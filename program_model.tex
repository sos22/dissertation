\chapter{The program model}

{\Technique} models the part of the program which is directly involved
in a crash via the {\StateMachine} mechanism, but these are only
capable of analysing relatively small fragments of the program.  This
means that they cannot express global properties such as the structure
of the heap.  {\Technique} instead captures these properties in its
\gls{programmodel}, a model of some important aspects of the program
built before the main analysis starts.

The first two analyses used in building the \gls{programmodel} are
dynamic.  The first is used to build the program's control flow graph,
including identifying branches from library code into the main
program, and is described in
Section~\ref{sect:program_model:indirect_branches}.  The other dynamic
analysis builds a model of the program's aliasing behaviour when
accessing global and heap-allocated structures; this is described in
Section~\ref{sect:program_model:dynamic_alias}.

The other analyses are all static.  The first analysis classifies
instructions into functions, including identifying most tail calls and
non-contiguous functions. This is described in
Section~\ref{sect:program_model:functions}.  This function table is
then used to perform two function-local static analyses: an aliasing
analysis for accesses to the stack, described in
Section~\ref{sect:program_model:static_aliasing}, and a known-value
analysis for characterising certain properties of registers, described
in Section~\ref{sect:program_model:fixed_regs}.

\subsection{Why is the stack handled differently from the heap?}
\todo{This doesn't really belong here.  It doesn't really belong
  anywhere else, either, though.}

The decision to handle the stack differently from the heap perhaps
requires further explanation.  This is a fundamentally pragmatic
decision.  Ideally, {\technique} would use only sound static analyses
to resolve aliasing problems, minimising the risk of false positives
and negatives, but this is simply not practical when analysing the
program's heap.  The heap structure can itself be very complicated, in
terms of which parts might contain pointers to which other parts, and
the information needed to derive this structure can be scattered
throughout the program.  This means that even with access to the
program's source deriving a useful model of the heap is a fearsome
challenge\needCite{}; doing it with just a binary is completely
infeasible.

The stack, on the other hand, has a much simpler structure.  Many
stack locations will have no pointers to them at all (except for the
stack pointer itself), and those which do rarely participate in
complicated object graphs.  Even better, all of the information needed
to characterise the structure of a given function's stack frame is
usually present in the function itself.  These two facts combine to
make a (mostly) sound static analysis feasible in many interesting
cases.

\todo{Also useful: stack accesses are more frequent than heap ones, so
  a dynamic analysis will have less overhead if it confines itself to
  only considering heap accesses.}

\section{Indirect branch analysis}
\label{sect:program_model:indirect_branches}

\todo{This is arguably too simple to be worth talking about, much less
  giving a whole section to.}

{\Technique} relies on being able to determine which addresses a given
branch might branch to in order to build its \glspl{cfg} (see
Section~\ref{sect:derive:build_static_cfg}).  This is trivial for
direct branches, but more difficult for indirect ones.
{\Implementation} solves this problem using a very simple dynamic
analysis, based on Valgrind\needCite{}, which simply records every
indirect branch instruction executed in the program, including both
the source and destination instruction pointers.  The main analysis
then assumes that only edges observed during dynamic analysis are
possible in the running program.

This is clearly unsound, as there might always be some behaviour which
is possible for the program but which is not observed by the dynamic
analysis.  This can lead to {\technique} building incomplete
\glspl{cfg}, and hence missing some bugs.  This is not, in practice, a
particularly series issue: most programs contain only a modest number
of dynamic branches, most of which can branch to only a modest number
of distinct targets, and so it does not take an enormous amount of
time for the dynamic analysis to achieve reasonable coverage.  Even
when coverage is incomplete, the most common effect is simply to cause
the main analysis to miss some bugs in code which was not run under
the dynamic analysis.  This is usually a tolerable weakness.

\todo{Hmm... The patch strategy generation step assumes it can find
  all of the predecessors of an instruction in order to fix the
  corrupted instruction problem.  Getting that wrong will crash the
  program when you try to fix it.  There's an explanation of why it's
  not usually a big deal in the section on patch strategy generation,
  but it wouldn't hurt to recap here.}

\subsection{Finding the predecessors of instructions}
\label{sect:program_model:instr_predecessors}

Recall that the algorithm for building a \gls{crashingthread}'s static
control-flow graph, given in
Section~\ref{sect:derive:build_static_cfg} depends on being able to
find the predecessor instructions of any instruction.  That is, it
assumes that there is some function $\mathit{pred}(i)$ which maps from
one of the program's instructions $i$ to the set of all instructions
in the program which might execute immediately before $i$.  The
indirect branch analysis allows {\technique} to build a complete
\gls{cfg} for most programs, and hence to implement the
$\mathit{pred}$ easily.  The key observation is that, for most
programs on most operating systems, the processor will only start
executing instructions from the main program binary following an
indirect branch from a library\footnote{The only common exceptions on
  Linux are the \texttt{\_start} routine, which can be trivially
  located by parsing the ELF meta-data, and signal handlers, which
  {\technique} does not consider.  Ordinary branches from shared
  libraries to the main program will always be indirect, as shared
  libraries must support being linked into multiple programs.}, and so
the indirect branch table contains all ``roots'' of the program CFG.
Simply disassembling forwards from these roots then gives the entire
CFG of all reachable instructions in the program binary.

There is one common exception to this rule: JIT compilers\needCite{}.
A JIT compiler will often generate code which branches back into the
JIT itself, and these branches can be direct\editorial{Cite, maybe?
  It's a bit obvious, but wouldn't hurt to have a real example.}.  The
dynamic analysis will not include these branches and so
{\implementation} will behave incorrectly.  The fundamental problem
here is not actually with the dynamic analysis, but rather with
{\technique} itself, as it assumes that all relevant instructions will
be available when it performs the analysis, and that is not always
true for JIT compilers.  One possible approach to this problem would
be to use another dynamic analysis which could capture execution
traces from the program and to then perform the analysis using those
traces, which would avoid the issue.  I have not investigated this in
detail\editorial{Well, kind of.  I did do a version using a DRS, which
  is pretty much the same idea, but it was very different from the
  thing I've described here, and I don't really want to talk about it,
  so I 'm going pretend it doesn't exist.}.

\section{Dynamic alias analysis}
\label{sect:program_model:dynamic_alias}

\todo{There's a tension here between precision and soundness which
  I've completely ignored.  Oh well.}

{\Technique} relies on a dynamic analysis to model memory accesses to
non-stack memory locations.  This analysis is responsible for building
an \gls{dynamicaliasingtable}, showing which pairs of instructions
might conceivably access the same memory locations.  In the pessimal
case, where every instruction might alias with every other, this table
will contain $n^2$ entries, where $n$ is the number of memory
accessing instructions in the program.  That would be unfeasibly large
for non-trivial programs.  Fortunately, the table is in practice very
sparse, and so is usually perfectly manageable.

\todo{This is a slightly abstract way of describing this.  I think
  it's still probably the best way of thinking about it, but perhaps
  not the best way of explaining it.}

The intuition behind this analysis is that most fields in most data
structures are accessed by a relatively small number of instructions
in the program, and so if it were possible to identify the field being
accessed by a given instruction then that would make it easy to
determine whether two instructions might interfere.  More concretely,
if we had an instruction $\mathit{instr\_to\_fields}(i)$ mapping from
an instruction to the set of fields which it can access then

\begin{displaymath}
\mathit{alias}(i, i') = (\mathit{instr\_to\_fields}(i) \cap \mathit{instr\_to\_fields}(i') \not= \varnothing)
\end{displaymath}

where $\mathit{alias}(i, i')$ is an alias function which is true when
there is some possibility of instructions $i$ and $i'$ accessing the
same memory location.  Unfortunately, the $\mathit{instr\_to\_field}$
function is difficult to express for binary programs, even with
perfect information, because fields are a high-level language
construct and {\technique} must operate at the level of machine code.

A little bit of algebra allows us to re-express $\mathit{alias}$ like
so:
\begin{displaymath}
\mathit{alias}(i, i') = \left(i' \in \bigcup_{f \in \mathit{instr\_to\_fields}(i)} \mathit{field\_to\_instrs}(f)\right)
\end{displaymath}

We now have two functions which are defined in terms of fields,
$\mathit{instr\_to\_fields}$ and $\mathit{field\_to\_instrs}$, rather
than one, which might appear to have made the problem worse.  On the
other hand, the composition of the two functions,
$\mathit{instr\_label}(i) = \{\mathit{field\_to\_instrs}(f) | f \in
\mathit{instr\_to\_fields}(i)\}$, does not require the caller to make
any reference to fields.  This suggests that it should be
implementable, and it is this function which the dynamic analysis
computes\footnote{In fact, the actual output of the {\implementation}
  dynamic analysis phase is the function $i \rightarrow \cup
  \mathit{instr\_label}(i)$, which flattens one level of
  $\mathit{instr\_label}$'s set structure, as that is the most
  convenient form for the later phases of the analysis.  Internally,
  it computes the full $\mathit{instr\_label}$ function.}.

The types of these functions are perhaps informative:

\begin{itemize}
\item $\mathit{instr\_to\_fields}: \mathit{instruction} \rightarrow \mathit{set}(\mathit{field})$
\item $\mathit{field\_to\_instrs}: \mathit{field} \rightarrow \mathit{set}(\mathit{instruction})$
\item $\mathit{instr\_label}: \mathit{instruction} \rightarrow \mathit{set}(\mathit{set}(\mathit{instruction}))$
\end{itemize}

In other words, $\mathit{instr\_label}$ is formed from
$\mathit{instr\_to\_fields}$ by identifying fields with sets of
instructions.  This can be thought of as analogous to the conceptual
switch between data types in a functional language and those in an
object-oriented language\marginpar{Little bit too philosophical}.  In
a functional language, a data type is identified with a set of
values\needCite{}, whereas in an object-oriented one the type is
identified with a set of operators which can be applied to
values\needCite{}.  In the same way, the intended semantics of this
analysis talks about the set of fields which an instruction can
access, which is in some sense like the set of values on which it can
operate, whereas the actual analysis discovers the set of instructions
with which it can interact, which is analogous to a set of operations.

\todo{Maybe draw an explicit analogy with de Bruijn indices?}

Given this conceptual work, implementing the dynamic analysis itself
is quite simple.  {\Implementation} does so using a Valgrind
skin\needCite{}.  The program's memory is divided into fixed-size
chunks, each of which has a label consisting of a set of accessing
instructions\footnote{{\Implementation} also tracks which accesses are
  reads, which writes, and which both, as this simplifies the
  implementation of the later analysis phases, but this does not
  meaningfully change the algorithm.}.  Any instruction which accesses
that memory chunk adds itself to the set.  The set of all labels
generated by the program, suitable indexed, is then an approximation
to the $\mathit{instr\_label}$ function; if the dynamic analysis is
able to observe all relevant program behaviour then it is precisely
equal to $\mathit{instr\_label}$.

This scheme, as presented, has one important weakness, which is that
it assigns labels to addresses in memory.  Mapping back to high-level
language constructs, this means that it is assuming that each address
in the program, once allocated, corresponds to the same type of data
structure throughout the program's life.  In other words, it assumes
that memory is type stable\needCite{}.  This is generally reasonable
for statically-allocated structures, such as those in the BSS
segment\needCite{}, but not for dynamically-allocated structures such
as those allocated via \texttt{malloc} or \texttt{operator new}.
{\Implementation} relies on being able to identify such dynamic memory
allocators so that it can reset the labels on memory addresses.  This
is easy for allocators provided by standard system libraries, such as
\texttt{malloc}, but much harder for program-specific allocators.  It
might be possible to identify such allocators using a variant of the
techniques described in \todo{cite the digging for data structures
  paper here}, but I have not investigated that at this time.
Instead, {\implementation} relies on manually annotating allocation
functions in the program.  This is not an unreasonable burden: for
mysql, the necessary annotations amount to an additional twenty lines
across the entire program, and none of the other programs examined in
the evaluation required any annotations at all.

{\Implementation} includes one minor refinement to the basic analysis
described above.  It is fairly common for programs to allocate new
heap structures using a function such as \texttt{malloc} and to then
initialise this structure using a series of stores.  These stores will
never race, so it would be helpful to avoid spending excessive time
considering what would happen if they did.  The approach taken is
simple: when a block is returned from \texttt{malloc} it is marked as
thread-private, and remains so until it is stored to non-stack memory.
Entries in the aliasing table then include a flag indicating whether
the access is thread-private or potentially racing.  Later analysis
phases can then assume that two thread-private accesses in different
threads cannot alias with each other.

\marginpar{Cross-thread stack accesses?} This policy might seem to be
overly conservative: a block of memory is marked as shared whenever a
pointer to it is stored into any non-stack memory, even when that
non-stack memory is itself marked as thread-private.  This is
necessary because the analysis does not attempt to track the heap
reachability graph, and in particular cannot map from one block to the
set of blocks reachable from that block.  It is therefore not safe to
``upgrade'' a block from thread-private to thread-shared if there is
any possibility of that block containing a thread-private pointer;
upgrading blocks early and pessimistically means that it is never
necessary to do so.

\section{Function discovery}
\label{sect:program_model:functions}

\todo{I need to check how IDA and CodeSurfer deal with this; I suspect
  the algorithm's pretty similar, just because there aren't that many
  options for doing this which are actually correct.  Comparison to
  the technique in Nebenzahl 2006 would also be worthwhile.}

\todo{This is a very long section given it's low level of
  non-obviousness.}

{\Technique}'s static analysis passes are all\editorial{All the ones
  I'm describing, anyway; there are a couple of cross-function ones,
  but they're not very interesting.} defined in function-local, but
function boundaries are not always apparent at the level of machine
code.  It is therefore necessary to, first, define what is meant by a
function, and, second, to show how to discover functions in a binary
program.  The definition used by {\technique} is simple:

\begin{itemize}
\item
  No instruction can be assigned to more than one function.
  \todo{There's a whole gaping chasm of complexity hiding under there
    (e.g. interaction with cross-function constant propagation), but I
    don't really want to talk about it, so I'm just going to ignore
    it.}
\item
  Every function has a single designated entry point instruction,
  referred to as the \gls{functionhead}.
\item
  The target of a \texttt{call} instruction is always the
  \glslink{functionhead}{head} of some function.  \todo{Maybe say
    something about the dummy calls used to implement PIC on 32 bit
    x86?}
\item
  Any branch instruction will either start and end in the same
  function or will end at a \gls{functionhead}, or both.  This
  includes the implicit branch to the next instruction at the end of
  an ordinary instruction.
\end{itemize}

Note that the instructions in a function do not have to be contiguous,
and so, for instance, code outlining is correctly
handled\editorial{Cite Zhou 2005 or US patent 2007/0089106, unless I
  can find something better.  Also mention that MSVC does this in real
  programs.}.

It is worthwhile discussing briefly how this definition interacts with
compiler tail-call elimination optimisations.  There are three
interesting cases:

\begin{itemize}
\item
  If a function is ever called normally, using a call-type branch, the
  fourth rule will ensure that its first instruction is a
  \gls{functionhead}, as desired.
\item
  If a function is tail-called from precisely one place, and is never
  invoked with a normal call instruction, no entry point will be
  created for it and it will be merged into its calling function.  The
  two functions will be analysed as if they were a single function.
\item
  If a function is tail-called from multiple different locations then
  the final rule will ensure that the join of the caller's CFGs is a
  \gls{functionhead}.  This is as desired: for simple tail calls, the
  join is the first instruction in the called function, which should
  indeed be marked as a head.
\end{itemize}

In other words, any function head inferred by these rules will
correspond to the first instruction of one of the program's actual
functions.  This is important because the platform ABI will place
various constraints on the state of the program at the start of
functions, and these constraints are very useful for the other static
analysis passes.

\todo{If I wanted to talk about intersecting function's this'd be the
  place to do it.}

\begin{figure}
\begin{algorithmic}[1]
  \State $\mathit{instrToHead} \gets \map{}$
  \State $\mathit{pending} \gets \queue{}$
  \State {Add all program entry points, as defined by ELF meta-data to $\mathit{pending}$\editorial{Fairly {\implementation}-specific}}
  \State {Add all targets of indirect calls discovered by the dynamic analysis to $\mathit{pending}$.}
  \While {$\mathit{pending}$ is not empty}
    \State $\mathit{head} \gets \mathit{pop}(\mathit{pending})$
    \State $p \gets \queue{\mathit{head}}$
    \While {$p$ is not empty}
      \State $i \gets \mathit{pop}(p)$
      \State $\mathit{oldH} \gets \mapIndex{\mathit{instrToHead}}{i}$
      \If {$\mathit{oldH} = \varnothing$} \Comment{New instruction in this function}
        \State $\mapIndex{\mathit{instrToHead}}{i} = \mathit{head}$
        \State {Disassemble $i$}
        \If {$i$ is a direct call to $i'$}
          \State {Add $i'$ to $\mathit{pending}$}
        \EndIf
        \State {$p = p + \textsc{succ}(i)$}
      \ElsIf {$\mathit{oldH} = \mathit{head}$} \Comment{Already found this instruction}
        \State \textbf{continue}
      \ElsIf {$\mathit{oldH} = i \vee i \in \mathit{pending}$} \Comment{Tail call into another known function}
        \State \textbf{continue}
      \Else \Comment {Tail call which contradicts existing assignment}
        \State {Remove all references to $\mathit{oldH}$ from $\mathit{instrToHead}$}
        \State {Add $i$ and $\mathit{oldH}$ to $\mathit{pending}$}
      \EndIf
    \EndWhile
  \EndWhile
\end{algorithmic}
\caption{Algorithm for assigning instructions to functions and
  identifying function heads.  The $\textsc{succ}(i)$ function finds
  all of the non-call successors of instruction $i$.  For a non-call
  instruction, this is the set of instructions which might execute
  immediately after $i$ (computing this may require reference to the
  results of the indirect branch analysis).  For a call instruction,
  it is the address to which the call will return. \todo{This is
    \emph{far} more detail than I wanted to go into on this.}}
\label{fig:function_head_alg}
\end{figure}

The full algorithm for discovering the program's function call
structure is given in Figure~\ref{fig:function_head_alg}.  It begins
with a list of all of the function heads which can be inferred from
the program's meta-data and the dynamic analysis.  It considers each
in turn, exploring forward through each function's control flow graph
and adding instructions to the $\mathit{instrToHead}$ map as it goes.
The only part which is perhaps surprising is on line 22, which handles
the case where the analysis finds a branch from one function to the
middle of another.  To see how this can happen, consider a program
with three functions, $a$, $b$, and $c$, where $a$ and $b$ both
tail-call into $c$, and suppose that $a$ is explored before $b$.  The
exploration of $a$ will discover $c$ and, since it is invoked by a
tail-call and is indistinguishable from a normal branch, will treat
$c$ as just another part of $a$.  When the algorithm moves on to
consider $b$, it will discover the tail-call to $c$, which is
currently just another instruction $a$.  The algorithm will then
realise it made a mistake when exploring $a$, remove function $a$ from
$\mathit{instrToHead}$, and record that it needs to explore both $a$
and $c$.  When it does explore $a$ it will correctly recognise the
branch to $c$ as a tail call and the algorithm will finish.

\section{Static alias analysis}
\label{sect:program_model:static_aliasing}

\todo{On the one hand, this is a bit noddy, and not really
  particularly insightful.  On the other hand, it is often quite
  useful, and I've not seen anyone else doing precisely the same
  thing.}

The dynamic aliasing analysis is effective at resolving aliasing
queries between instructions which access shared memory, but does not
provided any assistance with instructions which might access the local
stack.  {\Technique} instead handles these using a machine code-level
static function-local points-to analysis.  As with the dynamic
analysis, the results of this static analysis are used as input to
both the {\StateMachine} simplifier and the symbolic execution engine.

The key assumption made by this analysis is that locations in the
local stack frame are ``created'' when the function starts.  This
means that there cannot be any pointers to such locations before the
function starts, except for the stack pointer, whether in registers,
non-stack memory, or indeed the new stack frame itself, and the
program must not be able to calculate any from the pointers which it
does have.  A pointer here is defined to be something which is
eventually dereferenced, rather than something which just has a
numerical value which happens to match the local stack frame, so that,
for instance, pointers in dead registers are acceptable.  An important
corollary of this is that the address of the stack must have been
unknown when the program was compiled, and so statically constant
values cannot be stack pointers.

The analysis is structured as a least fixed point iteration which
builds a mapping \textit{configurations} from instructions to
\textsc{AliasConfig}s, defined in
Figure~\ref{fig:static_alias:abstract_domain}, starting from the state shown in
Figure~\ref{fig:static_alias:initial_config}, updating instructions
according to the relationships described in
Figure~\ref{fig:static_alias:update_rules}, and merging control-flow
joins using the $\sqcup$ operator defined in
Figure~\ref{fig:static_alias:join_op}.  I now describe these figures
in slightly more detail.

\paragraph{Figure~\ref{fig:static_alias:abstract_domain}, the abstract domain}

The aim of this analysis is to construct a mapping from instructions
to \textsc{configurations}, which are defined in
Figure~\ref{fig:static_alias:abstract_domain}.  This is less
complicated than it appears.  A \textsc{AliasConfig} can be either
$\bot$, indicating that the instruction is unreachable, or a it can
consist of a 3-tuple of $\mathit{regs}$, $\mathit{stackInStack}$, and
$\mathit{stackInOther}$.  Within that tuple:

\begin{itemize}
\item $\mathit{stackInStack}$ is true if there is any possibility that
  the function's local stack frame contains a pointer back to the
  frame when that instruction executes.
\item $\mathit{stackInOther}$ is true if there is any possibility that
  memory outside of the function's local frame might contain a pointer
  into the frame.
\item $\mathit{regs}$ is a function from registers to three-tuples of
  boolean flags showing what that register might point at:
  $\mathit{nonPointer}$, which is true if the register might contain a
  non-pointer value, $\mathit{stackPointer}$, which is true if it
  might point at the current stack frame, and $\mathit{otherPointer}$,
  which is true if it might contain a pointer to something other than
  the current stack frame.
\end{itemize}

\begin{figure}
  \begin{displaymath}
    \textsc{AliasConfig} = (\bot: \textsc{1}) + \left(\!\!\!\begin{array}{rl}%
      \mathit{regs}         :& \textsc{Reg} \rightarrow \left(\begin{array}{rl}%
        \mathit{nonPointer}   :& \textsc{Bool}\\
        \mathit{stackPointer} :& \textsc{Bool}\\
        \mathit{otherPointer} :& \textsc{Bool}
      \end{array}\right)\!\!\!\\
      \mathit{stackInStack} :& \textsc{Bool} \\
      \mathit{stackInOther} :& \textsc{Bool}
    \end{array}\right)
  \end{displaymath}
  \caption{Abstract interpretation domain for static alias analysis.}
  \label{fig:static_alias:abstract_domain}
\end{figure}

\paragraph{Figure~\ref{fig:static_alias:initial_config}, the initial configuration}

In the initial map, most instructions in the function are in
the \textsc{AliasConfig} $\bot$, indicating that they are currently
believed to be unreachable.  The exception is the function head
instruction, which has a fixed configuration given in the figure:

\begin{itemize}
\item At the start of the function, the \textsc{RSP} register
  definitely points at the current stack frame.
\item Argument registers could contain non-pointer values or pointers
  to things outside of the current stack frame, but cannot contain
  pointers to the current stack frame itself.  This reflects the
  assumption that the stack frame is created by the \texttt{call}
  instruction, and so there is no way for the caller to have obtained
  a pointer to it.
\item Similarly, the stack frame does not initially contain any
  pointers to itself.  Non-stack memory is likewise free of pointers
  to the frame.
\item Non-argument registers cannot contain any values at all.  This
  reflects the fact that the platform ABI leaves the value of all
  non-argument registers undefined at the start of a function, and so
  the function cannot depend on their values.  The analysis can
  therefore safely treat these registers as containing nothing at all.
\end{itemize}

This initial map is optimistic, but will have become conservative by
the time the fixed point iteration converges.

\begin{figure}
  \begin{displaymath}
    \begin{array}{rl}
      \textsc{AliasConfig}[\mathit{head}] = &\!\!\!\left(\!\!\!\begin{array}{lll}
        \mathit{regs} & \textsc{RSP} & = \left(\begin{array}{rl}
          \mathit{nonPointer} = &\!\!\!\mathit{false}\!\!\!\\
          \!\!\!\mathit{stackPointer} = &\!\!\!\mathit{true}\!\!\!\\
          \!\!\!\mathit{otherPointer} = &\!\!\!\mathit{false}\!\!\!
        \end{array}\right)\\
        \mathit{regs} & r \in \mathit{argRegs} &\!\!\!= \left(\begin{array}{rl}
          \mathit{nonPointer} = &\!\!\!\mathit{true}\!\!\!\\
          \!\!\!\mathit{stackPointer} = &\!\!\!\mathit{false}\!\!\!\\
          \!\!\!\mathit{otherPointer} = &\!\!\!\mathit{true}\!\!\!
        \end{array}\right)\\
        \mathit{regs} & \_ & = \left(\begin{array}{rl}
          \mathit{nonPointer} = &\!\!\!\mathit{false}\!\!\!\\
          \!\!\!\mathit{stackPointer} = &\!\!\!\mathit{false}\!\!\!\\
          \!\!\!\mathit{otherPointer} = &\!\!\!\mathit{false}\!\!\!
        \end{array}\right)\\
        \multicolumn{2}{l}{\mathit{stackInStack}} & = \mathit{false}\\
        \multicolumn{2}{l}{\mathit{stackInOther}} & = \mathit{false}\\
      \end{array}\!\!\!\right)\\
      \textsc{AliasConfig}[i] = &\!\!\!\bot\\
    \end{array}
  \end{displaymath}
  \caption{Starting point for the static alias analysis fixed point.
    $\mathit{head}$ is the \gls{functionhead} instruction and $i$ is
    any other instruction.}
  \label{fig:static_alias:initial_config}
\end{figure}

\paragraph{Figure~\ref{fig:static_alias:join_op}, the join operator $\sqcup$}

The join operator $\sqcup$ is used when multiple
\textsc{AliasConfig}s reach a single instruction over different
control-flow edges.  So, for instance, if instruction $a$ is reachable
from $b$ and $c$, the $E_b$ is the \textsc{AliasConfig} at the end
of $b$, and $E_c$ that at the end of $c$, the \textsc{AliasConfig}
at the start of $a$ will be $E_b \sqcup E_c$.  The $\sqcup$ operator
itself is quite simple: if either of the two predecessor instructions
is unreachable, take the \textsc{AliasConfig} from the other one,
and otherwise take a point-wise union of the two input
\textsc{AliasConfig}s.

\begin{figure}
  \begin{displaymath}
    \begin{array}{l}
    \bot \sqcup x    = x \\
    x    \sqcup \bot = x \\
    x    \sqcup y    = \left(\begin{array}{rl}
      \textit{regs r} = &\!\!\! \left(\begin{array}{rll}
        \mathit{nonPointer} = &\!\!\! (x.\textit{regs r}).\mathit{nonPointer} & \vee \\
                              & \multicolumn{2}{r}{(y.\textit{regs r}).\mathit{nonPointer}} \\
        \mathit{stackPointer} = &\!\!\!(x.\textit{regs r}).\mathit{stackPointer} & \vee \\
                                & \multicolumn{2}{r}{(y.\textit{regs r}).\mathit{stackPointer}} \\
        \!\!\!\mathit{nonStackPointer} = &\!\!\! (x.\textit{regs r}).\mathit{nonStackPointer} & \vee \\
                                   & \multicolumn{2}{r}{(y.\textit{regs r}).\mathit{nonStackPointer}}
        \end{array}\right)\\
      \!\!\!\mathit{stackInStack} = &\!\!\! x.\mathit{stackInStack} \vee y.\mathit{stackInStack} \\
      \!\!\!\mathit{stackInOther} = &\!\!\! x.\mathit{stackInOther} \vee y.\mathit{stackInOther}
    \end{array}\!\!\!\right)
    \end{array}
  \end{displaymath}
  \caption{Join operation $\sqcup$ on \textsc{AliasConfig}s.}
  \label{fig:static_alias:join_op}
\end{figure}

\paragraph{Figure~\ref{fig:static_alias:update_rules}, the rule for calculating an end-of-instruction \textsc{AliasConfig} from a start-of-instruction one}

The analysis relies on being able to generate the
\textsc{AliasConfig} at the end of an instruction given the
instruction and the \textsc{AliasConfig} at the start of the
instruction.  If the starting \textsc{AliasConfig} is $\bot$ then
the ending one is also $\bot$.  Otherwise, the analysis uses the rules
shown in Figure~\ref{fig:static_alias:update_rules}.

\begin{figure}
  \begin{displaymath}
    \begin{array}{l}
    \textsc{Store reg}_1 \rightarrow \ast(\textsc{reg}_2) \hfill \circled{1}\\
    \hspace{10mm}\begin{array}{ll}
      \mathbf{let} & \mathit{pt}_1 = \mathit{pre}.\mathit{regs}(\textsc{reg}_1) \\
                   & \mathit{pt}_2 = \mathit{pre}.\mathit{regs}(\textsc{reg}_2) \\
      \mathbf{in} & \left(\begin{array}{rl}
        \mathit{regs} = &\!\!\! \mathit{pre}.\mathit{regs}[\textsc{reg}_2 = \mathit{pt}_2[\mathit{nonPointer} = \mathit{false}]] \\
        \!\!\!\mathit{stackInStack} = &\!\!\!\mathit{pre}.\mathit{stackInStack} \vee (\mathit{pt}_1.\mathit{stackPointer} \wedge \mathit{pt}_2.\mathit{stackPointer}) \\
        \!\!\!\mathit{stackInOther} = &\!\!\!\mathit{pre}.\mathit{stackInOther} \vee (\mathit{pt}_1.\mathit{stackPointer} \wedge \mathit{pt}_2.\mathit{otherPointer})
      \end{array}\!\!\!\right)
    \end{array}\\
    \hline
    \textsc{Load } {\ast}(\textsc{reg}_1) \rightarrow \textsc{reg}_2 \hfill \circled{2}\\
    \hspace{10mm}\begin{array}{ll}
      \mathbf{let} & \mathit{pt}_1 = \mathit{pre}.\mathit{regs}(\textsc{reg}_1) \\
      \mathbf{in}  & \mathit{pre}\left[\!\!\!\begin{array}{ll}
          \mathit{regs}[\textsc{reg}_1] = \mathit{pt}_1[\mathit{nonPointer} = \mathit{false}]\\
          \mathit{regs}[\textsc{reg}_2] = \left(\begin{array}{rl}
            \mathit{nonPointer} = &\!\!\!\mathit{true}\\
            \!\!\!\mathit{stackPointer} = &\!\!\!\!\!\begin{array}{l}
            (\mathit{pt}_1.\mathit{stackPointer} \wedge \mathit{pre}.\mathit{stackInStack}) \vee\\
            (\mathit{pt}_1.\mathit{otherPointer} \wedge \mathit{pre}.\mathit{stackInOther})
            \end{array}\\
            \!\!\!\mathit{otherPointer} = &\!\!\!\mathit{true}
          \end{array}\!\!\!\!\!\!\right)\\
        \end{array}\!\!\!\right]
    \end{array}\\
    \hline
    \textsc{Copy } k \rightarrow \textsc{reg}_1 \hfill \circled{3}\\
    \hspace{10mm}\mathit{pre}\left[regs[\textsc{reg}_1] = \left(\begin{array}{rl}
        \mathit{nonPointer} = & \!\!\!\smBadPtr{k}\\
        \mathit{stackPointer} = & \!\!\!\mathit{false}\\
        \mathit{otherPointer} = & \!\!\!{\neg}\smBadPtr{k}\\
      \end{array}\right)\right]\\
    \hline
    \textsc{Copy } k + \textsc{reg}_1 \rightarrow \textsc{reg}_2 \hfill \circled{4}\\
    \hspace{10mm}\mathit{pre}\left[regs[\textsc{reg}_2] = \left(\begin{array}{rl}
        \mathit{nonPointer} = & \!\!\!\mathit{pre}.\mathit{regs}[\textsc{reg}_1].\mathit{nonPointer}\\
        \mathit{stackPointer} = & \!\!\!\mathit{pre}.\mathit{regs}[\textsc{reg}_1].\mathit{stackPointer}\\
        \mathit{otherPointer} = & \!\!\!\mathit{pre}.\mathit{regs}[\textsc{reg}_1].\mathit{otherPointer} \vee {\neg}\smBadPtr{k}\\
      \end{array}\right)\right]\\
    \hline
    \textsc{Copy } k {\oplus} \textsc{reg}_1 \rightarrow \textsc{reg}_2 \hfill \circled{5}\\
    \hspace{10mm}\mathit{pre}\left[regs[\textsc{reg}_2] = \left(\begin{array}{rl}
        \mathit{nonPointer} = & \!\!\!\true\\
        \mathit{stackPointer} = & \!\!\!\mathit{pre}.\mathit{regs}[\textsc{reg}_1].\mathit{stackPointer}\\
        \mathit{otherPointer} = & \!\!\!\mathit{pre}.\mathit{regs}[\textsc{reg}_1].\mathit{otherPointer} \vee {\neg}\smBadPtr{k}\\
      \end{array}\right)\right]\\
    \hline
    \textsc{Copy } \textsc{reg}_1 {\oplus} \textsc{reg}_2 \rightarrow \textsc{reg}_3 \hfill \circled{6}\\
    \hspace{10mm}\begin{array}{ll}
      \mathbf{let} & \mathit{pt}_1 = \mathit{pre}.\mathit{regs}[\textsc{reg}_1]\\
                   & \mathit{pt}_2 = \mathit{pre}.\mathit{regs}[\textsc{reg}_2]\\
      \mathbf{in}  & \mathit{pre}\left[regs[\textsc{reg}_1] = \left(\begin{array}{rl}
          \mathit{nonPointer} = & \!\!\!\mathit{true}\\
          \mathit{stackPointer} = & \!\!\!\mathit{pt}_1.\mathit{stackPointer} \vee \mathit{pt}_2.\mathit{stackPointer}\\
          \mathit{otherPointer} = & \!\!\!\mathit{pt}_1.\mathit{otherPointer} \vee \mathit{pt}_2.\mathit{otherPointer}\\
        \end{array}\right)\right]
    \end{array}\\
    \hline
    \textsc{Call} \hfill \circled{7}\\
    \hspace{10mm}\textrm{See main text}
  \end{array}
  \end{displaymath}
  \caption{Rules for calculating the \textsc{AliasConfig} at the end
    of an instruction.  $\mathit{pre}$ is the \textsc{AliasConfig}
    at the start of the instruction, $\textsc{reg}_i$ are registers,
    $k$ is a constant, and $\oplus$ is any operator other than $+$.
    Further details are given in the main text.}
  \label{fig:static_alias:update_rules}
\end{figure}

\begin{itemize}
\item[\circled{1}] The first rule in the figure shows how to handle an
  instruction which stores the contents of register $\textsc{reg}_1$
  in the memory location referenced by register $\textsc{reg}_2$.  The
  update to the \textsc{AliasConfig} has three parts:

  \begin{itemize}
  \item $\mathit{regs}$ is updated to record the fact that, if this
    instruction completes, $\textsc{reg}_2$ is definitely a valid
    pointer.  The $a[b = c]$ notation in the figure is intended to
    evaluate to the $a$ structure with field $b$ set to $c$.
  \item $\mathit{stackInStack}$ is set to $\mathit{true}$ if
    $\textsc{reg}_1$ might point at the stack and $\textsc{reg}_2$
    might also point at the stack.
  \item $\mathit{stackInOther}$ is set to $\mathit{true}$ if
    $\textsc{reg}_1$ might point at the stack and $\textsc{reg}_2$
    might point at some non-stack location.
  \end{itemize}
\item[\circled{2}] The second rule shows how to handle an instruction
  which loads register $\textsc{reg}_2$ from the memory location
  referenced by $\textsc{reg}_1$.  The end-of-instruction
  \textsc{AliasConfig} is the same as the start-of-instruction one,
  with two exceptions:

  \begin{itemize}
  \item $\textsc{reg}_1$ is marked as being definitely a valid pointer.
  \item $\textsc{reg}_2$ is recalculated.  The value loaded might
    always be a non-pointer or a pointer to non-stack memory,
    regardless of the \textsc{AliasConfig} in which the load takes
    place.  It might also be a pointer to the stack if either the
    location loaded might be on the stack and the stack might contain
    pointers to itself, or if the location loaded might be outside the
    stack and non-stack memory might contain a pointer to the stack.
  \end{itemize}
\item[\circled{3}] This rule shows how to handle an instruction which
  sets a register to a constant.  The only change here is to
  recalculate $\mathit{regs}[\textsc{reg}_1]$:

  \begin{itemize}
  \item $\mathit{stackPointer}$ is set to $\mathit{false}$.  This
    reflects the assumption that stack frames are allocated by
    \texttt{call} instructions, and so their address cannot be
    statically known, and so no pointer to a stack frame can ever
    appear in a constant address.
  \item The new value of $\mathit{nonPointer}$ is calculated by
    comparing the constant $k$ to the memory map in the program's
    binary.  Any locations not present in that map are treated
    as being potential non-pointers, and any which are present
    are assumed to be definitely valid pointers.
  \item $\mathit{otherPointer}$ is likewise calculate from the
    program's memory map.  This is perhaps more surprising: setting
    $\mathit{otherPointer}$ to $\mathit{false}$ indicates that the
    register cannot be a valid pointer, but there clearly are valid
    memory locations in a running program which are not included in
    its binary, and so this would appear to be unsafe.  In practice,
    however, it is not, because this rule will only be triggered by
    compile-time constants, and most operating systems make it
    difficult to allocate memory at a fixed address except via the
    memory map in the binary\footnote{Doing so is inherently dangerous
      in any but the most simple programs, due to the risk of
      accidentally colliding with a dynamically allocated structure
      maintained by a library, and so it is essentially never done in
      real programs.}.
  \end{itemize}

\item[\circled{4}] The fourth rule handles instructions which set one
  register to be another plus a constant offset (including a constant
  offset of zero).  In this case, the configuration of one register is
  simply copied to the other, after setting $\mathit{otherPointer}$ if
  $k$ is a valid pointer.  The assumption here is that the program
  will never generate a pointer to the stack frame by means of a
  constant offset from a pointer to something else, or vice versa,
  which is reasonable given the assumption that frames are allocated
  by \texttt{call} instructions.

  This rule is potentially dangerous, in that it assumes that if $x$
  is a valid pointer then so is $x+k$, for any constant $k$.  This is
  not guaranteed to be true if $x$ is near to the end of an object or
  if $k$ is very large, but is in practice reasonable given common
  programming practice.

\item[\circled{5},\circled{6}] This analysis has no special knowledge
  for any operators other than $+$.  The result might always be a
  non-pointer, and might be a pointer to anything to which any of the
  inputs point.

\item[\circled{7}] Handling called sub-functions requires some care,
  as this is a function-local analysis and so the bodies of
  sub-functions are not available for analysis.  There are two
  interesting cases: that where the called function receives a pointer
  to the current stack frame, and that where it does not.  Recall that
  this analysis assumes that the program cannot invent new pointers to
  stack frames, and so if the called function does not receive any
  such pointers then it definitely cannot access the current frame and
  cannot store any pointers to it in non-stack memory or it registers.
  With the additional assumption that the program only modifies
  registers in ways which are consistent with the standard function
  call ABI, this implies that \textsc{AliasConfig} in
  Figure~\ref{fig:static_alias:call_instrs} is a safe approximation to
  the \textsc{AliasConfig} when the function returns.

  Of course, this assumes that {\implementation} can identify when the
  function receives any pointers to the current frame, and in
  particular that it can identify the function's arguments.
  {\Implementation}'s approach is to simply assume that all arguments
  are passed in the ABI's designated argument-passing registers and to
  then use a simple register liveness analysis to determine which
  registers each function depends on.  This is sound for functions
  which take a small number of arguments, such that all arguments are
  passed via registers\footnote{For the Linux AMD64 ABI, this means
    all functions which take six or fewer arguments \todo{simplifying
      a bit, but close}.}, but might produce incorrect results if a
  function takes a pointer to the stack in an argument which is itself
  passed on the stack.  A much more complete function characterisation
  step, such as that in \editorial{cite something appropriate; AM's
    type-directed decompilation will do, if nothing better turns up},
  would be necessary to correctly handle these.
  
\end{itemize}

\begin{figure}
  \subfigure[][Stack does not reach callee]{
    \hspace{-10mm}
    \begin{math}
      \begin{array}{rrl}
        \textit{regs} & \mathit{ret} = &\!\!\!\left(\!\!\!\begin{array}{rl}
          \mathit{nonPointer} = &\!\!\!\mathit{true} \\
          \mathit{stackPointer} = &\!\!\!\mathit{false} \\
          \mathit{otherPointer} = &\!\!\!\mathit{true} \\
        \end{array}\!\!\!\right)\\
        \textit{regs} & r \in \mathit{clobbered} = &\!\!\!\left(\!\!\!\begin{array}{rl}
          \mathit{nonPointer} = &\!\!\!\mathit{false} \\
          \mathit{stackPointer} = &\!\!\!\mathit{false} \\
          \mathit{otherPointer} = &\!\!\!\mathit{false} \\
        \end{array}\!\!\!\right)\\
        \textit{regs} & r = &\!\!\!\mathit{pre}.\textit{regs r} \\
        \multicolumn{2}{l}{\mathit{stackInStack}} = &\!\!\!\mathit{pre}.\mathit{stackInStack} \\
        \multicolumn{2}{l}{\mathit{stackInOther}} = &\!\!\!\mathit{pre}.\mathit{stackInOther}
      \end{array}
    \end{math}
  }
  \subfigure[][Stack does reach callee]{
    \hspace{-5mm}
    \begin{math}
      \begin{array}{rrl}
        \textit{regs} & \mathit{ret} = &\!\!\!\left(\!\!\!\begin{array}{rl}
          \mathit{nonPointer} = &\!\!\!\mathit{true} \\
          \mathit{stackPointer} = &\!\!\!\mathit{true} \\
          \mathit{otherPointer} = &\!\!\!\mathit{true} \\
        \end{array}\!\!\!\right)\\
        \textit{regs} & r \in \mathit{clobbered} = &\!\!\!\left(\!\!\!\begin{array}{rl}
          \mathit{nonPointer} = &\!\!\!\mathit{false} \\
          \mathit{stackPointer} = &\!\!\!\mathit{false} \\
          \mathit{otherPointer} = &\!\!\!\mathit{false} \\
        \end{array}\!\!\!\right)\\
        \textit{regs} & r = &\!\!\!\mathit{pre}.\textit{regs r} \\
        \multicolumn{2}{l}{\mathit{stackInStack}} = &\!\!\!\mathit{true} \\
        \multicolumn{2}{l}{\mathit{stackInOther}} = &\!\!\!\mathit{true}
      \end{array}
    \end{math}
  }
  \caption{Handling of \texttt{call} instructions.  \textit{clobbered}
    is the set of call clobbered registers in the platform ABI and
    \textit{ret} is the ABI return register. \todo{Tidy up to not hang
      off page}}
  \label{fig:static_alias:call_instrs}
\end{figure}

Most non-control flow instructions in the AMD64 instruction set can be
expressed in terms of these simple operations, but the full expansion
is quite tedious and I do not give details here.  

\todo{Need to eval how effective all of this actually is.}\smh{Ack;
  also maybe look at how effective it would be to use more distinct
  locations (i.e. a greater number of locations).}

\section{The known register analysis}
\label{sect:program_model:fixed_regs}

In many cases the value of a register at a particular instruction is
either a fixed constant or a fixed offset from another register.  This
analysis attempts to discover such fixed relationships.  The structure
of the analysis is broadly similar to the static alias analysis:
define a domain of \textsc{KRConfig}s
(Figure~\ref{fig:known_regs:configuration}), a join operator for them
(Figure~\ref{fig:known_regs:join_op}), and an abstract interpretation
relation for instructions (Figure~\ref{fig:known_regs:update_rule}),
and then build a mapping from instructions to \textsc{KRConfig}s
by taking the least fixed point.  I now describe these diagrams in
more detail.

\todo{Mention that this is most useful for figuring out DF, so that
  memcpy etc. work, and for getting rid of frame pointers.}

\paragraph{Figure~\ref{fig:known_regs:configuration}, the abstract domain}

Figure~\ref{fig:known_regs:configuration} gives the abstract domain
for the known register analysis.  A \textsc{KRConfig} is either
$\bot$, indicating that no paths to the instruction have yet been
discovered, or a mapping from \textsc{Reg}isters to
\textsc{RegValue}s.  A \textsc{RegValue} is in turn either
\textit{const k}, indicating that the register is the constant $k$,
\textit{offset r k}, indicating that this register is equal to $r +
k$, where $r$ is some other register, or $\mathit{invalid}$,
indicating that the register does not fit one of those forms.

The initial value for the fixed point iteration has every instruction
$\bot$ except for the \gls{functionhead} instruction, which is a map
which reflects whatever constraints the system ABI imposes at the
start of a function.  For Linux on AMD64, the only such constraint is
that the direction flag in \texttt{rflags} is equal to
1\editorial{Cite, maybe?}.  {\Implementation} stores the direction
flag in its own register \textsc{df}, and so the initial configuration
is just $\{ \textsc{df} \rightarrow \textit{const }1 \}$.

\begin{figure}
  \begin{displaymath}
    \begin{array}{rcl}
      \textsc{RegValue} & = & (\mathit{invalid}:1) + (\mathit{const}:\textsc{Integer}) + (\mathit{offset}:\left(\textsc{Reg}, \textsc{Integer}\right)) \\
      \\
      \textsc{KRConfig} & = & ({\bot}:1) + (\textsc{Reg} \rightarrow \textsc{RegValue})\\
    \end{array}
  \end{displaymath}
  \caption{\textsc{KRConfig} type, the domain for the known register
    analysis.}
  \label{fig:known_regs:configuration}
\end{figure}

\paragraph{Figure~\ref{fig:known_regs:join_op}, the join operator}

Figure~\ref{fig:known_regs:join_op} shows the join operator $\sqcup$
on \textsc{KRConfig}s.  If one of the predecessor states is
unreachable, the operator takes the \textsc{KRConfig} from its
other predecessor; otherwise, it performs a point-wise join of the
values for the individual registers using the $\cup$ operator.  The
$\cup$ operator is, again, simple: if its arguments are equal, it
returns them, and otherwise it returns \textit{invalid}.

\begin{figure}
  \begin{displaymath}
    \begin{array}{l}
      \begin{array}{rl}
        \cup : & \textsc{RegValue} \rightarrow \textsc{RegValue} \rightarrow \textsc{RegValue} \\
        \sqcup : & \textsc{KRConfig} \rightarrow \textsc{KRConfig} \rightarrow \textsc{KRConfig}
      \end{array}\\\\
      \begin{array}{rclcll}
        x & \cup & y & = & x                & \textrm{if } x = y\\
        x & \cup & y & = & \mathit{invalid} & \textrm{otherwise} \\
        \\
        \bot & \sqcup & x    & = & x \\
        x    & \sqcup & \bot & = & x \\
        x    & \sqcup & y    & = & (r \rightarrow \textit{x r} \cup \textit{y r})
      \end{array}
    \end{array}
  \end{displaymath}
  \caption{Join operator $\sqcup$ operator for \textsc{KRConfig}s.}
  \label{fig:known_regs:join_op}
\end{figure}

\paragraph{Figure~\ref{fig:known_regs:update_rule}, the update rule}

\begin{figure}
  \begin{displaymath}
    \begin{array}{l}
      \hspace{\columnwidth}~\\
      \textsc{Copy } k \rightarrow \textsc{reg} \hfill \circled{1}\\
      \hspace{10mm}\textit{reachable }\left\{\begin{array}{lll}
      \textsc{reg} & & \!\!\!\rightarrow \textit{const k}\\
      \textsc{reg}' & \multirow{2}{*}{\begin{math}
          \left|\hspace{-1mm}\begin{array}{l}
          \textit{old \textsc{reg}' = offset \textsc{reg} k'} \\
          \textit{otherwise}
          \end{array}\right.
        \end{math}
      }\hspace{-5mm}   & \!\!\!\rightarrow \mathit{invalid}\\
       & & \!\!\!\rightarrow \textit{pre }\textsc{reg}'\\
      \end{array}\hfill\right\}\quad\\

      \hline
      \textsc{Copy } k + \textsc{reg} \rightarrow \textsc{reg} \hfill \circled{2}\\
      \hspace{10mm}\textit{reachable }\left\{\begin{array}{lll}
        \textsc{reg} & \multirow{3}{*}{\begin{math}
            \left|\hspace{-1mm}\begin{array}{l}
              \textit{pre }\textsc{reg} = \mathit{invalid}\\
              \textit{pre }\textsc{reg} = \textit{const }k'\\
              \textit{pre }\textsc{reg} = \textit{offset r } k'\\
            \end{array}\right.
          \end{math}
        } & \!\!\!\rightarrow \mathit{invalid} \\
        & & \!\!\!\rightarrow \textit{const k + }k' \\
        & & \!\!\!\rightarrow \textit{offset r }(k + k')\\
        \vspace{-2mm}\\
        \textsc{reg}' & \multirow{2}{*}{\begin{math}
            \left|\hspace{-1mm}\begin{array}{l}
              \textit{pre }\textsc{reg}' = \textit{offset \textsc{reg} }k'\\
              \mathit{otherwise}\\
            \end{array}\right.
          \end{math}
        } & \!\!\!\rightarrow \textit{offset \textsc{reg} } (k'-k) \\
        & & \!\!\!\rightarrow \textit{pre }r'\\
      \end{array}\right\}\\

      \hline
      \textsc{Copy } k + \textsc{reg}_1 \rightarrow \textsc{reg}_2 \textrm{ where } \textsc{reg}_1 \not= \textsc{reg}_2 \hfill \circled{3}\\
      \hspace{10mm}\textit{reachable }\left\{\begin{array}{lll}
        \textsc{reg}_2 & & \!\!\!\rightarrow \textit{offset }\textsc{reg}_1 \textit{ k} \\
        \textsc{reg}' & \multirow{2}{*}{\begin{math}
            \left|\hspace{-1mm}\begin{array}{l}
              \textit{pre }\textsc{reg}' = \textit{offset }\textsc{reg}_2 \textit{ } k' \\
              \textit{otherwise}\\
            \end{array}\right.
          \end{math}
        } & \!\!\!\rightarrow \mathit{invalid}\\
          & & \!\!\!\rightarrow \textit{pre }\textsc{reg}'\\
      \end{array}\right\}\\

      \hline
      \textrm{Other updates of }\textsc{reg} \hfill \circled{4}\\
      \hspace{10mm}\textit{reachable }\left\{\begin{array}{lll}
        \textsc{reg} & & \!\!\!\rightarrow \textit{invalid} \\
        \textsc{reg}' & \multirow{2}{*}{\begin{math}
            \left|\hspace{-1mm}\begin{array}{l}
              \textit{pre }\textsc{reg}' = \textit{offset }\textsc{reg} \textit{ } k \\
              \textit{otherwise}\\
            \end{array}\right.
          \end{math}
        } & \!\!\!\rightarrow \mathit{invalid}\\
        & & \!\!\!\rightarrow \textit{pre }\textsc{reg}'\\
      \end{array}\right\}\\
      
    \end{array}
  \end{displaymath}
  \caption{Update rule for \textsc{KRConfig}s.}
  \label{fig:known_regs:update_rule}
\end{figure}

Figure~\ref{fig:known_regs:update_rule} shows how to calculate the
\textsc{KRConfig} at the end of an instruction from the
\textsc{KRConfig} at the start, when that is not $\bot$, for some
simple instructions.

\begin{itemize}
\item[\circled{1}] If the instruction sets a register to a constant $k$,
  the update rule first sets that register to \textit{const k} and then
  invalidates any other registers which are currently believed to be a
  fixed offset from that register.
\item[\circled{2}] This rule handles instructions which update a
  register by adding a constant.  There are two parts to this: the
  register which was updated must have the constant added to its
  value, and registers which were defined in terms of the register
  must have the constant subtracted from there value.
\item[\circled{3}] The third rule shows how to handle instructions
  which set a register $\textsc{reg}_2$ to be equal to another
  register $\textsc{reg}_1$ plus a constant $k$\footnote{Which might
    be zero or negative.}.  $\textsc{reg}_2$ is set to $\textit{offset
  }\textsc{reg}_1\textit{ k}$, and any registers defined in terms of
  $\textsc{reg}_2$ are invalidated.
\item[\circled{4}] The final rule handles all other types of
  instructions which update a register $\textsc{reg}$.  This rule
  simply invalidates the entry for $\textsc{reg}$ and all registers
  which are defined in terms of it.  This is quite conservative.  For
  instance, the instruction is adding two registers both of which have
  known values it would be easy to constant-fold the values together.
  This is unlikely to be helpful for programs produced with optimising
  compilers, as even a very basic optimising compiler will itself
  perform constant folding optimisations, and so there will be no
  further opportunity to apply them to the compiled binary.
\end{itemize}

\todo{Need to eval how often this works, as well.}
