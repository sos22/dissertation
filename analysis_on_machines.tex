Things which are worth discussing here:

\begin{itemize}
\item
  Give a very brief discussion of the actual analysis passes I use.
  Most of these will just be refs to the standard program slicing/decompilation/compilers literature.
  Deadcode and avail belong here.
\item
  Explicit discussion of the undefinedness analysis, because the argument which shows it to be sound is really rather subtle, and I've not seen anyone else make it.
\item
  Probably mention that we sometimes optimise on the assumption that assertion failures correspond to crashes and sometimes on the assumption that correspond to avoiding the crash, and discuss why that's necessary.
\item
  Maybe discuss getting rid of local survival constraints?
\item
  The function alias analysis and phi elimination analyses are kind of interesting; I haven't seen them anywhere else (and they're inherently cross-function, rather than the usual compilers approach of just inlining everything and then doing intra-function analysis on that, which is kind of fun).
\item
  The encoding of multiple fragments of program into a single \StateMachine, which reduces analysis costs quite a bit, is kind of interesting.
\item
  The encoding of the static analysis information into the \StateMachines, to expand the context sensitivity horizon, is also kind of interesting.
\end{itemize}

\section{Alias analysis}

SLI's \StateMachines represent a cross-function slice of the program's
machine code, including a large number of memory-accessing
instructions.  This presents a non-trivial alias analysis problem.
SLI uses several techniques to resolve the resulting aliasing queries:

\begin{itemize}
\item
  First, a dynamic analysis is used to build up a model of how the
  program behaves during normal operation.  This model is generally
  reasonably effective at determining whether instructions which
  access the heap or global data might conflict, but does not contain
  information on accesses to the local stack.
\item
  Next, a static analysis pass is used to determine which instructions
  might access local variables in the current stack frame.  This
  analysis is almost entirely function-local and is applied to every
  function in the program before the main analysis pass starts.
\item
  The results of this static analysis pass are incorporated into the
  \StateMachines in a way which automatically extends them to
  accurately reflect cross-function properties of the program.
\item
  The main alias analysis can then be run on the \StateMachines
  themselves, in conjunction with the other \StateMachine
  simplification passes, to resolve aliasing queries.
\end{itemize}

I now describe each of these phases in more detail.

\subsection{Dynamic analysis}

SLI relies on a dynamic analysis pass to build up a model of the
program's behaviour when it is running normally.  This model mostly
focuses on the possible aliasing relationships between memory accesses
outside of the program's stack; in other words, determining whether
two instructions might access the same piece of non-stack memory.
This is a full alias analysis, rather than a points-to analysis, and
so could in principle need to build up a full $O(n^2)$ table showing,
for each pair of instructions whether those instructions might alias.
Fortunately, that table is rather sparse for most programs, allowing
some significant simplifications to be made.

The intuition behind this analysis is that most fields in most data
structures are accessed by a relatively small number of places in the
program, and so if it were possible to identify the field being
accessed by a given instruction then that would make it easy to
determine whether two instructions might interfere.  Unfortunately,
that kind of higher-level information is not usually available when
analysing binary programs.  SLI sidesteps that problem by identifying
fields by the set of instructions which might access them and then
assuming that instructions might alias precisely when they are part of
the same field.  The result is an alias table which can be collected
in reasonable time using a simple dynamic analysis, which can resolve
aliasing queries quickly, and which requires a tolerable amount of
space (tens of megabytes for mysqld, for instance).

Collecting the aliasing table works like this.  Memory is divided
eight byte chunks, each of which has two set of accessing instructions
associated with it, one for read instructions and one for write.  Any
instruction which accesses that memory chunk adds itself to the
relevant set.  These sets in effect identify the structure field for
the memory chunk.  When the memory dies, due to the program either
exiting or calling some \verb|free|-like function, the relevant field
descriptors are transferred to a global set of fields.  This global
set then provides the final aliasing table.

\todo{Other bits: skip stack accesses, and try to identify
  thread-private memory.}

\subsection{Static analysis}

\todo{On the one hand, this is quite important.  On the other hand,
  it's incredibly tedious.  Not sure what to do about that.}

The most important whole-program analysis used by SLI is a simple
points-to analysis which determines, for any given combination of
register and instruction, whether that register at that instruction
might point into the current stack frame (defined below) or into other
memory.  This information is then used to improve the accuracy of the
\StateMachine-level aliasing analyses.  Determining whether a memory
access is to the current stack frame is important because, in most
programs, there are a lot of them, and is relatively easy, because,
again in most programs, there are relatively few pointers into the
stack frame.  This makes the problem a reasonable choice for solving
by whole-program static analysis.

The analysis used by SLI makes several important assumptions about the
program to be analysed:

\begin{itemize}
\item
  Most importantly, it assumes that the locations in a function's
  frame are ``created'' when the function is called, in the sense that
  any pointers into the stack frame which are actually dereferenced
  must have been created after the function started.  This does not
  mean that there cannot be any pointers into the frame at the start
  of the function; simply that, if there are any, they cannot be
  dereferenced.  Pointers in dead registers are acceptable, for
  instance, as are values which are ``semantically'' integers but
  happen to have the same numerical values as valid pointers.

\item
  The analysis assumes that the location of the stack was unknown when
  the program was compiled, and in particular that any statically
  constant values are not stack pointers.  This is in some sense a
  special case of the previous assumption, but is both important and
  non-obvious and so bears additional emphasis.

\item
  The analysis assumes that all functions take all of their arguments
  in registers, and that SLI can identify the set of registers which
  might be arguments.  In the case of AMD64 using a normal ABI this is
  true for the first six arguments to a function \needRef{}, and,
  since the vast majority of functions have fewer than six arguments
  \needRef{}, this is not usually a problem.

\item
  The analysis assumes that functions will either not return at all or
  will return to the instruction following the instruction from which
  it was called.  Note that this does not rule out functions such as
  \verb|setjmp| and \verb|longjmp|: \verb|longjmp| never returns and
  \verb|setjmp| always returns to the instruction after the one which
  called it.

\item
  The analysis assumes that the stack pointer at the end of a function
  is always the same as the stack pointer at the start of that
  function, if the function returns to the same place as it was called
  from.
\end{itemize}

Given those assumptions, the first stage of the analysis is to
identify functions in the program.  For the purposes of this analysis,
a function is defined to be a (possibly non-contiguous) set of
instructions with a distinguished element, referred to as the head of
the function, such that:

\begin{itemize}
\item
  For any non-return and non-call type branch from instruction $a$ to
  $b$ anywhere in the program, either $a$ and $b$ are in the same
  function or $b$ is the head of some function, and
\item
  for any call-type branch instruction from $a$ to $b$ $b$ is the head
  of a function.
\end{itemize}

The term ``branch'' here includes the implicit branch
from one ordinary instruction to the one which immediately follows it.
Within those constraints, SLI attempts to minimise the number of
distinct functions\footnote{Or, equivalently, it maximises the size of
  the individual functions.}\editorial{Well, ``attempts to''.  There's
  no guarantee that the set is actually minimal, but it usually is.}.

For most functions, invoked using call-type branches, this definition
is straightforward, and is equivalent to saying the first instruction
in the function is the head and the function contains all instructions
reachable from that instruction until the first return-type branch,
excluding called functions.  Tail-call elimination complicates things
slightly, though.  If a function is ever called in the usual way
(i.e. not via tail-call elimination) then the first rule will cause
its first instruction to be a function head, as desired.  If a
function is never called via a call-type branch, but is tail-call
eliminated into multiple calling functions, then the second rule will
cause the join of the two calling functions' CFGs to be a function
head, and this is generally correct for simple tail-call elimination.
If the called function is only called from one place, and that place
is a tail-call, then this definition will lead to the two functions
being merged, which will lead the analysis to produce an
overly-conservative solution but will not cause an actual error.

The only slight complexity involved in discovering functions within
this definition is the handling of indirect call and branch
instructions.  SLI solves this using information from the dynamic
analysis phase: it assumes that all of the possible targets of every
indirect branch are recorded in the dynamic information, which makes
the problem trivial.

The analysis itself is then structured as a simple iteration to a
fixed point.  The iteration's state is a table saying, for each
instruction, the estimated points-to configuration at the start of the
instruction and at the end.  The end-of-instruction configuration is a
simple function of the start-of-instruction configuration and the
instruction itself, and the start-of-instruction configuration is
itself simply the union of the end-of-instruction configurations for
all of the instructions which might possibly execute immediately
before the one being analysed.  Once those rules are satisfied by each
instruction in isolation the analysis is complete and the table
provides accurate information about the program's behaviour (provided
that the assumptions given above are true).

I now give a little more detail of the various steps.

\begin{itemize}
\item
  The initial state reflects the assumptions given above.  At the
  start of the analysis, most instructions are assigned a special
  ``unreachable'' state, which says nothing at all about their
  possible points-to state.  The exceptions are function-head
  instructions, which are given this initial state:

  \begin{itemize}
  \item The stack pointer points at the current stack frame.
  \item Other registers contain either non-pointer values or pointers
    to memory outside of the current frame.
  \item There are no pointers in the current stack frame which point
    back at the current stack frame.
  \item There are no pointers in main memory which can reach the
    current stack frame.
  \end{itemize}
\item
  If the instruction sets register $r$ to the expression $x$, the
  analysis computes the points-to set of the expression $x$ (which
  will be some combination of points-at-current-frame,
  points-outside-current-frame, and non-pointer-value) and sets it to
  $r$.  Calculating the points-to set is straightforward, with a few
  exceptions:

  \begin{itemize}
  \item
    Constant expressions.  Constant values are assumed to never point
    into the stack, which is probably unsurprising.  SLI will,
    however, also mark constant expressions as not pointing at
    anything at all if the constant address is not mapped by the
    program's main binary, which is perhaps slightly more surprising.
    One might suspect that this would be confused by, for instance,
    references to shared libraries, which appear to be constant from
    the point of view of application programmers but not at the
    machine code level.  They therefore do not trigger this heuristic.

    The only way that this heuristic could be wrong would be if the
    program uses a facility such as \verb|mmap| to explicitly map
    something at a fixed virtual address without reserving that
    address in its ELF information, and then uses that known address
    to access the mapped information directly.  This is an extremely
    unusual thing for a program to do, because it is inherently unsafe
    in the presence of dynamically loaded libraries.  Not including
    the address in the ELF phdrs means that dynamically loaded
    libraries might claim the desired address before the program is
    able to, in which case the manifestly constant addresses in the
    program's binaries will necessarily be incorrect (and if the
    program includes some mechanism to recover from this then there
    would be no advantages to hard-coding an address in the first
    place).  I have not found any programs which violate this
    heuristic.

  \item
    If the computed value is of the form $x + y$ then points-to sets
    are computed for $x$ and $y$ and the final points-to set is the
    union of them.  This, together with the constant rules, means that
    the analysis implicitly assumes that the program never computes a
    pointer into one memory region by adding a constant to a pointer
    into another one.  In particular, this means that functions cannot
    access memory in the calling function's frame at a fixed offset
    from the stack pointer.\editorial{That's actually a fairly major
      assumption; I'm kind of surprised how rarely it's false.}

  \item
    If the computed value is a load from memory then it might always
    be a pointer outside of the current stack frame, and might point
    at the current stack frame if either the address loaded might
    point at the current stack frame and the current stack frame might
    include references to itself or the address might point outside of
    the current frame and memory outside of the current frame might
    include pointers to the current frame\editorial{Rephrase}.
  \end{itemize}

\item
  If the instruction stores to memory then the analysis computes the
  points-to sets for the address and the value stored and updates the
  stack-frame-escaped flags as appropriate.

\item
  Instructions which call other functions require a moderate amount of
  care.  SLI handles them by checking whether there might be any
  pointers to the current stack frame in either argument registers or
  non-stack memory.  If there are then the stack is considered to have
  escaped into the called function, so the return register is marked
  as possibly pointing at the stack and both memory regions are marked
  as possibly containing pointers to the stack.  Argument registers
  here are the intersection of the argument registers defined by the
  ABI and those found to be live at the start of the called function
  by a separate register liveness analysis pass not described here.
\end{itemize}

\todo{There's a bit of a subtlety to do with instructions which
  early-exit e.g. setne, but it's not very important and it's a pain
  to explain, so I probably won't bother.}
  
The static analysis iterates these rules until it finds a fixed point.

\subsection{Encoding information into \StateMachines}

The information collected by the static analysis pass is
function-local, in the sense that it can tell whether a given
instruction accesses the current function's stack frame but cannot say
anything about other frames.  SLI's \StateMachines are, however,
cross-function, and so even the idea of a ``current function'' is not
entirely well-defined.  Solving this mismatch has two main steps.  The
first is to recover the function structure of the program, and hence
to assign identifiers to stack frames which might be relevant.  The
function-local information can then be extended to say which frames a
given pointer might refer to, rather than just providing a simple
does/does-not point at the current frame flag.

\subsubsection{Recovering the function call structure}

\todo{The current version of this algorithm is from the end of August;
  the first one was from the 14th.}

The initial \StateMachine building process can identify the start and
end of functions by recognising \verb|call| and \verb|ret|
instructions.  These are converted into \verb|StartFunction(rsp)| and
\verb|EndFunction(rsp)| side-effects which can be incorporated into
the machine in the usual way (where \verb|rsp| is the stack pointer at
the time of the instruction).  These side-effects have no effects on
the \StateMachine's actual behaviour, in the sense that removing them
cannot change the \StateMachine's final result, but provide useful
hints to the later analysis steps.  The task of this pass is to take
these \verb|StartFunction| and \verb|EndFunction| side-effects and use
them to determine the entire stack layout at every point in the
\StateMachine.

This might, at first, appear to be trivial.  To understand why it is
not, it is helpful to consider a few simple examples.  First, suppose
that the behaviour being investigated is in function \verb|f| and that
it is called from \verb|g|:

\begin{verbatim}
g() {
l1:  f();
l2:  f();
}
\end{verbatim}

Where the behaviour to be investigated is in the second call to
\verb|f| and the \StateMachine generated includes both calls.  The
analysis should assign different frame IDs to the two calls, and so
assigning the same frame ID to every instance of a given static
function would be incorrect.  At the same time, simply assigning a
different ID to every instance would also be incorrect.  For example:

\begin{verbatim}
g() {
    if (cond1)
       f();
}
h() {
    if (cond2)
       f();
}
\end{verbatim}

Suppose that the \StateMachine being generated has entry points for
both \verb|g| and \verb|h|.  Ideally, we would like to analyse the two
instances of \verb|f| only once, to avoid doing redundant work, and
this will only be possible if they are assigned the same frame ID.

The correct solution to this problem is to realise that stack frames
are dynamic constructs, allocated by \verb|call| instructions and
released by \verb|ret| ones, and that, due to the way loop unrolling
works during CFG generation\needCite{}, the \verb|StartFunction| and
\verb|EndFunction| themselves correspond to dynamic instances of those
instructions\editorial{That really isn't very well described.}.

\todo{Hmm.  I'm really not doing a good job of describing how this
  works.}

Conceptually, this pass is a simple constraint solving problem.  A
\verb|StartFunction| $x$ side-effect produces a constraint that
$stack(s1) = stack(s2) + frame$, where $stack(s1)$ is the stack at the
start of the side-effect, $stack(s2)$ is the stack at the end of it
and $frame$ is the frame associated with the side-effect, and
conversely for \verb|EndFunction| side-effects.  Ideally, SLI would
generate all of these constraints and solve them, and hence directly
determine the stack layout at every point in the \StateMachine, but
doing so is problematic because the number of variables which must be
solved for is not known initially.  The workaround for this is
fortunately rather simply to split the constraint solver into two
passes: one which determines the depth of the stack at each point in
the \StateMachine, and hence how many variables are needed, and
another which solves to determine the values of those variables.  The
resulting stack layouts are then encoded into the \StateMachine using
special \verb|StackLayout| side-effects at every entry point and by
attaching the relevant frame ID to each \verb|StartFunction| and
\verb|EndFunction| side effect.

\todo{I could say quite a lot about how this works, and there are some
  moderately interesting subtle bits to it, but I don't think the
  interestingness justifies the amount of space needed to describe
  them properly.}

\todo{What I should really do is figure out precisely what problems it
  causes when we get this wrong, which'd give me a pretty good handle
  on (a) how important it is and (b) what the best way of describing
  it is.}

Once frame IDs have been allocated and assigned to side effects, the
statically-determined aliasing information must be incorporated into
the \StateMachine.  This takes the form of special
\verb|PointerAliasing| side effects, which give points-to set for
specific registers, and annotations on the \verb|StackLayout| side
effects which indicate whether there might be any pointers to the
relevant stack frame in any part of memory at a given entry
point\editorial{So that flattens the pointed-at-by-memory and
  pointed-at-by-self flags of the static analysis into one flag; might
  want to explain what's going on there.}.  Register points-to
sets here have a small number of fields:

\begin{itemize}
\item
  A flag saying whether the register might point outside the stack
  completely\editorial{Need to think hard about what that means.}.
\item
  A set of frames which the register might point into.
\item
  A flag saying whether any frames outside of that set might be
  pointed at.
\end{itemize}

\todo{Good God I'm making a meal of explaining this.}

\subsection{The actual alias analysis}

\todo{Really need to look at some standard compiler alias analyses to
  figure out how novel this actually is.  It'll need some description
  regardless, because it's important and I'm pretty certain nobody
  else has tried it in this context, but the amount and type might
  change a bit.}

Two-step process, one of which builds up aliasing and points-to
tables, the other which actually uses them.  Using them is easy, so
only go into detail on building them.

Key data structures:

\begin{itemize}
\item
  Aliasing table -- for each memory accessing operation a set of other
  memory accessing operations which occur before it in the machine and
  which it might interfere with.  Also flags saying whether a given
  load might load the initial value of memory or something stored
  by something outside of this \StateMachine.
\item
  Points-to table -- a points-to class for each register and temporary.
\end{itemize}

The two tables have to be generated at the same time, as the PTT is a
function of the AT and the AT is a function of the PTT.  Iterate to a
fixed point.

Things to do with these tables:

\begin{itemize}
\item
  Forwarding from stores to loads.
\item
  If something gets loaded twice replace the second one with a copy
  c.f. compiler avail expression analysis.
\item
  Removing stores which are definitely never loaded.
\item
  Removing \verb|StackLayout|, \verb|StartFunction|,
  \verb|EndFunction| side effects which aren't useful any more.
\end{itemize}

Probably also want some discussion about when this is safe w.r.t.
multi-threaded behaviour.



\section{Other static analysis}

Most of the SLI analyses are applied solely to the \StateMachine
generated for the specific bug, and so can only consider behaviour
which is within the analysis window.  In effect, they are insensitive
to context beyond the bug horizon.  This helps to make them fast, but
can sometimes limit their effectiveness.  SLI therefore also uses a
small number of very simple whole-program analyses, still applied to
the program binary without access to the source code, in order to
compute a few simple context-sensitive facts about instructions in the
program before the main analysis process starts.  These facts are then
included in the generated \StateMachines, and so can be used by the
local analysis passes to improve their accuracy.  I now describe these
global analysis passes\editorial{And I sound like I have something
  unpleasant jammed up my arse.}.


\subsection{Frame pointer elimination}

One possibly surprising property of SLI is that it is somewhat more
effective on programs built with extensive compiler optimisation than
it is on unoptimised builds, as optimising compilers are generally
quite good at removing unimportant steps from the program.  The most
important compiler optimisation, from SLI's perspective, is frame
pointer elimination.  When frame pointers are in use the program
maintains two pointers into the current stack frame, the stack pointer
and the frame pointer, usually with a fixed offset between them, and
the compiler emits some stack accesses relative to the stack pointer
and some relative to the frame pointer.  This complicates alias
analysis for accesses to function-local variables.  SLI therefore uses
a static analysis to rewrite frame pointer-relative accesses into
stack pointer-relative ones wherever possible.

The core structure produced by this analysis is a table mapping
instructions in the program to the offset from the stack pointer to
the frame pointer when that instruction executes, or a special value
indicating that the offset is not a constant.  The analysis here is,
again, an iteration to a fixed point which builds an initial
approximation to a correct offset table and then refines it by
considering each instruction in isolation until they are all locally
correct, at which point the overall table will also be correct.

In more detail:

\begin{itemize}
\item
  The initial offset table only contains entries for all instructions
  which depend on the type of instruction:

  \begin{itemize}
  \item Function heads start off as having a non-constant offset.
  \item Instructions which set the offset to a known value have an
    entry reflecting that.  For x86, the most common such instruction
    is \verb|mov %rsp, %rbp|, which copies the stack pointer to the
    frame pointer and hence sets the offset to zero, and which appears in
    most function prologs.
  \item Other functions start off having an unknown offset.
  \end{itemize}
\item
  The entry state of an instruction is the join of all of its
  predecessor instructions' exit states.  The join rule here
  is:

  \begin{itemize}
  \item
    If any input state is not-a-constant then the output state is
    not-a-constant.
  \item
    Otherwise, if there are any known-constant inputs, the output
    state is known-constant if all of the inputs match and
    not-a-constant otherwise.
  \item
    Otherwise, all of the predecessor instructions have unknown
    offsets and the result is an unknown offset.
  \end{itemize}
\item
  The exit state is a function of the input state and the type of
  instruction.  Those which adjust the stack or frame pointers by some
  constant produce an output offset which is the input offset plus or
  minus that constant, as appropriate; those which set the offset to a
  known value produce that value as output (even when the input offset
  is not-a-constant), as already indicated; and those which update one
  or other of the pointers in some other way set the output state to
  not-a-constant.
\item
  Once the iteration has converged any instructions which are still in
  the unknown state have their state set to not-a-constant.
\end{itemize}

The resulting offset table accurately reflects the properties of the
program.

\section{Alias analysis}
\subsection{Alias analysis and identification of thread-local accesses}
There are two major complications to optimising memory accesses:

\begin{itemize}
\item
  Alias analysis.
  Most memory-related optimisations rely on being able to tell whether two pointers alias with each other.
  This is impossible in the general case, but it is possible to develop some useful (sound and unsound) approximations.
  This is very similar to the problem of the same name faced by optimising compilers, except that optimising compilers generally have more information (e.g. type information).
\item
  Checking whether an access is thread-local.
  Most races are mediated through memory, and all of the races targeted by SLI will involve some memory-accessing component.
  Correctly handling races restricts the available space of optimisations, often substantially.
  For instance, if a machine stores to a location and then immediately loads from it again then it might be tempting to forward the stored expression to the load, but this is invalid if another thread might store to the same location in between the two accesses.
  This is a common cause of races, and one which can be easily handled by SLI-like techniques, and so it is important not to eliminate it at the machine simplification stage.
\end{itemize}

If a DRS log is available then this is generally trivial, as all of the necessary information will be present in the log\editorial{should really have a ref to the DRS section, which'll have more details.}.
Otherwise, SLI uses a combination of static and dynamic analyses to solve these problems.

\subsubsection{Fallback resolution}
When neither the dynamic nor local-frame static analyses can answer an aliasing question, SLI attempts to resolve it using a peephole resolver.
To check whether \verb|A| and \verb|B| might possibly refer to the same memory location, SLI generates the expression \verb|A == B| and then passes it to the expression simplifier (see \S\ref{sect:peephole_simplification_expressions}).
If the result is the constant \verb|1| then \verb|A| and \verb|B| definitely alias; if it is the constant \verb|0| then they definitely don't; and otherwise they might alias and might not.
In the last case alias analysis, finally, fails.
Depending on the intended use of the alias result, SLI will then either choose a safe default or use a case split to consider both possibilities.

\section{Discussion of the SSA form used}
Because it's very subtly different from the form most commonly used in optimising compilers.
Also worth talking about the use of SDA form here as well.

\section{Other simplifications}

\begin{itemize}
\item
  Standard dead code elimination.  Very modest trickiness to do with
  handling dead stores, but really not very hard.
\item
  Fairly standard copy propagation thing.  There are some moderate
  subtleties in here to do with handling accesses through memory, and
  whether it's safe to forward from a store to a load when there might
  be interesting races going on, but nothing very exciting.  Ref
  cifuentes here.
\item
  Phi elimination pass.
\item
  Undefinedness optimisation?
\end{itemize}

\section{Variable unification}

The analysis can sometimes lead to there being multiple fragment in a
single \StateMachine which differ only in variable and register names.
The variable unification pass attempts to unify these fragments
together by renaming variables.  This is conceptually rather simple:
find all of the places in the \StateMachine where two states are
identical except for variable names, build a new fragment of
\StateMachine which is equivalent to both input fragments, and then
replace the old fragments with the new one.  The details are, however,
moderately subtle, and I now discuss them briefly.

First, the definition of ``identical except for variable names''
includes the successor pointers of the state but not the predecessor
pointers, so states do not have to be reachable from the same place
but must reach the same place after completing\editorial{This is kind
  of arbitrary; an almost identical analysis could use the converse
  constraint, but I've not bothered to implement that.}.

Building the unifying \StateMachine fragments requires a moderate
amount of care.  There are in general three components to building the
unifier:

\begin{itemize}
\item
  Unifying any inputs which the state might require.
\item
  Unifying any memory accesses issued by the state.
\item
  Unifying any output registers which the state might produce.
\end{itemize}

Unifying output registers is the simplest of these.  Suppose we have
two side effects which we wish to unify:
\verb|A: Copy reg1 = 5 then C| and \verb|B: Copy reg2 = 5 then C|.
The obvious unifier here is like this:

\begin{verbatim}
A': Copy reg1 = 5 then B'
B': Copy reg2 = reg1 then C
\end{verbatim}

And this is the one used by SLI.  While it is correct in almost all
cases, it is perhaps not obvious why it is correct.  In particular,
the \verb|A| state has gained an assignment to \verb|reg2| and the
\verb|B| state one to \verb|reg1|, and one might be concerned that
this might affect the \StateMachine's behaviour.  To see why, first
notice that there can be no assignments to \verb|reg2| before
\verb|A|: the \StateMachine is in static single assignment form, and
so there can be no assignments to \verb|reg2| except for \verb|B|, and
the \StateMachine is acyclic, so there can be no path from \verb|C| to
\verb|A| and hence none from \verb|B| to \verb|A|.  Likewise,
\verb|reg1| is uninitialised at \verb|B|.  Therefore, ignoring
\verb|Phi| nodes, any path through \verb|C| starting at \verb|A|
cannot depend on the value of \verb|reg2|, and likewise any path
starting at \verb|B| cannot depend on the value of \verb|reg1|, and so
modifying their values is safe.

\verb|Phi| side effects complicate the situation somewhat, as they can
take uninitialised variables as input\footnote{Recall that Phi side
  effects select the most recently assigned input variable, and so
  will ignore any uninitialised variables in their inputs.}.
\verb|Phi| side effects which do not take either of the registers as
inputs are obviously unaffected by this transformation, as are those
which take both\footnote{The only possible effect of the
  transformation is that such a side-effect might take reg1 as input
  rather than reg2, or vice versa, but since their values will
  necessarily be equal that is not a problem.}, but any which take
only one of the registers as input will potentially produce a
different value.  Consider, for instance, this example program:

\begin{verbatim}
l1: a = 5;
l2: if (x)
l3:   b = 7;
l4: else
l5:   c = 7;
l6: d = Phi(a, b);
\end{verbatim}

The final value of \verb|d| will be \verb|7| if \verb|x| is true and
\verb|5| otherwise.  Attempting to unify \verb|l3| and \verb|l5| will,
however, produce a program fragment like this:

\begin{verbatim}
l1 : a = 5;
l2 : if (x)
l3 :   goto l3';
l4 : else
l5 :   goto l3';
l3': b = 7;
l5': c = 7;
l6 : d = Phi(a, b);
\end{verbatim}

In this case, the final value of \verb|d| is always \verb|7|.  SLI
therefore detects this case and will not perform the optimisation when
it happens.

Unifying inputs is more complicated.  The approach used by SLI is to
insert additional \verb|Phi| side-effects which select appropriate
register inputs into new freshly-allocated output registers and to
then use those new registers in the expression to be unified.  For
example, consider a program like this:

\begin{verbatim}
l1: if (x) {
l2:    a = 73;
l3:    b = a + 7;
l4: } else {
l5:    c = 92;
l6:    b = c + 7;
l7: }
\end{verbatim}

This example is not in SSA form, as there are multiple assignments to
\verb|b|, but output variables have already been discussed and so
ignore that\editorial{Find a better example}.  We would now like to
unify the \verb|l3| and \verb|l6| statements.  One possible solution
would be this:

\begin{verbatim}
l1: if (x) {
l2:    a = 73;
l4: } else {
l5:    c = 92;
l7: }
l8: d = Phi(a, c)
l9: b = d + 7
\end{verbatim}

Building the unifier is simple when it is possible to do so: compare
the two expressions which are to be unified, building up a unifier
over registers as we do so, then iterate over all of the registers in
the unifier and comparing them to the \StateMachine's control flow to
determine whether a \verb|Phi| can select the right one (failing if
not), and then generate the unifier itself in the obvious way.

\todo{I'm a little worried here that this is really very similar to
  the unification algorithm, and the terminology is also very similar,
  but they're not *quite* the same, which might be a bit confusing.}

\todo{Should probably have a more realistic example, really; this one
  makes it look like this is something which won't happen very often,
  whereas actually it's quite useful.}

The final step of unifying side effects is unifying their memory
accesses, if they have any.  At this point, the extra level of
indirection between memory access identifiers and CFG nodes, discussed
in \S\todo{...}, becomes useful, as unifying two memory accesses
becomes simply a matter of allocating a new memory access identifier
whose CFG set is the union of the two input identifier's CFG
sets\editorial{Need to either say more or move this to some place a
  bit less obvious.}.

\section{CFG trimming}

Many of the instructions examined in the analysis will ultimately prove to be irrelevant to the final result, and so will be eliminated by the analysis
They will, however, continue to be represented in the \StateMachine CFG.
This pass eliminates these wherever possible.

\todo{Write me.}

\section{Unsound simplifications}

\section{The satisfiability checker}
This doesn't really belong here.  I should figure out where to put it.  I should also figure out whether I'd be better off using someone else's checker.

\section{The expression simplifier}
These are pretty much just local arithmetic rules.
I should describe them somewhere, but it'll probably just be a single big display with all of the rules I use.
Most of them are very stupid.

\section{Comparison to decompilation techniques}
Not sure what I'm going to put in here, but it seems kind of necessary.
Generally need to spend some more time looking at the decompilation literature.

\todo{One obvious decompilation technique which I'm \emph{not} using is local variable recovery.  Should really explain why I didn't use it.}

\section{Comparison to reverse slicing techniques}
Surprisingly few, but probably worth discussing here anyway.
Pretty much all of the existing slicing literature is on source-level stuff, rather than binary-level, but there are still a few parallels.

