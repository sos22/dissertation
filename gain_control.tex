\chapter{Gaining control of a running program}
\label{sect:enforce:gain_control}

\todo{This is maybe a bit short to have its own chapter?}

The discussion of building crash enforcers in
Section~\ref{sect:reproducing_bugs} assumes that it can gain control
of the program at any point.  This section describes one possible way
of doing so.  {\Implementation} does so by modifying the original
program to contain branches into the plan interpreter where necessary.
This has very low runtime overhead and avoids modifying the program's
state, but carries one important complication: the instruction at
which {\implementation} needs to gain control might be smaller than a
branch instruction, and so inserting the branch might cause additional
instructions in the program binary to be corrupted.  {\Implementation}
must then ensure that the program never attempts to execute one of
these corrupted instructions.  For performance reasons, we would also
like to minimise the number of instructions which must be run in the
interpreter.

\todo{Slightly abrupt?} This is most easily understood as a kind of
search problem\cite{Russell1995}.  The algorithm starts in a state in
which the original program is not modified at all, but with a list of
instructions at which it must gain control; it must then find a path
to a state in which it is guaranteed to gain control at all necessary
points and the program is guaranteed never to branch to a corrupted
instruction.  More explicitly, these states consist of three sets:

\begin{itemize}
\item $\mathit{Patch}$ --- instructions at which we have decided to place a
  branch instruction.
\item $\mathit{Cont}$ --- instructions which might not themselves be
  patched, but which must be processed in the interpreter.  The
  program will not branch to the interpreter when it encounters one of
  these instructions, but if the interpreter has already started when
  one needs to be run then the interpreter will continue, even if the
  enforcement plan itself does not require that.
\item $\mathit{MustInterpret}$ --- instructions where the enforcement
  plan requires us to gain control, but where the patches described in
  this state would be insufficient to do so.
\end{itemize}

The search process then generates additional states according to two
rules:

\begin{itemize}
\item
  \textbf{PatchDirect} An instruction is removed from
  $\mathit{MustInterpret}$ and added to $\mathit{Patch}$.  The
  analysis then examines the instructions which would be corrupted by
  that new entry point to determine whether there are any potential
  branches to them and, if so, adds those branches to
  $\mathit{MustInterpret}$.  The corrupted instructions are themselves
  added to $\mathit{Cont}$.

  This rule is only valid when the existing $\mathit{Patch}$ set does
  not itself contain any patch operations which would overlap with the
  new patch point, as these branch instructions would otherwise
  corrupt each other and lead to an unrealisable solution.
\item
  \textbf{Prefix} As with \textbf{PatchDirect}, an instruction is
  removed from $\mathit{MustInterpret}$, but in this case it is added
  to $\mathit{Cont}$ and any instruction which might branch to it
  added to $\mathit{MustInterpret}$.  The idea here is that if we can
  ensure that all instructions which execute immediately prior to an
  instruction $i$ are executed in the interpreter and if the
  interpreter continues interpreting when it sees a branch to $i$ then
  that is sufficient to ensure that $i$ is also run in the
  interpreter.
\end{itemize}

Any state with an empty $\mathit{MustInterpret}$ set then represents a
valid solution to the patching problem.  The number of instructions in
$\mathit{Cont}$ is a rough proxy for the number of additional
instructions which will have to be interpreted due to the corrupted
instructions problem, and hence for the overheads of the patching
process, and so the search process aims to minimise the
$\mathit{Cont}$ set.  Usefully, both rules increase the size of
$\mathit{Cont}$ and so a simple exploration strategy which always
selects the state in the queue with the smallest $\mathit{Cont}$ set,
applies both rules to that state, and places the two new states in the
queue is guaranteed to always find the minimal result, and this is the
strategy adopted by {\implementation}.

This algorithm relies on being able to calculate the predecessors of
an instruction, much like the \gls{cfg} building algorithm in
Section~\ref{sect:derive:build_static_cfg}, and it also uses the
program model to do so, as described in
Section~\ref{sect:program_model:instr_predecessors}.  \todo{This
  depends on some details of the program model which I haven't
  discussed yet.  Hmm.} There is one important difference, though: if
the \gls{cfg} building algorithm misses a predecessor, the analysis might be
incomplete, which is irritating but tolerable; if the patch strategy
misses a predecessor, the program will crash, which is much worse.
There is one important case where the program model returns incorrect
information, and that is when a function in the main program is called
by a library.  The program model does not include any information
about libraries, and so will be unable to locate all of the callers.
On the other hand, it can locate all of the \emph{called}
instructions.  {\Technique} can then avoid the issue by disabling any
rule which requires the predecessor set for an instruction which a
library branches to (so \textbf{Prefix} cannot be used for
instructions which are branched to from libraries and
\textbf{PatchDirect} cannot be used if it would corrupt any
instructions which are branched to from libraries).

This can sometimes lead to the patch problem being unsolvable if the
entire function is smaller than a single branch instruction, as in
that case it is possible that neither rule might be enabled at the
first instruction in the function.  In that case, {\implementation}
simply gives up and reports an error.  This is rarely a problem in
practice, as very few functions are that small without very aggressive
compiler optimisations, and most compilers, including
gcc\cite[Section~3.10]{Stallman2010} and LLVM\needCite{}, will at high
optimisation levels pad functions to a multiple of 16 bytes for
performance reasons.

An alternative approach would be to take control of the program using
debug breakpoints rather than jump instructions.  These are either a
single byte (for the \verb|int3| instruction) or no bytes at all (for
debug registers), and so avoid the instruction clobbering problem.
This would work, but would have a couple of important disadvantages:

\begin{itemize}
\item
  Debug breakpoints are far slower than branches.  This might be
  important if the critical section is to be inserted on a
  particularly hot code path and has a side-condition which usually
  fails.
\item
  Using debug breakpoints in this way would interfere with any other
  debugger which the developer might want to use.  With a branch-style
  patch, standard debuggers work without modification for any part of
  the program which has not been patched, whereas a breakpoint-style
  patch requires extensive coordination between the debugger and the
  patch mechanism for either to work at all.
\item
  Breakpoint registers are of strictly limited number on most
  architectures (four, on x86).  This means that they can never
  provide a complete solution by themselves.
\item
  On most UNIX-type operating systems, including Linux and FreeBSD,
  catching debug breakpoints requires modifying the program's signal
  handling configuration, which requires some level of coordination
  with the program to be modified.  It would be possible to use an
  alternative API, but this would require kernel modifications,
  complicating the use of the generated patches.
\end{itemize}

{\Implementation} therefore generates exclusively branch-style patches.
