\chapter{Simplifying {\StateMachines}}
\todo{I'm pretty much convinced now that I want to kill off the static
  analysis bit of aliasing analysis, because the complexity is quite
  high for a very marginal win.}

The {\StateMachines} generated by the algorithms in
Section~\ref{sect:derive:derive} are faithful representations of all
of the instructions which might have been executed by the program in
the \backref{analysis window}.  As such, they usually contain a large
amount of redundant information which is not relevant to the behaviour
being investigated.  {\Technique} therefore uses a number of related
techniques to simplify them and to eliminate irrelevant fragments.
The important ones are:

\begin{itemize}
\item
  Dead code elimination, to eliminate redundant updates to registers.
  This is particularly effective for eliminating updates to the
  \verb|rflags| register in the AMD64 architecture. This is
  essentially identical to the compiler-level optimisation of the same
  name\needCite{}, and so is not discussed in detail here.
\item
  Register copy propagation, which combines smaller updates to
  registers into a single higher-level operation which is usually
  easier to analyse.  To see why this is necessary, consider
  a fragment of code like this:

\begin{verbatim}
shl (%rax << $4) -> %rax
sub (%rax - $7) -> %rax
mul (%rax * $11) -> %rax
\end{verbatim}
  
  This will produce a three-state {\StateMachine} fragment, with one
  state for each instruction.  It would be more useful to produce a
  single state which set $rax$ to $((rax \times 16) - 7) \times 11$,
  and this simplification performs that transformation.  The algorithm
  used is a straightforward adaptation of that employed by
  dcc\needCite{}, and so is not discussed in detail here.

  One minor extension present in {\implementation} but not dcc is that
  {\implementation} can make use of \state{Assert} side-effects during
  this transformation, so that, for instance, if $x$ is asserted to be
  less than $7$ then the expression $x > 22$ can be rewritten to
  $\mathit{false}$.  This does not require any significant changes to
  the bulk of the algorithm, beyond a few simple rules describing when
  such rewrites are valid.
\item
  {\Technique} attempts to eliminate $\Phi$ expressions from generated
  {\StateMachines} by converting them into multi-terminal binary
  decision diagrams over the {\StateMachine}'s control flow predicates;
  this analysis is described in Section~\ref{sect:phi_elimination}.
\item
  {\Technique} attempts to eliminate memory accessing instructions
  using a cross-function alias technique which incorporates a fast but
  low-accuracy points-to analysis on the original binary with a slower
  but more accurate alias analysis applied to the {\StateMachine};
  this is described in Section~\ref{sect:alias_analysis}.
\end{itemize}

{\Implementation} also makes use of a number of simple arithmetic
simplifications, such as rewriting $x + 0$ to just $x$ or
$(\happensBefore{x}{y}) \vee (\happensBefore{y}{x})$ to
$\mathit{true}$, and constant-folding for most common operators.

\section{$\Phi$ elimination}
\label{sect:phi_elimination}

\todo{This whole section needs rewriting.}

The use of SSA form simplifies many parts of the analysis by breaking
up variables into their different live ranges, but complicates some
parts by introducing $\Phi$ side effects, which are themselves quite
difficult to analyse.  The reason that $\Phi$ side effects are hard to
analyse is that their behaviour depends on the {\StateMachine}'s
control flow, and this is not explicitly reified and so is unavailable
to the usual simplification machinery.  The $\Phi$ elimination pass
attempts to rectify this problem by converting the $\Phi$ effects into
expression BDDs (see Section~\ref{sect:state_machine_expr_language})
which examine the {\StateMachine}'s state and use it to select the
correct input to the $\Phi$.  In other words, the BDDs encode the
relevant parts of the {\StateMachine}'s control flow, and hence allow
the other simplification passes to use and manipulate them.

The approach used is simple: build a mapping from {\StateMachine}
states to boolean BDDs\footnote{For clarity, I refer to BDDs which
  evaluate to either true or false as boolean BDDs or just BBDDs, by
  contrast with MTBDDs which can evaluate to a more complex value set.
  Most existing literature refers to BBDDs as just BDDs.}  which are
true for precisely those executions in which the state will be
executed, extend this into a mapping from states to MTBDDs which show
which input to the $\Phi$ would be selected if the $\Phi$ were issued
immediately after that state, and then simply read off the MTBDD for
the actual $\Phi$ state.\editorial{Should be a mention of CDGs in
  there, so that the next section makes sense.}

\smh{Some duplication here.  Can you clarify/simplify?  (Maybe also
  give an example?)}

\subsection{Building the control dependence graph}

\label{sect:cdg}

The control dependence graph (CDG) is a standard compiler data
structure\needCite{} showing which nodes in a control flow graph are
control-dependent on which branch operations.  In other words, it
shows, for each statement, which control-flow conditions must be true
for that statement to execute, which must be false, and which do not
matter.  The control dependence graph used by {\technique} is a slight
extension of this concept: rather than simply listing the expressions
on which the node depends, it gives a BBDD which will evaluate to true
in precisely those executions in which the node executes.  This is
possible because {\technique}'s {\StateMachines} are finite and
acyclic.

The algorithm for building the graph is simple:

\begin{algorithmic}[1]
\State $\mathit{cdg}[\mathit{root}] = \mathit{const}(\mathit{true})$\smh{Presumably, you meant to italicise cdg itself to something?}
\While {Some state is unlabelled in $\mathit{cdg}$}
  \State {$n \gets $ select a state which has no label but whose predecessors are all labelled}
  \State {$\mathit{cdg}[n] \gets \bigvee \{\mathit{cdg}[p, n] \mid p \in \textrm{$n$'s predecessors}\}$}\Comment{Take union of all reaching paths}\smh{Maybe use a different font for comments?}
\EndWhile
\end{algorithmic}

Here, $\mathit{cdg}[x]$ is the condition for state $x$ to run.
$\mathit{cdg}[x, y]$ is the condition necessary for the edge from
state $x$ to $y$ to be traversed, defined by:

\begin{displaymath}
\mathit{cdg}[x, y] = \begin{cases}
  \bot                                     & \text{if there is no edge from $x$ to $y$} \\
  \mathit{cdg}[x] \wedge x.condition       & \text{if $x$ is an \state{If} state and $y = x.\mathit{trueTarget}$} \\
  \mathit{cdg}[x] \wedge {\neg}x.condition & \text{if $x$ is an \state{If} state and $y = x.\mathit{falseTarget}$} \\
  \mathit{cdg}[x]                          & \text{otherwise}
\end{cases}
\end{displaymath}

Note that the selection on line 3 is only guaranteed to be possible
because the {\StateMachine} is acyclic\footnote{This assumption is
  also the reason that {\technique}'s CDG is precise, whereas CDGs for
  full programs usually contain some approximations}.  Given that, the
algorithm is simple: to find the conditions under which a state might
execute, find the conditions under which all of its incoming edges
might execute and take the union.  Once all states are labelled the
CDG is complete.

\subsection{Building the $\Phi$ map}

Once the CDG is complete, the next step is to build the $\Phi$ map.
This is a map from states of the {\StateMachine} to MTBDDs which
select which input of the $\Phi$ would be selected if the $\Phi$ were
to be issued immediately after that state.  The complete map then
makes eliminating the $\Phi$ side-effect trivial.  The algorithm for
building the $\Phi$ map is shown in
Figure~\ref{fig:derive:phi:phi_map}.

\begin{figure}
\begin{algorithmic}[1]
\For {$i$ in inputs to the $\Phi$}\Comment {Build the initial map}
  \State {$s \gets $ state defining input $i$}
  \State {$pm[s] \gets const(i)$}
\EndFor
\While {the $\Phi$ state is unlabelled}
  \State {$n \gets $ select a state which has no label but whose predecessors are all labelled}
  \State {$g \gets \{ (\textit{cdg}[p, n], \textit{pm}[p]) \mid p \in \textrm{$n$'s predecessors}\}$}
  \State {$\textit{pm}[n] \gets \textsc{simplify}(\textsc{flatten}(g), cdg[n])$}
\EndWhile
\end{algorithmic}
\caption{Building the $\Phi$ map}
\label{fig:derive:phi:phi_map}
\end{figure}

\smh{Ok -- I found this \emph{very} confusing and I think you can
  explain it more easily.  GMTBDD = trad abstraction; you essentially
  just want to use the CDG predicate to project the appropriate value
  from $\Phi$.}

Line 7, the definition of $g$, is the core of the algorithm.  $g$ is a
set of pairs of a control predicate and an MTBDD.  Each pair
represents one way of reaching the current state, where the predicate
says when the {\StateMachine} will follow that path and the MTBDD says
what the $\Phi$ would evaluate to if the {\StateMachine} followed that
path.  The \textsc{flatten} function then converts these sets back
into a simple MTBDD in a way which respects variable ordering and
keeps the MTBDD in reduced form.  This is guaranteed to be possible
and unambiguous because the construction of the CDG ensures that, for
every state, precisely one of the control predicates evaluates to
$\mathit{true}$.

\todo{\textsc{Flatten} itself is a fairly straightforward variant of
  the standard BDD zip algorithm, but has lots of nasty special cases,
  so I don't really want to have to describe it.  At the same time, I
  can't find a cite for anyone doing precisely the same thing, so
  perhaps I ought to.}

As a minor optimisation, the algorithm shown simplifies the MTBDD for
state $n$ under the assumption that the CDG condition for $n$ is true;
in other words, under the assumption that $n$ is actually run.  The
\textsc{simplify} function is shown in
Figure~\ref{fig:derive:phi_elimination:simplify}.  The basic idea is
to define a $\textsc{implies}(a, b)$ operation which evaluates to $a$
when $b$ is true or the special $\bot$ value when $b$ is false and to
then use this to zip the original MTBDD with the assumption BDD.  This
lifts the original MTBDD $a$ to a new MTBDD which evaluates to $a(k)$
for any configuration $k$ where the assumption is true or to $\bot$
where the assumption is false.  We then define an \textsc{unlift}
operation which removes all paths through the MTBDD to these $\bot$
values, returning the MTBDD to its original range.  The net effect is
to produce a new MTBDD which evaluates to the same thing as the
original whenever the assumption is true, but potentially tests fewer
input variables while it does so.\smh{Very unclear! example?  Isn't
  this effectively just computing the path condition?}

\todo{This is supposed to be removing the effects of any common
  dominator of both predecessor states.}

As a further minor optimisation, not shown in the figure,
{\implementation} performs an initial reachability test to determine
which states in the {\StateMachine} might eventually reach the $\Phi$
state which is to be eliminated and does not attempt to calculate
labels for any which do not.  The result of the analysis is unchanged.

\begin{figure}
\begin{algorithmic}
\Function{simplify}{$thing:MTBDD(k)$, $assumption:BDD$ $\rightarrow \bot + MTBDD(k)$}
  \State \Return $\textsc{unlift}(\textsc{zip}(thing, assumption, \textsc{implies}))$
\EndFunction
\Function{implies}{$a:k$, $b:bool$ $\rightarrow k + \bot$}
  \If{$b$}
    \State \Return $a$
  \Else
    \State \Return $\bot$
  \EndIf
\EndFunction
\Function{unlift}{$inp:MTBDD(k + \bot) \rightarrow \bot + MTBDD(k)$}
  \If{$inp = const(\bot)$}
    \State \Return $\bot$
  \ElsIf{$inp = const(k)$}
    \State \Return $inp$
  \Else
    \State $if(cond, t, f) \gets inp$
    \State $t' \gets \textsc{unlift}(t)$
    \State $f' \gets \textsc{unlift}(f)$
    \If{$t' = \bot$}
      \State \Return $f'$
    \ElsIf{$f' = \bot$}
      \State \Return $t'$
    \Else
      \State \Return $if(cond, t', f')$
    \EndIf
  \EndIf
\EndFunction
\end{algorithmic}
\caption{The MTBDD $simplify$ algorithm.  The notation $k:t$ indicates
  a variable $k$ of type $t$, $a \rightarrow b$ indicates a function
  from arguments $a$ to a return value of type $t$, $\bot + k$
  indicates type $k$ augmented with the value $\bot$, and $MTBDD(k)$
  is the type of MTBDDs whose terminals are constants of type $k$.
  Not shown: \textsc{unlift} also reduces the MTBDD as it builds it,
  removing any newly-introduced duplicate nodes.}
\label{fig:derive:phi_elimination:simplify}
\end{figure}

\todo{I really don't want to have to define $zip$, because it's fiddly
  and not very interesting, but I can't find a cite for it.}

\todo{Need to think much harder about variable ordering within the
  BDDs.  Currently using pointer order, which is rather silly.  Also
  need to mention that the ordering is consistent within any run of
  the pass, but not necessarily between multiple runs.}\smh{Well, you
  need some order -- or are you hoping to simplify by relabelling?  Is
  this worth it?}

\todo{Might be worth explicitly calling out that the CDG can be shared
  between different $\Phi$ effects, but the $\Phi$ map needs to be
  rebuilt each time?}\smh{Possibly.  As mentioned, this section is
  among the most confusing so far.}

\subsection{Conversion to SSA}
\label{sect:ssa}

\todo{I just moved this from the building phase without updating it.
  It probably needs rewriting for its new context, if I decide to keep
  it.}

{\STateMachines} are, for the most part, maintained in a variant of
static single assignment (SSA) form.  SSA is a standard compiler
intermediate representation in which each variable has at most one
static assignment\needCite{}\smh{Cytron et al TOPLAS 91?  or some
  textbook from AM's class?}.  Variables which are assigned to
multiple times are converted into families of related variables
(usually referred to as ``versions'' or ``generations'' of the
variable), each of which is assigned to precisely once.  This has the
effect of breaking up the live ranges of long-lived variables, which
can expose other useful optimisations.  Most uses of the original
variable will be converted into references to a specific member of one
of these families; the only case in which this is not possible is
where the correct member to use depends on the program's control flow,
and in that case special $\Phi$ nodes are inserted into the program
which select an appropriate member depending on the immediately
proceeding control flow.  These $\Phi$ nodes are themselves
unrealisable on most hardware, and so the program must be converted
back from SSA form after being optimised and before being lowered to
machine code.

Many of the compiler optimisations for which SSA is helpful are also
relevant to {\technique}, and so {\technique} also converts its
{\StateMachines} (which are analogous to a compiler's intermediate
representation) into SSA form.  The details of the SSA form are,
however, very slightly different to the conventional one: whereas a
compiler-style $\Phi$ node examines the program's preceding control
flow and maps from incoming control-flow edges to input variables, a
{\technique} one examines the order in which variables have been
assigned to and selects whichever was updated most recently (from a
specified set).  This has several important implications:

\begin{itemize}
\item
  Converting this form of SSA back into a non-SSA form can sometimes
  requires additional temporary variables to record which version of a
  particular variable has been most recently assigned to, whereas the
  more conventional control-flow based form does not.  This would be
  an additional complication in a compiler, but is not a problem for
  {\technique}, which never has to perform that conversion.
\item
  A {\StateMachine}'s control flow graphs can be modified without
  needing to update $\Phi$ nodes.  For example, suppose that a
  {\StateMachine} is as shown on the left of
  Figure~\ref{fig:ssa_cfg1}, and suppose that further analysis shows
  that the assignment of $z$ is dead.  We would like to remove the
  assignment and turn the {\StateMachine} into the one shown on the
  right.  This is correct as-is using {\technique}'s $\Phi$ semantics.
  If a simple control-flow based definition of $\Phi$ were used
  instead then we would also need to convert the $\Phi$ node at l1
  into $x_3 = \Phi(x_1, x_2, x_2)$, as the l1 state now has three
  control-flow predecessors.  There are, of course, many solutions to
  this problem in the standard compiler literature\needCite{}, but all
  add complexity which is unnecessary and unuseful in this context.
\end{itemize}

Most algorithms for converting to SSA form will work equally well with
either form, including that used by \implementation, and so no details
are given here; see \needCite{} for more information\editorial{blah}.

\todo{I'd be surprised if I'm the first person to come up with this...}

Note that while {\StateMachine}-level variables, including registers,
are converted to single static assignment form, memory accesses are
not.  This is because it is not always clear from the {\StateMachine}
when two \state{Store} operations modify the same memory location,
which makes the conversion process far more difficult.  \todo{Maybe
  cite Van Emmerik 2007 here?}

\todo{Maybe mention that LLVM and gcc do something very similar?}

\todo{Not actually sure how interesting this is, now that I've written
  it down.  It's all true, and it does make a bit of difference to the
  symbolic execution stuff, but I could probably rewrite to drop it
  completely without leaving a particularly obvious hole.}

\begin{figure}
\begin{tikzpicture}
  \node (start) {start};
  \node [below right=of start] (b) {$x_2 = 6$};
  \node [below = of b](c) {if ($\ldots$)};
  \node [below = of c] (d) {$y_1 = 1$};
  \node [below right = of c] (e) {$y_2 = 2$};
  \node [below = of d] (f) {$z = 3$};
  \node [left = of f] (a) {$x_1 = 5$};
  \node [below = of a] (g) {l1: $x_3 = \Phi(x_1, x_2)$};
  \draw[->] (start) -- (a);
  \draw[->] (start) -- (b);
  \draw[->] (a) -- (g);
  \draw[->] (b) -- (c);
  \draw[->] (c) -- (d);
  \draw[->] (c) -- (e);
  \draw[->] (d) -- (f);
  \draw[->] (e) -- (f);
  \draw[->] (f) -- (g);
\end{tikzpicture}
\begin{tikzpicture}
  \node (start) {start};
  \node [below right=of start] (b) {$x_2 = 6$};
  \node [below = of b](c) {if ($\ldots$)};
  \node [below = of c] (d) {$y_1 = 1$};
  \node [below right = of c] (e) {$y_2 = 2$};
  \node [left = of d] (a) {$x_1 = 5$};
  \node [below = of a] (g) {l1: $x_3 = \Phi(x_1, x_2)$};
  \draw[->] (start) -- (a);
  \draw[->] (start) -- (b);
  \draw[->] (a) -- (g);
  \draw[->] (b) -- (c);
  \draw[->] (c) -- (d);
  \draw[->] (c) -- (e);
  \draw[->] (d) -- (g);
  \draw[->] (e) -- (g);
\end{tikzpicture}
\caption{Optimising an SSA-form machine}
\label{fig:ssa_cfg1}
\end{figure}

\section{Alias analysis}
\label{sect:alias_analysis}

\todo{Really need to look at some standard compiler alias analyses to
  figure out how novel this actually is.  It'll need some description
  regardless, because it's important and I'm pretty certain nobody
  else has tried it in this context, but the amount and type might
  change a bit.}

Memory-accessing operations are often difficult to analyse because it
is hard to determine when two pointers refer to the same memory
location.  {\Technique} therefore tries to eliminate memory accesses
whenever possible.  The basic approach used is to derive, for each
\state{Load} operation, an expression BDD which evaluates to the value
loaded, allowing the \state{Load} to be replaced with a simple
\state{Copy}.  Eliminating \state{Load}s will then make the
\state{Store} operations redundant, and so they can also be
eliminated.  The result is a {\StateMachine} which is far simpler, in
two senses: it has fewer states, and the states which remain have
simpler semantics.

At a high level, the algorithm used is similar to that used to
eliminate \state{$\Phi$} states (see
section~\ref{sect:phi_elimination}): consider each \state{Load} state
$l$ in turn and, for each one, define functions over states $i$
$before_l(i)$ and $after_l(i)$ which show what the result of issuing
the \state{Load} immediately before or after (respectively) $i$.  This
is only possible if the {\StateMachine} contains every \state{Store}
which might alias with $l$, but, subject to that constraint, it is
always possible to build the complete map.  The state $l$ can then be
replaced by a \state{Copy} from $before_l(l)$.

The functions $before_l$ and $after_l$ are themselves relatively
straightforward:

\begin{itemize}
\item $after_l(i)$ depends on $before_l(i)$ and the type of state $i$.
  For \state{Store} states, it is a BDD which tests whether the store
  aliases with $l$ and returns the \state{Store}'s data if it does and
  $before_l(i)$ otherwise.  For non-\state{Store} states, it is simply
  $before_l(i)$.
\item For non-initial states $s$, $before_l(s)$ depends on
  $after_l(p)$ for every predecessor $p$ of $s$ and on the condition
  associated with the $p$ to $s$ edge.  The final result is an
  expression BDD which is equal to $after_l(p)$ whenever $cdg[p, s]$
  is true, where $cdg$ is the control dependence graph as defined in
  section~\ref{sect:cdg}.  \todo{Building those BDDs is remarkably
    intricate (it took me days to come up with an algorithm which
    worked and was reasonably efficient, and I'm currently on the
    third rev of it), but also kind of boring.  Not sure what to do
    about that.  Could just pretend it's trivial.}  Note that the
  $cdg$ ensures, by construction, that precisely one such condition is
  true in any configuration\editorial{define?} which can reach
  $s$\editorial{That isn't obvious; I believe it mostly because the
    experiments haven't found any cases where it's false.}.  If $s$ is
  unreachable then the value of $before_l(s)$ does not matter, and so
  this definition is sufficiently unambiguous.

  \todo{I'm ignoring sub-word accesses here.  Should probably be more
    explicit about that.}

\item $before_l(r)$, where $r$ is the first state in the
  {\StateMachine}, is simply $\smLoad{addr}$, where $addr$ is the
  address expression from $l$.  In other words, issuing a load at the
  start of the {\StateMachine} always returns the initial value of
  memory.
\end{itemize}

These rules can be solved recursively to find $before_l(i)$ for any
$l$ and $i$, and hence to eliminate any \state{Load} for which every
\state{Store} is available\footnote{Note that this recursion is only
  guaranteed to terminate because {\StateMachines} are acyclic.}.

These definitions leave one important aspect undefined: how to
determine whether a \state{Store} and \state{Load} might alias.  In
some cases, this is trivial: $RSP+8$ and $RSP+72$ are never going to
alias, regardless of the value of $RSP$ or the structure of memory.
Other cases are much more difficult.  {\Technique} has two approaches
for doing so: a dynamic analysis, used to model accesses to the heap
and other global data, and a static one, used to model accesses to the
stack.  These are described in
sections~\ref{sect:derive_manip:dynamic_analysis} and
\ref{sect:derive_manip:static_analysis}, respectively.
Section~\ref{sect:derive_manip:synthesis_aliasing} then describes how
to combine the results of these two analyses and use them to perform
alias resolution.

\subsection{Mapping the program's function call structure to the {\StateMachine} structure}
\label{sect:derive_manip:synthesis_aliasing}

The static analysis is expressed in terms of accesses to the current
function's local frame, but {\StateMachines} are potentially
cross-function and so even the idea of a ``current function'' is not
entirely well-defined.  It is therefore necessary to recover the
program's function call structure, determining where function local
frames are created and destroyed and the boundaries between them, and
to then match this structure to the data produced by the static
analysis.

Deriving the function call structure is reasonably straightforward:
frames are created by \texttt{call} instructions, destroyed by
\texttt{ret} instructions, and the boundaries between frames are given
by the value of the stack pointer when one of these instructions
executes.  The \texttt{call} and \texttt{ret} instructions are easily
encoded into \state{StartFunction} and \state{EndFunction} annotation
states, with any leftover \texttt{ret} instructions used to determine
the stack when the {\StateMachine} starts and used to form
\state{StackLayout} annotation states at each entry
point\editorial{LS}.

The only slight subtlety lies in deciding when to treat two
\texttt{call} instructions as starting equivalent stack frames, and
hence when they should be assigned the same frame identifier.  If two
\texttt{call}s call the same function then giving them the same frame
identifier potentially allows the relevant states in the
{\StateMachine} to be merged together, and hence reduces future
analysis work.  On the other hand, assigning them different
identifiers usually allows the static analysis to be encoded with
greater fidelity, which can improve the results of alias analysis.
{\Implementation} solves this problem by assigning functions different
frame identifiers whenever possible, subject to the constraint that
all paths to a given {\StateMachine} state must always result in the
same call stack.

Consider, for example, the program fragments shown in
Figures~\ref{fig:derive:assign_frames1}
and~\ref{fig:derive:assign_frames2}.  In the first figure there are
two calls to \texttt{f}.  One starts after the other has finished, and
so they can be assigned different frame identifiers without causing
any {\StateMachine} states to be executed in different stack contexts;
this ensures that the local variables of the two invocations of
\texttt{f} can be correctly disambiguated by the alias analysis.  The
second figure also shows two static calls to \texttt{f}, but here any
given path will only invoke \texttt{f} once, at the end of the path.
All paths will therefore share a single representation of \texttt{f}
and the two calls to \texttt{f} will share a single frame identifier.
This allows the results of the alias analysis on \texttt{f} to be
effectively shared across the two paths, reducing computation costs.

\todo{That's not a brilliant explanation of this.  If nothing else, it
  implies that this is purely a performance thing, whereas there are
  actually some correctness constraints here as well.}

\subsection{Incorporating static analysis results into the {\StateMachine}}

The next step, once frame IDs have been allocated and the stack layout
determined, is to incorporate the information from the static analysis
into the {\StateMachine}, and in particular determining which frames
might be present in registers or memory when the {\StateMachine}
starts.  For frames allocated by \state{StartFunction} annotations
this is trivial: such frames are never present in memory or in
registers\footnote{Except possibly for the stack pointer itself; see
  later.} when the {\StateMachine} starts.  This reflects the
fundamental assumption that there are no live pointers to a function's
local variables before that function starts.  Likewise, the frame at
the top of the stack when the {\StateMachine} starts is easily
handled, as it is perfectly modelled by the static analysis, and the
information can be simply added to the appropriate \state{StackLayout}
annotation state.  The other frames on the initial stack require more
care.  The approach taken by {\implementation} is to find the call
site which allocated a frame and to derive an over-approximation of
the correct points-to configuration from the points-to configuration
at that point.

As an example, suppose that the initial \state{StackLayout} annotation
has frames $F_1$, $F_2$ and $F_3$, and that the inlining
context\footnote{This can be obtained from the {\StateMachine}'s CFG;
  see section~\ref{sect:derive:cross_function_cfgs}.} is $a$, $b$,
$c$.  This indicates that the {\StateMachine} starts in function $c$,
which was called from function $b$, which was called from function
$a$; that function $c$'s frame is $F_3$; that function $b$'s frame is
$F_2$; and that function $a$'s frame is $F_1$.  The static analysis
can easily determine whether any pointers to $F_3$ might have reached
registers or memory, and so this information is simply copied into the
\state{StackLayout} annotation.  Determining whether any registers
might contain a pointer to $F_2$ is more difficult, as it requires
analysing both $c$ and $b$ and the static analysis is function-local.
It is, however, still possible to usefully constrain the locations
which might point at $F_2$ by examining the points-to configuration at
the \texttt{call} instruction in $b$ which invoked $c$.  In
particular, if there is no way for $c$ to have acquired a pointer to
$F_2$, whether via an argument register or through non-stack memory,
then it cannot have created any additional pointers to
$F_2$\footnote{This is closely analogous to the way the static
  analysis itself handles called sub-functions, as discussed in the
  previous section.}, and so it is safe to assume that a variable can
only contain a pointer to $F_2$ if the \texttt{call}-time
\backref{points-to configuration} allowed that location to point to
$F_2$\editorial{Rephrase.}.  On the other hand, if $c$ could have
received a pointer to $F_2$ then there is no way to constrain what it
could have done with it, and so any variable might point at $F_2$ by
the time the {\StateMachine} starts.  Similarly, the
\backref{points-to configuration} at the \texttt{call} from $a$ to $b$
can often be used to constrain the possible pointers to
$F_1$\footnote{One might reasonably ask what happens if a function
  does not have a \texttt{call}er; for instance, if it is the start
  function of some thread.  Such situations are extremely rare in
  practice.  Almost all operating systems start new programs and
  threads in some library function which then calls into the program's
  actual start-thread or start-program function, and so all program
  functions in practice have a \texttt{call}er.  {\Implementation}
  does not analyse library code, and so the library function's frame
  is itself never an issue.}\editorial{That footnote might be a bit
  too glib.}.

\todo{Maybe talk about interactions with cross-thread \StateMachines?
  They're not actually very interesting, but it's kind of an obvious
  omission.}

\subsection{Using the static analysis to constrain aliasing}

The {\StateMachine} has now been augmented with a number of annotation
states: \state{StackLayout}s, showing the initial stack layout and
which initial stack frames can be pointed at from where;
\state{StartFunction}s and \state{EndFunction}s, showing where stack
frames are created and destroyed; and \state{ImportRegister}s, showing
which frames program registers might point at.  The next step is to
use this information to determine which stack frames each memory
access might refer to, and hence to determine when the accesses might
alias.

This phase of the analysis builds two main data structures: the
aliasing table itself, which is a set of pairs of \state{Load} and
\state{Store} operations such that the \state{Store} might provide
some information which is loaded by the \state{Load}, and a points-to
table, which specifies where each {\StateMachine} temporary can point,
expressed as a set of stack frames and a flag indicating whether it
can point at non-stack memory.  These two structures are defined in
terms of each other and the analysis is structured as a simple
fixed-point iteration between them: the points-to table is used to
refine an initial, conservative\editorial{Why?  Should be able to use
  aggressive tables safely, and the results would probably be
  better.}, aliasing table, which is then used to refine the points-to
table, which is used to further refine the aliasing table, and so on,
until both tables have fully converged.  The resulting tables
accurately capture the possible memory-accessing behaviours of the
{\StateMachine}.

Defining the initial contents of the two tables is straightforwards.
The points-to table contains an entry for every {\StateMachine}
variable indicating that that variable might point at anything.  The
aliasing table contains an entry for every \state{Load} listing every
\state{Store} in the {\StateMachine}, subject to three constraints:

\begin{itemize}
\item \state{Store}s are not included in the set if simple arithmetic
  considerations show that they cannot alias with the \state{Load}.
  For instance, a \state{Store} to $RSP+8$ will not be included in the
  set for a \state{Load} of $RSP+32$.  \todo{I could maybe give
    details of the arithmetic rules used here, but it'd be long,
    obvious, and boring.}
\item \state{Store}s are only included in the set if there is some
  path through the {\StateMachine} which passes through the
  \state{Store} before reaching the \state{Load}.  This gives the
  analysis a modest level of control-flow sensitivity.
\item If the dynamic analysis recorded any information about the
  \state{Load} then the set of \state{Store}s is constrained to be
  consistent with that information.  The static analysis is not
  usually able to meaningfully improve on the information collected by
  the dynamic analysis, but incorporating it here simplifies the
  implementation.
\end{itemize}

These initial tables are clearly safe, in the sense that any data flow
pattern which the {\StateMachine} can actually achieve is also allowed
by the tables, but they are more conservative than is necessary.
{\Technique} therefore refines them slightly before using them.  This
has two phases: refining the alias table using the points-to table,
and refining the points-to table using the alias table.  The two
phases alternate until the tables converge.

Consider the alias refinement phase first.  This phase considers each
entry in the alias table, and hence each aliasing pair of \state{Load}
and \state{Store} operations, in turn.  For each pair, it calculates a
points-to set for each address; if they do not overlap, the pair can
be removed from the table.  Calculating the points-to set for an
address, $\mathit{pts}(\mathit{addr})$, is straightforward:

\begin{itemize}
\item
  The first step is to compare the address expression to the stack
  layout at this state.  If the address can be arithmetically shown to
  point to a particular stack frame then the points-to set is simply
  that frame.  If the address cannot be shown to point to a particular
  frame, but does mention the stack pointer, it is treated as being
  able to possibly point at any frame.  Otherwise, if it does not
  explicitly mention the stack pointer and cannot be shown to point at
  a particular frame, the analysis proceeds by cases.  \todo{That
    could maybe do with more detail.}
\item
  For a {\StateMachine} temporary $t$, the result is simply the
  points-to table's entry for $t$.
\item
  For an operator $x \oplus y$, the result is the union of the
  points-to sets for $x$ and $y$.  It might be possible to refine this
  rule somewhat for particular operators $\oplus$, but I have not
  needed to do so yet.  The rules for non-binary operators are
  analogous.
\item
  The rules for an initial memory expression $\smLoad{\mathit{addr}}$
  are more complicated.  The algorithm first derives the points-to set
  for $\mathit{addr}$, to find which memory locations the $\smLoad{}$
  might access, and then compares these to the memory layout implied
  by the \state{StackLayout} annotations to find what pointers those
  locations might contain.  The resulting points-to set are trimmed to
  remove any frames which are not live at both the \state{Load} and
  \state{Store} states.  The final result is the union of the
  remaining sets.  \todo{Formalise a little, maybe?}
\end{itemize}

There is a slight complication here, which is that the address fields
of \state{Store} and \state{Load} states are not simple expression,
but are instead BDDs with expressions at their leaves.
{\Implementation} therefore calculates points-to sets for each leaf of
the two BDDs, to form new BDDs over points-to sets, intersects the two
new BDDs, and then takes the union of the leaves of the result.
\todo{Should maybe mention the implications of that?  They're not very
  interesting, but it's an obvious omission at the moment.
  Alternatively: could move this to a footnote so as to de-emphasise
  it a bit.}

Refining the points-to table is also quite simple.  The refinement
process considers each entry in the variable in the {\StateMachine} in
turn and attempts to refine its points-to set using the current
points-to and alias tables.  The refinement of a variable depends on
the type of state which defined that variable\footnote{Recall that
  {\StateMachines} are in static single assignment form, and so there
  will always be precisely one such state.}:

\begin{itemize}
\item
  \state{Copy} $\mathit{expr} {\rightarrow} \mathit{tmp}$, which
  evaluates $\mathit{expr}$ and sets the {\StateMachine} variable
  $\mathit{tmp}$ to that value.  The new points-to set for
  $\mathit{tmp}$ will be $\mathit{old} \wedge
  \mathit{pts}(\mathit{expr})$, where $\mathit{old}$ is the current
  points-to set for $\mathit{tmp}$, $\mathit{pts}$ calculates the
  points-to set for $\mathit{expr}$ using the current points-to table,
  and $\wedge$ is the intersection of points-to sets.
\item
  \state{$\Phi$} $\{\mathit{tmp}_i : \mathit{expr}_i\} {\rightarrow}
  \mathit{tmp}$, which sets $\mathit{tmp}$ to one of the
  $\mathit{expr}_i$ depending on which of the $tmp_i$ has been most
  recently assigned to.  The new points-to set for $\mathit{tmp}$ will
  be $\mathit{old} \wedge (\bigvee \mathit{pts}(\mathit{expr}_i))$, where
  $\vee$ is the union of points-to sets.
\item
  \state{ImportRegister} $\mathit{tid}, \mathit{reg}, \mathit{pts}
  \rightarrow \mathit{tmp}$, which sets $\mathit{tmp}$ to the value of
  register $\mathit{reg}$ in thread $\mathit{tid}$ and notes that it
  has points-to set $\mathit{pts}$.  This case is trivial: the new
  points-to set for $\mathit{tmp}$ is simply $\mathit{pts}$.
\item
  \state{Load} $\ast\mathit{addr} \rightarrow \mathit{tmp}$, which
  evaluates $\mathit{addr}$ to obtain an address and copies the
  contents of memory at that address to $\mathit{tmp}$.  For this
  case, the analysis examines the aliasing table to find all
  \state{Store}s $S$ which might alias with the load and sets the new
  points-to set of $\mathit{tmp}$ to $old \wedge (\bigvee_{s \in
    S}(\mathit{pts}(s_{\mathit{data}})) \vee \mathit{pts}(\smLoad{\mathit{addr}}))$,
  where $s_{\mathrm{data}}$ is the data field of the \state{Store}
  $s$.
\end{itemize}



These two refinement phases are iterated until both the points-to and
aliasing tables completely converge.  The resulting aliasing table can
then be used to constrain the set of \state{Store}s which each
\state{Load} might interact with, and hence to produce simpler
load-replacing expression BDDs during the \state{Load} elimination
simplification.

\todo{The aliasing table can also be used to eliminate some
  \state{Store}s, but that's kind of an awkward thing to work in here,
  and is also kind of obvious.}

\smh{Ok, some nice text and I definitely feel that I have a better
  understanding of what you've done.  But: (i) still need to define
  some appropriate concepts and terms for them.  Will make text
  easier.  (ii) Need overall ``flowchart'' or similar diagram to
  explain things at a higher level of abstraction.  After this can go
  into details, coming up for air from time to time.  (iii) Use
  running examples to illustrate.}
