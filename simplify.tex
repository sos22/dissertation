\todo{I'm pretty much convinced now that I want to kill off the static
  analysis bit of aliasing analysis, because the complexity is quite
  high for a very marginal win.}

The {\StateMachines} generated by the algorithms in
Section~\ref{sect:derive:derive} are faithful representations of all
of the instructions which might have been executed by the program in
the \backref{analysis window}.  As such, they usually contain a large
amount of redundant information which is not relevant to the behaviour
being investigated.  {\Technique} therefore uses a number of related
techniques to simplify them and to eliminate irrelevant fragments.
The important ones are:

\begin{itemize}
\item
  Dead code elimination, to eliminate redundant updates to registers.
  This is particularly effective for eliminating updates to the
  \verb|rflags| register in the AMD64 architecture. This is
  essentially identical to the compiler-level optimisation of the same
  name\needCite{}, and so is not discussed in detail here.
\item
  Register copy propagation, which combines smaller updates to
  registers into a single higher-level operation which is usually
  easier to analyse.  To see why this is necessary, consider
  a fragment of code like this:

\begin{verbatim}
shl (%rax << $4) -> %rax
sub (%rax - $7) -> %rax
mul (%rax * $11) -> %rax
\end{verbatim}
  
  This will produce a three-state {\StateMachine} fragment, with one
  state for each instruction.  It would be more useful to produce a
  single state which set $rax$ to $((rax \times 16) - 7) \times 11$,
  and this simplification performs that transformation.  The algorithm
  used is a straightforward adaptation of that employed by
  dcc\needCite{}, and so is not discussed in detail here.

  One minor extension present in {\implementation} but not dcc is that
  {\implementation} can make use of \state{Assert} side-effects during
  this transformation, so that, for instance, if $x$ is asserted to be
  less than $7$ then the expression $x > 22$ can be rewritten to
  $\mathit{false}$.  This does not require any significant changes to
  the bulk of the algorithm, beyond a few simple rules describing when
  such rewrites are valid.
\item
  {\Technique} attempts to eliminate $\Phi$ expressions from generated
  {\StateMachines} by converting them into multi-terminal binary
  decision diagrams over the {\StateMachine}'s control flow predicates;
  this analysis is described in Section~\ref{sect:phi_elimination}.
\item
  {\Technique} attempts to eliminate memory accessing instructions
  using a cross-function alias technique which incorporates a fast but
  low-accuracy points-to analysis on the original binary with a slower
  but more accurate alias analysis applied to the {\StateMachine};
  this is described in Section~\ref{sect:alias_analysis}.
\end{itemize}

{\Implementation} also makes use of a number of simple arithmetic
simplifications, such as rewriting $x + 0$ to just $x$ or
$(\happensBefore{x}{y}) \vee (\happensBefore{y}{x})$ to
$\mathit{true}$, and constant-folding for most common operators.

\section{$\Phi$ elimination}
\label{sect:phi_elimination}

\todo{This whole section needs rewriting.}

The use of SSA form simplifies many parts of the analysis by breaking
up variables into their different live ranges, but complicates some
parts by introducing $\Phi$ side effects, which are themselves quite
difficult to analyse.  The reason that $\Phi$ side effects are hard to
analyse is that their behaviour depends on the {\StateMachine}'s
control flow, and this is not explicitly reified and so is unavailable
to the usual simplification machinery.  The $\Phi$ elimination pass
attempts to rectify this problem by converting the $\Phi$ effects into
expression BDDs (see Section~\ref{sect:state_machine_expr_language})
which examine the {\StateMachine}'s state and use it to select the
correct input to the $\Phi$.  In other words, the BDDs encode the
relevant parts of the {\StateMachine}'s control flow, and hence allow
the other simplification passes to use and manipulate them.

The approach used is simple: build a mapping from {\StateMachine}
states to boolean BDDs\footnote{For clarity, I refer to BDDs which
  evaluate to either true or false as boolean BDDs or just BBDDs, by
  contrast with MTBDDs which can evaluate to a more complex value set.
  Most existing literature refers to BBDDs as just BDDs.}  which are
true for precisely those executions in which the state will be
executed, extend this into a mapping from states to MTBDDs which show
which input to the $\Phi$ would be selected if the $\Phi$ were issued
immediately after that state, and then simply read off the MTBDD for
the actual $\Phi$ state.\editorial{Should be a mention of CDGs in
  there, so that the next section makes sense.}

\smh{Some duplication here.  Can you clarify/simplify?  (Maybe also
  give an example?)}

\subsection{Building the control dependence graph}

\label{sect:cdg}

The control dependence graph (CDG) is a standard compiler data
structure\needCite{} showing which nodes in a control flow graph are
control-dependent on which branch operations.  In other words, it
shows, for each statement, which control-flow conditions must be true
for that statement to execute, which must be false, and which do not
matter.  The control dependence graph used by {\technique} is a slight
extension of this concept: rather than simply listing the expressions
on which the node depends, it gives a BBDD which will evaluate to true
in precisely those executions in which the node executes.  This is
possible because {\technique}'s {\StateMachines} are finite and
acyclic.

The algorithm for building the graph is simple:

\begin{algorithmic}[1]
\State $\mathit{cdg}[\mathit{root}] = \mathit{const}(\mathit{true})$\smh{Presumably, you meant to italicise cdg itself to something?}
\While {Some state is unlabelled in $\mathit{cdg}$}
  \State {$n \gets $ select a state which has no label but whose predecessors are all labelled}
  \State {$\mathit{cdg}[n] \gets \bigvee \{\mathit{cdg}[p, n] \mid p \in \textrm{$n$'s predecessors}\}$}\Comment{Take union of all reaching paths}\smh{Maybe use a different font for comments?}
\EndWhile
\end{algorithmic}

Here, $\mathit{cdg}[x]$ is the condition for state $x$ to run.
$\mathit{cdg}[x, y]$ is the condition necessary for the edge from
state $x$ to $y$ to be traversed, defined by:

\begin{displaymath}
\mathit{cdg}[x, y] = \begin{cases}
  \bot                                     & \text{if there is no edge from $x$ to $y$} \\
  \mathit{cdg}[x] \wedge x.condition       & \text{if $x$ is an \state{If} state and $y = x.\mathit{trueTarget}$} \\
  \mathit{cdg}[x] \wedge {\neg}x.condition & \text{if $x$ is an \state{If} state and $y = x.\mathit{falseTarget}$} \\
  \mathit{cdg}[x]                          & \text{otherwise}
\end{cases}
\end{displaymath}

Note that the selection on line 3 is only guaranteed to be possible
because the {\StateMachine} is acyclic\footnote{This assumption is
  also the reason that {\technique}'s CDG is precise, whereas CDGs for
  full programs usually contain some approximations}.  Given that, the
algorithm is simple: to find the conditions under which a state might
execute, find the conditions under which all of its incoming edges
might execute and take the union.  Once all states are labelled the
CDG is complete.

\subsection{Building the $\Phi$ map}

Once the CDG is complete, the next step is to build the $\Phi$ map.
This is a map from states of the {\StateMachine} to MTBDDs which
select which input of the $\Phi$ would be selected if the $\Phi$ were
to be issued immediately after that state.  The complete map then
makes eliminating the $\Phi$ side-effect trivial.  The algorithm for
building the $\Phi$ map is shown in
Figure~\ref{fig:derive:phi:phi_map}.

\begin{figure}
\begin{algorithmic}[1]
\For {$i$ in inputs to the $\Phi$}\Comment {Build the initial map}
  \State {$s \gets $ state defining input $i$}
  \State {$pm[s] \gets const(i)$}
\EndFor
\While {the $\Phi$ state is unlabelled}
  \State {$n \gets $ select a state which has no label but whose predecessors are all labelled}
  \State {$g \gets \{ (\textit{cdg}[p, n], \textit{pm}[p]) \mid p \in \textrm{$n$'s predecessors}\}$}
  \State {$\textit{pm}[n] \gets \textsc{simplify}(\textsc{flatten}(g), cdg[n])$}
\EndWhile
\end{algorithmic}
\caption{Building the $\Phi$ map}
\label{fig:derive:phi:phi_map}
\end{figure}

\smh{Ok -- I found this \emph{very} confusing and I think you can
  explain it more easily.  GMTBDD = trad abstraction; you essentially
  just want to use the CDG predicate to project the appropriate value
  from $\Phi$.}

Line 7, the definition of $g$, is the core of the algorithm.  $g$ is a
set of pairs of a control predicate and an MTBDD.  Each pair
represents one way of reaching the current state, where the predicate
says when the {\StateMachine} will follow that path and the MTBDD says
what the $\Phi$ would evaluate to if the {\StateMachine} followed that
path.  The \textsc{flatten} function then converts these sets back
into a simple MTBDD in a way which respects variable ordering and
keeps the MTBDD in reduced form.  This is guaranteed to be possible
and unambiguous because the construction of the CDG ensures that, for
every state, precisely one of the control predicates evaluates to
$\mathit{true}$.

\todo{\textsc{Flatten} itself is a fairly straightforward variant of
  the standard BDD zip algorithm, but has lots of nasty special cases,
  so I don't really want to have to describe it.  At the same time, I
  can't find a cite for anyone doing precisely the same thing, so
  perhaps I ought to.}

As a minor optimisation, the algorithm shown simplifies the MTBDD for
state $n$ under the assumption that the CDG condition for $n$ is true;
in other words, under the assumption that $n$ is actually run.  The
\textsc{simplify} function is shown in
Figure~\ref{fig:derive:phi_elimination:simplify}.  The basic idea is
to define a $\textsc{implies}(a, b)$ operation which evaluates to $a$
when $b$ is true or the special $\bot$ value when $b$ is false and to
then use this to zip the original MTBDD with the assumption BDD.  This
lifts the original MTBDD $a$ to a new MTBDD which evaluates to $a(k)$
for any configuration $k$ where the assumption is true or to $\bot$
where the assumption is false.  We then define an \textsc{unlift}
operation which removes all paths through the MTBDD to these $\bot$
values, returning the MTBDD to its original range.  The net effect is
to produce a new MTBDD which evaluates to the same thing as the
original whenever the assumption is true, but potentially tests fewer
input variables while it does so.\smh{Very unclear! example?  Isn't
  this effectively just computing the path condition?}

\todo{This is supposed to be removing the effects of any common
  dominator of both predecessor states.}

As a further minor optimisation, not shown in the figure,
{\implementation} performs an initial reachability test to determine
which states in the {\StateMachine} might eventually reach the $\Phi$
state which is to be eliminated and does not attempt to calculate
labels for any which do not.  The result of the analysis is unchanged.

\begin{figure}
\begin{algorithmic}
\Function{simplify}{$thing:MTBDD(k)$, $assumption:BDD$ $\rightarrow \bot + MTBDD(k)$}
  \State \Return $\textsc{unlift}(\textsc{zip}(thing, assumption, \textsc{implies}))$
\EndFunction
\Function{implies}{$a:k$, $b:bool$ $\rightarrow k + \bot$}
  \If{$b$}
    \State \Return $a$
  \Else
    \State \Return $\bot$
  \EndIf
\EndFunction
\Function{unlift}{$inp:MTBDD(k + \bot) \rightarrow \bot + MTBDD(k)$}
  \If{$inp = const(\bot)$}
    \State \Return $\bot$
  \ElsIf{$inp = const(k)$}
    \State \Return $inp$
  \Else
    \State $if(cond, t, f) \gets inp$
    \State $t' \gets \textsc{unlift}(t)$
    \State $f' \gets \textsc{unlift}(f)$
    \If{$t' = \bot$}
      \State \Return $f'$
    \ElsIf{$f' = \bot$}
      \State \Return $t'$
    \Else
      \State \Return $if(cond, t', f')$
    \EndIf
  \EndIf
\EndFunction
\end{algorithmic}
\caption{The MTBDD $simplify$ algorithm.  The notation $k:t$ indicates
  a variable $k$ of type $t$, $a \rightarrow b$ indicates a function
  from arguments $a$ to a return value of type $t$, $\bot + k$
  indicates type $k$ augmented with the value $\bot$, and $MTBDD(k)$
  is the type of MTBDDs whose terminals are constants of type $k$.
  Not shown: \textsc{unlift} also reduces the MTBDD as it builds it,
  removing any newly-introduced duplicate nodes.}
\label{fig:derive:phi_elimination:simplify}
\end{figure}

\todo{I really don't want to have to define $zip$, because it's fiddly
  and not very interesting, but I can't find a cite for it.}

\todo{Need to think much harder about variable ordering within the
  BDDs.  Currently using pointer order, which is rather silly.  Also
  need to mention that the ordering is consistent within any run of
  the pass, but not necessarily between multiple runs.}\smh{Well, you
  need some order -- or are you hoping to simplify by relabelling?  Is
  this worth it?}

\todo{Might be worth explicitly calling out that the CDG can be shared
  between different $\Phi$ effects, but the $\Phi$ map needs to be
  rebuilt each time?}\smh{Possibly.  As mentioned, this section is
  among the most confusing so far.}

\section{Alias analysis}
\label{sect:alias_analysis}

\todo{Really need to look at some standard compiler alias analyses to
  figure out how novel this actually is.  It'll need some description
  regardless, because it's important and I'm pretty certain nobody
  else has tried it in this context, but the amount and type might
  change a bit.}

Memory-accessing operations are often difficult to analyse because it
is hard to determine when two pointers refer to the same memory
location.  {\Technique} therefore tries to eliminate memory accesses
whenever possible.  The basic approach used is to derive, for each
\state{Load} operation, an expression BDD which evaluates to the value
loaded, allowing the \state{Load} to be replaced with a simple
\state{Copy}.  Eliminating \state{Load}s will then make the
\state{Store} operations redundant, and so they can also be
eliminated.  The result is a {\StateMachine} which is far simpler, in
two senses: it has fewer states, and the states which remain have
simpler semantics.

At a high level, the algorithm used is similar to that used to
eliminate \state{$\Phi$} states (see
section~\ref{sect:phi_elimination}): consider each \state{Load} state
$l$ in turn and, for each one, define functions over states $i$
$before_l(i)$ and $after_l(i)$ which show what the result of issuing
the \state{Load} immediately before or after (respectively) $i$.  This
is only possible if the {\StateMachine} contains every \state{Store}
which might alias with $l$, but, subject to that constraint, it is
always possible to build the complete map.  The state $l$ can then be
replaced by a \state{Copy} from $before_l(l)$.

The functions $before_l$ and $after_l$ are themselves relatively
straightforward:

\begin{itemize}
\item $after_l(i)$ depends on $before_l(i)$ and the type of state $i$.
  For \state{Store} states, it is a BDD which tests whether the store
  aliases with $l$ and returns the \state{Store}'s data if it does and
  $before_l(i)$ otherwise.  For non-\state{Store} states, it is simply
  $before_l(i)$.
\item For non-initial states $s$, $before_l(s)$ depends on
  $after_l(p)$ for every predecessor $p$ of $s$ and on the condition
  associated with the $p$ to $s$ edge.  The final result is an
  expression BDD which is equal to $after_l(p)$ whenever $cdg[p, s]$
  is true, where $cdg$ is the control dependence graph as defined in
  section~\ref{sect:cdg}.  \todo{Building those BDDs is remarkably
    intricate (it took me days to come up with an algorithm which
    worked and was reasonably efficient, and I'm currently on the
    third rev of it), but also kind of boring.  Not sure what to do
    about that.  Could just pretend it's trivial.}  Note that the
  $cdg$ ensures, by construction, that precisely one such condition is
  true in any configuration\editorial{define?} which can reach
  $s$\editorial{That isn't obvious; I believe it mostly because the
    experiments haven't found any cases where it's false.}.  If $s$ is
  unreachable then the value of $before_l(s)$ does not matter, and so
  this definition is sufficiently unambiguous.

  \todo{I'm ignoring sub-word accesses here.  Should probably be more
    explicit about that.}

\item $before_l(r)$, where $r$ is the first state in the
  {\StateMachine}, is simply $\smLoad{addr}$, where $addr$ is the
  address expression from $l$.  In other words, issuing a load at the
  start of the {\StateMachine} always returns the initial value of
  memory.
\end{itemize}

These rules can be solved recursively to find $before_l(i)$ for any
$l$ and $i$, and hence to eliminate any \state{Load} for which every
\state{Store} is available\footnote{Note that this recursion is only
  guaranteed to terminate because {\StateMachines} are acyclic.}.

These definitions leave one important aspect undefined: how to
determine whether a \state{Store} and \state{Load} might alias.  In
some cases, this is trivial: $RSP+8$ and $RSP+72$ are never going to
alias, regardless of the value of $RSP$ or the structure of memory.
Other cases are much more difficult.  {\Technique} has two approaches
for doing so: a dynamic analysis, used to model accesses to the heap
and other global data, and a static one, used to model accesses to the
stack.  These are described in
sections~\ref{sect:derive_manip:dynamic_analysis} and
\ref{sect:derive_manip:static_analysis}, respectively.
Section~\ref{sect:derive_manip:synthesis_aliasing} then describes how
to combine the results of these two analyses and use them to perform
alias resolution.


\smh{Ok, some nice text and I definitely feel that I have a better
  understanding of what you've done.  But: (i) still need to define
  some appropriate concepts and terms for them.  Will make text
  easier.  (ii) Need overall ``flowchart'' or similar diagram to
  explain things at a higher level of abstraction.  After this can go
  into details, coming up for air from time to time.  (iii) Use
  running examples to illustrate.}
