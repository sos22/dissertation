\section{Motivation and overview}
\label{sect:intro:overview}

Commodity hardware is becoming increasingly concurrent, whether due to
more packages per machine, per cores per package, or more threads per
core, and software is adapting to make use of this greater
concurrency.  This promises greater performance, but at the same time
introduces a greater risk of serious concurrency bugs.  Concurrency
bugs are particularly difficult to fix, due to their inherent
non-determinism, and so it would be useful to have some tools to
assist in this.  This dissertation presents automated techniques to
discover, characterise, reproduce, and then fix a certain class of
concurrency bugs.

\begin{figure}
\begin{center}
  \begin{tikzpicture}
    [block/.style = rectangle,draw,fill=blue!20,
      line/.style = draw, -latex']
    \node [block,draw] (dynamic) {Running program};
    \node [block,draw, below = of dynamic] (model) {\Gls{programmodel}};
    \node [block,draw, below = of model] (statemachines) {\STateMachines};
    \node [block,draw, below = of statemachines] (candidates) {\Glspl{verificationcondition}};
    \node [block,draw, below = of candidates] (repro) {Reproduction};
    \node [block,draw, below = of repro] (fix) {{\Genfixes}};
    \path [line,->] (dynamic) to node [right] {Dynamic analysis} (model);
    \path [line,->] (model) to node [right] {Program abstraction} (statemachines);
    \path [line,->] (statemachines) to node [right] {Symbolic execution} (candidates);
    \path [line,->] (candidates) to node [right] {\Glspl{bugenforcer}} (repro);
    \path [line,->] (dynamic.west) to [bend right=45] (repro.west);
    \path [line,->] (repro) to node [right] {{\Genfix} generation} (fix);
  \end{tikzpicture}
\end{center}
\caption{Basic pipeline}
\label{fig:basic_pipeline}
\end{figure}

The basic approach is shown in Figure~\ref{fig:basic_pipeline}.  The
process starts by building a \gls{programmodel}, showing how the
program behaves when it is operating normally, using a combination of
static and dynamic analyses (\autoref{sect:program_model}).  This
\gls{programmodel} is then used to locate potentially relevant
fragments of the program and to build {\StateMachines} which model
their behaviour (\autoref{sect:derive}).  These {\StateMachines} are
then symbolically executed to determine whether they might exhibit the
bug under investigation, and, if so, under what circumstances; the
results are summarised as a set of \glspl{verificationcondition}.
This set usually contains a large number of false positives, and so
the next step is to prune it back down using \glspl{bugenforcer}:
special schedulers which, when applied to the running program, make it
far more likely that the bug will reproduce quickly
(\autoref{sect:reproducing_bugs}).  Any bugs which do reproduce can
then be passed to the final {\genfix} generation phase which binary
patches the program to introduce synchronisation which eliminates the
bug (\autoref{sect:fix_global_lock}).

One important decision which must be made when designing this kind of
tool is how much higher-level semantic information to use, and in
particular whether to operate at the level of machine code or source
code.  Working with machine code gives a tool the most precise
description of the program's behaviour, as concurrency bugs often
depend on the precise details of compiler optimisations, and is
inherently language-independent; on the other hand, source code
provides far more useful information, and so is usually far easier to
analyse.  {\Technique} takes the extreme position of operating on
machine code as far as possible, only relying on access to source code
when absolutely necessary.  For some non-trivial programs, including
Thunderbird and pbzip2, the technique can produce useful results
without any access to source code at all, and for most others very
little information is needed (MySQL, for instance, required two
functions related to memory allocation to be manually annotated).

\section{Contributions}

This dissertation makes several contributions:

\begin{itemize}
\item
  Suggest a novel method of finding concurrency-related bugs given
  only a binary program and some way of running it.
\item
  Describe how the bug description produced when trying to find a bug
  can be used to automatically fix the same bug or to make the bug
  more easily reproducible.
\item
  Evaluate these techniques, showing that they can find and fix bugs
  in simple programs quickly, and that the analysis techniques scale
  up to find real bugs in realistic programs.
\end{itemize}

I give a detailed description of {\technique} and some results
obtained using \implementation, my implementation of it.  These
include details of the fixes generated for a selection of bugs, both
artificial ones and some from real programs (including one which was
unknown to the author before writing the tool), and a demonstration
that the analysis scales to realistically large programs with
acceptable computational cost.  I also show that the fixes generated
often have sufficiently low overhead to be useful; usually a few
percent in more realistic tests.

\section{Type of bug considered}

{\Technique} considers only a subset of concurrency bugs: those where
one thread, referred to as the ``\gls{crashingthread}'', is reading
from shared data structure while another thread, the
``\gls{interferingthread}'', simultaneously modifies it, and these
concurrent updates cause the crashing thread to crash quickly.  In a
little more detail:

\begin{itemize}
\item The threads must be operating on a data structure located
  somewhere in shared memory.  The data structure does not need to be
  in contiguous memory, and does not need to correspond to any
  higher-level concept of a data structure such as a C++
  \texttt{class} or \texttt{struct}, but it does need to be in
  process-accessible memory.  Structures on the filesystem, for
  instance, are not considered.
\item The crashing thread must crash in a detectable way.  The
  simplest case is a hardware-detected fault such as referencing bad
  memory or dividing by zero, but more complex types of fault could
  also be supported, if a suitable detector can be implemented.
  {\Implementation} includes detectors for hardware-detected faults,
  assertion-failure type errors, and some types of double-free error.
\item The crash must be caused by the concurrent updates.  There must
  be some regions of the crashing and interfering threads such that
  running those regions in parallel can crash but running them
  atomically, in either order, will not.
\item The crashing thread must crash quickly.  {\Technique} uses a
  finite \gls{analysiswindow} \gls{alpha} and will only consider
  reordering concurrent operations which occur at most \gls{alpha}
  instructions before the crash.  Bugs which require knowledge of the
  program behaviour beyond that window cannot be analysed.
  Equivalently, {\technique} only considers bugs which can be fixed by
  small critical sections containing fewer than $\alpha$ dynamic
  instructions.  \gls{alpha} can, in principle, be arbitrarily large,
  but computational constraints mean that in practice it will be
  limited to a few dozen to a few hundred instructions, depending on
  the program to be analysed and how much information about the bug is
  available before analysis starts.
\end{itemize}

This clearly does not include every possible type of concurrent bug
(it does not, for instance, include any but the most trivial deadlock
bugs, and complicated memory corruption bugs are difficult to handle),
but it does include some interesting bugs.

\begin{figure}
\begin{centering}
\hfill
\begin{tabular}{p{8cm}l}
Crashing thread:\hfill         & Interfering thread: \\
\\
1: Load $t_0$ from loc1        & 6: Load $t_3$ from loc1 \\
2: Store $t_0$ to loc2         & 7: Store $t_3$ to loc2 \\
\textit{Something complicated} & 8: Store $t_3 + 1$ to loc2 \\
3: Load $t_1$ from loc1        & \\
4: Load $t_2$ from loc2        & \\
5: Crash if $t_1 = t_2$ & \\
\end{tabular}
\hfill
\end{centering}
\caption{Example of threads with mandatory concurrency.}
\label{fig:mandatory_concurrency1}
\end{figure}

\begin{figure}
\begin{centering}
\hfill
\begin{tabular}{p{8cm}l}
Crashing thread:          & Interfering thread: \\
\\
3: Load $t_1$ from loc1   & 6: Load $t_3$ from loc1 \\
4: Load $t_2$ from loc2   & 7: Store $t_3$ to loc2 \\
5: Crash if $t_1 = t_2$   & 8: Store $t_3 + 1$ to loc2
\end{tabular}
\hfill
\end{centering}
\caption{Truncation of the example in Figure~\ref{fig:mandatory_concurrency1}.}
\label{fig:mandatory_concurrency2}
\end{figure}

This definition has one non-obvious property: the ``quality'' of the
results is not monotonically increasing with \gls{alpha}.  Consider,
for instance, the example in Figure~\ref{fig:mandatory_concurrency1}.
The \gls{crashingthread} copies the contents of loc1 to loc2, performs
some complex local calculation, and then asserts that loc1 and loc2
are no longer equal.  Meanwhile, the \gls{interferingthread} sets loc1
and loc2 to $t_3$, then sets loc2 to $t_3 + 1$.  Running the
\gls{crashingthread} atomically is guaranteed to lead to a crash, and
so this bug is not caused by concurrent updates, according to the
definition used by {\technique}, and will not be reported by
{\technique}.  On the other hand, if the threads were truncated as
shown in Figure~\ref{fig:mandatory_concurrency2} a candidate bug would
be reported.  The program will always avoid crashing when the two
threads are run atomically, in either order, but interleaving their
statements might cause a crash, and so this program does have a
concurrency bug which can be detected by {\technique}.  The original
program threads clearly did contain a bug, and truncating them is
equivalent to reducing $\alpha$, and so decreasing $\alpha$ has
removed a false negative from the set of reported bugs.

This is an irritating property for {\technique} to have.  Fortunately,
one could reasonably expect this problem to be rare in real programs.
In general, real programs will only contain bugs which are at least a
little bit difficult to reproduce.  In this case, that means that the
complicated local computation in the \gls{crashingthread} must take
long enough to be almost certain that the \gls{interferingthread} will
intercede, which means that it must, at the very least, be large
relative to scheduling jitter.  At the same time, this problem will
only occur if the local computation is of a similar size to
{\technique}'s \gls{analysiswindow}.  Practical considerations mean
that the \gls{analysiswindow} can rarely exceed a hundred or so
instructions, which is usually very small relative to scheduling
jitter.  As such, it is quite unlikely that any bug could
simultaneously have a large enough complex local calculation to avoid
being fixed quickly by the programmer, but also a large enough one
that it could trigger this behaviour.

\section{Execution model}

In addition to restricting the class of bugs, {\technique} also
somewhat restricts the class of programs and execution environments
which it considers.  In particular, it uses a somewhat restrictive
memory model:

\begin{itemize}
\item It assumes that the processor implements a strongly-ordered
  memory model, so that accesses issued by a single are never
  reordered.  It therefore cannot precisely model the behaviour of any
  processor in widespread use today.  On the other hand, this is a
  reasonable approximation for the behaviour of the widely-used AMD64
  architecture\needCite{}, where memory reordering is a rare and
  highly constrained event.  This limitation is not, therefore,
  completely crippling.

\item {\Technique} also assumes that the program does not make use of
  any unaligned memory accesses.  For many architectures this would be
  a sound assumption, as most processors do not support such accesses.
  Even where they are supported, they are generally very slow, and so
  most programs will avoid them wherever possible.  This is therefore
  not a major limitation.

\item {\Technique} assumes that the only thing which could possibly
  modify memory is the program itself, and system calls invoked by the
  program.  It does not consider, for instance, races which are
  mediated through inter-process shared memory, or the effects of APIs
  like \texttt{ptrace}.

\item Similarly, it uses a very simple model of the program's address
  space in which every address is either fully accessible or
  completely inaccessible.  Read-only or no-execute memory\needCite{},
  for instance, is not considered.

\item The only type of concurrency considered is multi-threading.
  {\Technique} does not, for instance, consider the effects of
  asynchronous signal handlers or other upcall mechanisms.
\end{itemize}

These assumptions are clearly not trivial, and do exclude some
interesting behaviour.  On the other hand, they are not excessively
strong, either, and much interesting concurrency behaviour will still
fall within these parameters.

{\Technique} also assumes that the program has no real-time
constraints, so that it can safely delay program instructions
arbitrarily.  This is necessary for both the fixes and the
\glspl{bugenforcer} generated by {\technique} to make sense.

\section{Background}

\todo{I'm not entirely convinced this section adds all that much.}

\subsection{The happens-before graph}

A happens-before graph\needCite{} is a directed acyclic graph whose
vertices are events in the program execution and which contains an
edge from event A to event B if A is ordered before B by some form of
synchronisation or communication operation.  They are usually used to
describe the ordering of operations in weakly-ordered concurrent
systems, where there is not guaranteed to be a single total ordering
of all events, as they can conveniently express the possibility of two
events being unordered with respect to each other.  They are a natural
way of describing the memory ordering associated with a particular
concurrency bug, and it is in that capacity that {\technique} uses
them.

There is a slight conceptual subtlety here.  As discussed above,
{\technique} assumes that the processor implements a strongly ordered
memory model, so that memory accesses form a total order.  The
happens-before graph for a single execution is therefore a trivial
straight line.  Representing this with a full happens-before graph is
needless complexity.  The advantage of using a happens-before graph is
that it allows certain aspects of the ordering to be discarded, so
that several total orderings can be considered at the same time with
no additional work.  In other words, a {\technique} happens-before
graph represents a family of total orderings, rather than a single
partial ordering.  This makes very little practical difference.


