\section{Motivation and overview}
\label{sect:intro:overview}

Commodity hardware is becoming increasingly concurrent, whether due to
more packages per machine, more cores per package, or more threads per
core, and software is adapting to make use of this greater
concurrency.  This promises potentially greatly increased performance.
Unfortunately, it also promises greatly reduced reliability.
Highly-concurrent software is particularly prone to complex,
unpredictable, and hard-to-reproduce bugs, and so as concurrent
software development techniques become more widespread, especially
amongst less able developers, we should expect to see the frequency of
serious bugs in important software increase.  It would therefore be
useful to have some tools to help
\begin{wrapfigure}{r}{6.9cm}
  \vspace{-5mm}
\centerline{
  \begin{tikzpicture}
    [block/.style = rectangle,draw,fill=blue!20,
      line/.style = draw, -latex']
    \node [block,draw] (dynamic) {Running program};
    \node [block,draw, below = of dynamic] (model) {\Gls{programmodel}};
    \node [block,draw, below = of model] (statemachines) {\STateMachines};
    \node [block,draw, below = of statemachines] (candidates) {\Glspl{verificationcondition}};
    \node [block,draw, below = of candidates] (repro) {Reproduction};
    \node [block,draw, below = of repro] (fix) {{\Genfixes}};
    \path [line,->] (dynamic) to node [right] {Dynamic analysis} (model);
    \path [line,->] (model) to node [right] {Program abstraction} (statemachines);
    \path [line,->] (statemachines) to node [right] {Symbolic execution} (candidates);
    \path [line,->] (candidates) to node [right] {\Glspl{bugenforcer}} (repro);
    \path [line,->] (dynamic.west) to [bend right=45] (repro.west);
    \path [line,->] (repro) to node [right] {{\Genfix} generation} (fix);
  \end{tikzpicture}
}
\caption{Basic pipeline}
\label{fig:basic_pipeline}
\end{wrapfigure}
developers deal with this kind of
error.  This dissertation presents automated techniques to discover,
characterise, reproduce, and then fix a certain class of concurrency
bug.

The basic approach is shown in Figure~\ref{fig:basic_pipeline}.  The
process starts by building a \gls{programmodel}, showing how the
program behaves when it is operating normally, using primarily dynamic
analysis (\autoref{sect:program_model}).  This \gls{programmodel} is
then used to locate potentially relevant fragments of the program and
to build {\StateMachines} which model their behaviour
(\autoref{sect:derive}).  These {\StateMachines} are then symbolically
executed to determine whether they might exhibit the bug under
investigation, and, if so, under what circumstances; the results are
summarised as a set of \glspl{verificationcondition}.  This set
usually contains a large number of false positives, and so the next
step is to prune it back down using \glspl{bugenforcer}: special
schedulers which, when applied to the running program, make it far
more likely that the bug will reproduce quickly
(\autoref{sect:reproducing_bugs}).  Any bugs which do reproduce can
then be passed to the final {\genfix} generation phase which binary
patches the program to introduce synchronisation which eliminates the
bug (\autoref{sect:fix_global_lock}).

One important decision which must be made when designing this kind of
tool is how much higher-level semantic information to use, and in
particular whether to operate at the level of machine code or source
code.  Working with machine code gives a tool the most precise
description of the program's behaviour, as concurrency bugs often
depend on the precise details of compiler optimisations, and is
inherently language-independent; on the other hand, source code
provides far more useful information, and so is usually far easier to
analyse.  {\Technique} takes the extreme position of operating on
machine code as far as possible, only relying on access to source code
when absolutely necessary.  For some non-trivial programs, including
Thunderbird and pbzip2, the technique can produce useful results
without any access to source code at all, and for most others very
little information is needed (MySQL, for instance, required two
functions related to memory allocation to be manually annotated).

\section{Contributions}

This dissertation makes several contributions:

\begin{itemize}
\item
  Suggest a novel method of finding concurrency-related bugs given
  only a binary program and some way of running it.
\item
  Describe how the bug description produced when trying to find a bug
  can be used to automatically fix the same bug or to make the bug
  more easily reproducible.
\item
  Evaluate these techniques, showing that they can find and fix bugs
  in simple programs quickly, and that the analysis techniques scale
  up to find real bugs in realistic programs.
\end{itemize}

I give a detailed description of {\technique} and some results
obtained using \implementation, my implementation of it.  These
include details of the fixes generated for a selection of bugs, both
artificial ones and some from real programs (including one which was
unknown to the author before writing the tool), and a demonstration
that the analysis scales to realistically large programs with
acceptable computational cost.  I also show that the fixes generated
often have sufficiently low overhead to be useful.

\section{Type of bug considered}
\label{sect:types_of_bugs}

{\Technique} considers only a subset of concurrency bugs: those where
one thread, referred to as the ``\gls{crashingthread}'', is reading
from shared data structure while another thread, the
``\gls{interferingthread}'', simultaneously modifies it, and these
concurrent updates cause the crashing thread to crash quickly.  In a
little more detail:

\begin{itemize}
\item The threads must be operating on a data structure located
  somewhere in shared memory.  The data structure does not need to be
  in contiguous memory, and does not need to correspond to any
  higher-level concept of a data structure such as a C++
  \texttt{class} or \texttt{struct}, but it does need to be in
  process-accessible memory.  Structures on the filesystem, for
  instance, are not considered.
\item The crashing thread must crash in a detectable way.  The
  simplest case is a hardware-detected fault such as referencing bad
  memory or dividing by zero, but more complex types of fault could
  also be supported, if a suitable detector can be implemented.
  {\Implementation} includes detectors for hardware-detected faults,
  assertion-failure type errors, and some types of double-free error.
\item The crash must be caused by the concurrent updates.  There must
  be some regions of the crashing and interfering threads such that
  running those regions in parallel can crash but running them
  atomically, in either order, will not.
\item The crashing thread must crash quickly.  {\Technique} uses a
  finite \gls{analysiswindow} \gls{alpha} and will only consider
  reordering concurrent operations which occur at most \gls{alpha}
  instructions before the crash.  Bugs which require knowledge of the
  program behaviour beyond that window cannot be analysed.
  Equivalently, {\technique} only considers bugs which can be fixed by
  small critical sections containing fewer than $\alpha$ dynamic
  instructions.  \gls{alpha} can, in principle, be arbitrarily large,
  but computational constraints mean that in practice it will be
  limited to a few dozen to a few hundred instructions, depending on
  the program to be analysed and how much information about the bug is
  available before analysis starts.
\end{itemize}

This clearly does not include every possible type of concurrent bug
(it does not, for instance, include any but the most trivial deadlock
bugs, and complicated memory corruption bugs are difficult to handle),
but it does include some interesting ones.

\subsection{Order-violation bugs}

\begin{figure}
\begin{centering}
\hfill
\begin{tabular}{p{8cm}l}
Crashing thread:\hfill         & Interfering thread: \\
\\
1: Load $t_0$ from loc1        & 6: Load $t_3$ from loc1 \\
2: Store $t_0$ to loc2         & 7: Store $t_3$ to loc2 \\
\textit{Something complicated} & 8: Store $t_3 + 1$ to loc2 \\
3: Load $t_1$ from loc1        & \\
4: Load $t_2$ from loc2        & \\
5: Crash if $t_1 = t_2$ & \\
\end{tabular}
\hfill
\end{centering}
\caption{An order violation bug.}
\label{fig:mandatory_concurrency1}
\end{figure}

\begin{figure}
\begin{centering}
\hfill
\begin{tabular}{p{8cm}l}
Crashing thread:          & Interfering thread: \\
\\
1: Load $t_0$ from loc1        & 6: Load $t_3$ from loc1 \\
2: Store $t_0+1$ to loc2       & 7: Store $t_3$ to loc2 \\
\textit{Something complicated} & 8: Store $t_3 + 1$ to loc2 \\
3: Load $t_1$ from loc1        & \\
4: Load $t_2$ from loc2        & \\
5: Crash if $t_1 = t_2$ & \\
\end{tabular}
\hfill
\end{centering}
\caption{Partial fix for the bug in Figure~\ref{fig:mandatory_concurrency1}.}
\label{fig:mandatory_concurrency2}
\end{figure}

The class of bugs described above does not include order violation
bugs, and so {\technique} will never report any.  Order violation bugs
in the program can, however, still sometimes affect the results.
Consider, for instance, the threads shown in
Figure~\ref{fig:mandatory_concurrency1}.  These threads have an order
violation bug, in that the thread on the left will crash if it can get
from statement 2 to statement 3 before the thread on the right
executes.  As expected, {\technique} will discover that running the
left-hand thread in isolation always leads to a crash, and so will not
report a bug here.  Suppose now that the ordering violation bug is
fixed as shown in Figure~\ref{fig:mandatory_concurrency2}.
{\Technique} \emph{will} report a potential bug in this program:
running the two threads atomically, in either order, will not crash,
for any starting values of loc1 and loc2, but interleaving them might
(consider, for instance, the order 1, 2, 6, 7, 3, 4, 5).  In other
words, the ordering violation bug has hidden the atomicity violation
one and {\technique} will not find either.

This is an irritating property for {\technique} to have.  Fortunately,
it is unlikely to be a serious issue in real programs.  Most
concurrency bugs in real programs tend to be at least moderately
difficult to reproduce, as otherwise the developers of the software
will fix them quickly, and for this class of bugs that means that the
complicated computation between statements 2 and 3 must take long
enough that the interfering thread is almost certain to intercede.
That means, at a minimum, that it must be large relative to the
system's scheduling jitter.  This will usually be at least tens of
microseconds, and is often several milliseconds, which is usually
sufficient to execute thousands to hundreds of thousands of
instructions.  At the same time, there is only any possibility of an
ordering violation bug hiding an atomicity violation one if both bugs
fit into {\technique}'s \gls{analysiswindow}, which, in practice,
cannot exceed a couple of hundred instructions.  As such, it is quite
unlikely that real programs would be able to trigger this behaviour.

\section{Execution model}

In addition to restricting the class of bugs, {\technique} also
somewhat restricts the class of programs and execution environments
which it considers.  In particular, it assumes that the processor
implements a strongly-ordered memory model in which memory accesses
are never re-ordered.  This is not actually true for any processors in
wide-spread use today, but is a reasonable approximation for the
widely-used AMD64 architecture\needCite{}.  Architectures with a
weaker memory ordering, such as Alpha\needCite{} or ARM\needCite{},
would require more involved processing.

{\Technique} also assumes that the program has no real-time
constraints.  This means that it cannot find any bugs which manifest
by violating one of those constraints.  It also means that the bug
fixing and bug reproducing stages, which rely on being able to
arbitrarily delay instructions, might introduce additional such bugs.
Modelling real-time properties of a program is in general very hard,
even with unrestricted access to a program's source code and
significant manual assistance\needCite{}; attempting to infer the
necessary properties directly from machine code is completely
impractical.  Fortunately, most programs have only very soft real-time
constraints, which can be violated on occasion with only moderate loss
of functionality, and so this is a tolerable limitation.

{\Implementation}, my implementation of {\technique}, makes some
additional simplifying assumptions beyond these:

\begin{itemize}
\item It assumes that the program does not make use of any unaligned
  memory accesses.  For many architectures this would be a sound
  assumption, as most processors do not support such accesses.  Even
  where they are supported, they tend very slow, and so most programs
  will avoid them as far as possible.  This is therefore a reasonable
  approximation.

\item Similarly, it uses a very simple model of the program's address
  space in which every address is either fully accessible or
  completely inaccessible.  Read-only and no-execute memory\needCite{}
  are not considered.

\item The only type of concurrency considered is multi-threading.
  {\Implementation} does not, for instance, consider the effects of
  asynchronous signal handlers or other upcall mechanisms.

\item It assumes that the only thing which could possibly modify
  memory is the program itself and system calls invoked by the
  program.  It does not consider, for instance, races which are
  mediated through inter-process shared memory, or the effects of APIs
  like \texttt{ptrace}.
\end{itemize}

Fixing the first three would be a simple matter of programming, with
no deep conceptual changes needed.  Removing the final assumption
would require {\implementation} to be able to model the behaviour of
several processes at once, which would require significant changes to
the implementation but only modest ones to {\technique} itself.
