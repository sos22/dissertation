\section{Motivation and overview}
\label{sect:intro:overview}

Commodity hardware is becoming increasingly concurrent, whether due to
more packages per machine, per cores per package, or more threads per
core, and software is adapting to make use of this greater
concurrency.  This promises greater performance, but at the same time
introduces a greater risk of serious concurrency bugs.  Concurrency
bugs are particularly difficult to fix, due to their inherent
non-determinism, and so it would be useful to have some tools to
assist in this.  This dissertation presents automated techniques to
discover, characterise, reproduce, and then fix a certain class of
concurrency bugs.

\begin{figure}
\begin{center}
  \begin{tikzpicture}
    [block/.style = rectangle,draw,fill=blue!20,
      line/.style = draw, -latex']
    \node [block,draw] (dynamic) {Running program};
    \node [block,draw, below = of dynamic] (model) {\Gls{programmodel}};
    \node [block,draw, below = of model] (statemachines) {\STateMachines};
    \node [block,draw, below = of statemachines] (candidates) {\Glspl{verificationcondition}};
    \node [block,draw, below = of candidates] (repro) {Reproduction};
    \node [block,draw, below = of repro] (fix) {Fixes};
    \path [line,->] (dynamic) to node [right] {Dynamic analysis} (model);
    \path [line,->] (model) to node [right] {Program abstraction} (statemachines);
    \path [line,->] (statemachines) to node [right] {Symbolic execution} (candidates);
    \path [line,->] (candidates) to node [right] {\Glspl{bugenforcer}} (repro);
    \path [line,->] (dynamic.west) to [bend right=45] (repro.west);
    \path [line,->] (repro) to node [right] {Fix generation} (fix);
  \end{tikzpicture}
\end{center}
\caption{Basic pipeline}
\label{fig:basic_pipeline}
\end{figure}

The basic approach is shown in Figure~\ref{fig:basic_pipeline}.  The
process starts by building a \gls{programmodel}, showing how the
program behaves when it is operating normally, using a combination of
static and dynamic analyses (\autoref{sect:program_model}).  This
\gls{programmodel} is then used to locate potentially relevant fragments
of the program and to build {\StateMachines} which model their
behaviour (\autoref{sect:derive}).  These {\StateMachines} are
then \backref{symbolically executed} to determine whether they might
exhibit the bug under investigation, and, if so, under what
circumstances; the results are summarised as a set
of \glspl{verificationcondition}.  This set usually contains a large
number of false positives, and so the next step is to prune it back
down using \glspl{bugenforcer}: special schedulers which, when applied
to the running program, make it far more likely that the bug will
reproduce quickly (\autoref{sect:reproducing_bugs}).  Any bugs which do
reproduce can then be passed to the final \backref{fix generation}
phase which binary patches the program to introduce synchronisation
which eliminates the bug (\autoref{sect:fix_global_lock}).

One important decision which must be made when designing this kind of
tool is how much higher-level semantic information to use, and in
particular whether to operate at the level of machine code or source
code.  Working with machine code gives a tool the most precise
description of the program's behaviour, as concurrency bugs often
depend on the precise details of compiler optimisations, and is
inherently language-independent; on the other hand, source code
provides far more useful information, and so is usually far easier to
analyse.  {\Technique} takes the extreme position of operating on
machine code as far as possible, only relying on access to source code
when absolutely necessary.  For some non-trivial programs, including
Thunderbird and pbzip2, the technique can produce useful results
without any access to source code at all, and for most others very
little information is needed (MySQL, for instance, required two
functions related to memory allocation to be manually annotated).

\section{Contributions}

This dissertation makes several contributions:

\begin{itemize}
\item
  Suggest a novel method of finding concurrency-related bugs given
  only a binary program and some way of running it.
\item
  Describe how the bug description produced when trying to find a bug
  can be used to automatically fix the same bug or to make the bug
  more easily reproducible.
\item
  Evaluate these techniques, showing that they can find and fix bugs
  in simple programs quickly, and that the analysis techniques scale
  up to find real bugs in realistic programs.
\end{itemize}

I give a detailed description of {\technique} and some results
obtained using \implementation, my implementation of it.  These
include details of the fixes generated for a selection of bugs, both
artificial ones and some from real programs (including one which was
unknown to the author before writing the tool), and a demonstration
that the analysis scales to realistically large programs with
acceptable computational cost.  I also show that the fixes generated
often have sufficiently low overhead to be useful; usually a few
percent in more realistic tests.

\section{Execution model and type of bug considered}

{\Technique} considers only a subset of concurrency bugs: those where
one thread, referred to as the ``crashing thread'', is reading from
shared data structure while another thread, the ``interfering
thread'', simultaneously modifies it, and these concurrent updates
cause the crashing thread to crash quickly.  In a little more detail:

\begin{itemize}
\item The threads must be operating on a data structure located
  somewhere in shared memory.  The data structure does not need to be
  in contiguous memory, and does not need to correspond to any
  higher-level concept of a data structure such as a C++
  \texttt{class} or \texttt{struct}, but it does need to be in
  process-accessible memory.  Structures on the filesystem, for
  instance, are not considered.
\item The crashing thread must crash in a detectable way.  The
  simplest case is a hardware-detected fault such as referencing bad
  memory or dividing by zero, but more complex types of fault could
  also be supported, if a suitable detector can be implemented.
  {\Implementation} includes detectors for hardware-detected faults,
  assertion-failure type errors, and some types of double-free error.
\item The crash must be caused by the concurrent updates.  There must
  be some regions of the crashing and interfering threads such that
  running those regions in parallel can crash but running them
  atomically, in either order, will not.
\item The crashing thread must crash quickly.  {\Technique} uses a
  finite \introduction{analysis window} \introduction{$\alpha$} and
  will only consider reordering concurrent operations which occur at
  most \backref{$\alpha$} instructions before the crash.  Bugs which
  require knowledge of the program behaviour beyond that window cannot
  be analysed.  Equivalently, {\technique} only considers bugs which
  can be fixed by small critical sections containing fewer than
  $\alpha$ dynamic instructions.  \backref{$\alpha$} can, in
  principle, be arbitrarily large, but computational constraints mean
  that in practice it will be limited to a few dozen to a few hundred
  instructions, depending on the program to be analysed and how much
  information about the bug is available before analysis starts.
\end{itemize}

This clearly does not include every possible type of concurrent bug
(it does not, for instance, include any but the most trivial deadlock
bugs, and complicated memory corruption bugs are difficult to handle),
but it does include some interesting bugs.
Section~\ref{sect:future_work:generalising} considers some possible
ways of relaxing some of these restrictions.

In addition to restricting the class of bugs, {\technique} also
somewhat restricts the class of programs and execution environments
which it considers.  In particular, it uses a somewhat restrictive
memory model:

\begin{itemize}
\item It assumes that the processor implements a strongly-ordered
  memory model, so that accesses issued by a single are never
  reordered.  It therefore cannot precisely model the behaviour of any
  processor in widespread use today.  On the other hand, this is a
  reasonable approximation for the behaviour of the widely-used AMD64
  architecture\needCite{}, where memory reordering is a rare and
  highly constrained event.  This limitation is not, therefore,
  completely crippling.

\item {\Technique} also assumes that the program does not make use of
  any unaligned memory accesses.  For many architectures this would be
  a sound assumption, as most processors do not support such accesses.
  Even where they are supported, they are generally very slow, and so
  most programs will avoid them wherever possible.  This is therefore
  not a major limitation.

\item {\Technique} assumes that the only thing which could possibly
  modify memory is the program itself, and system calls invoked by the
  program.  It does not consider, for instance, races which are
  mediated through inter-process shared memory, or the effects of APIs
  like \texttt{ptrace}.

\item Similarly, it uses a very simple model of the program's address
  space in which every address is either fully accessible or
  completely inaccessible.  Read-only or no-execute memory\needCite{},
  for instance, is not considered.

\item The only type of concurrency considered is multi-threading.
  {\Technique} does not, for instance, consider the effects of
  asynchronous signal handlers or other upcall mechanisms.
\end{itemize}

These assumptions are clearly not trivial, and do exclude some
interesting behaviour.  On the other hand, they are not excessively
strong, either, and much interesting concurrency behaviour will still
fall within these parameters.

\section{Background}

\todo{I'm not entirely convinced this section adds all that much.}

\subsection{Program slicing}

Program slicing\cite{Weiser1981} is a family of techniques for
extracting the parts of a program which are relevant to some
particular behaviour.  A slice might, for instance, include all
statements which might be involved in computing the value of some
variable, or all of those which modify some data structure.  The core
technique used is a form of dependency-chasing: some initial set of
program statements are marked as necessary, then everything which
influences one of the necessary statements is marked as necessary, and
the process repeats until the set of necessary statements reaches a
fixed point.  The slice then consists of all of the statements in the
necessary set, combined so that they form a valid program.  There are
many variants of this basic algorithm, using different definitions of
``influence'' and different approaches for recombining the statements,
but the intent in all cases is to form a simplified version of the
program which can be used to more easily investigate some aspect of
its behaviour.

Program slices are conceptually very similar to {\StateMachines},
which also approximate a program so as to allow some aspect to be more
easily explored.  The key difference is that a classical program slice
will be expressed in the same language as the original program,
whereas as a {\StateMachine} translates the program's machine code
into an analysis language.  This makes them much easier to analyse,
but at the expense of making them more difficult to derive.

\subsection{The happens-before graph}

A happens-before graph\needCite{} is a directed acyclic graph whose
vertices are events in the program execution and which contains an
edge from event A to event B if A is ordered before B by some form of
synchronisation or communication operation.  They are usually used to
describe the ordering of operations in weakly-ordered concurrent
systems, where there is not guaranteed to be a single total ordering
of all events, as they can conveniently express the possibility of two
events being unordered with respect to each other.  They are a natural
way of describing the memory ordering associated with a particular
concurrency bug, and it is in that capacity that {\technique} uses
them.

There is a slight conceptual subtlety here.  As discussed above,
{\technique} assumes that the processor implements a strongly ordered
memory model, so that memory accesses form a total order.  The
happens-before graph for a single execution is therefore a trivial
straight line.  Representing this with a full happens-before graph is
needless complexity.  The advantage of using a happens-before graph is
that it allows certain aspects of the ordering to be discarded, so
that several total orderings can be considered at the same time with
no additional work.  In other words, a {\technique} happens-before
graph represents a family of total orderings, rather than a single
partial ordering.  This makes very little practical difference.


