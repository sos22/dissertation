\todo{Need to think about names of things.  In particular, it'd be a
  good idea to have different names for the \emph{technique} and the
  \emph{implementation}; the current text conflates them a bit, and
  giving them different names might make that a bit more clear.  I've
  used \technique{} and \implementation{} in a few places as
  placeholders.}

\todo{The main thing missing here is a well-defined thesis.  Need to come
  up with one pretty damn quickly.}


\section{Motivation and overview}

Commodity hardware is becoming increasingly concurrent, whether due to
more packages per machine, per cores per package, or more threads per
core, and software is adapting to make use of this greater
concurrency.  This promises greater performance, but at the same time
introduces a greater risk of serious concurrency bugs.  Even worse,
concurrency bugs often depend on details of the program which are only
apparent at the machine code level, rather than at source level.
There is therefore a need for tools to help discover and fix
concurrency bugs by directly analysing program binaries.  This
dissertation presents one possible approach to doing so: a set of
techniques which can, first, slice a binary program to find parts
which are relevant to a specific bug; second, analyse those slices to
determine whether the bug might actually happen in real executions;
and, third, automatically generate a fix for the bug.  \smh{Very
strong} The same analysis can also be used to find all possible bugs
of a specific class in the program.  The approach used requires
minimal programmer involvement; in some interesting cases it is
completely automated.

This is a demanding goal.  To make it feasible, {\technique} considers
only a restricted class of concurrency bug: two threads interacting,
with one modifying some shared structures while a second reads it,
causing the reading thread to crash quickly in a recognisable
way.\todo{Need a bit more here.}

\section{Overview of approach}
\label{sect:intro:overview}

\begin{figure}
\begin{center}
  \begin{tikzpicture}
    [block/.style = rectangle,draw,fill=blue!20,
      line/.style = draw, -latex']
    \node [block,draw] (dynamic) {Running program};
    \node [block,draw, below = of dynamic] (model) {\introduction{Program model}};
    \node [block,draw, below = of model] (statemachines) {\introduction{\StateMachines}};
    \node [block,draw, below = of statemachines] (candidates) {\introduction{Candidate bugs}};
    \node [block,draw, below = of candidates] (repro) {Reproduction};
    \node [block,draw, below = of repro] (fix) {Fixes};
    \path [line,->] (dynamic) to node [right] {\introduction{Dynamic analysis}} (model);
    \path [line,->] (model) to node [right] {\introduction{Program abstraction}} (statemachines);
    \path [line,->] (statemachines.west) to node [left] {\introduction{{\StateMachine} simplification}} (statemachines.west);
    \path [line,->] (statemachines) to node [right] {\introduction{Symbolic execution}} (candidates);
    \path [line,->] (candidates) to node [right] {\introduction{Bug enforcers}} (repro);
    \path [line,->] (dynamic.west) to (repro.west);
    \path [line,->] (repro) to node [right] {\introduction{Fix generation}} (fix);
  \end{tikzpicture}
  \caption{Basic pipeline}
\end{center}
\label{fig:basic_pipeline}
\end{figure}

The basic structure of {\technique} is shown in
Figure~\ref{fig:basic_pipeline}.  The process starts by building a
\backref{model} of the program's behaviour using \backref{dynamic
  analysis} (Section~\ref{sect:dynamic_analysis}).  This
\backref{model} is then used to locate potentially relevant fragments
of the program and to build {\StateMachines} which model their
behaviour (Section~\ref{sect:derive}).  {\Technique} then uses a
variety of analysis passes to simplify the {\StateMachines} and to
remove any information which is not relevant to the bug under
investigation (Section~\ref{sect:simplify}).  These simplified
{\StateMachines} are then \backref{symbolically executed} to determine
whether they might exhibit the bug under investigation, and, if so,
under what circumstances; the results are summarised as a set of
candidate bugs (Section~\ref{sect:symbolic_execution}).  This set
usually contains a large number of false positives, and so the next
step is to prune it back down using \backref{bug enforcers}: special
schedulers which, when applied to the running program, make it far
more likely that the bug will reproduce quickly
(Section~\ref{sect:bug_enforcers}).  Any bugs which do reproduce can
then be passed to the final \backref{fix generation} phase which
binary patches the program to introduce synchronisation which
eliminates the bug (Section~\ref{sect:fix_generation}).

\smh{c/f RX?}

I give a detailed description of \technique{} and some results
obtained using \implementation, my implementation of it.  These
include details of the fixes generated for a selection of bugs, both
artificial ones and some from real programs, and a demonstration that
the analysis scales to realistically large programs with acceptable
computational cost.  This includes bugs which were unknown to the
author before writing the tool\editorial{Or, more precisely, one bug
  which was unknown to me before writing the tool but which was known
  to the program's original developers, and which is a trivial
  variation of a bug which I did know about.  But, you know, take your
  victories where you can get them.}.  I also show that the fixes
generated often have sufficiently low overhead to be useful; usually a
few percent in more realistic tests.  Finally, I give the results of a
small set of experiments intended to show that \implementation{} is a
correct implementation of \technique{}.\editorial{Does this really
  belong here?}


\section{Contributions}

\begin{itemize}
\item
  Suggest a novel method of finding concurrency-related bugs given
  only a binary program and some way of running it.
\item
  Describe how the bug description produced when trying to find a bug
  can be used to automatically fix the same bug.
\item
  Propose several techniques for simplifying cross-function and
  cross-thread slices of binary programs.
\item
  Evaluate these techniques, showing that they can find and fix bugs
  in simple programs quickly, and that the analysis techniques
  \todo{(just about)} scale up to find real bugs in realistic
  programs.
\item
  I further give some evidence that my implementation of these
  techniques is itself correct, so that the other parts of the
  evaluation are likely to be correct.
\end{itemize}

\todo{Maybe include explicit comparisons to related work in
  here?}\smh{Maybe in intro, but not right here.}

\section{Background}
\subsection{BDDs}

A binary decision diagram, or BDD, is an efficient graphical
representation of a function from a finite set of boolean input
variables to a single boolean result.  An example is shown in
Figure~\ref{fig:intro:example_bdd}.  The graph can be evaluated from
the root downwards, with each node representing a test of one of the
input variables.  If the variable is true, evaluation follows the
solid edge to the next node, and if it is false evaluation follows the
dashed edge.  Eventually, evaluation reaches one of the two leaf
nodes, $\mathit{true}$ and $\mathit{false}$, which gives the final
result of the function.  \todo{Cite Lee 1995, maybe?}  While not shown
in the example, BDDs can share sub-graphs, often allowing more complex
functions to be represented compactly.  Indeed, the BDDs used by
{\implementation} (and most other systems) always share nodes whenever
it is possible to do so.  This makes the BDD representation of a
function canonical up to the order in which the variables are
tested\editorial{Cite Bryant 1986.}.

\todo{BDDs allow efficient implementation of many important
  operators...}

\begin{figure}
  \begin{tikzpicture}
    \node (x) [BddNode] {$x$};
    \node (y) [BddNode, below right = of x] {$y$};
    \node (z) [BddNode, below left = of y] {$z$};
    \node (true) [BddLeaf, below left = of z] {$\mathit{true}$};
    \node (false) [BddLeaf, below right = of z] {$\mathit{false}$};
    \draw [BddTrue] (x) -- (y);
    \draw [BddFalse] (x) -- (false);
    \draw [BddTrue] (y) -- (true);
    \draw [BddFalse] (y) -- (z);
    \draw [BddTrue] (z) -- (true);
    \draw [BddFalse] (z) -- (false);
  \end{tikzpicture}
  \caption{Example BDD, representing the expression $x \wedge (y \vee z)$}
  \label{fig:intro:example_bdd}
\end{figure}

\subsection{Memory model}

{\Technique} makes several important simplifying assumptions about the
program's memory model:

\begin{itemize}
\item
  {\Technique} assumes that all memory accesses are issued
  immediately, and are immediately visible to all other threads and
  processors.  In other words, it assumes a strongly ordered memory
  model.  This is a reasonable approximation for the AMD64
  architecture targeted by {\implementation}\footnote{The only
    permissible behaviour which is not modelled is that stores can be
    reordered to after loads, and even this is extremely rare.}, but
  would need to be reconsidered if an implementation were desired for
  less weakly ordered systems such as IA64 or the Alpha.
\item
  {\Technique} assumes that the program does not use misaligned memory
  accesses.  These are extremely rare in most programs, due to their
  very poor performance\editorial{Cite, maybe?}, and so this is not a
  major limitation.
\item
  {\Implementation} assumes that if a memory location is initialised
  with a write of size $n$ then any subsequent loads from that
  location will be of size $m \le n$.  In other words, it assumes that
  any load will consume data from at most one store.  \todo{It might
    actually be easier to remove that limitation than to justify it.}
\end{itemize}

\subsection{The happens-before graph}

The happens-before graph for a parallel execution is a partial
ordering over events in the execution, showing which events occur
before which other events\needCite{}.  For {\technique}, these events
are the memory-accessing states of the {\StateMachine}, and it is
assumed that there is some total order on them


Just say what a happens-before graph is, and give the syntax I've used
for querying it.  Discuss full-transitive versus transitively-reduced
graph (probably only needs a sentence).  Mention that we sometimes
have to query happens-before edges for memory accesses which never
actually happened, and explain what that means.

\subsection{Program slicing}

\emph{Very} brief discussion of what program slicing actually is.  I
don't think it's a particularly useful way of thinking about things,
but it's what all the related work does.

\section{Type of bug considered}
\label{sect:intro:types_of_bugs}

While {\StateMachines} are themselves a reasonably general mechanism
for modelling fragments of a program, {\implementation} considers only
a restricted class of possible bugs: those where two threads are
accessing the same structure, with one reading and one writing, and
this concurrency causes the reading thread to crash quickly and
recognisably.  In slightly more detail:

\begin{itemize}
\item The bug must be reproducible with only \emph{two} threads
  interacting.  Bugs which require three or more threads are not
  considered.  There might, of course, be additional threads in the
  program, and this is acceptable provided that those other threads
  are not needed to reproduce the bug.
\item The \emph{structure} is defined quite loosely: it is the (often
  non-contiguous) collection of memory locations which are accessed by
  both threads.  There is no requirement that it match up with any
  language- or programmer- level definition of structure such as
  \verb|struct| in C or a \verb|class| in C++ (and in fact such
  higher-level constructs do not exist in the binaries which
  {\technique} analyses).
\item One thread $R$ must be \emph{reading} from the structure while
  another $W$ is \emph{writing} to it.  Bugs in which both threads are
  simultaneously updating the same structure are not considered\smh{Is
    this a big restriction?  Can we not frame many bugs into this
    shape?}.  {\Technique} also assumes that the two threads
  communicate only via memory.  These two assumptions mean that the
  $R$ thread cannot influence the $W$ thread's behaviour in any way;
  this is referred to as the \introduction{W isolation} property
\item Only \emph{concurrency} bugs are considered: there must be some
  critical sections in the $R$ and $W$ threads such that running those
  sections atomically with respect to each other is guaranteed not to
  crash but interleaving them might.  In a little more detail:

  \begin{itemize}
  \item Running the $R$ thread's critical section atomically before
    the $W$ thread's critical section starts must not result in a
    crash.  This is the \introduction{R atomic} property.
  \item Similarly, running the $R$ thread's critical section
    atomically after the $W$ thread's critical section has completed
    must also not crash.  This is the \introduction{W atomic}
    property.
  \item There must be some interleaving of the two critical sections
    which does lead to a crash.  This is the \introduction{Crash
      possible} property.
  \end{itemize}
\item The reading threads must crash \emph{quickly}.  {\Technique}
  uses a finite \introduction{analysis window} \introduction{$\alpha$}
  and will only consider reordering concurrent operations which occur
  at most $\alpha$ instructions before the crash.  Bugs which require
  knowledge of the program behaviour beyond that window cannot be
  analysed.  Another way of thinking about this is to say that
  {\technique} only considers bugs which can be fixed by small
  critical sections containing fewer than $\alpha$ dynamic
  instructions.  $\alpha$ can, in principle, be arbitrarily large, but
  computational constraints mean that in practice it will be limited
  to a few dozen to a few hundred instructions, depending on the
  program to be analysed and how much information about the bug is
  available before analysis starts.
\item The tool\editorial{What tool?} must be able to \emph{recognise}
  that the program has crashed.  For the purposes of my prototype
  {\implementation}, this means that it must suffer either a fatal
  page fault, or trigger an assertion failure, or free the same block
  of memory twice.  This could be extended to more general classes of
  crash using techniques such as TESLA\needCite{} or invariant
  inference\needCite{}.

  \todo{It occurs to me that it'd actually be fairly easy to add at
    least one test which uses a programmer-provided invariant, which'd
    make this a little bit more convincing.  The invariant language
    would have to be fairly weak, so it wouldn't be particularly
    *useful*, but it'd be rhetorically handy.}

\end{itemize}

\smh{Why is this important (or not)?  i.e. can we reduce other bugs to
  this or extend the technique?}

\todo{I did have a reference saying that some reasonable proportion of
  real concurrency bugs falls into that category (like 30\% or so),
  but (a) I've lost the reference, and (b) having actually played with
  this a bit, I \emph{really} don't believe that any more.}\smh{hmm}

This is clearly only a subset of possible concurrency bugs.
Nevertheless, it does include some interesting bugs in real programs,
and the restrictions imposed significantly reduce the cost of
performing the analysis.  \editorial{I don't actually show that
  restricting to two threads helps reduce cost, and doing so would be
  a pain in the backside.  It's not even clear what it means to say
  that requiring a recognisable crash reduces cost, since the thing
  wouldn't even work if you didn't have that.} \todo{Need more here.}

For clarity, this dissertation will, for the most part, only discuss
this class of bugs.  Section~\ref{sect:future_work:generalising}
considers possible ways of relaxing some of these restrictions.

\todo{Might be worth explaining how to expression double-free bugs in
  this form?}

\smh{Ok -- generally decent intro missing some structure, and still
  missing the high level story in a para -- maybe fix up fig 2.1 with
  annotations?}

