\chapter{Introduction}

\label{sect:intro:overview}

Commodity hardware is becoming increasingly concurrent, whether due to
more packages per machine, more cores per package, or more threads per
core, and this change brings the promise of greatly increased
performance.  Unfortunately, it also promises greatly reduced
reliability.  Highly-concurrent software is infamously prone to
complex, unpredictable, and hard-to-reproduce bugs, and as it becomes
more widespread, especially amongst less able developers, we should
expect to see the frequency of serious bugs in important software
increase.  This dissertation presents automated techniques to help
developers discover,
\begin{wrapfigure}{r}{5.4cm}
  \vspace{-14pt}
  \begin{figgure}
    \centerline{
      \begin{tikzpicture}
        [block/.style = {rectangle,draw,fill=white},
          node distance = 1.2]
        \node [block] (dynamic) {Running program};
        \node [block,below = of dynamic] (model) {\Gls{programmodel}};
        \node [block,below = of model] (statemachines) {\STateMachines};
        \node [block,below = of statemachines] (candidates) {\Glspl{verificationcondition}};
        \node [block,below = of candidates] (repro) {Reproduction};
        \node [block, below = of repro] (fix) {Fixes};
        \draw [->] (dynamic) to node [right] {\shortstack[l]{Dynamic\\analysis}} (model);
        \draw [->] (model) to node [right] {\shortstack[l]{Program\\abstraction}} (statemachines);
        \draw [->] (statemachines) to node [right] {\shortstack[l]{Symbolic\\execution}} (candidates);
        \draw [->] (candidates) to node [right] {\shortstack[l]{Crash\\enforcement}} (repro);
        \draw [->] (dynamic.west)
          -- ++(-.1,0)
          ..controls +(-0.3,0) and +(0, 0.3) .. ++(-0.5,-0.5)
          -- ++(0,-6.29)
          ..controls +(0,-0.3) and +(-0.3,0) .. ++( 0.5,-0.5)
          -- (repro.west);
        \draw [->] (repro) to node [right] {\shortstack[l]{Fix\\generation}} (fix);
      \end{tikzpicture}
    }
    \vspace{-4pt}
    \caption{System overview}
    \label{fig:basic_pipeline}
  \end{figgure}
  \vspace{-24pt}
\end{wrapfigure}
characterise, reproduce, and fix a certain class of concurrency bug.

The basic approach is shown in \autoref{fig:basic_pipeline}.  The
process starts by building an \gls{programmodel}, showing how the
program accesses memory when it is operating normally, using primarily
dynamic analysis (\autoref{sect:program_model}).  The
\gls{programmodel} is then used to locate potentially relevant
fragments of the program and to build {\StateMachines} which model
their behaviour (\autoref{sect:derive}).  These {\StateMachines} can
in turn be symbolically executed to determine whether they might
exhibit some hypothesised bug, and, if so, under what circumstances,
with the results summarised as a set of \glspl{verificationcondition}.
This set usually contains a large number of false positives, and so
the next step is to prune it back down using \glspl{bugenforcer}:
co-programs which, when loaded into a program, adjust its scheduling
so as to make it far more likely that the bug will reproduce quickly
(\autoref{sect:reproducing_bugs}).  Bugs which fail to reproduce when
the program is run under one of these enforcers are discarded and the
remainder passed to a final fix generation phase which generates
binary patches which eliminate the bug
(\autoref{sect:fix_global_lock}).

An important decision which must be made when designing this kind of
tool is how much higher-level semantic information to use, and in
particular whether to operate at the level of machine code or source
code.  Working with machine code gives a tool the most precise
description of the program's behaviour, as\editorial{non sequitur}
concurrency bugs often depend on the precise details of compiler
optimisations; on the other hand, source code provides far more useful
information, and so is usually far easier to analyse.  {\Technique}
takes the extreme position of operating on machine code as far as
possible, only relying on access to source code when absolutely
necessary.  For some non-trivial programs, including Thunderbird and
pbzip2, the technique can produce useful results without any access to
source code at all, and for most others very little information is
needed (MySQL, for instance, required higher-level information for
just two functions).  {\Technique} explores a particularly difficult
version of the program analysis problem, and hence provides a form of
lower bound on what can be achieved in an environment which is
particularly ill-suited to this kind of technique.

\section{Contributions}

This dissertation makes several contributions:
\begin{itemize}
\item
  Suggest a novel method of finding concurrency-related bugs given
  only a binary program and some way of running it.
\item
  Describe how the bug description produced when trying to find a bug
  can be used to automatically fix the bug or to make it more easily
  reproducible.
\item
  Evaluate these techniques, showing that they can find and fix bugs
  in simple programs quickly, and that the analysis techniques scale
  up to find real bugs in realistic one.
\end{itemize}
I give a detailed description of {\technique} and some results
obtained using \implementation, my prototype implementation.  These
include details of the fixes generated for a selection of bugs, both
artificial ones and some from real programs (including two which were
unknown to me before writing the tool), along with a demonstration
that the analysis scales to realistically large programs with
acceptable computational cost.  I also show that the fixes generated
typically have sufficiently low overhead to be useful in practice.

\section{Type of bug considered}
\label{sect:types_of_bugs}

{\Technique} considers only a subset of concurrency bugs: those where
one thread, referred to as the ``\gls{crashingthread}'', is reading
from a shared data structure while another thread, the
``\gls{interferingthread}'', simultaneously modifies it, and these
concurrent updates cause the crashing thread to crash quickly.  In a
little more detail\editorial{Not a sentence}:
\begin{itemize}
\item The threads must be operating on a data structure located
  somewhere in shared memory.  This structure does not need to be in
  contiguous memory, and does not need to correspond to any
  higher-level concept of a data structure such as a C++
  \texttt{class} or \texttt{struct}, but it does need to be in
  process-accessible memory.  Structures on the filesystem, for
  instance, are not considered.
\item The \gls{crashingthread} must crash in a detectable way.  The
  simplest case is a hardware-detected fault such as referencing bad
  memory or dividing by zero, but more complex types of fault could
  also be supported, if a suitable detector can be implemented.
  {\Implementation} includes detectors for hardware-detected faults,
  assertion failures, and some types of double-free error.
\item The crash must be caused by the concurrent updates.  There must
  be some regions of the crashing and interfering threads such that
  running those regions in parallel can crash but running them
  atomically, in either order, will not.
\item The crashing thread must crash ``quickly''.  {\Technique} uses a
  finite \gls{analysiswindow} \gls{alpha} and will only consider
  reordering concurrent operations which occur at most \gls{alpha}
  instructions before the crash.  Bugs which require knowledge of the
  program behaviour beyond that window cannot be analysed.
  \gls{alpha} can, in principle, be arbitrarily large, but
  computational constraints mean that in practice it will be limited
  to a few dozen to a few hundred instructions, depending on the
  program to be analysed and how much information about the bug is
  available before analysis starts.
\end{itemize}
This clearly does not include every possible type of concurrent bug
(it does not, for instance, include any but the most trivial deadlock
bugs, and complicated memory corruption bugs are difficult to handle),
but does include some interesting ones.  \todo{More.  Should possibly
  give a name to the type of bug?}

\subsection{Order-violation bugs}

\begin{sanefig}
{\hfill}
\begin{tabular}{p{8cm}l}
Crashing thread:\hfill         & Interfering thread: \\
\\
1: Load $t_0$ from loc1        & 6: Load $t_3$ from loc1 \\
2: Store $t_0$ to loc2         & 7: Store $t_3$ to loc2 \\
\textit{Complicated local computation} & 8: Store $t_3 + 1$ to loc2 \\
3: Load $t_1$ from loc1        & \\
4: Load $t_2$ from loc2        & \\
5: Crash if $t_1 = t_2$ & \\
\end{tabular}
{\hfill}
\caption{An order violation bug. The complicated local computation
  does not modify loc1 or loc2.}
\label{fig:mandatory_concurrency1}
\end{sanefig}

\begin{sanefig}
\begin{centering}
\hfill
\begin{tabular}{p{8cm}l}
Crashing thread:          & Interfering thread: \\
\\
1: Load $t_0$ from loc1        & 6: Load $t_3$ from loc1 \\
2: Store $t_0+1$ to loc2       & 7: Store $t_3$ to loc2 \\
\textit{Complicated local computation} & 8: Store $t_3 + 1$ to loc2 \\
3: Load $t_1$ from loc1        & \\
4: Load $t_2$ from loc2        & \\
5: Crash if $t_1 = t_2$ & \\
\end{tabular}
\hfill
\end{centering}
\caption{Partial fix for the bug in
  \autoref{fig:mandatory_concurrency1}.}
\label{fig:mandatory_concurrency2}
\end{sanefig}

\noindent
The class of bugs described above does not include order violation
bugs, and so {\technique} will never report any.  Order violation bugs
in the program can, however, still sometimes affect the results.
Consider, for instance, the threads shown in
\autoref{fig:mandatory_concurrency1}.  These threads have an order
violation bug, in that the thread on the left will crash if it gets
from statement 2 to statement 3 before the thread on the right
executes.  Running the left-hand thread in isolation always leads to a
crash and so, within the definition used by {\technique}, this program
does not have a concurrency bug and no bug will be discovered,
reproduced, or fixed.

Suppose now that the ordering violation bug is fixed as shown in
\autoref{fig:mandatory_concurrency2}.  This program is ``more
correct'' than the previous one, in the sense that any instruction
interleaving which causes the new program to crash will also crash the
old one and some which crash the old will not crash the new, but
{\technique} \emph{will} report a potential bug in the new program.
Running the two new threads atomically, in either order, will never
crash, but interleaving them might (consider, for instance, the order
1, 2, 6, 7, 3, 4, 5).  The ordering violation bug effectively hid an
atomicity violation one, preventing {\technique} from finding either.

This non-monotonicity is an undesirable property for {\technique} to
have.  In practice, though, it is unlikely to be a serious problem.
Concurrency bugs in real programs tend to be at least moderately
difficult to reproduce, as easily reproduced bugs are generally fixed
quickly.  For this kind of bug, that implies that the local
computation must take time at least on the same order as operating
system scheduling effects, which usually ranges from a few tens of
microseconds to a few milliseconds.  On a modern process, that is
sufficient time to execute thousands to hundreds of thousands of
instructions, vastly exceeding {\technique}'s \gls{analysiswindow},
and {\technique} is highly unlikely to capture an ordering violation
bug in the same window as an atomicity violation one.  If two bugs are
analysed in different windows then neither can hide the other, and so
there is little risk of an ordering violation disguising an atomicity
violation.

\subsection{Execution model}

In addition to restricting the class of bugs, {\technique} also
restricts the execution environment by assuming a strongly-ordered
memory model, so that memory accesses are seen by all processors in
the order in which they appear in the program.  This is a reasonable
approximation for the widely-used x86 architecture, as that platform
rarely reorders memory accesses issued by a single
processor~\cite[Section 8.2]{Intel2009}.  Architectures with a weaker
memory ordering, such as Alpha~\cite[Section 5.6]{Compaq2002} or
ARM~\cite[Section 5.3.4]{ARM2007}, would require more involved
processing to correctly capture the more complicated concurrency
semantics.

\section{Model of program modification}
\label{sect:intro:theory_of_fixing}

{\Technique} relies on being able to modify a program's behaviour in
order to reproduce and then to fix bugs, and aims to do so soundly, in
the sense that it should never introduce additional bugs.  This is not
entirely well-defined without access to a formal specification of the
program's desired behaviour.  It might be, for instance, that the
program is designed to investigate the possible ways in which a
particular processor can interleave memory accesses, and to report its
results by either exiting normally or crashing with an unhandled page
fault.  There is no general way for an automated tool to distinguish
such a program from one which is intended to always exit normally but
occasionally crashes due to an accidental race condition.  Any fix for
the latter would break the former.

\todo{Needs editing.} This issue is most easily understood by
recasting the problem from changing the behaviour of the program to
changing the behaviour of the system on which it is running.  A
program will usually be designed to work with a broad class of
computers, rather than with one particular physical system, and so if
it exhibits behaviour A on one system and behaviour B on another then
forcing it to exhibit behaviour B on both systems is unlikely to
introduce a bug into a correct program.  This gives a reasonable
definition of what it means to soundly modify a program without a
specification: a modification is safe if it is equivalent to moving
the program from one computer system to another similar one.  For
{\technique}, two systems are similar if they differ only in the time
taken to run instructions, and so modifications which amount to simply
delaying a particular operation are safe.  This makes {\technique}
safe for use on any systems which do not have real-time constraints.

\section{Graph generating grammars}
\label{sect:intro:graph_grammar}

\begin{sanefig}
  {\hfill}
  \tikzstyle{graphNT}+=[text width=1cm,fill=white]
  \begin{tabular}{lcclccrcc}
    \graphNT{$n$} & $\Rightarrow$ & \raisebox{-6mm}{\begin{tikzpicture}
        \node (n) {A};
        \node (nn) [style=graphNT, below=.5 of n] {$3n+1$};
        \draw[->] (n) -- (nn);
      \end{tikzpicture}} & \production{1} & \hspace{1cm} &
    \graphNT{2} & $\Rightarrow$ & \raisebox{-6mm}{
      \begin{tikzpicture}
        \node (2) {C};
        \node (1) [style = graphNT, below = .5 of 2] {1};
        \draw[->] (2) -- (1);
      \end{tikzpicture}
    } & \production{3} \\
    \graphNT{$m$} & $\Rightarrow$ & \raisebox{-6mm}{\begin{tikzpicture}
        \node (m) {B};
        \node (mm) [style=graphNT, below left =.5 of m] {$\frac{m}{2}$};
        \node (mmm) [style=graphNT, below right = .5 of m] {$\frac{m}{2} - 2$};
        \draw[->] (m) -- (mm);
        \draw[->] (m) -- (mmm);
    \end{tikzpicture}} & \production{2} & &
    \graphNT{4} & $\Rightarrow$ & \raisebox{-6mm}{
      \begin{tikzpicture}
        \node (4) {D};
        \node (2) [style=graphNT, below = .5 of 4] {2};
        \draw[->] (4) -- (2);
      \end{tikzpicture}
    } & \production{4} \\
  \end{tabular}
  {\hfill}
  \caption{Productions for the example graph generating grammar.  The
    terminals of this grammar are capital letters and the
    non-terminals are positive integers in boxes. $n$ matches odd
    integers and $m$ matches even integers other than two and four.
    Circled numbers are labels used to refer to the productions in the
    text.}
  \label{fig:intro:graph_grammar}
\end{sanefig}
\begin{sanefig}
  \newcommand{\arrowwidth}{0.03}
  \newcommand{\arrowhead}{0.05}
  \newcommand{\arrowlength}{0.8}
  \newcommand{\arrowdecoration}{}
  \newcommand{\labelledarrow}[1]{
    \hspace{-3.5mm}
    \begin{tikzpicture}
      \draw [\arrowdecoration] (0,-\arrowwidth) -- ++(\arrowlength,0);
      \draw [\arrowdecoration] (0,\arrowwidth) -- ++(\arrowlength,0);
      \draw [\arrowdecoration] (\arrowlength - \arrowhead + 0.01, 0 - \arrowwidth - \arrowhead) -- (\arrowlength + \arrowhead / 3 + \arrowwidth / 3, 0) -- (\arrowlength - \arrowhead + 0.01, \arrowwidth + \arrowhead);
      \node at (\arrowlength / 2,0) [above] {#1};
    \end{tikzpicture}\hspace{-3.5mm}
  }
  \tikzstyle{graphNT}+=[text width=1em, fill=white]
  \centerline{
  \begin{tikzpicture}[baseline=(r.base)]
    \node [style=graphNT] (r) {3};
  \end{tikzpicture}
  \labelledarrow{\production{1}}
  \begin{tikzpicture}[baseline=(r.base)]
    \node (r) {A\!\!};
    \node [left=-8pt of r] (r3) {\graphNT{3}:};
    \node [below = of r, style=graphNT] (s) {10};
    \draw[->] (r) -- (s);
  \end{tikzpicture}
  \labelledarrow{\production{2}}
  \begin{tikzpicture}[baseline=(r.base)]
    \node (r) {A\!\!};
    \node [left=-8pt of r] (r3) {\graphNT{3}:};
    \node [below=.57 of r] (s) {B\!\!};
    \node [left=-8pt of s] (r10) {\graphNT{10}:};
    \node [below=of s, style=graphNT] (t) {5};
    \draw[->] (r) -- (s);
    \draw[->] (s) -- (t);
    \draw[->] (s.east) .. controls +(.2,0) and +(0,-.2) .. ++(.33,.3) -- ++(0,0.55) .. controls +(0,.2) and +(.2,0) .. ++(-.3,.3) -- (r.east);
  \end{tikzpicture}
  \labelledarrow{\production{1}}
  \begin{tikzpicture}[baseline=(r.base)]
    \node (r) {A\!\!};
    \node [left=-8pt of r] (r3) {\graphNT{3}:};
    \node [below=.57 of r] (s) {B\!\!};
    \node [left=-8pt of s] (r10) {\graphNT{10}:};
    \node [below=.57 of s] (t) {A\!\!};
    \node [left=-8pt of t] (r10) {\graphNT{5}:};
    \node [below=of t,style=graphNT] (u) {16};
    \draw[->] (r) -- (s);
    \draw[->] (s) -- (t);
    \draw[->] (t) -- (u);
    \draw[->] (s.east) .. controls +(.2,0) and +(0,-.2) .. ++(.33,.3) -- ++(0,0.55) .. controls +(0,.2) and +(.2,0) .. ++(-.3,.3) -- (r.east);
  \end{tikzpicture}
  \labelledarrow{\production{2}}
  \begin{tikzpicture}[baseline=(r.base)]
    \node (r) {A\!\!};
    \node [left=-8pt of r] (r3) {\graphNT{3}:};
    \node [below=.57 of r] (s) {B\!\!};
    \node [left=-8pt of s] (r10) {\graphNT{10}:};
    \node [below=.57 of s] (t) {A\!\!};
    \node [left=-8pt of t] (r10) {\graphNT{5}:};
    \node [below=.57 of t] (u) {B\!\!};
    \node [left=-8pt of u] (r16) {\graphNT{16}:};
    \path (u.south) ++(-.7,-1) node [style=graphNT] (v) {8};
    \path (u.south) ++(.6,-1) node [style=graphNT] (w) {6};
    \draw[->] (r) -- (s);
    \draw[->] (s) -- (t);
    \draw[->] (t) -- (u);
    \draw[->] (u) -- (v);
    \draw[->] (u) -- (w);
    \draw[->] (s.east) .. controls +(.2,0) and +(0,-.2) .. ++(.33,.3) -- ++(0,0.55) .. controls +(0,.2) and +(.2,0) .. ++(-.3,.3) -- (r.east);
  \end{tikzpicture}
  \labelledarrow{\production{2}}
  \begin{tikzpicture}[baseline=(r.base)]
    \node (r) {A\!\!};
    \node [left=-8pt of r] (r3) {\graphNT{3}:};
    \node [below=.57 of r] (s) {B\!\!};
    \node [left=-8pt of s] (r10) {\graphNT{10}:};
    \node [below=.57 of s] (t) {A\!\!};
    \node [left=-8pt of t] (r10) {\graphNT{5}:};
    \node [below=.57 of t] (u) {B\!\!};
    \node [left=-8pt of u] (r16) {\graphNT{16}:};
    \path (u.south) ++(-.7,-.85) node [inner sep = 1.5 pt] (v) {B};
    \node [left=-7pt of v] (r8) {\graphNT{8}:};
    \path (v.south) ++(-.5,-1) node [style=graphNT] (x) {4};
    \path (v.south) ++(.5,-1) node [style=graphNT] (y) {2};
    \path (u.south) ++(.6,-1) node [style=graphNT] (w) {6};
    \draw[->] (r) -- (s);
    \draw[->] (s) -- (t);
    \draw[->] (t) -- (u);
    \draw[->] (u) -- (v);
    \draw[->] (u) -- (w);
    \draw[->] (v) -- (x);
    \draw[->] (v) -- (y);
    \draw[->] (s.east) .. controls +(.2,0) and +(0,-.2) .. ++(.33,.3) -- ++(0,0.55) .. controls +(0,.2) and +(.2,0) .. ++(-.3,.3) -- (r.east);
  \end{tikzpicture}
  }
  \centerline{
  \labelledarrow{$\cdots$}
  \begin{tikzpicture}[baseline=(r10.base)]
    \node (r) {A\!\!};
    \node [left=-8pt of r] (r3) {\graphNT{3}:};
    \node [below=.57 of r] (s) {B\!\!};
    \node [left=-8pt of s] (r10) {\graphNT{10}:};
    \node [below=.57 of s] (t) {A\!\!};
    \node [left=-8pt of t] (r10) {\graphNT{5}:};
    \node [below=.57 of t] (u) {B\!\!};
    \node [left=-8pt of u] (r16) {\graphNT{16}:};
    \path (u.south) ++(-.7,-.85) node [inner sep = 1.5 pt] (v) {B};
    \node [left=-7pt of v] (r8) {\graphNT{8}:};
    \path (v.south) ++(-.6,-.85) node [inner sep = 1.5pt] (x) {D};
    \node [left=-7pt of x] (r4) {\graphNT{4}:};
    \path (v.south) ++(.7,-.85) node [inner sep = 1.5pt] (y) {C};
    \node [left=-7pt of y] (r2) {\!\!\graphNT{2}:};
    \path (u.south) ++(.9,-.85) node (w) {\!\!\graphNT{6}:\!\!\!\!};
    \path (r2 -| w) node (z) {\!\!\graphNT{1}:\!\!\!\!};
    \node [right=1pt of z] [inner sep = 1.5pt] (z2) {A};
    \node [right=1pt of w] [inner sep = 1.5pt] (z3) {B};
    \draw[->] (r) -- (s);
    \draw[->] (s) -- (t);
    \draw[->] (t) -- (u);
    \draw[->] (u) -- (v);
    \draw[->] (u) -- (z3);
    \draw[->] (v) -- (x);
    \draw[->] (v) -- (y);
    \draw[->] (x) -- (r2);
    \draw[->] (y) -- (z);
    \draw[->] (z3) -- (z2);
    \draw[->] (s.east) .. controls +(.2,0) and +(0,-.2) .. ++(.33,.3) -- ++(0,0.55) .. controls +(0,.2) and +(.2,0) .. ++(-.3,.3) -- (r.east);
    \draw[->] (z2.south) .. controls +(0,-.2) and +(.2,0) .. ++(-.3,-.3) -- ++(-2.15,0) .. controls +(-.2,0) and +(0,-.2) .. ++(-.3,.25) -- (x.south);
    \draw[->] (z3.north) -- ++(0,4.08) .. controls +(0,.2) and +(.2,0) .. ++(-.3,.3) -- (r.east);
  \end{tikzpicture}
  \labelledarrow{}
  \begin{tikzpicture}[baseline=(r10.base)]
    \node (r) {\!\!A\!\!};
    \node [below=.57 of r] (s) {\!\!B\!\!};
    \node [below=.57 of s] (t) {\!\!A\!\!};
    \node [below=.57 of t] (u) {\!\!B\!\!};
    \path (u.south) ++(-.8,-.85) node [inner sep = 1.5 pt] (v) {B};
    \path (v.south) ++(-.55,-.85) node [inner sep = 1.5pt] (x) {D};
    \path (v.south) ++(.55,-.85) node [inner sep = 1.5pt] (y) {C};
    \path (u.south) ++(.8,-.85) node [inner sep = 1.5pt] (z3) {B};
    \path (y -| z3) node [inner sep = 1.5pt] (z2) {A};
    \draw[->] (r) -- (s);
    \draw[->] (s) -- (t);
    \draw[->] (t) -- (u);
    \draw[->] (u) -- (v);
    \draw[->] (u) -- (z3);
    \draw[->] (v) -- (x);
    \draw[->] (v) -- (y);
    \draw[->] (x) -- (y);
    \draw[->] (y) -- (z2);
    \draw[->] (z3) -- (z2);
    \draw[->] (s.east) .. controls +(.2,0) and +(0,-.2) .. ++(.33,.3) -- ++(0,0.55) .. controls +(0,.2) and +(.2,0) .. ++(-.3,.3) -- (r.east);
    \draw[->] (z2.south) .. controls +(0,-.2) and +(.2,0) .. ++(-.3,-.3) -- ++(-1.55,0) .. controls +(-.2,0) and +(0,-.2) .. ++(-.3,.25) -- (x.south);
    \draw[->] (z3.north) -- ++(0,4.08) .. controls +(0,.2) and +(.2,0) .. ++(-.3,.3) -- (r.east);
  \end{tikzpicture}
  }
  \caption[Expansion of a non-terminal using the productions in
    \autoref{fig:intro:graph_grammar}]{Expansion of the non-terminal
    \graphNT{$\mathrm{3}$} using the productions in
    \autoref{fig:intro:graph_grammar}.  Circled numbers above the
    arrows show which production was used at each step.}
  \label{fig:intro:graph_grammar:expansion}
\end{sanefig}

\noindent
Several of the algorithms in this dissertation are described in terms
of graph generating node-replacement grammars, and so I now give a
brief overview of this formalism with reference to the example in
\autoref{fig:intro:graph_grammar}.  This figure shows a simple grammar
which produces directed graphs of (terminal) capital letters starting
from a single (non-terminal) integer.  The grammar works by matching a
pattern (on the left of the $\Rightarrow$) against some non-terminal
in the graph, possibly defining some match variables (in the example,
$n$ and $m$) in the process, generating a new fragment of graph (on
the right of the $\Rightarrow$), and replacing the original
non-terminal with the fragment.  This repeats until there are no more
non-terminals in the graph.  A non-terminal can only be generated at
most once; if a non-terminal is generated multiple times then it is
only added the first time and subsequent instances re-use the first
one.

\autoref{fig:intro:graph_grammar:expansion} shows how to apply this
grammar to the initial non-terminal \graphNT{3}.  The only production
that matches this initial graph is \production{1} with $n = 3$, which
generates a new terminal A with a single successor non-terminal
\graphNT{$3n+1$} = \graphNT{10}.  This non-terminal matches production
\production{2} with $m=10$, producing a terminal B and non-terminals
\graphNT{$\frac{m}{2}$}=\graphNT{5} and \graphNT{$\frac{m}{2}-2$} =
\graphNT{3}.  The \graphNT{3} non-terminal has already been generated
once, and so rather than adding a new node to the graph the grammar
adds an edge back to the previous one.  The grammar continues
expanding non-terminals in this way until none remain, producing the
graph at the bottom left of the figure.  It then forgets which
non-terminal generated each terminal, producing the final graph at the
bottom right of the figure.

\section{Structure of this dissertation}

This dissertation will, over the following chapters, present a
detailed description of how {\technique} works, starting with the
mechanism used to find and characterise bugs (\autoref{sect:derive})
and then moving on to describe how it first reproduces
(\autoref{sect:reproducing_bugs}) and then fixes
(\autoref{sect:fix_global_lock}) those bugs.  With the technique
itself described, I then give some experimental results obtained with
my prototype implementation {\implementation} (\autoref{chapter:eval})
and compare the technique to existing work in this area
(\autoref{chapter:related_work}).  Finally, I conclude and present
some possible avenues for future work (Chapters~\ref{sect:fw}
and~\ref{sect:concl}).
