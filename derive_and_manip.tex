\label{chapter:derive_manip}
\section{Description of \StateMachines}
\label{sect:derive:description}

\todo{There's a pretty reasonable argument that this should be in the
  introduction.}

The core data structure used by {\technique} is the \StateMachine.
These consist of two main components: a
\introduction{summary}\editorial{Need a less generic-sounding name.}
of part of the program, in a simple (non-Turing complete) analysis
language, and an annotated fragment of the program's
\introduction{control flow graph (CFG)}, showing how states in the
\backref{summary} map to instructions in the program.

\backref{Summaries} themselves are acyclic graphs of
\introduction{analysis states}, which can be thought of as roughly
corresponding to possible points in the program's execution.  The
states themselves fall into one of four classes:

\begin{description}
\item[\introduction{Choice states}] These states have two successors
  and decide between them based on a condition, expressed as a
  \backref{BBDD}.  These states are used both to model the underlying
  program's control flow and also, by means of $\happensBeforeEdge$
  expressions, the possible interleavings of interacting threads.
\item[\introduction{Terminal states}] These states have no successors
  at all; if a \backref{summary} reaches one of these states it must
  terminate.  There are three \backref{terminal states}:

  \begin{description}
  \item \state{Crash}, which is used when the bug of interest
    reproduces.
  \item \state{Survive}, which is used when the bug of interest
    definitely does not reproduce.  Note that this does not
    necessarily indicate that the program is correct in any
    higher-level sense; it simply means that particular bug which is
    currently being investigated is not going to be triggered.
  \item \state{Unreached}, which is used to indicate that the rest of
    the analysis framework should ignore this path.  This can be used
    to prevent the analysis from considering certain uninteresting
    instruction interleavings.  It is also used if one of the
    intermediate analysis steps detects an inconsistency in the
    \backref{summary}, such as when reproducing the bug of interest
    requires the program to have suffered a fatal error earlier in its
    execution\editorial{Not quite what I want to say: inconsistency is
      within a path, but that example doesn't make that very clear.}.
  \end{description}
\item[\introduction{Side effect states}] These states have a single
  successor and represent the side-effects of program instructions,
  such as accessing memory or modifying registers.  The initial
  {\StateMachine} generated for a program fragment will usually have
  several \backref{side effect states} for each program instruction,
  especially on a CISC instruction set such as AMD64, but the various
  {\StateMachine} simplification passes will usually be able to
  eliminate most of these such that most program instructions have no
  \backref{side effect states} at all.  \backref{Side effect states}
  can also be used to update the {\StateMachine}'s own temporary
  variables (and in fact in a fully simplified {\StateMachine} the
  majority of \backref{side effect states} will update the
  {\StateMachine} temporaries rather than program registers).
\item[\introduction{Annotation states}] These states do not represent
  a change in the program's state, but instead represent a property
  which is guaranteed to be true if the {\StateMachine} reaches that
  \backref{analysis state}.  Examples of such properties include the
  layout of a particular thread's stack or the aliasing configuration
  of certain registers.  These states are mostly derived from the
  program model, and are used to integrate the results of the initial
  \backref{static} and \backref{dynamic analysis phases} into the
  {\StateMachines}.
\end{description}

The \backref{CFG} component of the {\StateMachine} is used to map
\backref{analysis states} back to instructions in the original
program.  In particular, it shows how loops in the original program
were unrolled, where control-flow edges had to be removed in order to
make the \backref{summary} acyclic, and where the \backref{summary}
crosses function boundaries.  This is necessary to relate properties
of the {\StateMachines}, such as the \backref{verification conditions}
of \backref{candidate bugs}, back to the behaviour of the original
program, which is necessary to generate \backref{bug enforcers} and
\backref{fixes}.  For multi-threaded {\StateMachines}, there will be
one \backref{CFG} for each thread.\editorial{Doesn't really say what
  the CFGs \emph{are}.}

\begin{figure}
\begin{verbatim}
400694: mov    global_ptr,%rax
40069b: test   %rax,%rax
40069e: je     4006ad
4006a0: mov    global_ptr,%rax
4006a7: movl   $0x5,(%rax)
\end{verbatim}
\caption{A fragment of machine code.  The {\StateMachine} for this fragment is shown in Figure~\ref{fig:derive:single_threaded_machine}}
\label{fig:derive:single_threaded_machine_inp}
\end{figure}

\begin{figure}
  \begin{minipage}{35mm}
    \begin{subfloat}
      \begin{tikzpicture}
        \node (cfg6) at (0,2) [CfgInstr] {cfg6:400694};
        \node (cfg5) [CfgInstr, below=of cfg6] {cfg5:40069b};
        \node (cfg4) [CfgInstr, below=of cfg5] {cfg4:40069e};
        \node (cfg3) [CfgInstr, below=of cfg4] {cfg3:4006a0};
        \draw[->] (cfg6) -- (cfg5);
        \draw[->] (cfg5) -- (cfg4);
        \draw[->] (cfg4) -- (cfg3);
      \end{tikzpicture}
      \caption{\backref{CFG} fragment}
    \end{subfloat}
  \end{minipage}
  \begin{subfloat}
    \begin{minipage}{70mm}
      \begin{tikzpicture}
        \node (l1) at (0,2) [stateSideEffect] {l1: \state{Load} tmp1 $\leftarrow$ global\_ptr AT cfg6 };
        \node (l2) [stateIf, below=of l1] {l2: \state{If} (0 == tmp1)};
        \node (l4) [stateSideEffect, below=of l2] {l4: \state{Load} tmp2 $\leftarrow$ global\_ptr AT cfg3 };
        \node (l3) [stateTerminal, right=of l4] {l3: \state{Survive} };
        \node (l5) [stateIf, below=of l4] {l5: \state{If} (BadPtr(tmp2))};
        \node (l6) [stateTerminal, below=of l5] {l6: \state{Crash}};
        \draw[->] (l1) -- (l2);
        \draw[->,ifTrue] (l2) -- (l3);
        \draw[->,ifFalse] (l2) -- (l4);
        \draw[->] (l4) -- (l5);
        \draw[->,ifFalse] (l5) -- (l3);
        \draw[->,ifTrue] (l5) -- (l6);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program \backref{summary}}
  \end{subfloat}
  \caption{{\StateMachine} generated from the machine code in
    Figure~\ref{fig:derive:single_threaded_machine_inp}, assuming that
    the bug to be investigated is a crash at 4006a7.  Dotted lines
    leaving an \state{If} state indicate the false branch; solid ones
    indicate the true branch.}\smh{Captions in italics?}
  \label{fig:derive:single_threaded_machine}
\end{figure}

Figure~\ref{fig:derive:single_threaded_machine} shows an example of a
simple single-threaded \StateMachine\footnote{This is the read-side of
  the simple\_toctou test discussed in more detail in
  \S\ref{sect:eval:art:simple_toctou}.}.  It illustrates a simple
time-of-check, time-of-use race: the program loads from
\verb|global_ptr| twice in quick succession, validating the result of
the first and using the result of the second.  The translation to a
{\StateMachine} is hopefully reasonably clear: the \backref{CFG} on
the left covers all of the relevant instructions and the
\backref{summary} on the right expresses the relevant part of their
behaviour.  It is trivial to read off from these diagrams that the
program might crash if some other thread modifies \verb|global_ptr| in
between the two loads and will otherwise survive.  Notice that
\verb|4006a7|, the instruction which crashes, is not itself
represented in the \backref{CFG}: by the time that instruction
executes, the program is either doomed to crash or has definitely
avoided the bug, and so that instruction is irrelevant to determining
when (and whether) the bug can actually happen, and so it is not
included in the {\StateMachine}.

\begin{figure}[t]
\begin{verbatim}
4008fb: movq   $0x0,global_ptr
\end{verbatim}
\caption{Other side of the racing code shown in Figure~\ref{fig:derive:single_threaded_machine_inp}.}
\label{fig:derive:single_threaded_machine_write_inp}
\end{figure}

\begin{figure}[t]
  \begin{minipage}{50mm}
    \begin{subfloat}
      \begin{tikzpicture}
        \path (0,0mm) -- (3cm, 0cm);
        \node (cfg8) at (0,0) [CfgInstr] {cfg8:4008fb};
      \end{tikzpicture}
      \vspace{-20mm}\caption{\backref{CFG} fragment}
    \end{subfloat}
  \end{minipage}
  \begin{subfloat}
    \begin{minipage}{70mm}
      \begin{tikzpicture}
        \node (l7) [stateSideEffect] {l7: \state{Store} 0 $\rightarrow$ global\_ptr AT cfg8 };
      \end{tikzpicture}
    \end{minipage}
    \caption{Program \backref{summary}}
  \end{subfloat}
  \caption{{\STateMachine} generated from the code in
    Figure~\ref{fig:derive:single_threaded_machine_write_inp}.  In
    this case, the code to be represented has only a single
    instruction, and so the {\StateMachine} is very
    simple.}\todo{Looks a bit silly.}
  \label{fig:derive:single_threaded_machine_write}
\end{figure}

\begin{sidewaysfigure}
  \begin{tikzpicture}
    \node (lA) [stateIf] { \state{If} $\happensBefore{\mai{cfg6}{thread1}}{\mai{cfg8}{thread2}}$ };
    \node (lB) [stateSideEffect, below = of lA] { l1: \state{Load} tmp1 $\leftarrow$ global\_ptr AT cfg6:thread1 };
    \node (lC) [stateSideEffect, below right = of lA] {l7: \state{Store} 0 $\rightarrow$ global\_ptr AT cfg8:thread2 };
    \node (lD) [stateIf, below = of lB] { l2: \state{If} (0 == tmp1) };
    \node (lE) [stateTerminal, below = of lC] { \state{Unreached} };
    \node (lF) [stateIf, below left = of lD] {\state{If} $\happensBefore{\mai{cfg3}{thread1}}{\mai{cfg8}\mai{thread2}}$ };
    \node (lG) [stateTerminal, below right = of lD] {\state{Survive}};
    \node (lH) [stateTerminal, below right = of lF] {\state{Unreached}};
    \node (lI) [stateSideEffect, below = of lF] {l7: \state{Store} 0 $\rightarrow$ global\_ptr AT cfg8:thread2 };
    \node (lJ) [stateSideEffect, below = of lI] {l4: \state{Load} tmp2 $\leftarrow$ global\_ptr AT cfg3:thread1 };
    \node (lK) [stateIf, below = of lJ] { l5: \state{If} $BadPtr(tmp2)$ };
    \node (lL) [stateTerminal, below left = of lK] { \state{Crash} };
    \node (lM) [stateTerminal, below right = of lK] { \state{Survive} };
    \draw[->,ifTrue] (lA) -- (lB);
    \draw[->,ifFalse,draw] (lA) -- (lC);
    \draw[->] (lB) -- (lD);
    \draw[->] (lC) -- (lE);
    \draw[->,ifTrue] (lD) -- (lG);
    \draw[->,ifFalse] (lD) -- (lF);
    \draw[->,ifTrue] (lF) -- (lH);
    \draw[->,ifFalse] (lF) -- (lI);
    \draw[->] (lI) -- (lJ);
    \draw[->] (lJ) -- (lK);
    \draw[->,ifTrue] (lK) -- (lL);
    \draw[->,ifFalse] (lK) -- (lM);
  \end{tikzpicture}
  \caption{\backref{Summary} component of the cross-product of the
    {\StateMachine} shown in
    figures~\ref{fig:derive:single_threaded_machine} and
    \ref{fig:derive:single_threaded_machine_write}.  The \backref{CFG}
    component just contains the fragments from the two input
    {\StateMachines}.}
  \label{fig:derive:cross_thread}
\end{sidewaysfigure}

\begin{figure}
  \begin{tikzpicture}
    \node (lA) [stateSideEffect] {\state{Assert} $0 \not= InitMemory(global\_ptr)$ and $cfg6:thread1 \happensBefore cfg7:thread2$};
    \node (lB) [stateIf, below = of lA] {\state{If} $cfg3:thread1 \happensBefore cfg7:thread2$ };
    \node (lC) [stateTerminal, below left = of lB] {\state{Survive}};
    \node (lD) [stateTerminal, below right = of lB] {\state{Crash}};
    \draw [->] (lA) -- (lB);
    \draw [->,ifTrue] (lB) -- (lC);
    \draw [->,ifFalse] (lB) -- (lD);
  \end{tikzpicture}
  \caption{\STateMachine from figure~\ref{fig:derive:cross_thread}
    after {\StateMachine} simplification.}
  \label{fig:derive:cross_thread_opt}
\end{figure}

\STateMachines become somewhat more interesting when they capture the
results of multiple threads.  Figure~\ref{fig:derive:cross_thread}
shows an example of a \introduction{cross-product \StateMachine},
which captures all of the interleavings of the two input
{\StateMachines} which the analysis needs to consider.  There are a
couple of interesting new features here:

\begin{itemize}
\item Several new states have been created and existing ones
  duplicated.  In particular, some memory accesses have now been
  duplicated to multiple places in the \StateMachine.
\item
  $\happensBeforeEdge$ expressions.  These allow the {\StateMachine}
  to query the program's happens-before graph.
  $\happensBefore{\mai{cfgA}{threadB}}{\mai{cfgC}{threadD}}$ is true
  precisely when memory access $A$ in thread $B$ happens before memory
  access $C$ in thread $D$.  These memory accesses will usually
  correspond to specific instructions in the program, but this is not
  guaranteed.

  \todo{Actual implementation has another layer of indirection here,
    so that CFG nodes don't need to match up precisely with memory
    accesses, which is useful if one instruction makes multiple
    accesses or if we decide to merge multiple program accesses into
    one summary-level access.  I'm optimistic that I'll be able to get
    away without explaining that, though.}
\item
  Paths in which either the read or write \backref{summary} finish
  without the other starting will end in an unreached state, so they
  will not be considered by the later analysis phases.  Although not
  apparent in this simple example, the algorithm used by SLI also uses
  partial-order reduction\needCite{} to further reduce the number of
  interleavings to be considered\editorial{Need to be more precise
    about that.}.
\end{itemize}

The {\StateMachine} shown in Figure~\ref{fig:derive:cross_thread}
correctly captures the interaction of the two input {\StateMachines}
but is more complicated than it needs to be.  {\Technique} therefore
performs a few simplifications on the \StateMachine, detailed later,
before passing the {\StateMachine} to the symbolic execution engine;
the results are shown in Figure~\ref{fig:derive:cross_thread_opt}.

\STateMachines have a couple of other features not shown in this
example:

\begin{itemize}
\item
  \introduction{Control-flow expressions}.  Much as {\StateMachines}
  can query the happens-before graph of a program using
  $\happensBeforeEdge$ expressions, they can also query the control
  flow within a given thread using $\mathrm{Entry}$ and
  $\mathrm{ControlFlow}$ expressions.  An
  $\entryExpr{\mai{threadA}{cfgB}}$ expression is true if thread $A$
  entered its \backref{CFG} fragment at $\mathrm{cfgB}$, while
  $\controlEdge{threadA}{cfgB}{cfgC}$ is true if thread $A$
  transitions from \backref{CFG} node $B$ to node $C$.  Note that the
  value of $\mathrm{Control}$ expression does not depend on where in
  the {\StateMachine} it is evaluated: the control flow within a
  {\StateMachine} is (conceptually) independent of that of the
  original program.
\item
  $\Phi$ side-effects.  These are described later when discussing the
  SSA form used; they have somewhat different semantics from the
  $\Phi$ nodes used in optimising compilers.
\item
  \state{StartAtomic} and \state{EndAtomic} side-effects.  These
  indicate that a given fragment of the {\StateMachine} should execute
  atomically, and hence restrict the cross-product
  {\StateMachine}\smh{This is a nice concept, wonder if it needs a
    better name?}.  They are used both to represent instructions with
  the \verb|LOCK| prefix (which execute atomically) and some
  library-level functions such as
  \verb|pthread_mutex_lock|\editorial{Should probably have a forwards
    ref to discussion of handling library functions.}.
\item
  Initial-memory expressions, written $LD(addr)$.  {\STateMachines}
  can always access the contents of memory as it was when the
  {\StateMachine} started, ignoring any subsequent updates.  It is
  often easier to analyse conditions expressed in terms of these
  expressions than equivalent conditions expressed in terms of
  explicit memory-accessing operations, and the results are usually
  easier to understand.

  Note that the address part of the expression is evaluated against
  the \emph{current} state of the {\StateMachine}, and can therefore
  reference {\StateMachine} variables and so forth, even though the
  load itself is evaluated against a snapshot of the program's initial
  memory.\smh{Hmm -- seems complicated/arbitrary -- why?  Can you
    clarify?}
\end{itemize}

\section{Deriving \StateMachines}
\label{sect:derive:derive}

\todo{This is much bigger than I would like, given that it's not
  actually all that interesting.}

The way in which {\StateMachines} are built depends on how much
information is available before the analysis starts.  The simplest
case is that only the instruction pointer for the instruction which
crashed is available, and so I discuss that case first.  I then go on
to describe how to generalise this algorithm to operate when no
information is available at all, such as when trying to discover a
currently-unknown bug (Section~\ref{sect:derive:unknown_bugs}), and
when more information is available about the program's state at the
time of the crash (Section~\ref{sect:derive:from_core_dump}).

\todo{I'm not convinced that this is a good way of discussing this,
  because those generalisations are pretty obvious and this is
  building them up to be a little more clever than they really are.
  On the other hand, it does make the description of the main
  algorithm a bit clearer.}

\subsection{Building the read thread's \StateMachine}

The input to this phase of the analysis is the raw instruction pointer
for the thread which crashed, at the time of the crash, plus the
program binary and \backref{program model}.  It must use these to
build a {\StateMachine} representing the final \backref{$\alpha$}
instructions executed by the read thread.  The approach used has
several stages:

\begin{itemize}
\item First, determine which instructions need to be included in the
  {\StateMachine}.  This will be a fragment of the program's control
  flow graph which includes every instruction which the crashing
  thread might have executed in the \backref{$\alpha$} instructions
  immediately prior to crashing, and as few other instructions as
  possible.  This is described in more detail in
  Section~\ref{sect:derive:build_static_cfg}.
\item Next, unroll any loops in that control flow graph fragment such
  that all cyclic paths contain at least \backref{$\alpha$}
  instructions.  At that point, the cycles can be safely broken
  without changing the program's behaviour within the
  \backref{analysis window}.  This unrolled, acyclic CFG forms the
  \backref{CFG} component of the {\StateMachine}.  This is described
  in more detail in Section~\ref{sect:derive:handling_loops}.
\item The acyclic \backref{CFG} can then be compiled to produce the
  initial \backref{summary} component of the {\StateMachine}.  Each
  instruction in the \backref{CFG} is translated independently and the
  resulting fragments are then stitched back together to form the
  \backref{summary}.  This is described in
  Section~\ref{sect:derive:compile_cfg}.
\item The \backref{summary} is then compared to the \backref{program
  model}, producing \backref{annotation states} as necessary.
  Detailed discussion of how these annotations are generated is
  deferred to Section~\ref{sect:alias_analysis}, which also describes
  the main consumer of them.
\item Finally, the \backref{summary} is converted to
  \introduction{static single assignment} form (SSA).  The SSA form
  used by {\technique} is described in Section~\ref{sect:ssa}.
\end{itemize}

The resulting {\StateMachine} captures all of the relevant information
from this thread and can be consumed by the rest of the analysis
framework.

\subsection{Building the read thread's static control-flow graph}
\label{sect:derive:build_static_cfg}
The first stage, once a potentially-crashing instruction has been
selected for investigation, is to build a static control-flow graph
containing all of the instructions which might appear in the
\backref{analysis window}.  This is done by starting with a trivial
CFG containing just the crashing instruction and then expanding it
backwards, one instruction at a time, until every needed instruction
has been discovered.

\begin{figure}
\begin{algorithmic}[1]
\State $depth \gets 0$
\State $pendingAtDepth \gets \queue{targetInstrAddress}$
\State $result \gets \map{}$
\While{$depth < \alpha$}
  \State $pendingAtNextDepth \gets \queue{}$
  \While{$\neg{}empty(pendingAtDepth)$}
    \State $currentInstr \gets pop(pendingAtDepth)$
    \If {$result \textrm{ has entry for } currentInstr$}
      \State \textbf{continue}
    \EndIf
    \State $current \gets \text{decode instruction at } currentInstr$
    \State $\mapIndex{result}{currentInstr} \gets current$
    \State $predecessors \gets \text{predecessors of } currentInstr$\smh{How do we compute predecessors?}
    \State Add $predecessors$ to $pendingAtNextDepth$
  \EndWhile
  \State $pendingAtDepth \gets pendingAtNextDepth$
  \State $depth \gets depth + 1$
\EndWhile
\end{algorithmic}
\caption{Building a read-side static control flow graph within a
  single function.  Computing the predecessors of an instruction is
  non-trivial and is discussed in more detail in the text below.}
\label{fig:derive:static_read_cfg_single_function}
\end{figure}

The simple case is that all of the needed instructions are contained
within a single function.  In that case, the algorithm is as shown in
Figure~\ref{fig:derive:static_read_cfg_single_function}.  This simply
implements a depth-limited breadth-first search starting at the
potentially-crashing instruction and exploring backwards through the
program's control flow.  Note that this can result in a CFG with
multiple roots.

There is a slight subtlety on line 13, which determines the
predecessors of a given instruction.  This is not always obvious,
given only a binary program, for three reasons:

\begin{itemize}
\item
  The program might contain indirect branches.  It is difficult to
  determine statically where these might branch to.  A conservative
  approach would be to assume that they might branch anywhere, but
  this leads to unmanageably complex CFGs even for trivial programs.
  At the same time, ignoring them completely means that many important
  program paths will be missed.
\item
  Many instruction sets include variable-length instructions, and so
  there might be several overlapping instructions which all finish at
  the start of the instruction currently being investigated.  In most
  programs, only one of these will ever be executed, and it is
  important to pick the right one.
\item
  It is not always possible to identify which parts of a program are
  instructions and which data, which makes it difficult to determine
  whether a given sequence of bytes which happens to have the right
  format to be a branch instruction should be treated as one.
\end{itemize}

{\Implementation} solves this problem using a combination of
\backref{static} and \backref{dynamic analysis}.  First, the
\backref{dynamic analysis} tracks the targets of all indirect branch
and call instructions.  This makes the first problem trivial (assuming
that the dynamic analysis is complete).  This information also
includes a list of all of the functions in the program which are
called by the operating system or by library functions\footnote{Shared
  libraries, in the usual model, cannot statically assume anything
  about the memory layout of the program which they are to be linked
  against, and so all branches from a shared library into the main
  program will be indirect.}.  Knowing all entry points into the main
program, plus all branches within the main program, is sufficient for
a simple static analysis to enumerate every instruction in the main
program, and this then allows the second and third problems to be
solved as well.\editorial{I want to mention the \backref{program
    model} in there somewhere, but can't quite fit it in.}

\todo{But that doesn't quite work for types of run-time generated code
  other than shared libraries.  Might need to say something about
  that.}\smh{Hmm -- perhaps .. (this would matter e.g. for obfuscated
  code like burnintest)... But maybe could defer to end f the
  section/or chapter or dissertation?}

\subsection{Handling loops in the read thread's CFG}
\label{sect:derive:handling_loops}

There may be loops in the CFGs generated by the algorithm in
Section~\ref{sect:derive:build_static_cfg}, but {\technique} requires
that the {\StateMachines} be finite and acyclic.  These loops must
therefore be eliminated, and they must be eliminated in a way which is
guaranteed to preserve all paths of length \backref{$\alpha$} ending
at the instruction being investigated.  The approach {\technique}
takes is to unroll the loops, duplicating instructions as necessary,
until every path from a root of the CFG to the target instruction is
either free from cycles or of length greater than \backref{$\alpha$}.
The remaining cycles can then be eliminated without changing the
program's behaviour within the \backref{analysis window}.

\begin{figure}
\begin{tikzpicture}
  [node distance=1 and 0.3]
  \begin{scope}
    \node (A) at (0,2) [CfgInstr] {A};
    \node (B) [CfgInstr] [below=of A] {B}; 
    \node (C) [CfgInstr] [below=of B] {C}; 
    \node (D) [CfgInstr] [below=of C] {D}; 
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C.east) to [bend right=90] (B.east) node (edge1) [right] {};
    \begin{pgfonlayer}{bg}
      \node (box1) [fill=black!10,fit=(A) (B) (C) (D) (edge1)] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=4cm]
    \node (A) at (0,2) [CfgInstr] {A};
    \node (B) [CfgInstr] [below=of A] {B}; 
    \node (C) [CfgInstr] [below=of B] {C}; 
    \node (D) [CfgInstr] [below=of C] {D};  
    \node (C') [CfgInstr] [right=of C] {C'};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (B) to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B);
    \begin{pgfonlayer}{bg}
      \node (box2) [fill=black!10,fit=(A) (B) (C) (D) (C')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=8cm]
    \node (A) at (0,2) [CfgInstr] {A};
    \node (B) [CfgInstr] [below=of A] {B};
    \node (B') [CfgInstr] [right=of B] {B'};
    \node (C) [CfgInstr] [below=of B] {C};
    \node (D) [CfgInstr] [below=of C] {D};
    \node (C') [CfgInstr] [right=of C] {C'};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B');
    \begin{pgfonlayer}{bg}
      \node (box3) [fill=black!10,fit=(A) (B) (C) (D) (C') (B')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=12cm]
    \node (A) at (0,2) [CfgInstr] {A};
    \node (B) [CfgInstr] [below=of A] {B};
    \node (B') [CfgInstr] [right=of B] {B'};
    \node (C) [CfgInstr] [below=of B] {C};
    \node (C') [CfgInstr] [right=of C] {C'};
    \node (C'') [CfgInstr] [right=of C'] {C''};
    \node (D) [CfgInstr] [below=of C] {D};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') -- (C');
    \draw[->] (C'') to [bend right=10] (B');
    \draw[->] (B') to [bend right=10] (C'');
    \begin{pgfonlayer}{bg}
      \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
    \end{pgfonlayer}
  \end{scope}
  \draw[->,thick] (box1) -- (box2) node [above,midway] {duplicate C};
  \draw[->,thick] (box2) -- (box3) node [above,midway] {duplicate B};
  \draw[->,thick] (box3) -- (box4) node [above,midway] {duplicate C'};
  \draw[->,thick] (box4) -- +(2.5,0) node [above,midway] {...};
\end{tikzpicture}
\caption{A CFG containing a cycle.}
\label{fig:cyclic_cfg}
\end{figure}

\begin{figure}
\begin{center}
\begin{tikzpicture}
  [node distance=1 and 0.3]
  \node (A) at (0,2) [CfgInstr] {A};
  \node (B) [CfgInstr] [below=of A] {B};
  \node (B') [CfgInstr] [right=of B] {B'};
  \node (C) [CfgInstr] [below=of B] {C};
  \node (C') [CfgInstr] [right=of C] {C'};
  \node (C'') [CfgInstr] [above right=of B'] {C''};
  \node (D) [CfgInstr] [below=of C] {D};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) -- (D);
  \draw[->] (C') -- (B);
  \draw[->] (A) -- (B');
  \draw[->] (B') -- (C');
  \draw[->] (C'') -- (B');
  \begin{pgfonlayer}{bg}
    \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
  \end{pgfonlayer}\smh{Center?}
\end{tikzpicture}
\end{center}
\caption{Fully unrolled version of the CFG in
  Figure~\ref{fig:cyclic_cfg}, preserving all paths of length six or
  fewer instructions.  Note that an additional root has been
  introduced at C''.}\todo{Underful hbox}
\label{fig:unrolled_cyclic_cfg}
\end{figure}

As an example, consider the CFG shown at the left of
Figure~\ref{fig:cyclic_cfg}, which contains a loop between
instructions B and C.  This loop must be removed from the CFG while
maintaining all paths which terminate at D and which contain
\backref{$\alpha$} or fewer instructions.  The algorithm starts by
performing a depth-first traversal backwards through the graph from D
until it finds an edge which closes a cycle.  In this case, that is
the edge from C to B.  SLI will therefore break this edge by
duplicating the instruction at the start of the edge, C, along with
all of its incoming edges (in this case, just the B to C edge).  The C
to B edge can then be redirected to be from C' to B, producing the
next diagram in the sequence.  All paths which were possible in the
old graph will also be possible in the new one, if duplicated nodes
are treated as semantically equivalent, and the loop is now one
instruction further away from the target instruction D.  The process
then repeats, moving the cycle steadily further and further away from
D until all paths ending of length \backref{$\alpha$} ending at D are
acyclic, at which point the cycle can be safely removed from the
graph.

Note that the edge which is modified is the back edge, from C to B,
which points ``away from D'', and not the forwards edge from B to C.
Trying to break the B to C edge would have moved the cycle away from A
rather than away from D, which would not be helpful.

\begin{figure}
\begin{algorithmic}[1]
  \While {graph is not cycle-free}
     \State $edge \gets \textsc{findEdgeToBeBroken}(targetInstr)$
     \If {$edge$ is at least $N_r$ instructions from target instruction}
        \State {Erase $edge$ from graph}
     \Else
        \State {$newNode \gets$ duplicate of $edge.source$}
        \For {$i$ incoming edge of $edge.source$}
           \State {Create a new edge from $i.source$ to $newNode$}
        \EndFor
        \State {Replace $edge$ with an edge from $newNode$ to $edge.destination$}
     \EndIf
  \EndWhile
\end{algorithmic}
\caption{Loop unrolling and cycle breaking algorithm.
  \textsc{findEdgeToBeBroken} simply performs a depth-first search of
  the graph backwards from $targetInstr$ and returns the first edge
  which completes a cycle.}
\label{fig:derive:read:unroll_cycle_break}
\end{figure}

The complete algorithm is shown in
Figure~\ref{fig:derive:read:unroll_cycle_break}.  This algorithm is
guaranteed to preserve all paths of length $N_r$ which end at the
target instruction.  There are only two places in the algorithm which
remove existing edges, so consider each in turn.  The first is the
erasure on line 4.  This can only ever affect edges whose shortest
path to a target is at least $N_r$ instructions long, and so cannot
eliminate any paths to a target of length $N_r$, and is therefore
safe.  The other is the replacement step at line 10, which replaces an
edge from $edge.source$ to $edge.destination$ with one from $newNode$
to $edge.destination$.  This is safe provided that every path to
$newNode$ has a matching path to $edge.source$, which is ensured by
duplicating all of $edge.source$'s incoming edges to $newNode$.  At
the same time, no additional paths will be introduced, because every
path to $newNode$ has a matching path to $edge.source$.

\todo{Is it worth doing a proof of termination as well?}\smh{No,
  although you might comment on the expected running time...}

\subsection{Handling cross-function CFGs.}

\label{sect:derive:cross_function_cfgs}

There is, of course, no guarantee that all of the instructions in the
\backref{analysis window} will be contained within a single function,
and if they are not then {\technique} must be able to generate
appropriate cross-function \backref{CFG}s.  It does so by partially
inlining functions as necessary to restore the problem to the
single-function case.  This means that instructions must be labelled
by both the pointer of the instruction itself and also by its inlining
context, which is simply the stack of functions into which it has been
inlined\editorial{Not terribly clear.}.  The only slight complications
are that the inlining context is not necessarily known when CFG
exploration starts, and it might be necessary to consider the same
instruction in several contexts.

\todo{That's kind of a stupid way of doing this.  Should only need to
  duplicate instructions when the inlining context matters, which is
  pretty much just when we see both the start and end of the
  function.}

There are two important cases to consider: backing into another
function, where the exploration starts in one function and must be
extended backwards into the end of another one, and backing out of
one, where the exploration starts in one function and must be extended
to include that function's callers.  Backing into a function is
simple: the analysis finds the functions which are to be
called\footnote{There might be more than one function if the
  instruction is a dynamic call.  In that case, the \backref{program
    model} is used to find the set of all possibly-called functions
  and they are all treated as possible predecessors.}, finds all of
the return instructions in those functions, and treats those as the
predecessor of the current instruction with an appropriately extended
inlining context.

Backing out of a function is more complex.  In this case, the analysis
must consider all possible callers of the target function and inline
the target function into each.  

\todo{As I write this I realise that the way I've done this is really,
  really stupid.  I should probably fix that before writing any more
  about it.}\smh{An example or two would probably help, too.}

Tail calls do not require any particular special handling here.  If
the exploration reaches the start of a function then all branches to
that instruction will be treated as possible predecessors of it,
regardless of whether they come from call instructions or simple
branch instructions.

\subsection{Compiling the CFG to a \StateMachine}
\label{sect:derive:compile_cfg}

\todo{Should mention that I use libVEX\smh{probably not at this level
    of abstraction.} to convert instructions to a slightly saner
  intermediate format before converting to {\StateMachine} fragments,
  rather than doing it directly.}

The algorithm presented so far can build the \backref{CFG} component
of the {\StateMachine}.  The next step is to convert that
\backref{CFG} into the \backref{summary} component.  The
{\StateMachine} analysis language is powerful enough to make
translating individual instructions in isolation completely
straightforward.  Connecting them together is, however,
slightly more difficult, as the edges in the \backref{CFG} no
longer match up precisely with those in the original program,
so that, for instance, an instruction which would normally
have a single successor might have multiple successors in the
\backref{CFG}, or one which would normally have multiple successors
might only have one.  There are three cases which require
special care:

\begin{itemize}
\item
  Some edges will be erased from the \backref{CFG}, so that the
  program can branch from instruction A to instruction B but the
  \backref{CFG} does not allow that to happen.  These are simply
  converted to branches to the special \state{Unreached} state,
  reflecting the fact that these paths are of no interest to the rest
  of the analysis.

\item
  Some additional edges will have been introduced which do not
  correspond to anything in the original program.  In the example in
  Figure~\ref{fig:unrolled_cyclic_cfg}, instruction A had a single
  successor, B, in the original program, but has multiple successors
  in the unrolled \backref{CFG}.  {\Technique} handles these by
  converting them into \StateMachine-level control flow using
  $ControlFlow$ expressions, so that the {\StateMachine} fragment for
  A will be something like ``\state{If} ($\controlEdge{threadId}{A}{B}$)
  \{fragment for B\} else \{fragment for B'\}''.

\item
  The \backref{CFG} can sometimes have multiple roots.  In this case,
  the first state of the {\StateMachine} will be a test on special
  $Entry()$ expressions which will select an appropriate fragment of
  {\StateMachine} to start with.  In the example, the first state will
  be something like

  \state{If} $(\entryExpr{\mai{threadId}{A}})$ \\
  \{fragment for A\} \\
  else \\
  \{fragment for C''\}\editorial{Ugly ugly ugly}

  The \backref{CFG} has two roots, A and C'', and this selects an
  appropriate place from which to start the \backref{summary}
  according to where the thread entered the \backref{CFG} (as
  indicated by the Entry expression).\editorial{Gah.}

\end{itemize}

As a somewhat unrealistic example, suppose that the CFG in
Figure~\ref{fig:cyclic_cfg} had been generated from a program
something like this:

\begin{verbatim}
A: MOV rdx -> rcx
B: LOAD *(rcx) -> rcx
C: JMP_IF_NOT_EQ *(rcx + 8), 0, B
D: STORE $0 -> *(rcx)
\end{verbatim}

The \verb|JMP_IF_NOT_EQ| instruction is supposed to indicate that
\verb|C| loads from the memory at \verb|rcx+8|, jumping to \verb|B| if
it is non-zero and proceeding to \verb|D| otherwise.  This will
produce an unrolled CFG as in Figure~\ref{fig:unrolled_cyclic_cfg}, as
already discussed, and a {\StateMachine} as shown in
Figure~\ref{fig:state_machine_for_cyclic_cfg}.

At this stage special side-effects are added to the {\StateMachine} to
represent the results of the earlier whole-program static analysis.
Discussion of these effects is deferred to
section~\ref{sect:alias_analysis} which describes the static analysis
which generates them and the {\StateMachine} simplifications which use
them.

\begin{figure}
\begin{tikzpicture}
  \node[stateIf,initial] (l1) {\state{If} $Entry(A)$};
  \node[stateSideEffect,below left = of l1] (l2) {A: \state{Copy} $\mathit{rdx}$ to $\mathit{rcx}$};
  \node[stateIf,below = of l2] (l3) {\state{If} $\controlEdge{threadId}{A}{B}$};
  \node[stateSideEffect,below = of l3] (l4) {B: \state{Load} $\mathit{rcx}$ to $\mathit{rcx}$};
  \node[stateSideEffect,below = of l4] (l5) {C: \state{Load} $\mathit{rcx}+8$ to $\mathit{tmp}$};
  \node[stateIf,below = of l5] (l6) {\state{If} $\mathit{tmp} = 0$};
  \node[stateIf,below = of l6] (l7) {D: \state{If} $\mathit{BadPtr}(\mathit{rcx})$};
  \node[stateSideEffect,below right = of l3] (l8) {B': \state{Load} $\mathit{rcx}$ to $\mathit{rcx}$};
  \node[stateSideEffect,below = of l8] (l9) {C': \state{Load} $\mathit{rcx}+8$ to $\mathit{tmp}$};
  \node[stateIf,below = of l9] (l10) {\state{If} $\mathit{tmp} = 0$};
  \node[stateSideEffect,below right = of l1] (l11) {C'': \state{Load} $\mathit{rcx}+8$ to $\mathit{tmp}$};
  \node[stateIf,below = of l11] (l12) {\state{If} $\mathit{tmp} = 0$};
  \node[stateTerminal,below left = of l7] (lBeta) {\state{Crash}};
  \node[stateTerminal,below right = of l7] (lGamma) {\state{Survive}};
  \node[stateTerminal,right = of lGamma] (lAlpha) {\state{Unreached}};
  \draw[->,ifTrue] (l1) -- (l2);
  \draw[->,ifFalse] (l1) -- (l11);
  \draw[->] (l2) -- (l3);
  \draw[->,ifFalse] (l3) -- (l8);
  \draw[->,ifTrue] (l3) -- (l4);
  \draw[->] (l4) -- (l5);
  \draw[->] (l5) -- (l6);
  \draw[->,ifFalse] (l6) -- (lAlpha);
  \draw[->,ifTrue] (l6) -- (l7);
  \draw[->,ifFalse] (l7) -- (lGamma);
  \draw[->,ifTrue] (l7) -- (lBeta);
  \draw[->] (l8) -- (l9);
  \draw[->] (l9) -- (l10);
  \draw[->,ifTrue] (l10) -- (lAlpha);
  \draw[->,ifFalse] (l10) -- (l4);
  \draw[->] (l11) -- (l12);
  \draw[->,ifTrue] (l12) -- (lAlpha);
  \draw[->,ifFalse] (l12) -- (l8);
\end{tikzpicture}
\caption{{\STateMachine} generated from the CFG shown in
  Figure~\ref{fig:cyclic_cfg}.}\todo{Check this very carefully.}
\label{fig:state_machine_for_cyclic_cfg}
\end{figure}

\subsection{Conversion to SSA}
\label{sect:ssa}

\todo{Maybe move this to the section on $\Phi$ elimination?  That's
  the only place I use the odd SSA form bits.}

{\STateMachines} are, for the most part, maintained in a variant of
static single assignment (SSA) form.  SSA is a standard compiler
intermediate representation in which each variable has at most one
static assignment\needCite{}\smh{Cytron et al TOPLAS 91?  or some
  textbook from AM's class?}.  Variables which are assigned to
multiple times are converted into families of related variables
(usually referred to as ``versions'' or ``generations'' of the
variable), each of which is assigned to precisely once.  This has the
effect of breaking up the live ranges of long-lived variables, which
can expose other useful optimisations.  Most uses of the original
variable will be converted into references to a specific member of one
of these families; the only case in which this is not possible is
where the correct member to use depends on the program's control flow,
and in that case special $\Phi$ nodes are inserted into the program
which select an appropriate member depending on the immediately
proceeding control flow.  These $\Phi$ nodes are themselves
unrealisable on most hardware, and so the program must be converted
back from SSA form after being optimised and before being lowered to
machine code.

Many of the compiler optimisations for which SSA is helpful are also
relevant to {\technique}, and so {\technique} also converts its
{\StateMachines} (which are analogous to a compiler's intermediate
representation) into SSA form.  The details of the SSA form are,
however, very slightly different to the conventional one: whereas a
compiler-style $\Phi$ node examines the program's preceding control
flow and maps from incoming control-flow edges to input variables, a
{\technique} one examines the order in which variables have been
assigned to and selects whichever was updated most recently (from a
specified set).  This has several important implications:

\begin{itemize}
\item
  Converting this form of SSA back into a non-SSA form can sometimes
  requires additional temporary variables to record which version of a
  particular variable has been most recently assigned to, whereas the
  more conventional control-flow based form does not.  This would be
  an additional complication in a compiler, but is not a problem for
  SLI, which never has to perform that conversion.
\item
  A {\StateMachine}'s control flow graphs can be modified without
  needing to update $\Phi$ nodes.  For example, suppose that a
  {\StateMachine} is as shown on the left of
  Figure~\ref{fig:ssa_cfg1}, and suppose that further analysis shows
  that the assignment of $z$ is dead.  We would like to remove the
  assignment and turn the {\StateMachine} into the one shown on the
  right.  This is correct as-is using SLI's $\Phi$ semantics.  If a
  simple control-flow based definition of $\Phi$ were used instead
  then we would also need to convert the $\Phi$ node at l1 into $x_3 =
  \Phi(x_1, x_2, x_2)$, as the l1 state now has three control-flow
  predecessors.  There are, of course, many solutions to this problem
  in the standard compiler literature\needCite{}, but all add
  complexity which is unnecessary and unuseful in this context.
\end{itemize}

Most algorithms for converting to SSA form will work equally well with
either form, including that used by \implementation, and so no details
are given here; see \needCite{} for more information\editorial{blah}.

\todo{I'd be surprised if I'm the first person to come up with this...}

Note that while {\StateMachine}-level variables, including registers,
are converted to single static assignment form, memory accesses are
not.  This is because it is not always clear from the {\StateMachine}
when two \state{Store} operations modify the same memory location,
which makes the conversion process far more difficult.  \todo{Maybe
  cite Van Emmerik 2007 here?}

\todo{Maybe mention that LLVM and gcc do something very similar?}

\todo{Not actually sure how interesting this is, now that I've written
  it down.  It's all true, and it does make a bit of difference to the
  symbolic execution stuff, but I could probably rewrite to drop it
  completely without leaving a particularly obvious hole.}

\begin{figure}
\begin{tikzpicture}
  \node (start) {start};
  \node [below right=of start] (b) {$x_2 = 6$};
  \node [below = of b](c) {if ($\ldots$)};
  \node [below = of c] (d) {$y_1 = 1$};
  \node [below right = of c] (e) {$y_2 = 2$};
  \node [below = of d] (f) {$z = 3$};
  \node [left = of f] (a) {$x_1 = 5$};
  \node [below = of a] (g) {l1: $x_3 = \Phi(x_1, x_2)$};
  \draw[->] (start) -- (a);
  \draw[->] (start) -- (b);
  \draw[->] (a) -- (g);
  \draw[->] (b) -- (c);
  \draw[->] (c) -- (d);
  \draw[->] (c) -- (e);
  \draw[->] (d) -- (f);
  \draw[->] (e) -- (f);
  \draw[->] (f) -- (g);
\end{tikzpicture}
\begin{tikzpicture}
  \node (start) {start};
  \node [below right=of start] (b) {$x_2 = 6$};
  \node [below = of b](c) {if ($\ldots$)};
  \node [below = of c] (d) {$y_1 = 1$};
  \node [below right = of c] (e) {$y_2 = 2$};
  \node [left = of d] (a) {$x_1 = 5$};
  \node [below = of a] (g) {l1: $x_3 = \Phi(x_1, x_2)$};
  \draw[->] (start) -- (a);
  \draw[->] (start) -- (b);
  \draw[->] (a) -- (g);
  \draw[->] (b) -- (c);
  \draw[->] (c) -- (d);
  \draw[->] (c) -- (e);
  \draw[->] (d) -- (g);
  \draw[->] (e) -- (g);
\end{tikzpicture}
\caption{Optimising an SSA-form machine}
\label{fig:ssa_cfg1}
\end{figure}

\subsection{Generating CFGs from core dumps}
\label{sect:derive:from_core_dump}

In many cases there may be more information available about the bug
than just the crashing instruction pointer.  In many particular, the
contents of the processor stack at the time of the crash might be
available\footnote{This would, for instance, be main source of
  information available in a Windows minidump \todo{cite?}, many other
  platforms include a stack snapshot in addition to more extensive
  information \todo{maybe some more cites?}.}.  {\Technique} can use
this snapshot, when available, to restrict the set of read-side
\backref{CFG}s which must be considered.

The most important information in the stack is the sequence of
functions which were called to reach the current instruction, which is
equivalent to {\technique}'s inlining contexts.  Extracting this
information is, however, non-trivial if the program to be analysed
lacks frame pointers and debug information.  {\Technique} has two
strategies for solving this problem:

\begin{itemize}
\item
  A static analysis, run on the binary before attempting to analyse
  the core dump, which attempts to map from instruction addresses to
  the offset between the current stack pointer and the address of the
  current function's return address.  When this analysis succeeds it
  makes it easy to determine from the core dump where the function
  will return to, and hence where it was called from, but it will not
  always succeed.  In particular, the \verb|alloca| function can cause
  that offset to change at run-time, and so no static analysis will
  ever succeed.
\item
  An abstract interpreter, which attempts to interpret the program's
  machine code forwards from the point of the crash to determine what
  it would have done if it hadn't crashed.  This proceeds until it
  reaches a \verb|ret| instruction, at which point determining the
  return address is again straightforward.
\end{itemize}

\todo{Need to talk about when you use each, and why you need both.
  Which is potentially awkward, because I've forgotten myself.}

Knowing the contents of the call stack at the time of the crash
effectively tells us what the correct inlining context to use is,
which can then be used to constrain the backing-out-of-function case
discussed above.

\subsection{Finding unknown bugs}
\label{sect:derive:unknown_bugs}

\todo{I don't really have anything clever to say here.}

The previous sections described how to derive the read thread's
{\StateMachine} when the analysis already knows, from some external
source, which instruction the program will crash at.  This is useful
when {\technique} is being used to investigate a known bug, but
{\technique} can also be used to look for previously unknown ones.
The scheme used is quite simple: enumerate all of the instructions in
a program which might crash due to a bug of the class being
investigated and then consider each independently in turn.  This is
obviously only feasible if the majority of instructions can be
dismissed quickly.  Fortunately, they can be: {\implementation} takes,
on average, a few tens of milliseconds per instruction on fairly
modest hardware, allowing even large programs with millions of
instructions to be analysed in a few days\editorial{Put some actual
  numbers in here.}.  Parallelisation would allow this time to be
reduced further if necessary.

Identifying instructions which might crash depends on the type of bug
which is to be investigated, but is generally straightforwards.
{\Implementation} considers two types of crash:

\begin{itemize}
\item Assertion-failure type crashes.  These are caused by the program
  calling a function such as \verb|__assert_fail| or \verb|abort|
  provided by an operating system library.  Finding such functions is
  generally straightforward given the usual dynamic linker
  information, and the initial whole-program static analysis phase can
  then find all callers of those functions\editorial{Forward ref}.
\item Bad pointer dereferences.  Any memory-accessing instruction
  could potentially dereference a bad pointer, and so
  {\implementation} simply enumerates all memory accessing
  instructions discovered by the initial static analysis.
\end{itemize}

These potentially-crashing instructions are then considered
independently in turn.\smh{aren't there still a \emph{lot} of these?
  (i.e. it's the backwards search that's important here, not the insn
  choice?)}

\subsubsection{Stack canonicalisation}

The aim of the {\StateMachine} building algorithm is to share work
between different contexts in which the crashing instruction is found
by combining the contexts into a single {\StateMachine}.  This is much
easier if the we can arrange for the stack pointer to always have the
same value at the end of the {\StateMachine}, regardless of where the
{\StateMachine} starts, so that local variable accesses in the
function containing the purported crashing instruction match up
properly\footnote{Of course, this means that stack references near the
  start of the {\StateMachine} will be less likely to match up.  This
  is usually a reasonable trade-off, as all paths through the
  {\StateMachine} end in the same way but might start in completely
  different function contexts.}.  This is not completely trivial if
the starting points are themselves in different functions with
different inlining contexts.  This problem can be solved by examining
the generated {\StateMachine} and determining, for each entry point,
how the stack pointer changes between that entry point and the end and
then inserting an opposite change immediately before the entry point.
Most of the time, the only change to the stack pointer is adding or
subtracting a constant, and so this is easy.  Otherwise, it fails.
The fact that the stack has been massaged in this way is recorded in
the {\StateMachine} structure so that it can be undone later when
building data-dependent crash enforcers.

\todo{Possibly interesting: this means that once you've converted to
  SSA, the initial generation of the stack pointer refers to its value
  at the \emph{end} of the machine, whereas for every other register
  it refers to the value at the \emph{beginning}.}

\subsection{Building the write thread's \StateMachines}
\label{sect:derive:write_side}

At this stage, {\technique} has build the read thread's
{\StateMachine} for some bug which is to be investigated.  The next
step is to build the write thread's {\StateMachine}.  According to the
bug definition in Section~\ref{sect:intro:types_of_bugs}, we should
now consider every possible sequence of \backref{$\alpha$}
instructions in the program, convert each to a {\StateMachine}, and
consider every possible interleaving of the read thread's
{\StateMachine} with each of these write {\StateMachines}.  This would
clearly be completely impractical for all but the most trivial
programs.  Fortunately, it is possible to significantly reduce the set
of sequences which must be considered by using the \backref{program
  model}.

The \backref{program model} includes, for each instruction which reads
from memory, a list of all of the instructions which might store to
the same memory location\footnote{As usual, this list is only complete
  when the \backref{dynamic analysis} from which the \backref{program
    model} is built is itself complete.}.  This instructions in this
list are referred to as the \introduction{interfering
  stores}\editorial{Need a less generic-sounding name} for the read
thread.  We can then immediately reduce the set of sequences which
need to be considered to just those which include some instruction
from the \backref{interfering stores} set, which is already a useful
reduction.  Two further observations make it possible to reduce the
set of sequences further:

\begin{itemize}
\item Any instructions in the write thread after the last
  \backref{interfering store} cannot possible influence the behaviour
  of the read thread, and so cannot possibly affect whether the
  program crashes.  They can therefore be completely discarded.
\item Instructions prior to the first \backref{interfering store} can
  also be discarded.  This is perhaps less obvious.  Discarding these
  instructions is safe only because of the \backref{W isolation}
  property.  The write thread cannot load from any locations which are
  stored to by the read thread, and so, in particular, its load
  operations cannot race with any operations in the read thread.
  Their only possible effect is to restrict the possible values of
  thread-local state, such as machine registers or stack locations,
  when the write thread starts.  In the absence of such restrictions,
  {\technique} will consider all possible initial values for this
  state, and thus discarding these restrictions is safe.

  On the other hand, it is not always desirable to do so, if the
  restrictions would have provided useful hints to later phases of the
  analysis.  For instance, if the bug to be investigated is a bad
  pointer dereference, knowing that the value stored into a shared
  structure is a pointer to the write thread's stack, and hence
  definitely valid, is often useful.  The approach taken by
  {\implementation} is to first generate a set of \backref{CFGs} which
  all start with an \backref{interfering store} and then extend them
  backwards to include a little bit of preceding context, as long as
  doing so is unambiguous and does not exceed the \backref{analysis
    window}.  ``Unambiguous'' here means that {\implementation} will
  replace a \backref{CFG} root instruction A with an instruction B if
  B is always the instruction executed immediately before
  A.\editorial{Not a nice way of phrasing that.}
\end{itemize}

The procedure for building write-side {\StateMachines} is then a
variant of that used for building read-side ones: find all of the
potentially interfering store instructions, build an acyclic CFG which
includes all traces of appropriate length which start and end with an
interfering store, potentially extend it with a little bit of extra
context, and then compile the CFG down to a {\StateMachine}.  The
details are, however, slightly different, and I describe them in the
next couple of sections.

\subsubsection{Finding relevant stores}

The first phase of building the write-side {\StateMachines} is to
determine which stores in the program might possibly interfere with
the read-side {\StateMachine}.  As indicated, this is straightforward:
simply take all \state{Load} states in the read thread's
{\StateMachine} and compare them to the \backref{program model} to
find all of the store instructions which might possibly interfere with
them.

As a minor optimisation, {\implementation} first attempts to remove
\state{Load}s which are only used to select between one of several
possible \backref{annotation states}.  Doing so is also simple: remove
all of the \backref{annotation states} from the read-side
{\StateMachine} and the simplify the resulting {\StateMachine} as far
as possible.  Any \state{Load}s which are discarded by these
simplifications can then be ignored when building the set of
\backref{interfering stores}.  This might, for instance, be useful if
a \state{Load} is dead\editorial{in the sense of dead code
  elimination} except for being used to select between several
possible \state{PointerAliasing} annotations; such \state{Load}s are
almost never relevant to the bug being investigated, and so
considering their interactions with the write thread is usually a
waste of time.  \todo{Now I write it out like that, this sounds like a
  much less good idea.  Hmm.}

Note that \state{Store} operations in the read-side {\StateMachine}
are not considered at this stage, even though {\technique} does handle
some kinds of write-write races.  That is safe, again, because of the
W isolation property: the write thread can never load any locations
written by the read thread, and so if the stored value is ever loaded
it must be via a \state{Load} side-effect in the read thread, and any
potentially interfering stores in remote threads will be detected
because the dynamic aliasing model will report that they alias with
that \state{Load} side-effect.

\subsubsection{Build write thread CFGs}
\todo{This has far more pages than it really deserves, although most
  of them are diagrams, so I guess it's not too bad.}

The input to this phase of the analysis is the set of
\backref{interfering store} instructions, and the analysis must build
a collection of acyclic CFGs which cover all possible paths through
the program which start and end with one of those instructions and
which contain at most \backref{$\alpha$} instructions.  This is easier
than building the read thread CFGs in the sense that both ends of the
CFG are ``bounded'' by some well-defined instruction, whereas read
thread CFGs potentially extend arbitrarily far backwards; it is harder
in the sense that write thread CFG builder must also cluster the
interfering instructions, deciding which should be included in a
single trace and which analysed independently, whereas read thread
CFGs concern only a single instruction.  The overall result is that
the write thread CFGs tend to be significantly smaller and easier to
analyse than read thread ones but the unrolling algorithm itself is
slightly more involved.

The first phase of the algorithm is to build a (possibly) cyclic
fragment of the original program's control flow graph which includes
all instructions which might possibly be included in one of the final
traces.  This is simple: starting from each potentially interfering
instruction, {\technique} explores forwards for \backref{$\alpha$}
instructions, merges the resulting CFG fragments, and then discards
any instructions which cannot reach a potentially interfering
instruction within \backref{$\alpha$} instructions.  These CFG
fragments may cross function boundaries; the details are the same as
those for read thread CFGs in most important respects, and I do not
repeat them here\editorial{Well, the fact that you're exploring
  forwards rather than backwards makes it a bit different, but not in
  an interesting way.}.

The next step is to remove the cycles from the CFG.  As with read
thread CFGs, this is accomplished by duplicating nodes so as to unroll
loops until any path which uses the loop more than once must be longer
than \backref{$\alpha$} instructions, at which point the loop-closing
edges can be safely discarded.  There is, however, one important
difference: in the read thread CFG, we are interested in any path
which terminates at a specific point, whereas in the write thread CFG
we need to preserve any path which goes between any members of a set
of interfering instructions.  This makes it more difficult to
determine when a loop has been unrolled sufficiently, as it is no
longer sufficient to just check the distance to a nominated target
instruction.  {\Technique} solves this problem by labelling each node
in the graph with information about where it might occur in an
interesting path.  This label contains an entry for every possibly
interfering instruction specifying:

\begin{enumerate}
\item
  The number of instructions on the shortest path from that
  interfering instruction to the labelled node (the ``min from''
  distance), and
\item
  The number of instructions on the shortest path from the labelled
  node to the interfering instruction or any of its duplicates (the
  ``min to'' distance).
\end{enumerate}

\smh{Possibly define/use I = set of interfering instructions or $I_d$
  = duplicates will make lots of text concise...}

The asymmetry, taking the distance from a ``true'' interfering
instruction and to any duplicate of an interfering instruction, is
perhaps surprising.  The key observation is that every path which
starts at a duplicated interfering instruction will have a matching
path which starts at the original interfering instruction, and so the
ones which start at the duplicate instruction are
redundant\footnote{The symmetrical statement is also true: every path
  which ends in a duplicate interfering instruction has a matching
  path which ends at a true interfering instruction.  It would
  therefore also be correct to discard paths which \emph{end} at a
  duplicate interfering instruction.  It would not, however, be
  correct to combine the two observations and discard all paths which
  either start or end with duplicate instructions, as there would then
  be little point in having those duplicate instructions.}  It is
therefore safe to discard any nodes $r$ where

\begin{displaymath}
\min_{s \in I}\mathit{min\_from}(s, r) + \min_{s \in I_d}\mathit{min\_to}(s, r)
\end{displaymath}

exceeds \backref{$\alpha$}, where $I$ is the set of
\backref{interfering stores} and $I_d$ the set of \backref{interfering
  stores} and their duplicates.  The complete algorithm is then as
shown in Figure~\ref{fig:derive:store_cfg_unroll_alg}.

\begin{figure}
\begin{algorithmic}
  \State {Compute initial labelling of graph}
  \For {$t$ in the set of potentially-relevant stores}
    \While {graph rooted at $t$ is not cycle-free}
       \State $\mathit{edge} \gets \textsc{findEdgeToBeBroken}(t, \{\})$
       \State $\mathit{newLabel} \gets \textsc{combineLabels}(\text{current label of } \mathit{edge}.\mathit{start}, \text{current label of } \mathit{edge}.\mathit{end})$
       \If {$\min_s(\mathit{newLabel}.\mathit{minFrom}(s)) + \min_s(\mathit{newLabel}.\mathit{minTo}(s)) > N_w$}
           \State {remove $\mathit{edge}$}
       \Else
           \State $\mathit{newNode} \gets \text{duplicate } \mathit{edge}.\mathit{end}$
           \For {Edges $e$ leaving $\mathit{edge}.\mathit{end}$}
              \State {Create a new edge from $\mathit{newNode}$ to $e.\mathit{end}$}
           \EndFor
           \State {Set label of $\mathit{newNode}$ to $\mathit{newLabel}$}
           \State {Replace $\mathit{edge}$ with an edge from $\mathit{edge}.\mathit{start}$ to $\mathit{newNode}$}
           \State {Recalculate $\mathit{min\_from}$ for $\mathit{edge}.\mathit{end}$ and its successors, if necessary}
       \EndIf
    \EndWhile
  \EndFor
\end{algorithmic}
\caption{Loop unrolling algorithm for write thread CFGs.
  \textsc{findEdgeToBeBroken} and \textsc{combineLabels} are described
  in the text below.}
\label{fig:derive:store_cfg_unroll_alg}
\end{figure}

Note that in this algorithm duplicating a node duplicates its
\emph{outgoing} edges, whereas when building a read thread CFG the
\emph{incoming} edges are duplicated.  This reflects the fact that
write thread CFGs are built up forwards from the interfering
instructions while read thread CFGs are built up backwards from the
target instruction.

The algorithm relies on two utility functions:

\begin{itemize}
\item \textsc{findEdgeToBeBroken} just finds the closing edge of some
  cycle in the graph.  The precise choice of edge is not
  important\editorial{I \emph{think} it'll converge on the same thing
    regardless, but it might be nice to show that.  It's certainly
    guaranteed to be correct, but confluence would also be a nice
    property.}.  In {\implementation}'s implementation, this is a
  breadth-first search starting from some arbitrarily chosen root of
  the CFG and reporting the first edge to close a cycle.  If the graph
  reachable from that root is acyclic then {\implementation} moves on
  to the next root.  If the sub-graph reachable from every root is
  acyclic then the whole graph is acyclic and nothing more needs to be
  done.
\item \textsc{combineLabels} is also simple, and is responsible for
  computing the label for the new node which would be produced by
  duplicating $\mathit{edge}.\mathit{end}$.  This node will have the
  same outgoing edges as $\mathit{edge}.\mathit{end}$, and so the same
  $min\_to$ label, and a single incoming edge from
  $\mathit{edge}.\mathit{start}$, and hence a $\mathit{min\_from}$
  label which is just $\mathit{edge}.\mathit{start}$'s
  $\mathit{min\_from}$ with one added to every value.
\end{itemize}

The resulting CFG can then be compiled to a {\StateMachine} in the
same way as a read thread's CFG is.  The only major difference is that
the write thread's CFGs can sometimes contain disjoint components, in
which case each such component is compiled to a separate
{\StateMachine}.

As an example, consider this cyclic CFG:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B} edge [in=30,out=-30,loop] ();
  \node (C) [TrueCfgInstr, below=of B] {C};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & \multicolumn{2}{c}{min to} & \multicolumn{2}{c}{min from} & overall min\\
             & A & C & A & C \\
      A & 0 & 2 & 0 & 1 & 0\\
      B & 2 & 1 & 1 & 2 & 2\\
      C & 1 & 0 & 2 & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}

\todo{All of these diagrams need checking over carefully.  I
  rearranged the column headings, need to make sure that I rearranged
  the data to match.}

Blue nodes indicate the interfering instructions.  The overall min
column is the minimum min\_to value plus the minimum min\_from one; it
gives the number of edges on the shortest path involving a given node
which starts at a true interfering instruction and ends at any
interfering instruction, whether true or a duplicate.  Assume, for the
purposes of the example, that $N_w$ is five.  A depth-first search
starting at A will find the cycle from B back to itself and attempt to
break that cycle by duplicating B.  The resulting graph will look like
this:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B} edge [in=210,out=150,loop,killEdge] ();
  \node (B1) [NewCfgInstr, right=of B] {B1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,swungEdge] (B1) to [bend left=10] (B);
  \draw[->] (B1) -- (C);
  \draw[->] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & \multicolumn{2}{l}{min to} & \multicolumn{2}{l}{min from} & overall min\\
         & A & C & A & C \\
      A  & 0 & 2 & 0 & 1 & 0\\
      B  & 2 & 1 & 1 & 2 & 2\\
      C  & 1 & 0 & 2 & 0 & 0\\
      B1 & 2 & 1 & 2 & 3 & 3\\
    \end{tabular}
  };
\end{tikzpicture}

New nodes are shown in red, as is the edge which is modified, and
edges which have been removed are shown crossed through.  Notice that
whereas the shortest cyclic path starting at A was previously A,B,B,
of length 3, it is now A, B, B1, B1, of length 4.  Suppose that the
next depth-first iteration discovers the edge from C to A.  The
algorithm will then break this edge by duplicating A:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (A1) [NewCfgInstr,right=of C] {A1};
  \draw[->] (A) -- (B);
  \draw[->,swungEdge] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->] (B1) -- (C);
  \draw[->] (B1) to [bend left=10] (B);
  \draw[->] (C) -- (A1);
  \draw[->,killEdge] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & \multicolumn{2}{l}{min to} & \multicolumn{2}{l}{min from} & overall min\\
         & A & C & A & C\\
      A  & 0 & 2 & 0 & $\infty$ & 0\\
      A1 & 0 & 2 & 3 & 1 & 1\\
      B  & 2 & 1 & 1 & 2 & 2\\
      C  & 1 & 0 & 2 & 0 & 0\\
      B1 & 2 & 1 & 2 & 3 & 3\\
    \end{tabular}
  };
\end{tikzpicture}

Suppose it now selects the B1 to B edge as the cycle-completing edge.
It will then duplicate B:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (B2) [NewCfgInstr, right=of B1] {B2};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (A1) [DupeCfgInstr,right=of C] {A1};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,killEdge] (B1) to [bend left=10] (B);
  \draw[->,swungEdge] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & \multicolumn{2}{l}{min to} & \multicolumn{2}{l}{min from} & overall min\\
         & A & C & A & C\\
      A  & 0 & 2 & 0 & $\infty$ & 0\\
      A1 & 0 & 2 & 3 & 1 & 1\\
      B  & 2 & 1 & 1 & 2 & 2\\
      C  & 1 & 0 & 2 & 0 & 0\\
      B1 & 2 & 1 & 2 & 3 & 3\\
      B2 & 2 & 1 & 3 & 4 & 4\\
    \end{tabular}
  };
\end{tikzpicture}

The length of the shortest cyclic path start at A has again increased,
this time from four to five.  Now duplicate B because of the A1 to B
cycle-completing edge:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (B2) [CfgInstr, right=of B1] {B2};
  \node (A1) [DupeCfgInstr,right=of C] {A1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (B3) [NewCfgInstr, below=of A1] {B3};
  \draw[->] (A) -- (B);
  \draw[->,killEdge] (A1) -- (B);
  \draw[->,swungEdge] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & \multicolumn{2}{l}{min to} & \multicolumn{2}{l}{min from} & overall min\\
         & A & C & A & C\\
      A  & 0 & 2 & 0 & $\infty$ & 0\\
      A1 & 0 & 2 & 3 & 1        & 1\\
      B  & 2 & 1 & 1 & $\infty$ & 2\\
      C  & 1 & 0 & 2 & 0        & 0\\
      B1 & 2 & 1 & 2 & 3        & 3\\
      B2 & 2 & 1 & 3 & 4        & 4\\
      B3 & 2 & 1 & 4 & 2        & 3\\
    \end{tabular}
  };\smh{overall min hangs over right hand margin}
\end{tikzpicture}

The next cycle-completing edge considered is that from B2 to B1.  In
this case, the new label would have an overall minimum of 5, matching
$N_w$, and so there can be no paths through the new node which start
with an interfering instruction and which end at an interfering
instruction or a duplicate of it, and so the edge is simply deleted:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (B2) [CfgInstr, right=of B1] {B2};
  \node (A1) [DupeCfgInstr,right=of C] {A1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (B3) [CfgInstr, below=of A1] {B3};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->,killEdge] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & \multicolumn{2}{l}{min to} & \multicolumn{2}{l}{min from} & overall min\\
         & A & C & A & C\\
      A  & 0 & 2 & 0 & $\infty$ & 0\\
      A1 & 0 & 2 & 3 & 1        & 1\\
      B  & 2 & 1 & 1 & $\infty$ & 2\\
      C  & 1 & 0 & 2 & 0        & 0\\
      B1 & 2 & 1 & 2 & 3        & 3\\
      B2 & 2 & 1 & 3 & 4        & 4\\
      New label & 2 & 1 & 4 & 5 & 5\\
    \end{tabular}
  };
\end{tikzpicture}

This process iterates, removing one cycle-completing edge at a time,
until the graph is completely acyclic\editorial{I used to have more
  intermediate steps in here, but they were really boring.}:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (B2) [CfgInstr, right=of B1] {B2};
  \node (A1) [DupeCfgInstr,right=of C] {A1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (B3) [CfgInstr, below=of A1] {B3};
  \node (C1) [DupeCfgInstr, below=of B3] {C1};
  \node (B4) [CfgInstr, right=of B3] {B4};
  \node (A2) [DupeCfgInstr, left=of C1] {A2};
  \node (C2) [DupeCfgInstr, below=of B4] {C2};
  \node (C3) [DupeCfgInstr, right=of A1] {C3};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) -- (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) -- (C3);
  \draw[->] (B3) -- (B4);
  \draw[->] (C) -- (A1);
  \draw[->] (C1) -- (A2);
  \draw[->] (B3) -- (C1);
  \draw[->] (B4) -- (C2);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (A2) (B) (B1) (B2) (B3) (C) (C1) (C2) (C3) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & \multicolumn{2}{l}{min to} & \multicolumn{2}{l}{min from} & overall min\\
         & A & C & A & C\\
      A  & 0 & 2 & 0 & $\infty$ & 0\\
      A1 & 0 & 2 & 3 & 1        & 1\\
      A2 & 0 & $\infty$ & 6 & 4 & 4\\
      B  & 2 & 1 & 1 & $\infty$ & 2\\
      B1 & 2 & 1 & 2 & $\infty$ & 3\\
      B2 & $\infty$ & 1 & 3 & $\infty$ & 4\\
      B3 & 2 & 1 & 4 & 2        & 3\\
      B4 & $\infty$ & 1 & 5 & 3        & 4\\
      C  & 1 & 0 & 2 & 0        & 0\\
      C1 & 1 & 0 & 5 & 3        & 4\\
      C2 & $\infty$ & 0 & 6 & 4        & 4\\
      C3 & $\infty$ & 0 & 4 & $\infty$ & 4\\
    \end{tabular}
  };
\end{tikzpicture}

As desired, the graph has been rendered acyclic while preserving all
paths of length up to five instructions.  As a minor optimisation,
{\implementation} will merge node B2 with B4 and C3 with C2 before
converting the CFG to a \StateMachine, as the nodes are semantically
identical and this results in a slightly simpler \StateMachine.

\smh{This is pretty good, though could be improved somewhat (layout at
  least!)}

\section{Simplifying {\StateMachines}}

The {\StateMachines} generated by the algorithms in
Section~\ref{sect:derive:derive} are faithful representations of all
of the instructions which might have been executed by the program in
the \backref{analysis window}.  As such, they usually contain a large
amount of redundant information which is not relevant to the behaviour
being investigated.  {\Technique} therefore uses a number of related
techniques to simplify them and to eliminate irrelevant fragments.
The important ones are:

\begin{itemize}
\item
  Dead code elimination, to eliminate redundant updates to registers.
  This is particularly effective for eliminating updates to the
  \verb|rflags| register in the AMD64 architecture. This is
  essentially identical to the compiler-level optimisation of the same
  name\needCite{}, and so is not discussed in detail here.
\item
  Register copy propagation, which combines smaller updates to
  registers into a single higher-level operation which is usually
  easier to analyse.  To see why this is necessary, consider
  a fragment of code like this:

\begin{verbatim}
shl (%rax << $4) -> %rax
sub (%rax - $7) -> %rax
mul (%rax * $11) -> %rax
\end{verbatim}
  
  This will produce a three-state {\StateMachine} fragment, with one
  state for each instruction.  It would be more useful to produce a
  single state which set $rax$ to $((rax \times 16) - 7) \times 11$,
  and this simplification performs that transformation.  The algorithm
  used is a straightforward adaptation of that employed by
  dcc\needCite{}, and so is not discussed in detail here.

  One minor extension present in {\implementation} but not dcc is that
  {\implementation} can make use of \state{Assert} side-effects during
  this transformation, so that, for instance, if $x$ is asserted to be
  less than $7$ then the expression $x > 22$ can be rewritten to
  $\mathit{false}$.  This does not require any significant changes to
  the bulk of the algorithm, beyond a few simple rules describing when
  such rewrites are valid.
\item
  {\Technique} attempts to eliminate $\Phi$ expressions from generated
  {\StateMachines} by converting them into multi-terminal binary
  decision diagrams over the {\StateMachine}'s control flow predicates;
  this analysis is described in Section~\ref{sect:phi_elimination}.
\item
  {\Technique} attempts to eliminate memory accessing instructions
  using a cross-function alias technique which incorporates a fast but
  low-accuracy points-to analysis on the original binary with a slower
  but more accurate alias analysis applied to the {\StateMachine};
  this is described in Section~\ref{sect:alias_analysis}.
\item
  The {\StateMachines} generated by {\technique} often contain
  fragments which differ only in variable names; the unification
  simplification, described in Section~\ref{sect:unification} attempts
  to unify these.
\end{itemize}

{\Implementation} also makes use of a number of simple arithmetic
simplifications, such as rewriting $x + 0$ to just $x$ or
$(\happensBefore{x}{y}) \vee (\happensBefore{y}{x})$ to
$\mathit{true}$, and constant-folding for most common operators.

\subsection{$\Phi$ elimination}
\label{sect:phi_elimination}

\todo{This whole section needs rewriting.}

The use of SSA form simplifies many parts of the analysis by breaking
up variables into their different live ranges, but complicates some
parts by introducing $\Phi$ side effects, which are themselves quite
difficult to analyse.  The reason that $\Phi$ side effects are hard to
analyse is that their behaviour depends on the {\StateMachine}'s
control flow, and this is not explicitly reified and so is unavailable
to the usual simplification machinery.  The $\Phi$ elimination pass
attempts to rectify this problem by converting the $\Phi$ effects into
multi-terminal reduced ordered binary decision
diagrams\needCite{}\smh{Clarke 93?}  (MTBDDs) which take as input the
{\StateMachine}'s existing control-flow predicates and which produce
as their output the input which will be selected by the $\Phi$.  The
$\Phi$ itself can then be replaced with this MTBDD, making the control
dependence explicit and simplifying later analyses.

\todo{Having played around a bit, I'm now absolutely convinced that
  using BDDs in a few other places would massively improve
  performance.  Might end up having to do that.  Actually, pulling the
  BDD discussion out a bit earlier might make this a bit clearer,
  anyway. Alternative: just assert that the algorithms exist here and
  put the actual definitions in an appendix.}

The approach used is simple: build a mapping from {\StateMachine}
states to boolean BDDs\footnote{For clarity, I refer to BDDs which
  evaluate to either true or false as boolean BDDs or just BBDDs, by
  contrast with MTBDDs which can evaluate to a more complex value set.
  Most existing literature refers to BBDDs as just BDDs.}  which are
true for precisely those executions in which the state will be
executed, extend this into a mapping from states to MTBDDs which show
which input to the $\Phi$ would be selected if the $\Phi$ were issued
immediately after that state, and then simply read off the MTBDD for
the actual $\Phi$ state.\editorial{Should be a mention of CDGs in
  there, so that the next section makes sense.}

\smh{Some duplication here.  Can you clarify/simplify?  (Maybe also
  give an example?)}

\subsubsection{Building the control dependence graph}

\label{sect:cdg}

The control dependence graph (CDG) is a standard compiler data
structure\needCite{} showing which nodes in a control flow graph are
control-dependent on which branch operations.  In other words, it
shows, for each statement, which control-flow conditions must be true
for that statement to execute, which must be false, and which do not
matter.  The control dependence graph used by {\technique} is a slight
extension of this concept: rather than simply listing the expressions
on which the node depends, it gives a BBDD which will evaluate to true
in precisely those executions in which the node executes.  This is
possible because {\technique}'s {\StateMachines} are finite and
acyclic.

The algorithm for building the graph is simple:

\begin{algorithmic}[1]
\State $\mathit{cdg}[\mathit{root}] = \mathit{const}(\mathit{true})$\smh{Presumably, you meant to italicise cdg itself to something?}
\While {Some state is unlabelled in $\mathit{cdg}$}
  \State {$n \gets $ select a state which has no label but whose predecessors are all labelled}
  \State {$\mathit{cdg}[n] \gets \bigvee \{\mathit{cdg}[p, n] \mid p \in \textrm{$n$'s predecessors}\}$}\Comment{Take union of all reaching paths}\smh{Maybe use a different font for comments?}
\EndWhile
\end{algorithmic}

Here, $\mathit{cdg}[x]$ is the condition for state $x$ to run.
$\mathit{cdg}[x, y]$ is the condition necessary for the edge from
state $x$ to $y$ to be traversed, defined by:

\begin{displaymath}
\mathit{cdg}[x, y] = \begin{cases}
  \bot                                     & \text{if there is no edge from $x$ to $y$} \\
  \mathit{cdg}[x] \wedge x.condition       & \text{if $x$ is an \state{If} state and $y = x.\mathit{trueTarget}$} \\
  \mathit{cdg}[x] \wedge {\neg}x.condition & \text{if $x$ is an \state{If} state and $y = x.\mathit{falseTarget}$} \\
  \mathit{cdg}[x]                          & \text{otherwise}
\end{cases}
\end{displaymath}

Note that the selection on line 3 is only guaranteed to be possible
because the {\StateMachine} is acyclic\footnote{This assumption is
  also the reason that {\technique}'s CDG is precise, whereas CDGs for
  full programs usually contain some approximations}.  Given that, the
algorithm is simple: to find the conditions under which a state might
execute, find the conditions under which all of its incoming edges
might execute and take the union.  Once all states are labelled the
CDG is complete.

\subsubsection{Building the $\Phi$ map}

Once the CDG is complete, the next step is to build the $\Phi$ map.
This is a map from states of the {\StateMachine} to MTBDDs which
select which input of the $\Phi$ would be selected if the $\Phi$ were
to be issued immediately after that state.  The complete map then
makes eliminating the $\Phi$ side-effect trivial.  The algorithm for
building the $\Phi$ map is shown in
Figure~\ref{fig:derive:phi:phi_map}.

\begin{figure}
\begin{algorithmic}[1]
\For {$i$ in inputs to the $\Phi$}\Comment {Build the initial map}
  \State {$s \gets $ state defining input $i$}
  \State {$pm[s] \gets const(i)$}
\EndFor
\While {the $\Phi$ state is unlabelled}
  \State {$n \gets $ select a state which has no label but whose predecessors are all labelled}
  \State {$g \gets \{ (\textit{cdg}[p, n], \textit{pm}[p]) \mid p \in \textrm{$n$'s predecessors}\}$}
  \State {$\textit{pm}[n] \gets \textsc{simplify}(\textsc{flatten}(g), cdg[n])$}
\EndWhile
\end{algorithmic}
\caption{Building the $\Phi$ map}
\label{fig:derive:phi:phi_map}
\end{figure}

\smh{Ok -- I found this \emph{very} confusing and I think you can
  explain it more easily.  GMTBDD = trad abstraction; you essentially
  just want to use the CDG predicate to project the appropriate value
  from $\Phi$.}

Line 7, the definition of $g$, is the core of the algorithm.  $g$ is a
set of pairs of a control predicate and an MTBDD.  Each pair
represents one way of reaching the current state, where the predicate
says when the {\StateMachine} will follow that path and the MTBDD says
what the $\Phi$ would evaluate to if the {\StateMachine} followed that
path.  The \textsc{flatten} function then converts these sets back
into a simple MTBDD in a way which respects variable ordering and
keeps the MTBDD in reduced form.  This is guaranteed to be possible
and unambiguous because the construction of the CDG ensures that, for
every state, precisely one of the control predicates evaluates to
$\mathit{true}$.

\todo{\textsc{Flatten} itself is a fairly straightforward variant of
  the standard BDD zip algorithm, but has lots of nasty special cases,
  so I don't really want to have to describe it.  At the same time, I
  can't find a cite for anyone doing precisely the same thing, so
  perhaps I ought to.}

As a minor optimisation, the algorithm shown simplifies the MTBDD for
state $n$ under the assumption that the CDG condition for $n$ is true;
in other words, under the assumption that $n$ is actually run.  The
\textsc{simplify} function is shown in
Figure~\ref{fig:derive:phi_elimination:simplify}.  The basic idea is
to define a $\textsc{implies}(a, b)$ operation which evaluates to $a$
when $b$ is true or the special $\bot$ value when $b$ is false and to
then use this to zip the original MTBDD with the assumption BDD.  This
lifts the original MTBDD $a$ to a new MTBDD which evaluates to $a(k)$
for any configuration $k$ where the assumption is true or to $\bot$
where the assumption is false.  We then define an \textsc{unlift}
operation which removes all paths through the MTBDD to these $\bot$
values, returning the MTBDD to its original range.  The net effect is
to produce a new MTBDD which evaluates to the same thing as the
original whenever the assumption is true, but potentially tests fewer
input variables while it does so.\smh{Very unclear! example?  Isn't
  this effectively just computing the path condition?}

\todo{This is supposed to be removing the effects of any common
  dominator of both predecessor states.}

As a further minor optimisation, not shown in the figure,
{\implementation} performs an initial reachability test to determine
which states in the {\StateMachine} might eventually reach the $\Phi$
state which is to be eliminated and does not attempt to calculate
labels for any which do not.  The result of the analysis is unchanged.

\begin{figure}
\begin{algorithmic}
\Function{simplify}{$thing:MTBDD(k)$, $assumption:BDD$ $\rightarrow \bot + MTBDD(k)$}
  \State \Return $\textsc{unlift}(\textsc{zip}(thing, assumption, \textsc{implies}))$
\EndFunction
\Function{implies}{$a:k$, $b:bool$ $\rightarrow k + \bot$}
  \If{$b$}
    \State \Return $a$
  \Else
    \State \Return $\bot$
  \EndIf
\EndFunction
\Function{unlift}{$inp:MTBDD(k + \bot) \rightarrow \bot + MTBDD(k)$}
  \If{$inp = const(\bot)$}
    \State \Return $\bot$
  \ElsIf{$inp = const(k)$}
    \State \Return $inp$
  \Else
    \State $if(cond, t, f) \gets inp$
    \State $t' \gets \textsc{unlift}(t)$
    \State $f' \gets \textsc{unlift}(f)$
    \If{$t' = \bot$}
      \State \Return $f'$
    \ElsIf{$f' = \bot$}
      \State \Return $t'$
    \Else
      \State \Return $if(cond, t', f')$
    \EndIf
  \EndIf
\EndFunction
\end{algorithmic}
\caption{The MTBDD $simplify$ algorithm.  The notation $k:t$ indicates
  a variable $k$ of type $t$, $a \rightarrow b$ indicates a function
  from arguments $a$ to a return value of type $t$, $\bot + k$
  indicates type $k$ augmented with the value $\bot$, and $MTBDD(k)$
  is the type of MTBDDs whose terminals are constants of type $k$.
  Not shown: \textsc{unlift} also reduces the MTBDD as it builds it,
  removing any newly-introduced duplicate nodes.}
\label{fig:derive:phi_elimination:simplify}
\end{figure}

\todo{I really don't want to have to define $zip$, because it's fiddly
  and not very interesting, but I can't find a cite for it.}

\todo{Need to think much harder about variable ordering within the
  BDDs.  Currently using pointer order, which is rather silly.  Also
  need to mention that the ordering is consistent within any run of
  the pass, but not necessarily between multiple runs.}\smh{Well, you
  need some order -- or are you hoping to simplify by relabelling?  Is
  this worth it?}

\todo{Might be worth explicitly calling out that the CDG can be shared
  between different $\Phi$ effects, but the $\Phi$ map needs to be
  rebuilt each time?}\smh{Possibly.  As mentioned, this section is
  among the most confusing so far.}

\subsection{Alias analysis}
\label{sect:alias_analysis}

\todo{Really need to look at some standard compiler alias analyses to
  figure out how novel this actually is.  It'll need some description
  regardless, because it's important and I'm pretty certain nobody
  else has tried it in this context, but the amount and type might
  change a bit.}

Memory-accessing operations are often difficult to analyse because it
is hard to determine when two pointers refer to the same memory
location.  {\Technique} therefore tries to eliminate memory accesses
whenever possible.  The basic approach used is to derive, for each
\state{Load} operation, an expression BDD which evaluates to the value
loaded, allowing the \state{Load} to be replaced with a simple
\state{Copy}.  Eliminating \state{Load}s will then make the
\state{Store} operations redundant, and so they can also be
eliminated.  The result is a {\StateMachine} which is far simpler, in
two senses: it has fewer states, and the states which remain have
simpler semantics.

At a high level, the algorithm used is similar to that used to
eliminate \state{$\Phi$} states (see
section~\ref{sect:phi_elimination}): consider each \state{Load} state
$l$ in turn and, for each one, define functions over states $i$
$before_l(i)$ and $after_l(i)$ which show what the result of issuing
the \state{Load} immediately before or after (respectively) $i$.  This
is only possible if the {\StateMachine} contains every \state{Store}
which might alias with $l$, but, subject to that constraint, it is
always possible to build the complete map.  The state $l$ can then be
replaced by a \state{Copy} from $before_l(l)$.

The functions $before_l$ and $after_l$ are themselves relatively
straightforward:

\begin{itemize}
\item $after_l(i)$ depends on $before_l(i)$ and the type of state $i$.
  For \state{Store} states, it is a BDD which tests whether the store
  aliases with $l$ and returns the \state{Store}'s data if it does and
  $before_l(i)$ otherwise.  For non-\state{Store} states, it is simply
  $before_l(i)$.
\item For non-initial states $s$, $before_l(s)$ depends on
  $after_l(p)$ for every predecessor $p$ of $s$ and on the condition
  associated with the $p$ to $s$ edge.  The final result is an
  expression BDD which is equal to $after_l(p)$ whenever $cdg[p, s]$
  is true, where $cdg$ is the control dependence graph as defined in
  section~\ref{sect:cdg}.  \todo{Building those BDDs is remarkably
    intricate (it took me days to come up with an algorithm which
    worked and was reasonably efficient, and I'm currently on the
    third rev of it), but also kind of boring.  Not sure what to do
    about that.  Could just pretend it's trivial.}  Note that the
  $cdg$ ensures, by construction, that precisely one such condition is
  true in any configuration\editorial{define?} which can reach
  $s$\editorial{That isn't obvious; I believe it mostly because the
    experiments haven't found any cases where it's false.}.  If $s$ is
  unreachable then the value of $before_l(s)$ does not matter, and so
  this definition is sufficiently unambiguous.

  \todo{I'm ignoring sub-word accesses here.  Should probably be more
    explicit about that.}

\item $before_l(r)$, where $r$ is the first state in the
  {\StateMachine}, is simply $\smLoad{addr}$, where $addr$ is the
  address expression from $l$.  In other words, issuing a load at the
  start of the {\StateMachine} always returns the initial value of
  memory.
\end{itemize}

These rules can be solved recursively to find $before_l(i)$ for any
$l$ and $i$, and hence to eliminate any \state{Load} for which every
\state{Store} is available\footnote{Note that this recursion is only
  guaranteed to terminate because {\StateMachines} are acyclic.}.

These definitions leave one important aspect undefined: how to
determine whether a \state{Store} and \state{Load} might alias.  In
some cases, this is trivial: $RSP+8$ and $RSP+72$ are never going to
alias, regardless of the value of $RSP$ or the structure of memory.
Other cases are much more difficult.  {\Technique} has two approaches
for doing so: a dynamic analysis, used to model accesses to the heap
and other global data, and a static one, used to model accesses to the
stack.  These are described in
sections~\ref{sect:derive_manip:dynamic_analysis} and
\ref{sect:derive_manip:static_analysis}, respectively.
Section~\ref{sect:derive_manip:synthesis_aliasing} then describes how
to combine the results of these two analyses and use them to perform
alias resolution.

The decision to handle the stack differently from the heap perhaps
requires further explanation.  This is a fundamentally pragmatic
decision.  Ideally, {\technique} would use only analyses to resolve
aliasing problems, minimising the risk of false positives and
negatives, but this is simply not practical when analysing the
program's heap.  The heap structure can itself be very complicated, in
terms of which parts might contain pointers to which other parts, and
the information needed to derive this structure can be scattered
throughout the program.  This means that even with access to the
program's source deriving a useful model of the heap is a fearsome
challenge\needCite{}; doing it with just a binary is completely
infeasible.

The stack, on the other hand, has a much simpler structure.  Many
stack locations will have no pointers to them at all (except for the
stack pointer itself), and those which do rarely participate in
complicated object graphs.  Even better, all of the information needed
to characterise the structure of a given function's stack frame is
usually present in the function itself.  These two facts combine to
make a (mostly) sound static analysis feasible in many interesting
cases.

\todo{Also useful: stack accesses are more frequent than heap ones, so
  a dynamic analysis will have less overhead if it confines itself to
  only considering heap accesses.}

\subsubsection{Dynamic analysis to characterise the heap}
\label{sect:derive_manip:dynamic_analysis}

\todo{There's a tension here between precision and soundness which
  I've completely ignored.  Oh well.}

{\Technique} relies on a dynamic analysis to model memory accesses to
non-stack memory locations.  This analysis is responsible for building
an \introduction{aliasing table}, showing which pairs of instructions
might conceivably access the same memory locations.  In the pessimal
case, where every instruction might alias with every other, this table
will contain $n^2$ entries, where $n$ is the number of memory
accessing instructions in the program, which would be unfeasibly large
for non-trivial programs; fortunately, the table is in practice very
sparse, and so is usually perfectly manageable.  

The intuition behind this analysis is that most fields in most data
structures are accessed by a relatively small number of instructions
in the program, and so if it were possible to identify the field being
accessed by a given instruction then that would make it easy to
determine whether two instructions might interfere.  Unfortunately,
that kind of higher-level information is not usually available when
analysing binary programs.  SLI sidesteps that problem by labelling
fields with the set of instructions which might access them;
determining whether two instructions might alias is then a matter of
determining whether they ever appear in the same field label.  The
result is an alias table which can be collected in reasonable time
using a simple dynamic analysis, which can resolve aliasing queries
quickly, and which requires a tolerable amount of space (tens of
megabytes for mysqld, for instance).

The dynamic analysis itself is quite simple, and is implemented as a
skin for Valgrind\needCite{}.  The program's memory is divided into
fixed-size chunks, each of which has a label consisting of two sets of
accessing instructions, one for read instructions and one for write.
Any instruction which accesses that memory chunk adds itself to the
relevant set (or to both sets, for read-modify-write type
instructions).  These labels in effect identify the field for the
memory chunk, and so adding an instruction to a set amounts to
re-labelling one of the fields.  Eventually, the memory chunk will be
released, due to the program either terminating or calling a
\verb|free|-like function, and at that point the label is frozen and
added to a global set of possible field labels.  This global set,
suitably indexed, forms the main aliasing table: the \state{Load}
elimination analysis considers two memory accesses to potentially
alias precisely when there is some entry in the global set which
mentions both accesses.

Of course, implementing this scheme requires the dynamic analysis tool
to be able to correctly identify \verb|free|-like functions.  Most of
these are standard functions present in system libraries, which can be
identified trivially, but it is also possible for a program to
implement its own memory management routines, and in this case
{\technique} depends on manually identifying these functions in order
to build up a precise aliasing table.  This is the only place in which
{\technique} relies on manual intervention.  If this information is
not available then the aliasing table is likely to be highly
inaccurate and the analysis is likely to take a long time to
converge\smh{What does that mean?}.  It might in some cases be
possible to infer the existence of these functions automatically; see,
for example \todo{I was convinced that there were some well-known
  existing papers doing just that, but now I can't find them.  Need to
  dig around some more}.  I have not investigated doing so at this
time.

{\Implementation} includes one minor refinement to this scheme.  It is
fairly common for programs to allocate new heap structures using a
function such as \texttt{malloc} and to then initialise this structure
using a series of stores.  These stores will never race (provided that
\texttt{malloc} is implemented correctly) and it would be helpful to
avoid spending excessive time considering the case where they do.  The
approach taken is simple: when a block is returned from
\texttt{malloc} it is marked as thread-private, and remains so until
it is stored to non-stack memory.  Entries in the aliasing table then
include a flag indicating whether the access is thread-private or
potentially racing.  Later analysis phases know\editorial{urk} that
two thread-private accesses in different threads cannot alias with
each other.

This policy might seem to be overly conservative: a block of memory is
marked as shared whenever a pointer to it is stored into any non-stack
memory, even when that non-stack memory is itself marked as
thread-private.  This is necessary because the analysis does not
attempt to track the heap reachability graph, and in particular cannot
map from one block to the set of blocks reachable from that block.  It
is therefore not safe to ``upgrade'' a block from thread-private to
thread-shared if there is any possibility of that block containing a
thread-private pointer; upgrading blocks early and pessimistically
means that it is never necessary to do so.  This rule can potentially
lead to large amounts of memory being marked as thread-shared when it
is not strictly necessary to do so, but does still allow simple
structure initialisation operations to be marked as thread private.
\todo{Yurk.}

\subsubsection{Static analysis to resolve stack accesses}

\label{sect:static_analysis}

The dynamic aliasing analysis is effective at resolving aliasing
queries between instructions which access shared memory, but does not
provided any assistance with instructions which might access the local
stack.  {\Technique} instead handles these using a machine code-level
static points-to analysis.  As with the dynamic analysis, the results
of this static analysis are used by the {\StateMachine} simplifiers to
restrict the alias analysis pass.

The analysis itself is a reasonably standard points-to analysis.  It
models each program register independently, but reduces memory to two
abstract locations: the current stack frame and everything else.  Each
register can point at either of the locations, or neither (if the
register is definitely not a valid pointer), or both (if the analysis
cannot determine where the register points).  The contents of the two
locations is modelled in a similar way: a location is either
completely free of pointers, or it might contain pointers to the
current stack frame, or it might contain pointers to non-stack memory,
or it might contain both types of pointers.  The complete state is
referred to as a points-to configuration, and the final result of the
analysis is a mapping from instructions in the function to the
configuration at that instruction.

\paragraph{Identifying functions}

\label{sect:function_head}

This analysis is function-local, and so it is important to specify
what is meant by a function, since they are not always apparent at the
level of machine code.  The definition used by {\technique} is simple:

\begin{itemize}
\item
  Every instruction must be assigned to some function.
\item
  No instruction can be assigned to more than one function.
\item
  Every function has a designated entry point instruction, referred to
  as the \introduction{function head}.
\item
  The target of a \texttt{call} instruction is always the
  \backref{head} of some function.
\item
  Any branch instruction will either start and end in the same
  function or will have a \backref{function head} as its target.  This
  includes the implicit branch to the next instruction at the end of
  an ordinary instruction.
\end{itemize}

Note that the instructions in a function do not have to be contiguous,
and so, for instance, code outlining is correctly
handled\editorial{Cite Zhou 2005 or US patent 2007/0089106, unless I
  can find something better.}.  Subject to those constraints,
{\technique} finds the assignment of instructions to functions which
minimises the number of distinct \backref{function heads}.

It is worthwhile discussing briefly how this definition interacts with
compiler tail-call elimination optimisations.  There are three
interesting cases:

\begin{itemize}
\item
  If a function is ever called normally, using a call-type branch, the
  fourth rule will ensure that its first instruction is a
  \backref{function head}, as desired.
\item
  If a function is tail-called from precisely one place, and is never
  invoked with a normal call instruction, no entry point will be
  created for it and it will be merged into its calling function.  The
  two functions will be analysed as if they were a single function.
  This leads to a somewhat less precise \backref{points-to table}, as
  the analysis cannot make any a-priori assumptions about the
  \backref{points-to configuration} at the start of the second
  function, but will not lead to any unsoundness.
\item
  If a function is tail-called from multiple different locations then
  the final rule will ensure that the join of the caller's CFGs is a
  \backref{function head}.  This is correct for simple tail-call
  elimination, in which a call instruction followed by a return one is
  replaced with a simple jump.  This is the only form of tail call
  supported by gcc and by LLVM\editorial{Might be worth a cite for
    that?}.  Optimisations such as function epilogue
  sharing\needCite{} might cause the join to be further through the
  function, though, and hence cause the function entry point to be
  incorrectly place.  I am not aware of any widely-used compiler which
  implements such an optimisation and so this is a largely theoretical
  concern\editorial{Might be worth asking on some compiler mailing
    lists to make sure I haven't missed anything.}\smh{Yes, sounds
    good}.
\end{itemize}

In other words, the function heads assigned by these rules will match
up with the actual first instructions of functions in the higher-level
language.  This is important because this analysis is primarily
concerned with identifying accesses to a function's local variables,
and correctly identifying the start of the function makes it easy to
define which memory locations are function-local.  If the value of the
stack pointer when the function starts is $RSP_0$, and the value at
instruction $i$ is $RSP_i$, then the local frame at $i$ consists of
everything between $RSP_i$ and $RSP_0$, plus any stack red zone (for
AMD64, this will be from $RSP_i-128$ to $RSP_i$).

\paragraph{Characterising the behaviour of functions}

The key assumption made by this analysis is that locations in the
local stack frame are ``created'' when the function starts.  This
means that there cannot be any pointers to such locations before the
function starts, except for the stack pointer, whether in registers,
non-stack memory, or indeed the new stack frame itself.  A pointer
here is defined to be something which is eventually dereferenced,
rather than something which just has a numerical value which happens
to match the local stack frame, so that, for instance, pointers in
dead registers are acceptable.  An important corollary of this is that
the address of the stack must have been unknown when the program was
compiled, and so statically constant values cannot be stack pointers.

\todo{This paragraph is repeating stuff I've already said.}  The
details of the analysis are then quite straightforward, and can be
most easily understood as a kind of abstract interpretation.  As
previously indicated, memory is modelled as two abstract locations,
one for the current frame and one for everything else, with the
program state modelled as a set of abstract variables, one for each
register and one for each of the locations.  The values of the
abstract variables are subsets of $\{\mathit{stack}, \mathit{other}\}$
indicating, for registers, which locations the variable might point
at, or, for locations, what types of pointers the location might
contain.  The complete state is referred to as a points-to
configuration.  For each instruction, the analysis derives two labels,
one for the start of the instruction and one for the end, where a
label is either a points-to configuration which over-approximates the
possible configurations of the program at that point or the special
value $\bot$ indicating that the point is unreachable.

Define the following symbols:

\begin{itemize}
\item $\mathit{pred}(i)$ is the set of instructions in this function
  which might execute immediately before instruction $i$.
\item $\mathit{before}(i)$ is the start-of-instruction label for
  instruction $i$, defined in terms of $\mathit{after}(i')$ for $i'$
  in $\mathit{pred}(i)$.
\item $\mathit{after}(i)$ is the end-of-instruction label for
  instruction $i$.  This is a function of $\mathrm{before}(i)$ and the
  type of instruction at $i$.
\item $\sqcup$ is the least upper bound operator on instruction
  labels, defined as:

  \begin{itemize}
  \item $\bot {\sqcup} l = l$ and $l {\sqcup} \bot = l$, for any
    instruction label $l$.
  \item $l {\sqcup} l' = l \cup l'$, for $l$ and $l'$ not equal to
    $\bot$, where $\cup$ is the element-wise union of all of the
    locations in $l$.
  \end{itemize}
\end{itemize}

$\mathit{before}$ and $\mathit{after}$ are defined in terms of each
other.  The final result is a least fixed point solution of those
functions.

Defining $\mathit{before}(i)$ in terms of $\mathit{after}(i)$ is then
easy: $\mathit{before}(i) = \sqcup_{i' \in
  \mathit{pred}(i)}\mathit{after}(i')$, so that the points-to
configuration at the start of an instruction is a conservative
combination of the points-to configurations of all of its reachable
predecessor instructions.  Defining $\mathit{after}(i)$ in terms of
$\mathit{before}(i)$ is no more difficult but requires more special
cases.  I give only a few interesting cases here; generalising to the
full AMD64 instruction set is straightforward.

\begin{itemize}
\item
  If $\mathit{before}(i)$ is $\bot$ then $\mathit{after}(i)$ is also
  $\bot$.
\item
  Load instructions: LOAD ${\ast}\mathit{reg} \rightarrow
  \mathit{reg}'$ loads from the memory pointed to by register
  $\mathit{reg}$ into register $\mathit{reg}'$.  $\mathit{after}(i)$
  is will be equal to $\mathit{before}(i)$ except for the entry for
  $reg'$, which is set to $\Cup_{l \in
    \mathit{before}(i)(\mathit{reg})}{\mathit{before}(i)(l)}$.  In
  other words, the analysis determines which locations $\mathit{reg}$
  might point at, determines what pointers those locations might
  contain, and then takes the union.
\item
  Store instructions: STORE $\mathit{reg}' \rightarrow
  {\ast}\mathit{reg}$ stores the value of register $\mathit{reg}'$
  into the memory location pointed to by register $\mathit{reg}$.  The
  points-to set for $reg$ and $reg'$ will be unchanged, but the
  points-to sets for the $\mathit{stack}$ and $\mathit{other}$
  locations might be updated.  If $\mathit{before}(i)(\mathit{reg})$
  includes $\mathit{stack}$ then $\mathit{after}(i)(\mathit{stack}) =
  \mathit{before}(i)(\mathit{stack}) \cup
  \mathit{before}(i)(\mathit{reg}')$; otherwise
  $\mathit{after}(i)(\mathit{stack}) =
  \mathit{before}(i)(\mathit{stack})$.  Likewise, if
  $\mathit{before}(i)(\mathit{reg})$ includes $\mathit{other}$ then
  $\mathit{after}(i)(\mathit{other}) =
  \mathit{before}(i)(\mathit{other}) \cup
  \mathit{before}(i)(\mathit{reg}')$, and otherwise
  $\mathit{after}(i)(\mathit{other}) =
  \mathit{before}(i)(\mathit{other})$.
\item
  Set register to constant: SET $k \rightarrow \mathit{reg}$ sets the
  register $\mathit{reg}$ to constant value $k$.
  $\mathit{after}(i)(\mathit{reg})$ will never contain
  $\mathit{stack}$, because the analysis assumes that there are no
  pointers to the stack when the function starts, and $k$ clearly
  existed before that.  Less obviously, $\mathit{other}$ might also be
  absent from $\mathit{after}(i)(\mathit{reg})$.  In particular, the
  analysis assumes that any constant values which are not pointers
  into the program's main binary are not pointers at all.  It is
  possible to write a program which violates this assumption by, for
  instance, passing in an explicit address to the \verb|mmap| system
  call, but this is a very unusual practice.  If nothing else,
  programs which use fixed addresses in that way risk conflicting with
  dynamically loaded libraries and hence become highly non-portable
  for little practical benefit.\editorial{Rephrase a bit.}

  \todo{This isn't actually very important, and it's a bit of a
    nuisance to describe.  Should maybe just kill it.}
\item
  \label{alias:static_analysis:binop_rule}
  Register-register computation: SET $\mathit{reg} \oplus
  \mathit{reg}' \rightarrow \mathit{reg}''$ sets $\mathit{reg}''$ to
  some binary function of $\mathit{reg}$ and $\mathit{reg}'$.
  Conservatively, {\technique} sets $\mathit{after}(i)(\mathit{reg}'')
  = \mathit{before}(i)(\mathit{reg}) \cup
  \mathit{before}(i)(\mathit{reg}')$ in this case.  It might be
  possible to improve on this for some specific $\oplus$.
\end{itemize}

We must now define the initial state for the fixed point iteration.
This is simple:

\begin{itemize}
\item
  The label on a function entry point is as follows:

  \begin{itemize}
  \item The stack pointer points at the current stack frame.
  \item Argument registers cannot point at the current stack frame,
    but might point at memory outside of that frame.
  \item Other registers cannot point at anything.  This is not
    entirely true, in the sense that the caller could have loaded
    anything at all into these registers, but, if the callee conforms
    to the ABI, it is guaranteed that the function will never
    dereference any of them.  Marking them as pointers would mean that
    the stack frame would be marked as containing pointers if the
    function ever saved them to the stack, which is common for
    call-preserved registers, reducing the accuracy of the analysis.
  \item Main memory can contain pointers to itself but not to the
    current stack frame.
  \item The current stack frame contains no pointers at all.
  \end{itemize}
\item
  The initial label for every other instruction is just $\bot$,
  reflecting the fact that we have not yet discovered any paths from
  the entry point to them.
\end{itemize}

The static analysis iterates until it finds a fixed point of these
rules.  The resulting labelling of instructions accurately models the
program's actual points-to behaviour, provided that the assumptions
discussed above all hold.

Handling called sub-functions requires some care, as this is a
function-local analysis and so the bodies of sub-functions are not
available for analysis.  Fortunately, reasonable results can be
obtained by making some simple assumptions which hold for the vast
majority of functions:

\begin{itemize}
\item
  Functions must obey the standard function call ABI.  In particular,
  functions must either return to the instruction immediately after
  the call instruction or not return at all, and, when they do return,
  they must not modify call-preserved registers, and should only
  return values through the designated return value register.
\item
  Functions must not ``invent'' pointers to their caller's stack frame
  by offsetting from the stack pointer, but must instead receive such
  pointers from their caller, whether as formal parameters or via
  memory.
\item
  The called function must take all of its arguments in the ABI's
  designated argument-passing registers.  Within that set,
  {\technique} uses a separate register liveness analysis to determine
  which argument registers a given function actually uses.  This
  liveness analysis is entirely conventional and is not discussed
  here, beyond noting that it analyses the entire program at once,
  across function boundaries, and handles library functions
  conservatively by assuming that they consume all ABI argument
  registers.

  Arguments passed via the stack are completely ignored by both the
  points-to and register liveness analyses (or, equivalently, they are
  assumed to never contain pointers to the current frame).  This is
  unsound.  In practice, however, this unsoundness is rarely
  important, as most function's arguments are passed via registers (at
  least for the standard AMD64 ABI).
\end{itemize}

Given those assumptions, defining $\mathit{after}$ for call
instructions is straightforward.  The analysis first determines
whether any of the inputs to the function (either arguments or
non-stack memory) might contain pointers to the current stack frame.
If they can then the analysis assumes that the function will write
that pointer to all accessible locations (the current stack frame,
non-stack memory, and the return value register); otherwise, the
called function is assumed to be a no-op.

\todo{Need to eval how effective all of this actually is.}\smh{Ack;
  also maybe look at how effective it would be to use more distinct
  locations (i.e. a greater number of locations).}

\paragraph{Mapping the program's function call structure to the {\StateMachine} structure}
\label{sect:derive_manip:synthesis_aliasing}

The static analysis is expressed in terms of accesses to the current
function's local frame, but {\StateMachines} are potentially
cross-function and so even the idea of a ``current function'' is not
entirely well-defined.  It is therefore necessary to recover the
program's function call structure, determining where function local
frames are created and destroyed and the boundaries between them, and
to then match this structure to the data produced by the static
analysis.

Deriving the function call structure is reasonably straightforward:
frames are created by \texttt{call} instructions, destroyed by
\texttt{ret} instructions, and the boundaries between frames are given
by the value of the stack pointer when one of these instructions
executes.  The \texttt{call} and \texttt{ret} instructions are easily
encoded into \state{StartFunction} and \state{EndFunction} annotation
states, with any leftover \texttt{ret} instructions used to determine
the stack when the {\StateMachine} starts and used to form
\state{StackLayout} annotation states at each entry
point\editorial{LS}.

The only slight subtlety lies in deciding when to treat two
\texttt{call} instructions as starting equivalent stack frames, and
hence when they should be assigned the same frame identifier.  If two
\texttt{call}s call the same function then giving them the same frame
identifier potentially allows the relevant states in the
{\StateMachine} to be merged together, and hence reduces future
analysis work.  On the other hand, assigning them different
identifiers usually allows the static analysis to be encoded with
greater fidelity, which can improve the results of alias analysis.
{\Implementation} solves this problem by assigning functions different
frame identifiers whenever possible, subject to the constraint that
all paths to a given {\StateMachine} state must always result in the
same call stack.

Consider, for example, the program fragments shown in
Figures~\ref{fig:derive:assign_frames1}
and~\ref{fig:derive:assign_frames2}.  In the first figure there are
two calls to \texttt{f}.  One starts after the other has finished, and
so they can be assigned different frame identifiers without causing
any {\StateMachine} states to be executed in different stack contexts;
this ensures that the local variables of the two invocations of
\texttt{f} can be correctly disambiguated by the alias analysis.  The
second figure also shows two static calls to \texttt{f}, but here any
given path will only invoke \texttt{f} once, at the end of the path.
All paths will therefore share a single representation of \texttt{f}
and the two calls to \texttt{f} will share a single frame identifier.
This allows the results of the alias analysis on \texttt{f} to be
effectively shared across the two paths, reducing computation costs.

\todo{That's not a brilliant explanation of this.  If nothing else, it
  implies that this is purely a performance thing, whereas there are
  actually some correctness constraints here as well.}

\paragraph{Incorporating static analysis results into the {\StateMachine}}

The next step, once frame IDs have been allocated and the stack layout
determined, is to incorporate the information from the static analysis
into the {\StateMachine}, and in particular determining which frames
might be present in registers or memory when the {\StateMachine}
starts.  For frames allocated by \state{StartFunction} annotations
this is trivial: such frames are never present in memory or in
registers\footnote{Except possibly for the stack pointer itself; see
  later.} when the {\StateMachine} starts.  This reflects the
fundamental assumption that there are no live pointers to a function's
local variables before that function starts.  Likewise, the frame at
the top of the stack when the {\StateMachine} starts is easily
handled, as it is perfectly modelled by the static analysis, and the
information can be simply added to the appropriate \state{StackLayout}
annotation state.  The other frames on the initial stack require more
care.  The approach taken by {\implementation} is to find the call
site which allocated a frame and to derive an over-approximation of
the correct points-to configuration from the points-to configuration
at that point.

As an example, suppose that the initial \state{StackLayout} annotation
has frames $F_1$, $F_2$ and $F_3$, and that the inlining
context\footnote{This can be obtained from the {\StateMachine}'s CFG;
  see section~\ref{sect:derive:cross_function_cfgs}.} is $a$, $b$,
$c$.  This indicates that the {\StateMachine} starts in function $c$,
which was called from function $b$, which was called from function
$a$; that function $c$'s frame is $F_3$; that function $b$'s frame is
$F_2$; and that function $a$'s frame is $F_1$.  The static analysis
can easily determine whether any pointers to $F_3$ might have reached
registers or memory, and so this information is simply copied into the
\state{StackLayout} annotation.  Determining whether any registers
might contain a pointer to $F_2$ is more difficult, as it requires
analysing both $c$ and $b$ and the static analysis is function-local.
It is, however, still possible to usefully constrain the locations
which might point at $F_2$ by examining the points-to configuration at
the \texttt{call} instruction in $b$ which invoked $c$.  In
particular, if there is no way for $c$ to have acquired a pointer to
$F_2$, whether via an argument register or through non-stack memory,
then it cannot have created any additional pointers to
$F_2$\footnote{This is closely analogous to the way the static
  analysis itself handles called sub-functions, as discussed in the
  previous section.}, and so it is safe to assume that a variable can
only contain a pointer to $F_2$ if the \texttt{call}-time
\backref{points-to configuration} allowed that location to point to
$F_2$\editorial{Rephrase.}.  On the other hand, if $c$ could have
received a pointer to $F_2$ then there is no way to constrain what it
could have done with it, and so any variable might point at $F_2$ by
the time the {\StateMachine} starts.  Similarly, the
\backref{points-to configuration} at the \texttt{call} from $a$ to $b$
can often be used to constrain the possible pointers to
$F_1$\footnote{One might reasonably ask what happens if a function
  does not have a \texttt{call}er; for instance, if it is the start
  function of some thread.  Such situations are extremely rare in
  practice.  Almost all operating systems start new programs and
  threads in some library function which then calls into the program's
  actual start-thread or start-program function, and so all program
  functions in practice have a \texttt{call}er.  {\Implementation}
  does not analyse library code, and so the library function's frame
  is itself never an issue.}\editorial{That footnote might be a bit
  too glib.}.

\todo{Maybe talk about interactions with cross-thread \StateMachines?
  They're not actually very interesting, but it's kind of an obvious
  omission.}

\paragraph{Using the static analysis to constrain aliasing}

The {\StateMachine} has now been augmented with a number of annotation
states: \state{StackLayout}s, showing the initial stack layout and
which initial stack frames can be pointed at from where;
\state{StartFunction}s and \state{EndFunction}s, showing where stack
frames are created and destroyed; and \state{ImportRegister}s, showing
which frames program registers might point at.  The next step is to
use this information to determine which stack frames each memory
access might refer to, and hence to determine when the accesses might
alias.

This phase of the analysis builds two main data structures: the
aliasing table itself, which is a set of pairs of \state{Load} and
\state{Store} operations such that the \state{Store} might provide
some information which is loaded by the \state{Load}, and a points-to
table, which specifies where each {\StateMachine} temporary can point,
expressed as a set of stack frames and a flag indicating whether it
can point at non-stack memory.  These two structures are defined in
terms of each other and the analysis is structured as a simple
fixed-point iteration between them: the points-to table is used to
refine an initial, conservative\editorial{Why?  Should be able to use
  aggressive tables safely, and the results would probably be
  better.}, aliasing table, which is then used to refine the points-to
table, which is used to further refine the aliasing table, and so on,
until both tables have fully converged.  The resulting tables
accurately capture the possible memory-accessing behaviours of the
{\StateMachine}.

Defining the initial contents of the two tables is straightforwards.
The points-to table contains an entry for every {\StateMachine}
variable indicating that that variable might point at anything.  The
aliasing table contains an entry for every \state{Load} listing every
\state{Store} in the {\StateMachine}, subject to three constraints:

\begin{itemize}
\item \state{Store}s are not included in the set if simple arithmetic
  considerations show that they cannot alias with the \state{Load}.
  For instance, a \state{Store} to $RSP+8$ will not be included in the
  set for a \state{Load} of $RSP+32$.  \todo{I could maybe give
    details of the arithmetic rules used here, but it'd be long,
    obvious, and boring.}
\item \state{Store}s are only included in the set if there is some
  path through the {\StateMachine} which passes through the
  \state{Store} before reaching the \state{Load}.  This gives the
  analysis a modest level of control-flow sensitivity.
\item If the dynamic analysis recorded any information about the
  \state{Load} then the set of \state{Store}s is constrained to be
  consistent with that information.  The static analysis is not
  usually able to meaningfully improve on the information collected by
  the dynamic analysis, but incorporating it here simplifies the
  implementation.
\end{itemize}

These initial tables are clearly safe, in the sense that any data flow
pattern which the {\StateMachine} can actually achieve is also allowed
by the tables, but they are more conservative than is necessary.
{\Technique} therefore refines them slightly before using them.  This
has two phases: refining the alias table using the points-to table,
and refining the points-to table using the alias table.  The two
phases alternate until the tables converge.

Consider the alias refinement phase first.  This phase considers each
entry in the alias table, and hence each aliasing pair of \state{Load}
and \state{Store} operations, in turn.  For each pair, it calculates a
points-to set for each address; if they do not overlap, the pair can
be removed from the table.  Calculating the points-to set for an
address, $\mathit{pts}(\mathit{addr})$, is straightforward:

\begin{itemize}
\item
  The first step is to compare the address expression to the stack
  layout at this state.  If the address can be arithmetically shown to
  point to a particular stack frame then the points-to set is simply
  that frame.  If the address cannot be shown to point to a particular
  frame, but does mention the stack pointer, it is treated as being
  able to possibly point at any frame.  Otherwise, if it does not
  explicitly mention the stack pointer and cannot be shown to point at
  a particular frame, the analysis proceeds by cases.  \todo{That
    could maybe do with more detail.}
\item
  For a {\StateMachine} temporary $t$, the result is simply the
  points-to table's entry for $t$.
\item
  For an operator $x \oplus y$, the result is the union of the
  points-to sets for $x$ and $y$.  It might be possible to refine this
  rule somewhat for particular operators $\oplus$, but I have not
  needed to do so yet.  The rules for non-binary operators are
  analogous.
\item
  The rules for an initial memory expression $\smLoad{\mathit{addr}}$
  are more complicated.  The algorithm first derives the points-to set
  for $\mathit{addr}$, to find which memory locations the $\smLoad{}$
  might access, and then compares these to the memory layout implied
  by the \state{StackLayout} annotations to find what pointers those
  locations might contain.  The resulting points-to set are trimmed to
  remove any frames which are not live at both the \state{Load} and
  \state{Store} states.  The final result is the union of the
  remaining sets.  \todo{Formalise a little, maybe?}
\end{itemize}

There is a slight complication here, which is that the address fields
of \state{Store} and \state{Load} states are not simple expression,
but are instead BDDs with expressions at their leaves.
{\Implementation} therefore calculates points-to sets for each leaf of
the two BDDs, to form new BDDs over points-to sets, intersects the two
new BDDs, and then takes the union of the leaves of the result.
\todo{Should maybe mention the implications of that?  They're not very
  interesting, but it's an obvious omission at the moment.
  Alternatively: could move this to a footnote so as to de-emphasise
  it a bit.}

Refining the points-to table is also quite simple.  The refinement
process considers each entry in the variable in the {\StateMachine} in
turn and attempts to refine its points-to set using the current
points-to and alias tables.  The refinement of a variable depends on
the type of state which defined that variable\footnote{Recall that
  {\StateMachines} are in static single assignment form, and so there
  will always be precisely one such state.}:

\begin{itemize}
\item
  \state{Copy} $\mathit{expr} {\rightarrow} \mathit{tmp}$, which
  evaluates $\mathit{expr}$ and sets the {\StateMachine} variable
  $\mathit{tmp}$ to that value.  The new points-to set for
  $\mathit{tmp}$ will be $\mathit{old} \wedge
  \mathit{pts}(\mathit{expr})$, where $\mathit{old}$ is the current
  points-to set for $\mathit{tmp}$, $\mathit{pts}$ calculates the
  points-to set for $\mathit{expr}$ using the current points-to table,
  and $\wedge$ is the intersection of points-to sets.
\item
  \state{$\Phi$} $\{\mathit{tmp}_i : \mathit{expr}_i\} {\rightarrow}
  \mathit{tmp}$, which sets $\mathit{tmp}$ to one of the
  $\mathit{expr}_i$ depending on which of the $tmp_i$ has been most
  recently assigned to.  The new points-to set for $\mathit{tmp}$ will
  be $\mathit{old} \wedge (\bigvee \mathit{pts}(\mathit{expr}_i))$, where
  $\vee$ is the union of points-to sets.
\item
  \state{ImportRegister} $\mathit{tid}, \mathit{reg}, \mathit{pts}
  \rightarrow \mathit{tmp}$, which sets $\mathit{tmp}$ to the value of
  register $\mathit{reg}$ in thread $\mathit{tid}$ and notes that it
  has points-to set $\mathit{pts}$.  This case is trivial: the new
  points-to set for $\mathit{tmp}$ is simply $\mathit{pts}$.
\item
  \state{Load} $\ast\mathit{addr} \rightarrow \mathit{tmp}$, which
  evaluates $\mathit{addr}$ to obtain an address and copies the
  contents of memory at that address to $\mathit{tmp}$.  For this
  case, the analysis examines the aliasing table to find all
  \state{Store}s $S$ which might alias with the load and sets the new
  points-to set of $\mathit{tmp}$ to $old \wedge (\bigvee_{s \in
    S}(\mathit{pts}(s_{\mathit{data}})) \vee \mathit{pts}(\smLoad{\mathit{addr}}))$,
  where $s_{\mathrm{data}}$ is the data field of the \state{Store}
  $s$.
\end{itemize}



These two refinement phases are iterated until both the points-to and
aliasing tables completely converge.  The resulting aliasing table can
then be used to constrain the set of \state{Store}s which each
\state{Load} might interact with, and hence to produce simpler
load-replacing expression BDDs during the \state{Load} elimination
simplification.

\todo{The aliasing table can also be used to eliminate some
  \state{Store}s, but that's kind of an awkward thing to work in here,
  and is also kind of obvious.}

\subsubsection{Frame pointer elimination}

\todo{Not entirely convinced this needs to be here at all.}

\todo{This is one of the simplest static analyses I do, but has the
  same structure as the more complex alias analysis, so I could
  consider pulling it up a bit?}

The frame pointer register, if present, complicates local variable
aliasing analysis\footnote{It is perhaps surprising that, at least in
  this respect, the output of an optimising compiler is usually easier
  to analyse than that of a non-optimising one.  This is not the only
  situation in which this happens; see the evaluation for other
  examples. \todo{Need to do that bit of the eval.}}.  The problem is
that some accesses to local variables will be expressed relative to
the stack pointer and some to the frame pointer, and so alias
resolution is likely to fail unless some relationship can be
established between them.  For most compilers, most of the time, that
relationship is straightforward: for any given instruction, the frame
pointer will be the stack pointer will some fixed offset.  If the
frame pointer could be replaced with the expression $RSP+k$, where
$RSP$ is the stack pointer and $k$ the offset, then this problem could
be avoided.  {\Technique} takes exactly that approach, using an
initial static analysis to derive $k$ for every possible instruction
in the program and then replacing the frame pointer with an
appropriate stack-relative expression\footnote{The main
  {\StateMachine}-level analysis cannot derive $k$, as $k$ is usually
  set by the function prologue and the {\technique} analysis window
  often starts in the middle of a function, so does not include the
  necessary information.}.

\begin{figure}
\begin{algorithmic}
  \For {Every instruction $i$ in the program}
     \State {$exitLabels[i] \gets \bot$}
     \State {$entryLabels[i] \gets \bot$}
  \EndFor
  \While {Not converged}
     \For {Every instruction $i$ in the program}
        \State {$entryLabels[i] \gets \bigsqcup\limits_{p \in predecessors(i)}exitLabels[p]$}
        \State {$exitLabels[i] \gets \textsc{exitConfig}(i, entry)$}
     \EndFor
  \EndWhile
  \State \Return $entryLabels$
\end{algorithmic}
\caption{Frame pointer offset algorithm.  \textsc{exitConfig} is
  described in the main text.}
\label{fig:derive:frame_pointer_alg}
\end{figure}

The static analysis is itself rather simple, and is shown in
Figure~\ref{fig:derive:frame_pointer_alg}.  Each instruction has an
entry label, showing the offset from the frame pointer to the stack
pointer at the start of the instruction, and an exit label, showing
the offset at the end of the instruction.  In addition, each label can
take the special value $\bot$, indicating that nothing is currently
known about the offset at that point, or the value $\top$, indicating
that the offset is not a constant.  The analysis is then structured as
an iteration to a fixed point of two rules: first $entryLabel[i]$ is
set to the union of the exit label's of all of $i$'s predecessors, and
then $exitLabel[i]$ is recomputed based on $entryLabel[i]$ and the
type of instruction at $i$.  \todo{Rewrite.}

A few things remain to define:

\begin{itemize}
\item
  $l \sqcup \bot = l$ and $\bot \sqcup l = l$, for any $l$.
\item
  $l \sqcup \top = \top$ and $\top \sqcup l = \top$, for any $l$.
\item
  $offset(k) \sqcup \mathit{offset}(k) = \mathit{offset}(k)$
\item
  $offset(k) \sqcup \mathit{offset}(k)' = \top$ for any $k \not= k'$.
\item
  The behaviour of $\textsc{exitConfig}(i, l)$ depends on the
  instruction $i$ being examined:

  \begin{itemize}
  \item If $i$ modifies neither the frame pointer nor the stack
    pointer then the result is $l$.
  \item If it sets the frame pointer to be equal to the stack pointer
    plus $k$ then the result is $\mathit{const}(k)$, and likewise if
    it sets the stack pointer to the frame pointer plus $k$ the result
    is $\mathit{const}(-k)$.
  \item If it adds $k$ to the frame pointer and $l$ is
    $\mathit{const}(k')$ then the result is $\mathit{const}(k'+k)$.
    For other values of $l$, the result is just $l$.  Conversely, if
    it adds $k$ to the stack pointer then the result is
    $\mathit{const}(k'-k)$.
  \item Otherwise, for any other modifications to the stack or frame
    pointers, the result is $\top$.
  \end{itemize}
\end{itemize}

Once these rules have converged any $\mathit{const}(k)$ entries in the
resulting table will correctly specify the offset between the stack
and frame pointers at that instruction.  The {\StateMachine} compiler,
described in Section~\ref{sect:derive:compile_cfg}, can then use this
information to remove most references to the frame pointer.

\todo{Need to eval how often this works.}

\subsubsection{Comparison to other aliasing analyses}

\todo{I'm not convinced this adds all that much; might just kill it.}

There is some standard terminology for describing compiler alias
analyses\editorial{Cite Hind 2001.}.  It does not apply perfectly in
this context, but here's an attempt to do so anyway:

\begin{itemize}
\item
  The analysis is partly flow-sensitive.  Two accesses are only
  considered to alias if the control flow of the {\StateMachine}
  allows them to occur in an appropriate order, but higher-order
  control flow dependencies are not tracked.  \todo{I did find a cite
    for another alias analysis which has the same kind of sensitivity,
    but I now seem to have lost it.  I should dig that up again.}
\item
  The analysis is not context-sensitive in the usual optimising
  compiler sense of examining a function's calling context, beyond
  that which has been incorporated into the {\StateMachine}.  On the
  other hand, the {\StateMachine}-level analysis does use context
  information from the static analysis phase.
\item
  The heap is not modelled at all.  Instead, the objects which are
  pointed at are frames on the stack, or a special value indicating
  that a value points at something other than a stack frame.
\item
  Aggregates, as such, do not really exist, and so aggregate modelling
  is not entirely meaningful.  To the extent that it does mean
  something, the analysis distinguishes aggregate fields.
\item
  The analysis operates on whole {\StateMachines}, and hence on small
  fragments of the original program.
\item
  The alias representation is a hybrid, including both an explicit
  alias table and a points-to table. \todo{Could actually build the
    aliasing table lazily, which might be a bit faster but would make
    everything much more confusing; ref demand driven aliasing
    analysis, maybe?}
\end{itemize}


\smh{Ok, some nice text and I definitely feel that I have a better
  understanding of what you've done.  But: (i) still need to define
  some appropriate concepts and terms for them.  Will make text
  easier.  (ii) Need overall ``flowchart'' or similar diagram to
  explain things at a higher level of abstraction.  After this can go
  into details, coming up for air from time to time.  (iii) Use
  running examples to illustrate.}
