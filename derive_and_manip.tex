\section{Discussion of the type of bug which {\technique} considers}
\label{sect:finding_bugs:finding_candidate_bugs:formal_definition}

\todo{This isn't really a good place for this.  It needs to be early,
  but possibly not this early.}

{\Technique} defines a bug to be a tuple $(P, R, W)$, where $P$ is a
program state and $R$ and $W$ are fragments of the program being
investigated which can start in state $P$, such that:

\begin{itemize}
\item[R atomic] If the program starts in state $P$ and executes the
  $R$ atomically then it will not crash before finishing $R$.  If the
  program cannot complete $R$ for some reason then it is assumed to
  not crash.
\item[W atomic] Similarly, if the program starts in state $P$ and
  executes $W$ then $R$ atomically then it will not crash before
  finishing $R$.
\item[Crash possible] If the program starts in state $P$ and then
  executes $R$ and $W$ in parallel, interleaving their instructions,
  it might crash.
\item[W isolation] In all of the executions considered, $W$ does not
  load from any locations which are stored to by $R$.
\end{itemize}

The first three constraints define what is meant by a two-thread
concurrency bug: running the program fragments in a non-concurrent
setting does not lead to a crash, but running them in parallel does.
{\Technique} only considers two-thread concurrency bugs.  It is, of
course, be possible to design a bug which requires the interaction of
three or more threads in order to reproduce, but such bugs tend to be
both rare in practice and extremely difficult to analyse in
practice\needCite{} and so, following common practice\needCite{}, I
completely ignore them here.

The fourth constraint, W isolation, is perhaps more controversial, and
restricts {\technique} to only consider those bugs which are caused by
one thread reading from a structure while another thread modifies it.
This is obviously a somewhat smaller class of bugs than concurrency
bugs in general, but is also much easier to analyse.  \todo{I really
  need to put something in the eval to show how much that costs/buys
  us.}

The definitions of ``program state'' and ``program fragment'' are
perhaps non-obvious.  For {\technique}, a program state consists of
the contents of all of program memory, the set of running threads, and
the value of all per-thread processor registers.  A program fragment
is a finite sequence of instructions which could be executed by some
thread in the program, or, to put it slightly differently, it is a
path through the program's control flow graph.  The length of these
fragments is the primary determinant of the time taken by
{\technique}'s analysis passes, with longer fragments generally taking
far longer to analyse\footnote{But not always; see
  section~\ref{sect:derive:benefits_of_longer_fragments}.}.
{\Technique} therefore limits the number of instructions in these
fragments to some fixed maximum size, referred to as the analysis
window; depending on the mode of operation and the program to be
analysed, this will be on the order of dozens to hundreds of
instructions.

\subsection{Crash summaries}

This analysis produces a series of crash summaries.
Each summary represents a (possibly infinite) set of bug tuples has several components:

\begin{itemize}
\item The read \StateMachine, corresponding to the $R$ member of the bug tuple.
\item The write \StateMachine, corresponding to the $W$ member of the bug tuple.
\item The verification condition, a predicate on the program's state which corresponds to the $S$ member of the bug tuple.
\item An aliasing table, which says which memory-accessing instructions in the read and write \StateMachines might access the same area of memory.
\end{itemize}

A crash summary represents a bug tuple $(R, W, S)$ if the summary's read \StateMachine includes the dynamic instruction trace $R$, its write \StateMachine includes the dynamic instruction trace $W$, and the verification condition is true of $S$.
The set of summaries produced by the analysis is defined to be complete if every possible bug tuple is represented by at least one crash summary, and sound if the crash summaries only represent valid bug tuples.
The analysis presented here is, in that sense, complete, subject to some caveats discussed in later sections, but not sound.

\section{Description of \StateMachines}
\label{sect:derive:description}


The {\StateMachines} themselves consist of three components:

\begin{itemize}
\item
  A slice of the program in a simple analysis language.  Programs in
  this language consist of a directed acyclic graph of analysis
  states, which themselves fall into one of three classes: terminals,
  with no successors; side-effects, with a single successor; or
  choices, with two successors.  The side-effects can express obvious
  program-level effects such as accessing memory or setting a
  particular register, but also less-obvious ones such as register
  aliasing configurations or restrictions on the set of program states
  which must be considered by later analysis steps.  Similarly, the
  expression language used for the conditions in choice states or the
  addresses for memory accessing side-effects can refer to simple
  things like the values of processor registers or the initial
  contents of memory, and can also express queries about the program's
  control flow or the happens-before graph.
\item
  One or two fragments of the program's control-flow graph (CFG),
  corresponding to the program fragments discussed in the bug
  definition in
  section~\ref{sect:finding_bugs:finding_candidate_bugs:formal_definition}.
  In the early analysis phases each {\StateMachine} represents the
  actions of only a single thread, so there is only one CFG fragment;
  later on, they can represent both threads involved in the bug, in
  which case there is one CFG fragment for each thread.  These
  fragments are unrolled such that every instruction in the program
  fragment corresponds to precisely one node in the control-flow
  graph.  They can cross function boundaries, in which case the called
  function is effectively inlined into the caller.
\item
  A table mapping memory access identifiers to sets of nodes in the
  control flow graph. This allows a single memory-accessing side
  effect in the \StateMachine{} to represent several instructions in
  the program, if they have similar effects\editorial{Which only
    really happens if the underlying program is unoptimised, but
    whatever}, or for a single instruction to be represented by
  multiple side-effects when its context is important to its
  operation.
\end{itemize}

The first component is usually by the far the most interesting and so
most examples will show only that, leaving the other two implicit.
All transformations used by {\technique} maintain all three components
in a consistent state.

\begin{figure}
  \begin{minipage}{50mm}
    \begin{subfloat}
      \begin{minipage}{50mm}
\begin{verbatim}
400694: mov    global_ptr,%rax
40069b: test   %rax,%rax
40069e: je     4006ad
4006a0: mov    global_ptr,%rax
4006a7: movl   $0x5,(%rax)
\end{verbatim}
      \end{minipage}
      \caption{Program code}
    \end{subfloat}
    \vspace{50pt}
    \begin{subfloat}
      \hspace{20mm}
      \begin{tikzpicture}
        \node (cfg6) at (0,2) [CfgInstr] {cfg6:400694};
        \node (cfg5) [CfgInstr, below=of cfg6] {cfg5:40069b};
        \node (cfg4) [CfgInstr, below=of cfg5] {cfg4:40069e};
        \node (cfg3) [CfgInstr, below=of cfg4] {cfg3:4006a0};
        \draw[->] (cfg6) -- (cfg5);
        \draw[->] (cfg5) -- (cfg4);
        \draw[->] (cfg4) -- (cfg3);
      \end{tikzpicture}
      \caption{Control-flow graph fragment}
    \end{subfloat}
    \begin{subfloat}
      \begin{tabular}{ll}
        CFG node & Memory access \\
                 & identifier \\
        cfg3     & mai1 \\
        cfg6     & mai2 \\
      \end{tabular}
      \caption{Memory access identifier table}
    \end{subfloat}
  \end{minipage}
  \begin{subfloat}
    \begin{minipage}{30mm}
      \begin{tikzpicture}
        \node (l1) at (0,2) [stateSideEffect] {l1: \state{Load} tmp1 $\leftarrow$ global\_ptr AT mai2 };
        \node (l2) [stateIf, below=of l1] {l2: \state{If} (0 == tmp1)};
        \node (l4) [stateSideEffect, below=of l2] {l4: \state{Load} tmp2 $\leftarrow$ global\_ptr AT mai1 };
        \node (l3) [stateTerminal, right=of l4] {l3: \state{Survive} };
        \node (l5) [stateIf, below=of l4] {l5: \state{If} (BadPtr(tmp2))};
        \node (l6) [stateTerminal, below=of l5] {l6: \state{Crash}};
        \draw[->] (l1) -- (l2);
        \draw[->,ifTrue] (l2) -- (l3);
        \draw[->,ifFalse] (l2) -- (l4);
        \draw[->] (l4) -- (l5);
        \draw[->,ifFalse] (l5) -- (l3);
        \draw[->,ifTrue] (l5) -- (l6);
      \end{tikzpicture}
    \end{minipage}
    \caption{\STateMachine}
  \end{subfloat}
  \label{fig:intro:single_threaded_machine}
  \caption{A fragment of machine code, and the {\StateMachine}
    generated for a bug which leads to a crash at 4006a7.  Circled
    lines leaving an \state{If} state indicate the false branch,
    uncircled ones indicate the true branch.}
\end{figure}

Figure~\ref{fig:intro:single_threaded_machine} shows an example of a
simple single-threaded \StateMachine\footnote{This is the read-side of
  the simple\_toctou test in \S\ref{sect:eval:art:simple_toctou}.}.
It illustrates a simple time-of-check, time-of-use race: the program
loads from \verb|global_ptr| twice in quick succession, validating the
result of the first and using the result of the second.  The
translation to a {\StateMachine} is hopefully reasonably clear: the
control-flow graph covers on the left all of the relevant instructions
and the flow chart on the right expresses the relevant part of their
behaviour.  It is trivial to read off from these diagrams that the
program might crash if some other thread modifies \verb|global_ptr| in
between the two loads and will otherwise survive.

\begin{figure}
  \begin{minipage}{70mm}
    \begin{subfloat}
      \begin{minipage}{50mm}
\begin{verbatim}
4008fb: movq   $0x0,global_ptr
\end{verbatim}
      \end{minipage}
      \caption{Program code}
    \end{subfloat}
    \vspace{50pt}
    \begin{subfloat}
      \hspace{20mm}
      \begin{tikzpicture}
        \node (cfg8) at (0,2) [CfgInstr] {cfg8:4008fb};
      \end{tikzpicture}
      \caption{Control-flow graph fragment}
    \end{subfloat}
    \begin{subfloat}
      \begin{tabular}{ll}
        CFG node & Memory access \\
                 & identifier \\
        cfg8     & mai3 \\
      \end{tabular}
      \caption{Memory access identifier table}
    \end{subfloat}
  \end{minipage}
  \begin{subfloat}
    \begin{minipage}{30mm}
      \begin{tikzpicture}
        \node (l7) at (0,2) [stateSideEffect] {l7: \state{Store} 0 $\rightarrow$ global\_ptr AT mai3 };
      \end{tikzpicture}
    \end{minipage}
    \caption{\STateMachine}
  \end{subfloat}
  \label{fig:intro:single_threaded_machine_write}
  \caption{The other side of the race in
    figure~\ref{fig:intro:single_threaded_machine}. \todo{Looks a bit
      silly.}}
\end{figure}

\begin{sidewaysfigure}
  \begin{tikzpicture}
    \node (lA) [stateIf] { \state{If} $mai2:thread1 \happensBefore mai3:thread2$ };
    \node (lB) [stateSideEffect, below = of lA] { l1: \state{Load} tmp1 $\leftarrow$ global\_ptr AT mai2:thread1 };
    \node (lC) [stateSideEffect, below right = of lA] {l7: \state{Store} 0 $\rightarrow$ global\_ptr AT mai3:thread2 };
    \node (lD) [stateIf, below = of lB] { l2: \state{If} (0 == tmp1) };
    \node (lE) [stateTerminal, below = of lC] { \state{Unreached} };
    \node (lF) [stateIf, below left = of lD] {\state{If} $mai1:thread1 \happensBefore mai3:thread2$ };
    \node (lG) [stateTerminal, below right = of lD] {\state{Survive}};
    \node (lH) [stateTerminal, below right = of lF] {\state{Unreached}};
    \node (lN) [stateSideEffect, below = of lF] {\state{Assert} $global\_ptr = global\_ptr$ };
    \node (lI) [stateSideEffect, below = of lN] {l7: \state{Store} 0 $\rightarrow$ global\_ptr AT mai3:thread2 };
    \node (lJ) [stateSideEffect, below = of lI] {l4: \state{Load} tmp2 $\leftarrow$ global\_ptr AT mai1:thread1 };
    \node (lK) [stateIf, below = of lJ] { l5: \state{If} $BadPtr(tmp2)$ };
    \node (lL) [stateTerminal, below left = of lK] { \state{Crash} };
    \node (lM) [stateTerminal, below right = of lK] { \state{Survive} };
    \draw[->,ifTrue] (lA) -- (lB);
    \draw[->,ifFalse,draw] (lA) -- (lC);
    \draw[->] (lB) -- (lD);
    \draw[->] (lC) -- (lE);
    \draw[->,ifTrue] (lD) -- (lG);
    \draw[->,ifFalse] (lD) -- (lF);
    \draw[->,ifTrue] (lF) -- (lH);
    \draw[->,ifFalse] (lF) -- (lN);
    \draw[->] (lN) -- (lI);
    \draw[->] (lI) -- (lJ);
    \draw[->] (lJ) -- (lK);
    \draw[->,ifTrue] (lK) -- (lL);
    \draw[->,ifFalse] (lK) -- (lM);
  \end{tikzpicture}
  \label{fig:intro:cross_thread}
  \caption{Cross-product of the \StateMachine shown in
    figures~\ref{fig:intro:single_threaded_machine} and
    \ref{fig:intro:single_threaded_machine_write}.}
\end{sidewaysfigure}

\begin{figure}
  \begin{tikzpicture}
    \node (lA) [stateSideEffect] {\state{Assert} $0 \not= InitMemory(global\_ptr)$ and $cfg6:thread1 \happensBefore cfg7:thread2$};
    \node (lB) [stateIf, below = of lA] {\state{If} $cfg3:thread1 \happensBefore cfg7:thread2$ };
    \node (lC) [stateTerminal, below left = of lB] {\state{Survive}};
    \node (lD) [stateTerminal, below right = of lB] {\state{Crash}};
    \draw [->] (lA) -- (lB);
    \draw [->,ifTrue] (lB) -- (lC);
    \draw [->,ifFalse] (lB) -- (lD);
  \end{tikzpicture}
  \label{fig:intro:cross_thread_opt}
  \caption{\STateMachine from figure~\ref{fig:intro:cross_thread}
    after \StateMachine simplification.}
\end{figure}

\STateMachines become somewhat more interesting when they capture the
results of multiple threads.  Figure~\ref{fig:intro:cross_thread}
shows an example of a cross-thread \StateMachine.  There are a couple
of interesting new features here:

\begin{itemize}
\item Several new states have been created and existing ones
  duplicated.  In particular, some memory accesses have now been
  duplicated to multiple places in the \StateMachine.
\item
  $\happensBefore$ expressions.  These allow the \StateMachine to
  query the program's happens-before graph.  $maiA:threadB
  \happensBefore maiC:threadD$ is true precisely when memory access
  $A$ in thread $B$ happens before memory access $C$ in thread $D$.
  These memory accesses will usually correspond to specific
  instructions in the program, but this is not guaranteed.
\item
  \state{Assert} side-effects and the unreached state.  These are used to
  give later stages of the analysis hints about which paths through
  the combined \StateMachine are likely to be interesting.  Assertion
  side-effects indicate that if some condition is false then the
  \StateMachine is not worth analysing; later analysis phases make
  heavy use of these hints.  Likewise, any path which reaches an
  unreached state is considered to be uninteresting.  These two
  mechanisms are of precisely equivalent power; SLI uses one or the
  other depending on which is more convenient at the time.
\item
  Paths in which either the store or load machine end without the
  other starting will end in an unreached state, so they will not be
  considered by the later analysis phases.  While not apparent in this
  simple example, the algorithm used by SLI also uses partial-order
  reduction\needCite{} to further reduce the number of interleavings
  to be considered\editorial{Need to be more precise about that.}.
\end{itemize}

The \StateMachine shown in figure~\ref{fig:intro:cross_thread}
correctly captures the interaction of the two input \StateMachines but
is more complicated than it needs to be.  SLI therefore performs a few
simplifications on the \StateMachine, detailed later, before passing
the \StateMachine to the symbolic execution engine; the results are
shown in figure~\ref{fig:intro:cross_thread_opt}\footnote{Similar
  simplifications are also applied to the single-threaded
  \StateMachines, but those are uninteresting in this case.}.

\STateMachines have a couple of other features not shown in this
example:

\begin{itemize}
\item
  Control-flow expressions.  Much as \StateMachines can query the
  happens-before graph of a program using $\happensBefore$
  expressions, they can also query the control flow within a given
  thread using $Entry$ and $ControlFlow$ expressions.  An
  $Entry(threadA:cfgB)$ expression is true if thread $A$ entered the
  CFG at CFG node $cfgB$, while $ControlFlow(threadA:cfgB{\rightarrow}cfgC)$ is
  true if thread $A$ transitions from CFG node $B$ to node $C$.  Note
  that the value of $ControlFlow$ expression does not depend on where
  in the \StateMachine it is evaluated: the control flow within a
  \StateMachine is (conceptually) independent of that of the original
  program.
\item
  Static analysis side effects.  These are used to import the results
  of the initial whole-program static analysis into the \StateMachine.
  The most important of these is the Alias side-effect, which gives a
  points-to set of a given \StateMachine-level variable.  These
  points-to sets are expressed over stack frames, plus a flag saying
  whether the variable might point at a non-stack location.
\item
  $\Phi$ side-effects.  These are described later when discussing the
  SSA form used; they have a somewhat different semantic from the
  $\Phi$ nodes used in optimising compilers.
\item
  The expression language used to compute values for side-effects
  (such as the value stored by a \state{Store} side effect or the
  address loaded by a \state{Load} one) can include multi-terminal
  binary decision diagrams\editorial{Cite Clark, Fujita et al. 1993}.
  This allows them to efficiently select one of a number of possible
  sub-expressions dependent on a set of boolean predicates.
\item
  Start and end atomic block side-effects.  These indicate that a
  given fragment of the {\StateMachine} should execute atomically, and
  hence restrict the cross-product {\StateMachine}.  They are used
  both to represent instructions with the \verb|LOCK| prefix (which
  execute atomically) and some library-level functions such as
  \verb|pthread_mutex_lock|\editorial{Should probably have a forwards
    ref to discussion of handling library functions.}.
\item
  Initial-memory expressions, written $LD(addr)$.  {\STateMachines}
  can always access the contents of memory as it was when the
  {\StateMachine} started, ignoring any subsequent updates.  It is
  often easier to analyse conditions expressed in terms of these
  expressions than equivalent conditions expressed in terms of
  explicit memory-accessing operations, and the results are usually
  easier to understand.

  Note that the address part of the expression is evaluated against
  the \emph{current} state of the {\StateMachine}, and can therefore
  reference {\StateMachine} variables and so forth, even though the
  load itself is evaluated against a snapshot of the program's initial
  memory.
\end{itemize}

%% They are also similar to the intermediate forms used by model
%% checkers such as SAL\editorial{Cite Park 2000; the Stanford Java
%% checker.}\editorial{Cite JPF and their arguments for not using
%% standard MC intermediate forms; they all apply here as well, and it
%% saves me having to argue it myself.}\editorial{Need to come up with
%% an argument for not just using SAL.}.  The key difference here is
%% less the nature of the intermediate form and more the way in which
%% it is used: SLI \StateMachines only model the part of a program
%% which might conceivably be relevant to some (real or hypothesised)
%% bug, whereas a model checker's intermediate form will usually
%% represent at least some aspect of the entire program component
%% which is to be analysed\editorial{Clumsy.  What I'm trying to say
%% here is that we slice on a different axis: model checkers build up
%% a model of the entire program which is relevant to the predicate
%% which they're checking, whereas SLI builds up a series of models
%% each of which is constrained on both the property (implicit) and
%% the place which we think might have a bug.  Also, describing it as
%% the key difference is kind of misleading: it's the key difference
%% here, but not really the most important one, which is what we use
%% the results for.}.

\section{Deriving \StateMachines}

\todo{This is much bigger than I would like, given that it's not
  actually all that interesting.}

The way in which {\StateMachines} are built depends on the mode in
which {\technique} is being used.  There are three main cases: build
\StateMachines{} for every possible bug in a program, build a single
\StateMachine{} for one bug starting from a core dump, or building a
\StateMachine{} from a deterministic replay system log.  The first of
these is the simplest, and so I discuss it first.

\subsection{Building read-side \StateMachines}

The aim of this phase of the algorithm is to find every fragment of
the program which might act as the first ($R$) element of the bug
tuple.  The simplest approach would be to simply enumerate every
dynamic fragment containing $N_r$ instructions which ends in a
memory-accessing instruction and convert each one independently into a
\StateMachine.  While it would be effective, this would be extremely
time consuming, especially for larger values of $N_r$, and would
perform a large amount of redundant work.  \Technique{} therefore uses
a slightly more involved approach which attempts to share useful work
between traces where possible.  The core idea here is to start with
the instruction at which the program is purported to be able to crash
and then expand backwards from there until every path is properly
represented, up to the limit imposed by the analysis window.  This
allows different paths to a given instruction to be analysed at the
same time, sharing some analysis work.

\subsubsection{Finding potentially-crashing instructions}

The first phase of this algorithm is to find all instructions in the
program which might crash, which is generally straightforward given
the information available at this stage.  {\Implementation} considers
two types of crash:

\begin{itemize}
\item Assertion-failure type crashes.  These are caused by the program
  calling a function such as \verb|__assert_fail| or \verb|abort|
  provided by an operating system library.  Finding such functions is
  generally straightforward given the usual dynamic linker
  information, and the initial whole-program static analysis phase can
  then find all callers of those functions\editorial{Forward ref}.
\item Bad pointer dereferences.  Any memory-accessing instruction
  could potentially dereference a bad pointer, and so
  {\implementation} simply enumerates all memory accessing
  instructions discovered by the initial static analysis.
\end{itemize}

These potentially-crashing instructions are then considered
independently in turn.

\subsubsection{Building the static control-flow graph}
The next step, once a potentially-crashing instruction has been
selected for investigation, is to build a static control-flow graph
containing all of the instructions which might appear in the analysis
window.  This is done by starting with a trivial CFG containing just
the crashing instruction and then expanding it backwards, one
instruction at a time, until every needed instruction has been
discovered.

\begin{figure}
\begin{algorithmic}[1]
\State $depth \gets 0$
\State $pendingAtDepth \gets \queue{targetInstrAddress}$
\State $result \gets \map{}$
\While{$depth < N_r$}
  \State $pendingAtNextDepth \gets \queue{}$
  \While{$\neg{}empty(pendingAtDepth)$}
    \State $currentInstr \gets pop(pendingAtDepth)$
    \If {$result \textrm{ has entry for } currentInstr$}
      \State \textbf{continue}
    \EndIf
    \State $current \gets \text{decode instruction at } currentInstr$
    \State $\mapIndex{result}{currentInstr} \gets current$
    \State $predecessors \gets \text{predecessors of } currentInstr$
    \State Add $predecessors$ to $pendingAtNextDepth$
  \EndWhile
  \State $pendingAtDepth \gets pendingAtNextDepth$
  \State $depth \gets depth + 1$
\EndWhile
\end{algorithmic}
\caption{Building a read-side static control flow graph within a
  single function. \todo{This is just depth-limited BFS; maybe drop
    the explicit algorithm?}}
\label{fig:derive:static_read_cfg_single_function}
\end{figure}

The simple case is that all of the needed instructions are contained
within a single instruction.  In that case, the algorithm is as shown
in figure~\ref{fig:derive:static_read_cfg_single_function}.  This
simply implements a depth-limited breadth-first search starting at the
potentially-crashing instruction and exploring backwards through the
program's control flow.  Note that this can result in a CFG with
multiple roots.

There is a slight subtlety on line 13, which determines the
predecessors of a given instruction.  This is not always obvious,
given only a binary program, for three reasons:

\begin{itemize}
\item
  The program might contain indirect branches.  It is difficult to
  determine statically where these might branch to.  A conservative
  approach would be to assume that they might branch anywhere, but
  this leads to unmanageably complex CFGs even for trivial programs.
  At the same time, ignoring them completely means that many important
  program paths will be missed.
\item
  The AMD64 instruction set includes variable-length instructions, and
  so there might be several overlapping instructions which all finish
  at the start of the instruction currently being investigated.  In
  most programs, only one of these will ever be executed, and it is
  important to pick the right one.
\item
  It is not always possible to identify which parts of a program are
  instructions and which data, which makes it difficult to determine
  whether a given sequence of bytes which happens to have the right
  format to be a branch instruction should be treated as one.
\end{itemize}

{\Implementation} solves this problem using a combination of static
and dynamic analysis.  First, the dynamic analysis tracks the targets
of all indirect branch and call instructions.  This makes the first
problem trivial (assuming that the dynamic analysis is complete).
This information also includes a list of all of the functions in the
program which are called by the operating system or by library
functions\footnote{Shared libraries, in the usual model, cannot
  statically assume anything about the memory layout of the program
  which they are to be linked against, and so all branches from a
  shared library into the main program will be indirect.}.  Knowing
all entry points into the main program, plus all branches within the
main program, is sufficient for a simple static analysis to enumerate
every instruction in the main program, and this then allows the second
and third problems to be solved as well.

\todo{But that doesn't quite work for types of run-time generated code
  other than shared libraries.  Might need to say something about
  that.}

\subsubsection{Handling loops in the CFG}

There may be loops in the CFGs generated by this algorithm, but
{\technique} requires that the {\StateMachines} be finite and acyclic.
These loops must therefore be eliminated in a way which is guaranteed
to preserve all paths of length $N_r$.  The approach {\technique}
takes is to unroll the loops, duplicating instructions as necessary,
until every path from a root of the CFG to the target instruction is
either free from cycles or of length greater than $N_r$.

\begin{figure}
\begin{tikzpicture}
  [node distance=1 and 0.3]
  \begin{scope}
    \node (A) at (0,2) [CfgInstr] {A};
    \node (B) [CfgInstr] [below=of A] {B}; 
    \node (C) [CfgInstr] [below=of B] {C}; 
    \node (D) [CfgInstr] [below=of C] {D}; 
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C.east) to [bend right=90] (B.east) node (edge1) [right] {};
    \begin{pgfonlayer}{bg}
      \node (box1) [fill=black!10,fit=(A) (B) (C) (D) (edge1)] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=4cm]
    \node (A) at (0,2) [CfgInstr] {A};
    \node (B) [CfgInstr] [below=of A] {B}; 
    \node (C) [CfgInstr] [below=of B] {C}; 
    \node (D) [CfgInstr] [below=of C] {D};  
    \node (C') [CfgInstr] [right=of C] {C'};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (B) to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B);
    \begin{pgfonlayer}{bg}
      \node (box2) [fill=black!10,fit=(A) (B) (C) (D) (C')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=8cm]
    \node (A) at (0,2) [CfgInstr] {A};
    \node (B) [CfgInstr] [below=of A] {B};
    \node (B') [CfgInstr] [right=of B] {B'};
    \node (C) [CfgInstr] [below=of B] {C};
    \node (D) [CfgInstr] [below=of C] {D};
    \node (C') [CfgInstr] [right=of C] {C'};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B');
    \begin{pgfonlayer}{bg}
      \node (box3) [fill=black!10,fit=(A) (B) (C) (D) (C') (B')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=12cm]
    \node (A) at (0,2) [CfgInstr] {A};
    \node (B) [CfgInstr] [below=of A] {B};
    \node (B') [CfgInstr] [right=of B] {B'};
    \node (C) [CfgInstr] [below=of B] {C};
    \node (C') [CfgInstr] [right=of C] {C'};
    \node (C'') [CfgInstr] [right=of C'] {C''};
    \node (D) [CfgInstr] [below=of C] {D};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') -- (C');
    \draw[->] (C'') to [bend right=10] (B');
    \draw[->] (B') to [bend right=10] (C'');
    \begin{pgfonlayer}{bg}
      \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
    \end{pgfonlayer}
  \end{scope}
  \draw[->,thick] (box1) -- (box2) node [above,midway] {duplicate C};
  \draw[->,thick] (box2) -- (box3) node [above,midway] {duplicate B};
  \draw[->,thick] (box3) -- (box4) node [above,midway] {duplicate C'};
  \draw[->,thick] (box4) -- +(2.5,0) node [above,midway] {...};
\end{tikzpicture}
\label{fig:cyclic_cfg}
\caption{A CFG containing a cycle.}
\end{figure}

\begin{figure}
\begin{tikzpicture}
  [node distance=1 and 0.3]
  \node (A) at (0,2) [CfgInstr] {A};
  \node (B) [CfgInstr] [below=of A] {B};
  \node (B') [CfgInstr] [right=of B] {B'};
  \node (C) [CfgInstr] [below=of B] {C};
  \node (C') [CfgInstr] [right=of C] {C'};
  \node (C'') [CfgInstr] [above right=of B'] {C''};
  \node (D) [CfgInstr] [below=of C] {D};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) -- (D);
  \draw[->] (C') -- (B);
  \draw[->] (A) -- (B');
  \draw[->] (B') -- (C');
  \draw[->] (C'') -- (B');
  \begin{pgfonlayer}{bg}
    \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
  \end{pgfonlayer}
\end{tikzpicture}
\label{fig:unrolled_cyclic_cfg}
\caption{Fully unrolled version of the CFG in
  figure~\ref{fig:cyclic_cfg}, preserving all paths of length six or
  fewer instructions.  Note that an additional root has been
  introduced at C''.}
\end{figure}

As an example, consider the CFG shown at the left of
figure~\ref{fig:cyclic_cfg}, which contains a loop between
instructions B and C.  This loop must be removed from the CFG while
maintaining all paths which terminate at D and which contain $N_r$
instructions.  The algorithm starts by performing a depth-first
traversal backwards through the graph from D until it finds an edge
which closes a cycle.  In this case, that is the edge from C to B.
SLI will therefore break this edge by duplicating the instruction at
the start of the edge, C, along with all of its incoming edges (in
this case, just the B to C edge).  The C to B edge can then be
redirected to be from C' to B, producing the next diagram in the
sequence.  All paths which were possible in the old graph will also be
possible in the new one, if duplicated nodes are treated as
semantically equivalent, and the loop is now one instruction further
away from the target instruction D.  The process then repeats, moving
the cycle steadily further and further away from D until all paths
ending of length $N_r$ ending at D are acyclic, at which point the
cycle can be safely removed from the graph.

Note that the edge which is modified is the back edge, from C to B,
which points ``away from D'', and not the forwards edge from B to C.
Trying to break the B to C edge would have moved the cycle away from A
rather than away from D, which would not be helpful.

\begin{figure}
\begin{algorithmic}[1]
  \While {graph is not cycle-free}
     \State $edge \gets \textsc{findEdgeToBeBroken}(targetInstr)$
     \If {$edge$ is at least $N_r$ instructions from target instruction}
        \State {Erase $edge$ from graph}
     \Else
        \State {$newNode \gets$ duplicate of $edge.source$}
        \For {$i$ incoming edge of $edge.source$}
           \State {Create a new edge from $i.source$ to $newNode$}
        \EndFor
        \State {Replace $edge$ with an edge from $newNode$ to $edge.destination$}
     \EndIf
  \EndWhile
\end{algorithmic}
\caption{Loop unrolling and cycle breaking algorithm.
  \textsc{findEdgeToBeBroken} simply performs a depth-first search of
  the graph backwards from $targetInstr$ and returns the first edge
  which completes a cycle.}
\label{fig:derive:read:unroll_cycle_break}
\end{figure}

This algorithm is guaranteed to preserve all paths of length $N_r$
which end at the target instruction.  There are only two places in the
algorithm which remove existing edges, so consider each in turn.  The
first is the erasure on line 4.  This can only ever affect edges whose
shortest path to a target is at least $N_r$ instructions long, and so
cannot eliminate any paths to a target of length $N_r$, and is
therefore safe.  The other is the replacement step at line 10, which
replaces an edge from $edge.source$ to $edge.destination$ with one
from $newNode$ to $edge.destination$.  This is safe provided that
every path to $newNode$ has a matching path to $edge.source$, which is
ensured by duplicating all of $edge.source$'s incoming edges to
$newNode$.  At the same time, no additional paths will be introduced,
because every path to $newNode$ has a matching path to $edge.source$.

\todo{Is it worth doing a proof of termination as well?}

\subsubsection{Handling cross-function CFGs.}

There is, of course, no particular guarantee that the entire trace
will be contained in a single function, and SLI must correctly handle
these cross-function CFGs.  It does so by partially inlining functions
as necessary to restore the problem to the single-function case.  This
means that instructions must be labelled by both the pointer of the
instruction itself and also by its inlining context, which is simply
the stack of functions into which it has been inlined\editorial{Not
  terribly clear.}.  The only slight complications are that the
inlining context is not necessarily known when CFG exploration starts,
and it might be necessary to consider the same instruction in several
contexts.

There are two important cases to consider: backing into another
function, where the exploration starts in one function and must be
extended backwards into the end of another one, and backing out of
one, where the exploration starts in one function and must be extended
to include that function's callers.  Backing into a function is
simple: the analysis finds the functions which are to be
called\footnote{There might be more than one function if the
  instruction is a dynamic call.  In that case, the dynamic analysis
  is used to find the set of all possibly-called functions and they
  are all treated as possible predecessors.}, finds all of the return
instructions in those functions, and treats those as the predecessor
of the current instruction with an appropriately extended inlining
context.

Backing out of a function is more complex.  In this case, the analysis
must consider all possible callers of the target function and inline
the target function into each.  

\todo{As I write this I realise that the way I've done this is really,
  really stupid.  I should probably fix that before writing any more
  about it.}

Tail calls do not require any particular special handling here.  If
the exploration reaches the start of a function then all branches to
that instruction will be treated as possible predecessors of it,
regardless of whether they come from call instructions or simple
branch instructions.

\subsubsection{Generating CFGs from core dumps}

Rather than trying to find all of the potential bugs in a program, SLI
can instead be used to investigate a specific bug which has been
reproduced to produce a core dump.  The procedure used is identical
except for the way in which read-side control-flow graphs are
generated, and so it is described here.  In this targeted mode, SLI
does not attempt to generate every possible read-side CFG, but instead
just generates those which might lead to the bug exhibited in the core
dump.  The core dump contains two critical pieces of information:

\begin{itemize}
\item
  The instruction which crashed.
\item
  The processor stack at the time of the crash.
\end{itemize}

Knowing the instruction which crashed constrains the CFG in an obvious
way, but the processor stack is moderately mode difficult to analyse.
In principle, it tells us where each currently-active function was
called from, which would usefully constrain the set of predecessors,
but extracting this information from a program binary without debug
information and without a frame pointer is non-trivial.  SLI has two
strategies for solving this problem:

\begin{itemize}
\item
  A static analysis, run on the binary before attempting to analyse
  the core dump, which attempts to map from instruction addresses to
  the offset between the current stack pointer and the address of the
  current function's return address.  When this analysis succeeds it
  makes it trivial to determine from the core dump where the function
  will return to, and hence where it was called from, but it will not
  always succeed.  In particular, the \verb|alloca| function can cause
  that offset to change at run-time, and so no static analysis will
  ever succeed.
\item
  An abstract interpreter, which attempts to interpret the program's
  machine code forwards from the point of the crash to determine what
  it would have done if it hadn't crashed.  This proceeds until it
  reaches a \verb|ret| instruction, at which determining the return
  address is again trivial.
\end{itemize}

\todo{Need to talk about when you use each, and why you need both.
  Which is potentially awkward, because I've forgotten myself.}

Knowing the contents of the call stack at the time of the crash
effectively tells us what the correct inlining context to use is,
which can then be used to constrain the backing-out-of-function case
discussed above.

\subsubsection{Expanding the CFG}
\label{sect:derive:benefits_of_longer_fragments}

\todo{Expanding the fragment sometimes makes things go faster if it
  means you get some useful context.  I'm not sure whether I want to
  discuss that (the implementation doesn't really make any use of it),
  but if I do it'll go here.}

\subsubsection{Compiling the CFG to a \StateMachine}
\label{sect:derive:compile_cfg}

\todo{Should mention that I use libVEX to convert instructions to a
  slightly saner intermediate format before converting to
  \StateMachine fragments, rather than doing it directly.}

The result of the previous phases is a control-flow graph containing
all of the instructions which might be present in the $R$ fragment of
the crash tuple, and this CFG must now be compiled into a
\StateMachine.  Were it not for the transformations performed in the
loop unrolling phase this would be straightforward: simply decode each
instruction in isolation and convert it into an appropriate fragment
of \StateMachine, and then stitch all of the fragments together again
in the obvious way.  The unrolling process introduces three major
complications:

\begin{itemize}
\item
  Some edges will be erased from the CFG, so that the program can
  branch from instruction A to instruction B but the CFG does not
  allow that to happen.  These are simply converted to branches to the
  special \state{Unreached} state, reflecting the fact that these
  paths are of no interest to the rest of the analysis.

\item
  Some additional edges will have been introduced which do not
  correspond to anything in the original program.  In the example,
  instruction A had a single successor, B, in the original program,
  but has multiple successors in the unrolled CFG.  SLI handles these
  by converting them into \StateMachine-level control flow using
  $ControlFlow$ expressions, so that the \StateMachine fragment for A
  will be something like ``if ($Control(A{\rightarrow}B)$) \{fragment for B\} else
  \{fragment for B'\}''.

\item
  Likewise, the CFG can sometimes have multiple roots.  In this case,
  the first state of the \StateMachine will be a test on the special
  $Entry()$ expressions which will select an appropriate fragment of
  \StateMachine to start with.  In the example, the first state will
  be ``if ($Entry(A)$) \{fragment for A\} else \{ fragment for
  C''\}''.
\end{itemize}

As a somewhat unrealistic example, suppose that the CFG in
figure~\ref{fig:cyclic_cfg} had been generated from a program
something like this:

\begin{verbatim}
A: MOV rdx -> rcx
B: LOAD *(rcx) -> rcx
C: JMP_IF_NOT_EQ *(rcx + 8), 0, B
D: STORE $0 -> *(rcx)
\end{verbatim}

The \verb|JMP_IF_NOT_EQ| instruction is supposed to indicate that
\verb|C| loads from the memory at \verb|rcx+8|, jumping to \verb|B| if
it is non-zero and proceeding to \verb|D| otherwise.  This will
produce an unrolled CFG as in figure~\ref{fig:unrolled_cyclic_cfg}, as
already discussed, and a {\StateMachine} as shown in
figure~\ref{fig:state_machine_for_cyclic_cfg}.

At this stage special side-effects are added to the {\StateMachine} to
represent the results of the earlier whole-program static analysis.
Discussion of these effects is deferred to
section~\ref{sect:alias_analysis} which describes the static analysis
which generates them and the {\StateMachine} simplifications which use
them.

\begin{figure}
\begin{tikzpicture}
  \node[stateIf,initial] (l1) {If $Entry(A)$};
  \node[stateSideEffect,below left = of l1] (l2) {A: Copy $rdx$ to $rcx$};
  \node[stateIf,below = of l2] (l3) {If $\controlEdge{A}{B}$};
  \node[stateSideEffect,below = of l3] (l4) {B: Load $rcx$ to $rcx$};
  \node[stateSideEffect,below = of l4] (l5) {C: Load $rcx+8$ to $tmp$};
  \node[stateIf,below = of l5] (l6) {If $tmp = 0$};
  \node[stateIf,below = of l6] (l7) {D: If $BadPtr(rcx)$};
  \node[stateSideEffect,below right = of l3] (l8) {B': Load $rcx$ to $rcx$};
  \node[stateSideEffect,below = of l8] (l9) {C': Load $rcx+8$ to $tmp$};
  \node[stateIf,below = of l9] (l10) {If $tmp = 0$};
  \node[stateSideEffect,below right = of l1] (l11) {C'': Load $rcx+8$ to $tmp$};
  \node[stateIf,below = of l11] (l12) {If $tmp = 0$};
  \node[stateTerminal,below left = of l7] (lBeta) {Crash};
  \node[stateTerminal,below right = of l7] (lGamma) {Survive};
  \node[stateTerminal,right = of lGamma] (lAlpha) {Unreached};
  \draw[->,ifFalse] (l1) -- (l2);
  \draw[->,ifTrue] (l1) -- (l11);
  \draw[->] (l2) -- (l3);
  \draw[->,ifFalse] (l3) -- (l8);
  \draw[->,ifTrue] (l3) -- (l4);
  \draw[->] (l4) -- (l5);
  \draw[->] (l5) -- (l6);
  \draw[->,ifFalse] (l6) -- (lAlpha);
  \draw[->,ifTrue] (l6) -- (l7);
  \draw[->,ifFalse] (l7) -- (lGamma);
  \draw[->,ifTrue] (l7) -- (lBeta);
  \draw[->] (l8) -- (l9);
  \draw[->] (l9) -- (l10);
  \draw[->,ifFalse] (l10) -- (lAlpha);
  \draw[->,ifTrue] (l10) -- (l4);
  \draw[->] (l11) -- (l12);
  \draw[->,ifFalse] (l12) -- (lAlpha);
  \draw[->,ifTrue] (l12) -- (l8);
\end{tikzpicture}
\caption{{\STateMachine} generated from the CFG shown in figure~\ref{fig:cyclic_cfg}.}
\label{fig:state_machine_for_cyclic_cfg}
\end{figure}

\subsubsection{Stack canonicalisation}

The aim of the {\StateMachine} building algorithm is to share work
between different contexts in which the crashing instruction is found
by combining the contexts into a single {\StateMachine}.  This is much
easier if the we can arrange for the stack pointer to always have the
same value at the end of the {\StateMachine}, regardless of where the
{\StateMachine} starts, so that local variable accesses in the
function containing the purported crashing instruction match up
properly.  This is not completely trivial if the starting points are
themselves in different functions with different inlining contexts.
This problem can be solved by examining the generated {\StateMachine}
and determining, for each entry point, how the stack pointer changes
between that entry point and the end and then inserting an opposite
change immediately before the entry point.  Most of the time, the only
change to the stack pointer is adding or subtracting a constant, and
so this is easy.  Otherwise, it fails.  The fact that the stack has
been massaged in this way is recorded in the {\StateMachine} structure
so that it can be undone later when building data-dependent crash
enforcers.

\todo{Possibly interesting: this means that once you've converted to
  SSA, the initial generation of the stack pointer refers to its value
  at the \emph{end} of the machine, whereas for every other register
  it refers to the value at the \emph{beginning}.}

\subsubsection{Conversion to SSA}
\label{sect:ssa}

{\STateMachines} are maintained in a variant of static single
assignment (SSA) form.  SSA form is a standard compiler intermediate
representation in which each variable has at most one static
assignment\needCite{}.  Variables which are assigned to multiple times
are converted into families of related variables (usually referred to
as ``versions'' or ``generations'' of the variable), each of which is
assigned to precisely once.  This has the effect of breaking up the
live ranges of long-lived variables, which can expose other useful
optimisations.  Most uses of the original variable will be converted
into references to a specific member of one of these families; the
only case in which this is not possible is where the correct member to
use depends on the program's control flow, and in that case special
$\Phi$ nodes are inserted into the program which select an appropriate
member depending on the immediately proceeding control flow.  These
$\Phi$ nodes are themselves unrealisable with reasonable overhead on
most hardware, and so the program must be converted back from SSA form
after being optimised and before being lowered to machine code.

Many of the compiler optimisations for which SSA is helpful are also
useful in the kinds of analyses used by SLI to determine whether a
program might crash, and so SLI also converts its {\StateMachines}
(which are analogous to a compiler's intermediate representation) into
SSA form.  The precise semantics of the SSA form are, however, very
slightly different to the more conventional one: whereas a
compiler-style $\Phi$ node examines the program's preceding control
flow and maps from incoming control-flow edges to input variables, an
SLI one examines the order in which variables have been assigned to
and selects whichever was updated most recently (from a specified
set).  This has several important implications:

\begin{itemize}
\item
  Converting this form of SSA back into a non-SSA form can sometimes
  requires additional temporary variables to record which version of a
  particular variable has been most recently assigned to, whereas the
  more conventional control-flow based form does not.  This would be
  an irritation in a compiler, but is not a problem for SLI, which
  never has to perform that conversion.
\item
  A {\StateMachine}'s control flow graphs can be modified without
  needing to update $\Phi$ nodes.  For example, suppose that a
  {\StateMachine} is as shown on the left of
  figure~\ref{fig:ssa_cfg1}, and suppose that further analysis shows
  that the assignment of $z$ is dead.  We would like to remove the
  assignment and turn the {\StateMachine} into the one shown on the
  right.  This is correct as-is using SLI's $\Phi$ semantics.  If a
  simple control-flow based definition of $\Phi$ were used instead
  then we would also need to convert the $\Phi$ node at l1 into $x_3 =
  \Phi(x_1, x_2, x_2)$, as the l1 state now has three control-flow
  predecessors.  There are, of course, many solutions to this problem
  in the standard compiler literature\needCite{}, but all add
  complexity which is unnecessary and unuseful in this context.
\end{itemize}

Most algorithms for converting to SSA form will work equally well with
either form, including that used by \implementation, and so no details
are given here; see \needCite{} for more information\editorial{blah}.

\todo{I'd be surprised if I'm the first person to come up with this...}

Note that while {\StateMachine}-level variables, including registers,
are converted to single static assignment form, memory accesses are
not.  In this respect {\technique} follows the same pattern as most
SSA-based compilers, including LLVM\needCite{} and gcc\editorial{I
  \emph{think} gcc does this; should probably check that.}.  This is
because converting memory accesses to SSA form requires first solving
the aliasing problem and determining when two memory accesses refer to
the same memory location, and this is impossible in the general case.
It is, of course, possible to identify some interesting cases in which
memory SSA is both possible and useful, in the style of
\editorial{cite Van Emmerik 2007}; {\technique}'s alias analysis pass,
described in section~\ref{sect:alias_analysis} can be seen as doing
something very similar to this.

\begin{figure}
\begin{tikzpicture}
  \node (start) {start};
  \node [below right=of start] (b) {$x_2 = 6$};
  \node [below = of b](c) {if ($\ldots$)};
  \node [below = of c] (d) {$y_1 = 1$};
  \node [below right = of c] (e) {$y_2 = 2$};
  \node [below = of d] (f) {$z = 3$};
  \node [left = of f] (a) {$x_1 = 5$};
  \node [below = of a] (g) {l1: $x_3 = \Phi(x_1, x_2)$};
  \draw[->] (start) -- (a);
  \draw[->] (start) -- (b);
  \draw[->] (a) -- (g);
  \draw[->] (b) -- (c);
  \draw[->] (c) -- (d);
  \draw[->] (c) -- (e);
  \draw[->] (d) -- (f);
  \draw[->] (e) -- (f);
  \draw[->] (f) -- (g);
\end{tikzpicture}
\begin{tikzpicture}
  \node (start) {start};
  \node [below right=of start] (b) {$x_2 = 6$};
  \node [below = of b](c) {if ($\ldots$)};
  \node [below = of c] (d) {$y_1 = 1$};
  \node [below right = of c] (e) {$y_2 = 2$};
  \node [left = of d] (a) {$x_1 = 5$};
  \node [below = of a] (g) {l1: $x_3 = \Phi(x_1, x_2)$};
  \draw[->] (start) -- (a);
  \draw[->] (start) -- (b);
  \draw[->] (a) -- (g);
  \draw[->] (b) -- (c);
  \draw[->] (c) -- (d);
  \draw[->] (c) -- (e);
  \draw[->] (d) -- (g);
  \draw[->] (e) -- (g);
\end{tikzpicture}
\caption{Optimising an SSA-form machine}
\label{fig:ssa_cfg1}
\end{figure}

\todo{Not actually sure how interesting this is, now that I've written it down.}

\subsection{Building write-side \StateMachines}
\label{sect:derive:write_side}

The definition of bug tuples given in
section~\ref{sect:finding_bugs:finding_candidate_bugs:formal_definition}
is existentially quantified over all write thread sequences of length
$N_w$ instructions.  As with the read-side {\StateMachines}, it would
be possible to enumerate all of these and attempt to analyse each, but
this would be extremely inefficient, and so {\technique} uses a
slightly more sophisticated approach.  The first observation required
here is that a write-side {\StateMachine} is only interesting if it
can influence the behaviour of the read-side {\StateMachine} in some
way, which means that the read-side {\StateMachine} must load from at
least one location which is stored to by the write-side
{\StateMachine}.  The dynamic aliasing model can find, for any
memory-loading instruction, a list of memory-storing instructions
which might access the same memory location from another thread, and
so find all of the stores which might influence the read-side
{\StateMachine}.  It is then sufficient to consider just those
write-thread traces which are of length less than or equal to $N_w$
and which start and end with one of these interfering accesses, and
this is usually a far smaller set.

It is obvious why the traces of interest must end with an interfering
store, as anything after the final store definitely cannot influence
the read-side {\StateMachine} and hence cannot influence whether the
program crashes.  It is perhaps less obvious why the trace must also
start with an interfering access: instructions before the first
interfering one can influence the behaviour of the interfering
instructions, and hence potentially the read-side {\StateMachine}.
This is not a problem because of the W isolation assumption discussed
in
section~\ref{sect:finding_bugs:finding_candidate_bugs:formal_definition},
which requires that the write {\StateMachine} never load any locations
stored to by the read {\StateMachine}.  That means that the prefix of
the write thread before the first interfering instruction cannot
possibly have any effect on the concurrent behaviour of the program,
and so only influences the sequential behaviour of the write thread.
In the absence of any other information {\technique} makes no
assumptions about the context in which {\StateMachines} operate and so
discarding this information is always safe.  On the other hand, it
might increase the complexity of the rest of the analysis, and so
{\implementation} will sometimes re-introduce such context
instructions according to some heuristics\editorial{Explain those
  heuristics somewhere.}.  \todo{Not sure how clear that para is.}

The procedure for building write-side {\StateMachines} is then a
variant of that used for building read-side ones: find all of the
potentially interfering store instructions, build an acyclic CFG which
includes all traces of appropriate length which start and end with an
interfering store, and then compile the CFG down to a {\StateMachine}.
The details are, however, slightly different, and I describe them in
the next couple of sections.

\subsubsection{Finding relevant stores}

The first phase of building the write-side \StateMachines is to
determine which stores in the program might possibly interfere with
the read-side {\StateMachine}.  As indicated, this is straightforward:

\begin{itemize}
\item
  Remove all of the optional annotations from the read-side
  \StateMachine.  These are the side effects which provide additional
  information to the analysis but do not themselves affect the
  semantics of the \StateMachine, such as \state{Assert} or
  \state{Stack-Layout} side effects.  This can sometimes allow the
  {\StateMachine} to be further simplified, which might allow
  some loads to be removed.  \todo{Do I need an example here?
    It's kind of boring, but the way this is written makes it
    sound a bit mysterious.}
\item
  Enumerate all of the \state{Load} side-effects in the read-side
  {\StateMachine} and map them, via the memory access identifier table
  and the CFG fragment, into memory loading instructions in the original program.
\item
  For each such memory loading instruction, query the dynamic aliasing
  table to find all of memory storing instructions which might access
  the same location while the location is not private to a particular
  thread.
\end{itemize}

Note that \state{Store} operations in the read-side {\StateMachine}
are not considered at this stage, even though {\technique} does handle
some kinds of write-write races.  That is safe, again, because of the
W isolation property: the write thread can never load any locations
written by the read thread, and so if the stored value is ever loaded
it must be via a \state{Load} side-effect in the read thread, and any
potentially interfering stores in remote threads will be detected
because the dynamic aliasing model will report that they alias with
that \state{Load} side-effect.

\subsubsection{Build write-side CFGs}
\todo{This has far more pages than it really deserves, although most
  of them are diagrams, so I guess it's not too bad.}

The input to this phase of the analysis is a set of potentially
relevant static store instructions, and the analysis must build a
collection of acyclic CFGs which cover all possible paths through the
program which start and end with one of those instructions and which
contain at most $N_w$ instructions.  This is easier than building the
read-side CFGs in the sense that both ends of the CFG are ``bounded''
by some well-defined instruction, whereas read-side CFGs potentially
extend arbitrarily far backwards; it is harder in the sense that
write-side CFG builder must also cluster the interfering instructions,
deciding which should be included in a single trace and which analysed
independently, whereas read-side CFGs contain only a single
instruction.  The overall result is that write-side CFGs tend to be
significantly smaller and easier to analyse than read-side ones but
the unrolling algorithm itself is slightly more involved.

The first phase of the algorithm is to build a (possibly) cyclic
fragment of the original program's control flow graph which includes
all instructions which might possibly be included in one of the final
traces.  This is simple: starting from each potentially interfering
instruction, {\technique} explores forwards for $N_w$ instructions,
merges the resulting CFG fragments, and then discards any instructions
which cannot reach an potentially interfering instruction within $N_w$
instructions.  These CFG fragments may cross function boundaries; the
details are the same as those for read-side CFGs in most important
respects, and I do not repeat them here\editorial{Well, the fact that
  you're exploring forwards rather than backwards makes it a bit
  different, but not in an interesting way.}.

The next step is to remove the cycles from the CFG.  As with read-side
CFGs, this is accomplished by duplicating nodes so as to unroll loops
until any path which uses the loop more than once must be longer than
$N_w$ instructions, at which point the loop-closing edges can be
safely discarded.  There is, however, one important difference: in the
read-side CFG, we are interested in any path which terminates at a
specific point, whereas in the write-side CFG we need to preserve any
path which goes between any members of a set of interesting
instructions.  This makes it more difficult to determine when a loop
has been unrolled sufficiently, as it is no longer sufficient to just
check the distance to a nominated target instruction.  {\Technique}
solves this problem by labelling each node in the graph with
information about where it might occur in an interesting path.  This
label contains an entry for every possibly interfering instruction
specifying:

\begin{itemize}
\item
  The number of instructions on the shortest path from that
  interfering instruction to the labelled node (the ``min from''
  distance).
\item
  The number of instructions on the shortest path from the labelled
  node to the interfering instruction or any of its duplicates (the
  ``min to'' distance).
\end{itemize}

The asymmetry, taking the distance from a ``true'' interfering
instruction and to any duplicate of an interfering instruction, is
perhaps surprising.  The key observation is that every path which
starts at a duplicated interfering instruction will have a matching
path which starts at the original interfering instruction, and so the
ones which start at the duplicate instruction are
redundant\footnote{The symmetrical statement is also true: every path
  which ends in a duplicate interfering instruction has a matching
  path which ends at a true interfering instruction.  It would
  therefore also be correct to discard paths which \emph{end} at a
  duplicate interfering instruction.  It would not, however, be
  correct to combine the two observations and discard all paths which
  either start or end with duplicate instructions, as there would then
  be little point in having those duplicate instructions.}  It is
therefore safe to discard any nodes $r$ where

\begin{displaymath}
\min_{s \in \textrm{interfering instructions}}min\_from(s, r) + \min_{s \in \textrm{interfering instructions and duplicates}}min\_to(s, r)
\end{displaymath}

exceeds $N_w$, and the node label makes this quantity trivial to
calculate.  The complete algorithm is then as shown in
figure~\ref{fig:derive:store_cfg_unroll_alg}.

\begin{figure}
\begin{algorithmic}
  \State {Compute initial labelling of graph}
  \For {$t$ in the set of potentially-relevant stores}
    \While {graph rooted at $t$ is not cycle-free}
       \State $edge \gets \textsc{findEdgeToBeBroken}(t, \{\})$
       \State $newLabel \gets \textsc{combineLabels}(\text{current label of } edge.start, \text{current label of } edge.end)$
       \If {$\min_s(newLabel.minFrom(s)) + \min_s(newLabel.minTo(s)) > N_w$}
           \State {remove $edge$}
       \Else
           \State $newNode \gets \text{duplicate } edge.end$
           \For {Edges $e$ leaving $edge.end$}
              \State {Create a new edge from $newNode$ to $e.end$}
           \EndFor
           \State {Set label of $newNode$ to $newLabel$}
           \State {Replace $edge$ with an edge from $edge.start$ to $newNode$}
           \State {Recalculate $min\_from$ for $edge.end$ and its successors, if necessary}
       \EndIf
    \EndWhile
  \EndFor
\end{algorithmic}
\caption{Loop unrolling algorithm for write-side CFGs.}
\label{fig:derive:store_cfg_unroll_alg}
\end{figure}

Note that in this algorithm duplicating a node duplicates its
\emph{outgoing} edges, whereas when building a read-side CFG the
\emph{incoming} edges are duplicated.  This reflects the fact that
write-side CFGs are built up forwards from the interfering
instructions while read-side CFGs are built up backwards from the
target instruction.  \todo{Might need an example to say why that's
  necessary.}

The algorithm relies on two utility functions:

\begin{itemize}
\item \textsc{findEdgeToBeBroken} just finds the closing edge of some
  cycle in the graph.  The precise choice of edge is not
  important\editorial{I \emph{think} it'll converge on the same thing
    regardless, but it might be nice to show that.  It's certainly
    guaranteed to be correct, but confluence would also be a nice
    property.}.  In {\implementation}'s implementation, this is a
  breadth-first search starting from some arbitrarily chosen root of
  the CFG and reporting the first edge to close a cycle.  If the graph
  reachable from that root is acyclic then {\implementation} moves on
  to the next root.  If the sub-graph reachable from every root is
  acyclic then the whole graph is acyclic and nothing more needs to be
  done.
\item \textsc{combineLabels} is also simple, and is responsible for
  computing the label for the new node which would be produced by
  duplicating $edge.end$.  This node will have the same outgoing edges
  as $edge.end$, and so the same $min\_to$ label, and a single
  incoming edge from $edge.start$, so a $min\_from$ label which is
  just $edge.start$'s $min\_from$ with one added to every value.
\end{itemize}

The resulting CFG can then be compiled to a {\StateMachine} in the
same way as a read-side CFG is.  The only major difference is that
write-side CFGs can sometimes contain disjoint components, in which
case each such component is compiled to a separate {\StateMachine}.

As an example, consider this cyclic CFG:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B} edge [in=30,out=-30,loop] ();
  \node (C) [TrueCfgInstr, below=of B] {C};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & min to A & min from A & min to C & min from C & overall min\\
      A & 0 & 0 & 2 & 1 & 0\\
      B & 2 & 1 & 1 & 2 & 2\\
      C & 1 & 2 & 0 & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}

Blue nodes indicate the interfering instructions.  The overall min
column is the minimum min\_to value plus the minimum min\_from one; it
gives the number of edges on the shortest path involving a given node
which starts at a true interfering instruction and ends at any
interfering instruction, whether true or a duplicate.  Assume, for the
purposes of the example, that $N_w$ is five.  A depth-first search
starting at A will find the cycle from B back to itself and attempt to
break that cycle by duplicating B.  The resulting graph will look like
this:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B} edge [in=210,out=150,loop,killEdge] ();
  \node (B1) [NewCfgInstr, right=of B] {B1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,swungEdge] (B1) to [bend left=10] (B);
  \draw[->] (B1) -- (C);
  \draw[->] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & min to A & min from A & min to C & min from C & overall min\\
      A  & 0 & 0 & 2 & 1 & 0\\
      B  & 2 & 1 & 1 & 2 & 2\\
      C  & 1 & 2 & 0 & 0 & 0\\
      B1 & 2 & 2 & 1 & 3 & 3\\
    \end{tabular}
  };
\end{tikzpicture}

New nodes are shown in red, as is the edge which is modified, and
edges which have been removed are shown crossed through.  Notice that
whereas the shortest cyclic path starting at A was previous A,B,B, of
length 3, it is now A, B, B1, B1, of length 4.  Suppose that the next
depth-first iteration discovers the edge from C to A.  The algorithm
will then break this edge by duplicating A:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (A1) [NewCfgInstr,right=of C] {A1};
  \draw[->] (A) -- (B);
  \draw[->,swungEdge] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->] (B1) -- (C);
  \draw[->] (B1) to [bend left=10] (B);
  \draw[->] (C) -- (A1);
  \draw[->,killEdge] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & min to A & min from A & min to C & min from C & overall min\\
      A  & 0 & 0 & 2 & $\infty$ & 0\\
      A1 & 0 & 3 & 2 & 1 & 1\\
      B  & 2 & 1 & 1 & 2 & 2\\
      C  & 1 & 2 & 0 & 0 & 0\\
      B1 & 2 & 2 & 1 & 3 & 3\\
    \end{tabular}
  };
\end{tikzpicture}

Suppose it now selects the B1 to B edge as the cycle-completing edge.
It will then duplicate B:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (B2) [NewCfgInstr, right=of B1] {B2};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (A1) [DupeCfgInstr,right=of C] {A1};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,killEdge] (B1) to [bend left=10] (B);
  \draw[->,swungEdge] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & min to A & min from A & min to C & min from C & overall min\\
      A  & 0 & 0 & 2 & $\infty$ & 0\\
      A1 & 0 & 3 & 2 & 1 & 1\\
      B  & 2 & 1 & 1 & 2 & 2\\
      C  & 1 & 2 & 0 & 0 & 0\\
      B1 & 2 & 2 & 1 & 3 & 3\\
      B2 & 2 & 3 & 1 & 4 & 4\\
    \end{tabular}
  };
\end{tikzpicture}

Again the minimum distance from A to a cycle has increased, from four
to five.  Now duplicate B because of the A1 to B cycle-completing
edge:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (B2) [CfgInstr, right=of B1] {B2};
  \node (A1) [DupeCfgInstr,right=of C] {A1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (B3) [NewCfgInstr, below=of A1] {B3};
  \draw[->] (A) -- (B);
  \draw[->,killEdge] (A1) -- (B);
  \draw[->,swungEdge] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & min to A & min from A & min to C & min from C & overall min\\
      A  & 0 & 0 & 2 & $\infty$ & 0\\
      A1 & 0 & 3 & 2 & 1        & 1\\
      B  & 2 & 1 & 1 & $\infty$ & 2\\
      C  & 1 & 2 & 0 & 0        & 0\\
      B1 & 2 & 2 & 1 & 3        & 3\\
      B2 & 2 & 3 & 1 & 4        & 4\\
      B3 & 2 & 4 & 1 & 2        & 3\\
    \end{tabular}
  };
\end{tikzpicture}

The next cycle-completing edge considered is that from B2 to B1.  In
this case, the new label would have an overall minimum of 5, matching
$N_w$, and so there can be no paths through the new node which start
with an interfering instruction and which end at an interfering
instruction or a duplicate of it, and so the edge is simply deleted:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (B2) [CfgInstr, right=of B1] {B2};
  \node (A1) [DupeCfgInstr,right=of C] {A1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (B3) [CfgInstr, below=of A1] {B3};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->,killEdge] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & min to A & min from A & min to C & min from C & overall min\\
      A  & 0 & 0 & 2 & $\infty$ & 0\\
      A1 & 0 & 3 & 2 & 1        & 1\\
      B  & 2 & 1 & 1 & $\infty$ & 2\\
      C  & 1 & 2 & 0 & 0        & 0\\
      B1 & 2 & 2 & 1 & 3        & 3\\
      B2 & 2 & 3 & 1 & 4        & 4\\
      New label & 2 & 4 & 1 & 5 & 5\\
    \end{tabular}
  };
\end{tikzpicture}

This process iterates, removing one cycle-completing edge at a time,
until the graph is completely acyclic\editorial{I used to have more
  intermediate steps in here, but they were really boring.}:

\begin{tikzpicture}
  \node (A) at (0,2) [TrueCfgInstr] {A};
  \node (B) [CfgInstr, below=of A] {B};
  \node (B1) [CfgInstr, right=of B] {B1};
  \node (B2) [CfgInstr, right=of B1] {B2};
  \node (A1) [DupeCfgInstr,right=of C] {A1};
  \node (C) [TrueCfgInstr, below=of B] {C};
  \node (B3) [CfgInstr, below=of A1] {B3};
  \node (C1) [DupeCfgInstr, below=of B3] {C1};
  \node (B4) [CfgInstr, right=of B3] {B4};
  \node (A2) [DupeCfgInstr, left=of C1] {A2};
  \node (C2) [DupeCfgInstr, below=of B4] {C2};
  \node (C3) [DupeCfgInstr, right=of A1] {C3};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) -- (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) -- (C3);
  \draw[->] (B3) -- (B4);
  \draw[->] (C) -- (A1);
  \draw[->] (C1) -- (A2);
  \draw[->] (B3) -- (C1);
  \draw[->] (B4) -- (C2);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (A2) (B) (B1) (B2) (B3) (C) (C1) (C2) (C3) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
      labels & min to A & min from A & min to C & min from C & overall min\\
      A  & 0 & 0 & 2 & $\infty$ & 0\\
      A1 & 0 & 3 & 2 & 1        & 1\\
      A2 & 0 & 6 & $\infty$ & 4 & 4\\
      B  & 2 & 1 & 1 & $\infty$ & 2\\
      B1 & 2 & 2 & 1 & $\infty$ & 3\\
      B2 & $\infty$ & 3 & 1 & $\infty$ & 4\\
      B3 & 2 & 4 & 1 & 2        & 3\\
      B4 & $\infty$ & 5 & 1 & 3        & 4\\
      C  & 1 & 2 & 0 & 0        & 0\\
      C1 & 1 & 5 & 0 & 3        & 4\\
      C2 & $\infty$ & 6 & 0 & 4        & 4\\
      C3 & $\infty$ & 4 & 0 & $\infty$ & 4\\
    \end{tabular}
  };
\end{tikzpicture}

As desired, the graph has been rendered acyclic while preserving all
paths of length up to five instructions.  As a minor optimisation,
{\implementation} will merge node B2 with B4 and C3 with C2 before
converting the CFG to a \StateMachine, as the nodes are semantically
identical and this results in a slightly simpler
\StateMachine\editorial{Well, kind of.  If you don't bother then the
  {\StateMachine} simplifier will do it for you later, but doing it
  early avoids some overhead.}.

\section{Simplifying {\StateMachines}}

The {\StateMachines} generated by this process are a faithful
representation of all of the instructions which might have been
executed by the program immediately prior to crashing, up to the size
of the analysis window.  As such, they usually contain a large amount
of redundant information which is not relevant to the behaviour being
investigated.  {\Technique} therefore uses a number of related
techniques to simplify them and to eliminate irrelevant fragments.
The important ones are:

\begin{itemize}
\item
  Dead code elimination, to eliminate redundant updates to registers.
  This is particularly effective for eliminating updates to the
  \verb|rflags| register in the AMD64 architecture. This is
  essentially identical to the compiler-level optimisation of the same
  name\needCite{}, and so is not discussed in detail here.
\item
  Register copy propagation, which combines smaller updates to
  registers into a single higher-level operation which is usually
  easier to analyse.  To see why this is necessary, consider
  a fragment of code like this:

\begin{verbatim}
shl (%rax << $4) -> %rax
sub (%rax - $7) -> %rax
mul (%rax * $11) -> %rax
\end{verbatim}
  
  This will produce a three-state {\StateMachine} fragment, with one
  state for each instruction.  It would be more useful to produce a
  single state which set $rax$ to $((rax \times 16) - 7) \times 11$,
  and this simplification performs that transformation.  The algorithm
  used is essentially the same as that employed by dcc\needCite{} and
  so is not described in detail here.

  One minor extension present in {\implementation} but not dcc is that
  {\implementation} can make use of \state{Assert} side-effects during
  this transformation, so that, for instance, if $x$ is asserted to be
  less than $7$ then the expression $x > 22$ can be rewritten to
  $false$.  This does not require any significant changes to the bulk
  of the algorithm, beyond a few simple rules describing when such
  rewrites are valid.  \todo{In other words, a whole bunch of fiddly
    special cases.  This isn't very interesting.}
\item
  {\Technique} attempts to eliminate memory accessing instructions
  using a novel cross-function alias technique which incorporates a
  fast but low-accuracy points-to analysis on the original binary with
  a slower but more accurate alias analysis applied to the
  {\StateMachine}; this is described in
  section~\ref{sect:alias_analysis}.
\item
  {\Technique} attempts to eliminate $\Phi$ expressions from generated
  {\StateMachines} by converting them into multi-terminal binary
  decision diagrams over the {\StateMachine}'s control flow predicates;
  this analysis is described in section~\ref{sect:phi_elimination}.
\item
  The {\StateMachines} generated by {\technique} often contain
  fragments which differ only in variable names; the unification
  simplification, described in section~\ref{sect:unification} attempts
  to unify these.
\item
  \todo{Probably mention that we sometimes optimise on the assumption
    that assertion failures correspond to crashes and sometimes on the
    assumption that correspond to avoiding the crash, and discuss why
    that's necessary.}
\end{itemize}

There are also a number of other minor simplifications:

\begin{itemize}
\item
  Various basic arithmetic simplifications, such as rewriting $x + 0$
  to just $x$ or $(x \happensBefore y) \vee (y \happensBefore x)$ to
  $true$, and constant-folding for most common operators.
\item
  A write-side {\StateMachine} which terminates without issuing any
  \state{Store} operations is unlikely to be useful, and so a simple
  control-flow analysis is used to turn any paths which do so into
  \state{Unreached} states.  Likewise, any fragments of a read-side
  {\StateMachine} which can reach only a single terminal state
  are replaced by that terminal state.
\item
  {\Technique} is interested only in concurrency bugs, but the
  {\StateMachines} generated often contain large fragments which
  operate on purely thread-local state.  These fragments are generally
  uninteresting, and so {\technique} attempts to convert them to
  \state{Assert} states whenever possible.  For example, the program
  fragment

\begin{verbatim}
f(x) {
   a = global_ptr;
   if (x % 7 == 3)
       return;
   assert(a == global_ptr);
}
\end{verbatim}

   might generate a {\StateMachine} something like this:

\begin{verbatim}
1: Load global_ptr to a
2: if x % 7 == 3 then survive else 3
3: Load global_ptr to tmp
4: if a == global_ptr then survive else crash
\end{verbatim}

   flattening the {\StateMachine}'s graph structure to text in the
   obvious way.  The test on \verb|x| is purely local, and so this
   machine can be converted to

\begin{verbatim}
1: Load global_ptr to a
2: Assert x % 7 != 3
3: Load global_ptr to tmp
4: if a == global_ptr then survive else crash
\end{verbatim}

   In and of itself this is not a particularly useful transformation.
   However, the semantics of \state{Assert} states mean that later
   phases of the analysis have much greater flexibility to either move
   or discard such assertions, which can allow further useful
   optimisations.  \todo{I should really come up with a better example
     here.}
\end{itemize}

\subsection{$\Phi$ elimination}
\label{sect:phi_elimination}

The use of SSA form simplifies many parts of the analysis by breaking
up variables into their different live ranges, but complicates some
parts by introducing $\Phi$ side effects, which are themselves quite
difficult to analyse.  The state unification simplification described
in section~\ref{sect:unification} can also introduce further $\Phi$
effects.  The reason that $\Phi$ side effects are hard to analyse is
that their behaviour depends on the {\StateMachine}'s control flow,
and this is not explicitly reified and so is unavailable to the usual
simplification machinery.  The $\Phi$ elimination pass attempts to
rectify this problem by converting the $\Phi$ effects into
multi-terminal reduced ordered binary decision diagrams\needCite{}
(MTBDDs) which take as input the {\StateMachine}'s existing
control-flow predicates which produce as their output the input which
will be selected by the $\Phi$.  The $\Phi$ itself can then be
replaced with this MTBDD, making the control dependence explicit and
simplifying later analyses.

\todo{Having played around a bit, I'm now absolutely convinced that
  using BDDs in a few other places would massively improve
  performance.  Might end up having to do that.  Actually, pulling the
  BDD discussion out a bit earlier might make this a bit clearer,
  anyway. Alternative: just assert that the algorithms exist here and
  put the actual definitions in an appendix.}

\todo{Also need to say that, for the purposes of this analysis, we
  assume that any expressions which aren't physically identical are
  completely independent.  That's definitely safe, but can sometimes
  lead to moderate conservativeness.  Later passes try to tidy things
  up a bit.}

\todo{The only reason this can ever fail is the
  {\StateMachine}-internal typesystem, which I've (deliberately)
  avoided discussing so far, which makes the presentation a little bit
  awkward.}

The approach used is simple: build a mapping from {\StateMachine}
states to boolean BDDs\footnote{For clarity, I refer to BDDs which
  evaluate to either true or false as boolean BDDs or just BBDDs, by
  contrast with MTBDDs which can evaluate to a more complex value set.
  Most existing literature refers to BBDDs as just BDDs.}  which are
true for precisely those executions in which the state will be
executed, extend this into a mapping from states to MTBDDs which show
which input to the $\Phi$ would be selected if the $\Phi$ were issued
immediately after that state, and then simply read off the MTBDD for
the actual $\Phi$ state.  This allows all $\Phi$ states to be
eliminated\editorial{Modulo typing issues, which I'm really going to
  need to come up with a better story for.}, significantly simplifying
later analysis phases.

\subsubsection{Building the control dependence graph}

The control dependence graph (CDG) is a standard compiler data
structure\needCite{} showing which nodes in a control flow graph are
control-dependent on which branch operations.  In other words, it
shows, for each statement, which control-flow conditions must be true
for that statement to execute, which must be false, and which do not
matter.  The control dependence graph used by {\technique} is a slight
extension of this concept: rather than simply listing the expressions
on which the node depends, it gives a BBDD which will evaluate to true
in precisely those executions in which the node executes.  This is
possible because {\technique}'s {\StateMachines} are finite and
acyclic.

The algorithm for building the graph is simple:

\begin{algorithmic}[1]
\State $cdg[root] = const(true)$
\While {Some state is unlabelled in $cdg$}
  \State {$n \gets $ select a state which has no label but whose predecessors are all labelled}
  \State {$cdg[n] \gets \bigvee \{cdg[p, n] \mid p \in \textrm{$n$'s predecessors}\}$}\Comment{Take union of all reaching paths}
\EndWhile
\end{algorithmic}

Here, $cdg[x]$ is the condition for state $x$ to run.  $cdg[x, y]$ is
the condition necessary for the edge from state $x$ to $y$ to be
traversed, defined by:

\begin{displaymath}
cdg[x, y] = \begin{cases}
  \bot                            & \text{if there is no edge from $x$ to $y$} \\
  cdg[x] \wedge x.condition       & \text{if $x$ is a control-flow node and $y = x.trueTarget$} \\
  cdg[x] \wedge {\neg}x.condition & \text{if $x$ is a control-flow node and $y = x.falseTarget$} \\
  cdg[x]                          & \text{otherwise}
\end{cases}
\end{displaymath}

Note that the selection on line 3 is only guaranteed to be possible
because the {\StateMachine} is acyclic\editorial{Is that obvious?  Or
  do I need to go and prove it?}.  This assumption is also the reason
that {\technique}'s CDG is precise, whereas CDGs for full programs
usually contain some approximations.  Apart from that, this algorithm
is obvious: to find the conditions under which a state might execute,
find the conditions under which all of its incoming edges might
execute and take the union.  Once all states are labelled the CDG is
complete.

\subsubsection{Building the $\Phi$ map}

Once the CDG is complete, the next step is to build the $\Phi$ map.
This is a map from states of the {\StateMachine} to MTBDDs which
select which input of the $\Phi$ would be selected if the $\Phi$ were
to be issued immediately after that state.  The complete map then
makes eliminating the $\Phi$ side-effect trivial.  The algorithm for
building the $\Phi$ map is shown in
figure~\ref{fig:derive:phi:phi_map}

\begin{figure}
\begin{algorithmic}[1]
\For {$i$ in inputs to the $\Phi$}\Comment {Build the initial map}
  \State {$s \gets $ state defining input $i$}
  \State {$pm[s] \gets const(i)$}
\EndFor
\While {the $\Phi$ state is unlabelled}
  \State {$n \gets $ select a state which has no label but whose predecessors are all labelled}
  \State {$g \gets \{ (cdg[p, n], pm[p]) \mid p \in \textrm{$n$'s predecessors}\}$}
  \State {$pm[n] \gets \textsc{simplify}(\textsc{flatten}(g), cdg[n])$}
\EndWhile
\end{algorithmic}
\caption{Building the $\Phi$ map}
\label{fig:derive:phi:phi_map}
\end{figure}

The core idea is that of a gated MTBDD (GMTBDD).  This is a set of
(BBDD, MTBDD) pairs where each BBDD is treated as a gate for the
matching MTBDD.  If all of the MTBDDs whose gate evaluates to $true$
evaluate to the same terminal then the GMTBDD evaluates to the same
terminal; otherwise, or if none of the gates evaluate to $true$, the
result of the GMTBDD is undefined.  These GMTBDDs can then be easily
flattened back into MTBDDs using the usual BDD zipping
algorithm\editorial{Kind of.  The point is that the algorithm is (a)
  really easy, (b) a trivial variant of standard algorithms, and (c) a
  pain in the backside to describe because it has lots of nasty edge
  cases.  I've not been able to find any actual cites for it,
  though.}. For each state, the algorithm builds a GMTBDD with an
entry for each edge to the state, setting the gate to the CDG
condition for that edge and the MTBDD to the $\Phi$ map entry for the
predecessor state.  The resulting GMTBDD, once all edges have been
processed, selects the correct $\Phi$ input for the state.  Note that
the construction of the CDG ensures that precisely one gate will be
enabled in any configuration of the input boolean variables, so that
the GMTBDD is always defined.

\todo{I just made up GMTBDDs.  Google doesn't reveal anything with the
  same name, but I suspect that I'm not the first person to come up
  with the idea.  Should go and have a proper look for prior work.}

As a minor optimisation, the algorithm shown simplifies the MTBDD for
state $n$ under the assumption that the CDG condition for $n$ is true;
in other words, under the assumption that $n$ is actually run.  The
\textsc{simplify} function is shown in
figure~\ref{fig:derive:phi_elimination:simplify}.  The basic idea is
to define a $\textsc{implies}(a, b)$ operation which evaluates to $a$
when $b$ is true or the special $\bot$ value when $b$ is false and to
then use this to zip the original MTBDD with the assumption BDD.  This
lifts the original MTBDD $a$ to a new MTBDD which evaluates to $a(k)$
for any configuration $k$ where the assumption is true or to $\bot$
where the assumption is false.  We then define an \textsc{unlift}
operation which removes all paths through the MTBDD to these $\bot$
values, returning the MTBDD to its original domain.  The net effect is
to produce a new MTBDD which evaluates to the same thing as the
original whenever the assumption is true, but potentially tests fewer
input variables while it does so.

As a further minor optimisation, not shown in the figure,
{\implementation} performs an initial reachability test to determine
which states in the {\StateMachine} might eventually reach the $\Phi$
state which is to be eliminated and does not attempt to calculate
labels for any which do not.  The result of the analysis is unchanged.

\begin{figure}
\begin{algorithmic}
\Function{simplify}{$thing:MTBDD(k)$, $assumption:BDD$ $\rightarrow \bot + MTBDD(k)$}
  \State \Return $\textsc{unlift}(\textsc{zip}(thing, assumption, \textsc{implies}))$
\EndFunction
\Function{implies}{$a:k$, $b:bool$ $\rightarrow k + \bot$}
  \If{$b$}
    \State \Return $a$
  \Else
    \State \Return $\bot$
  \EndIf
\EndFunction
\Function{unlift}{$inp:MTBDD(k + \bot) \rightarrow \bot + MTBDD(k)$}
  \If{$inp = const(\bot)$}
    \State \Return $\bot$
  \ElsIf{$inp = const(k)$}
    \State \Return $inp$
  \Else
    \State $if(cond, t, f) \gets inp$
    \State $t' \gets \textsc{unlift}(t)$
    \State $f' \gets \textsc{unlift}(f)$
    \If{$t' = \bot$}
      \State \Return $f'$
    \ElsIf{$f' = \bot$}
      \State \Return $t'$
    \Else
      \State \Return $if(cond, t', f')$
    \EndIf
  \EndIf
\EndFunction
\end{algorithmic}
\caption{The MTBDD $simplify$ algorithm.  The notation $k:t$ indicates
  a variable $k$ of type $t$, $a \rightarrow b$ indicates a function
  from arguments $a$ to a return value of type $t$, $\bot + k$
  indicates type $k$ augmented with the value $\bot$, and $MTBDD(k)$
  is the type of MTBDDs whose terminals are constants of type $k$.
  Not shown: \textsc{unlift} also reduces the MTBDD as it builds it,
  removing any newly-introduced duplicate nodes.}
\label{fig:derive:phi_elimination:simplify}
\end{figure}

\todo{I really don't want to have to define $zip$, because it's fiddly
  and not very interesting, but I can't find a cite for it.}

\todo{Do I need to give the $flatten$ algorithm as well?  It's even
  more tedious and annoying than these ones.}

\todo{Need to think much harder about variable ordering within the
  BDDs.  Currently using pointer order, which is rather silly.  Also
  need to mention that the ordering is consistent within any run of
  the pass, but not necessarily between multiple runs.}

\todo{Might be worth explicitly calling out that the CDG can be shared
  between different $\Phi$ effects, but the $\Phi$ map needs to be
  rebuilt each time?}

%%  This is pretty much just a generalisation of the heuristic used in
%%  Fujita, Matsunaga et al 1991 from BDDs to MTBDDs, so maybe I can
%%  just cite them rather than trying to justify it myself?
  
\subsection{Alias analysis}
\label{sect:alias_analysis}

\todo{Should probably mention that having a DRS log makes all of this
  redundant.}

SLI's {\StateMachines} represent a cross-function slice of the
program's machine code, including a large number of memory-accessing
instructions.  This presents a non-trivial alias analysis problem.
SLI uses several techniques to resolve the resulting aliasing queries:

\begin{itemize}
\item
  First, a dynamic analysis is used to build up a model of how the
  program behaves during normal operation.  This model is generally
  reasonably effective at determining whether instructions which
  access the heap or global data might conflict, but does not contain
  information on accesses to the local stack.
\item
  Next, a static analysis pass is used to determine which instructions
  might access local variables in the current stack frame.  This
  analysis is almost entirely function-local and is applied to every
  function in the program before the main analysis pass starts.
\item
  The results of this static analysis pass are incorporated into the
  {\StateMachines} in a way which automatically extends them to
  accurately reflect cross-function properties of the program.
\item
  The main alias analysis can then be run on the {\StateMachines}
  themselves, in conjunction with the other {\StateMachine}
  simplification passes, to resolve aliasing queries.
\end{itemize}

I now describe each of these phases in more detail.

\subsubsection{Dynamic analysis}

SLI relies on a dynamic analysis pass to build up a model of the
program's behaviour when it is running normally.  This model mostly
focuses on the possible aliasing relationships between memory accesses
outside of the program's stack; in other words, determining whether
two instructions might access the same piece of non-stack memory.
This is a full alias analysis, rather than a points-to analysis, and
so could in principle need to build up a full $O(n^2)$ table showing,
for each pair of instructions, whether those instructions might alias.
Fortunately, that table is rather sparse for most programs, allowing
some significant simplifications to be made.

The intuition behind this analysis is that most fields in most data
structures are accessed by a relatively small number of places in the
program, and so if it were possible to identify the field being
accessed by a given instruction then that would make it easy to
determine whether two instructions might interfere\editorial{I should
  probably find a cite for someone doing that at source level.}.
Unfortunately, that kind of higher-level information is not usually
available when analysing binary programs.  SLI sidesteps that problem
by identifying fields by the set of instructions which might access
them; determining whether two instructions might alias is then a
matter of determining whether they ever appear in the same field
label.  The result is an alias table which can be collected in
reasonable time using a simple dynamic analysis, which can resolve
aliasing queries quickly, and which requires a tolerable amount of
space (tens of megabytes for mysqld, for instance).

The dynamic analysis itself is quite simple.  The program's memory is
divided into eight byte chunks, each of which has a label consisting
of two set of accessing instructions, one for read instructions and
one for write.  Any instruction which accesses that memory chunk adds
itself to the relevant set.  These labels in effect identify the field
for the memory chunk, and so adding an instruction to a set amounts to
re-labelling one of the fields.  Eventually, the memory chunk will be
released, due to the program either terminating or calling a
\verb|free|-like function, and at that point the label is frozen and
added to a global set of possible field labels.  This global set,
suitably indexed, forms the main aliasing table.

Of course, implementing this scheme requires the dynamic analysis tool
to be able to correctly identify \verb|free|-like functions.  Most of
these are standard functions present in system libraries, which can be
identified trivially, but it is also possible for a program to
implement their own memory management routines, and in this case
{\technique} depends on manually identifying these functions in order
to build up a precise aliasing table.  This is the only place in which
{\technique} relies on manual intervention.  If this information is
not available then the aliasing table is likely to be highly
inaccurate and the analysis is likely to take a long time to converge.
It might in some cases be possible to infer the existence of these
functions automatically; see, for example \todo{I was convinced that
  there were some well-known existing papers doing just that, but now
  I can't find them.  Need to dig around some more}.  I have not
investigated doing so at this time.

{\Implementation} includes two minor refinements to this scheme:

\begin{itemize}
\item
  The analysis does not track accesses to the stack, beyond simply
  noting which instructions accessed the stack.  Most programs include
  a very large number of stack-accessing instructions, and so tracking
  them as well would significantly increase the already-significant
  cost of the analysis, and most such accesses are easily resolved
  using other analysis mechanisms, and so doing so would not actually
  be helpful. \todo{Need to eval how much that buys us.  In general,
    need to eval completeness and overheads of the dynamic model.
    Also, this isn't just a performance tweak, but avoids a lot of
    nastiness to do with tail calls, since they kill off the frame but
    aren't obvious in the instruction stream.}
\item
  The analysis attempts to identify accesses which are definitely
  thread-private, and marks them as such in the field labels.  Later
  analysis phases know to ignore these thread-private accesses when
  consider cross-thread aliasing but to use them for intra-thread
  aliasing.  The scheme for identifying them is very simple: a memory
  region returned by \verb|malloc| is marked as thread-private, and
  whenever a pointer to a memory region is stored in non-stack memory
  the region is marked as being thread-shared\footnote{The dynamic
    analysis checks every memory access to make sure that only the
    right thread ever accesses a thread-private memory region, and
    will report an error if another thread accesses it; no such errors
    were reported for any of the test programs.}.
\end{itemize}

\todo{I did have a bit more on context sensitivity here (i.e. looking
  back up the call stack in addition to the current RIP in order to
  identify instructions), but I don't think it's all that interesting.
  It didn't seem to help all that much.}

\subsubsection{Static analysis}

\label{sect:static_analysis}

\todo{On the one hand, this is quite important.  On the other hand,
  it's incredibly tedious.  Not sure what to do about that.}

\todo{Calling this a whole-program analysis is a bit of an abuse of
  terminology, as already discussed.}

The dynamic aliasing analysis is effective at resolving aliasing
queries between instructions which access shared memory, but does not
provided any assistance with instructions which might access the local
stack.  {\Technique}'s solution to that problem has three components.
First, a simple static points-to and escape analysis is applied to
each function in the program, before the main analysis starts, to
characterise which instructions might access that function's stack
frame.  Second, the results of that static analysis are incorporated
into the generated {\StateMachines}.  Finally, an alias analysis is
run on the thus augmented {\StateMachines} to determine whether a
given memory accessing side effect might access the stack, and, if so,
which stack frames it might access.

The initial static analysis examines one function at a time in the
program and determines how it manages its own stack frame, in terms of
whether any pointers to the frame are ever stored into non-stack
memory and which registers contain pointers to the frame.  This then
provides the main analysis with additional context beyond the normal
analysis window.  It would, of course, be possible to obtain the same
information by extending the analysis window, but this is often
expensive, and the use of a cheaper static analysis allows a far
larger context to be plausibly considered.  As an added bonus, the
results of the static analysis can be easily shared between multiple
{\StateMachines}, which is problematic for properties derived from the
{\StateMachines} themselves.

The information needed by the main analysis consists of meta-data
about stack frames:

\begin{itemize}
\item
  Which frames are active when the {\StateMachine} starts;
\item
  Where stack frames are created and destroyed;
\item
  Whether non-stack memory might contain pointers to stack frames,
  and, if so, which;
\item
  Which stack frames might contain pointers to themselves; and
\item
  Which registers might contain pointers to which stack frame.
\end{itemize}

It should be emphasised that the stack frames referred to here are
\emph{dynamic} constructs, created by \verb|call| instructions and
destroyed by \verb|ret| ones.  The way in which CFGs are unrolled
ensures that if a function can be called in two places then it will
have two nodes in the CFG; this makes it meaningful to say that a
\verb|call| CFG node creates a specific stack frame\editorial{not all
  that clear.}.

Note that the static analysis itself is entirely function-local.  The
only information it provides is a label saying, for each instruction
in a function, which registers might point into the current stack
frame and whether any pointers to the current frame have been stored
in either memory or the frame itself.  The cross-function stack
information is then synthesised from this by the main analysis when
needed.

\label{sect:function_head}
All phases of this analysis algorithm make one key assumption: that
the locations in a function's frame are ``created'' when the function
is called, so that there are no pointers into the frame before the
function starts.  It is further assumed that all functions have a
single entry point instruction and that the only way of activating a
function is by calling its entry point.  For this purpose, a pointer
is defined to be something which is dereferenced to refer to memory in
the stack frame, rather than something which just has a numerical
value which happens to match that of some location in the stack, so
that, for instance, pointers in dead registers are acceptable.  An
important corollary of this is that the address of the stack must have
been unknown when the program was compiled, and so statically constant
values cannot be stack pointers.

The static analysis phase also makes some assumptions about called
functions\footnote{The main analysis has no need of these assumptions
  as called functions are effectively inlined, and hence available for
  direct analysis.}:

\begin{itemize}
\item
  The analysis assumes that all of the input arguments to functions
  are in registers.  In the case of AMD64 using a normal ABI this is
  true for the first six arguments to a function \needCite{}, and,
  since the vast majority of functions have fewer than six arguments
  \needCite{}, this is not usually a problem.  \todo{But it can be
    sometimes.  It'd be nice to know how often, and how bad it is when
    it does happen.}

\item
  The analysis assumes that it can determine which registers, out of
  all of the possible argument registers, are used to pass parameters
  to a particular function.  {\Implementation} uses a separate
  register liveness analysis to do so: a register \verb|r| is
  considered to be a parameter to a function \verb|f| if \verb|r| is
  live at instruction \verb|f|.  This liveness analysis is entirely
  conventional and is not described here, beyond noting that it
  analyses the entire program at once, across function boundaries, but
  assumes that functions contained in libraries consume all six
  argument registers.

\item
  The analysis assumes that the program does not contain any
  statically-constant pointers which point at something outside of the
  main program.  This is a reasonable assumption to make because, on
  most modern systems, virtual addresses are assigned by the operating
  system rather than by the program, and so cannot possibly be known
  when the program is compiled and hence cannot be present as static
  constants.

  Alternatively, consider what sort of program might violate this
  assumption.  Assume the program is an ELF binary running on Linux,
  for the sake of exposition; other operating systems are similar.
  Such a program would have to have hard-coded certain virtual
  addresses without marking them as reserved in the ELF metadata.
  Since the dynamic linker runs before any part of the program, there
  is no way for the program to express its preference, and so no way
  for the dynamic linker to know that it must avoid mapping libraries
  into a particular address.  As such, any correct program with such
  hard-coded but unreserved addresses must have some fallback scheme
  for then the addresses are unavailable, in which case hard-coding
  the addresses would be pointless.  Of course, the program might
  simply be buggy and hard-code an address without a fallback, but
  this seems like a sufficiently unlikely sort of bug that I feel no
  particular guilt about ignoring it.  \todo{Not sure I needed that
    much detail there, really.}

\item
  The analysis assumes that functions roughly follow the usual ABI:

  \begin{itemize}
  \item They must not modify registers marked as call-preserved in the
    ABI, or if they do modify the registers must restore their values
    before returning.  This includes the stack pointer.
  \item They must either never return to the calling function at all
    or return to the instruction immediately after the \verb|call|
    which invoked them.  This is effectively another facet of the
    single-entry assumption which has already been discussed: if the
    called function were to return to the called function at some
    other instruction then that other instruction would constitute
    another entry point into the calling function.  Note that this
    does not rule out functions such as \verb|setjmp| and
    \verb|longjmp|\editorial{cite?}: \verb|longjmp| never returns and
    \verb|setjmp| always returns to the instruction after the one
    which called it.  The analysis might, however, produce an
    overly-conservative result if a called function never returns.
  \item They must not attempt to calculate pointers into the calling
    function's stack frame by reference to their own stack frame.
    Accessing the calling function's frame via references in one of
    the argument registers, or via pointers which were previously
    stored to main memory, is allowed.

    In other words, called functions must be written such that moving
    fields around in or adding padding to the calling function's frame
    does not affect their behaviour\editorial{Could be clearer.}.
  \end{itemize}

  The vast majority of functions conform to these assumptions.
  Indeed, most languages higher-level than assembly make it difficult
  to violate them even deliberately.
\end{itemize}

The first stage of the static analysis is to identify all of the
functions in the program and their entry point instructions.  For the
purposes of this analysis, a function is defined to be a (possibly
non-contiguous) set of instructions with a distinguished entry point
instruction, such that:

\begin{itemize}
\item
  For any non-return and non-call type branch from instruction $a$ to
  $b$ anywhere in the program, either $a$ and $b$ are in the same
  function or $b$ is the entry point instruction of some function.
  This includes the implicit branch from one ordinary instruction to
  the one which immediately follows it.
\item
  For any call-type branch instruction from $a$ to $b$, $b$ is the
  entry point of a function.
\end{itemize}

Within these constraints, SLI minimises the number of distinct entry
point instructions\footnote{Or, equivalently, it maximises the size of
  the individual functions.}\editorial{Might need to explain what the
  minimum assignment actually is?  It's kind of obvious, really.}.
Indirect branch instructions are handled using information from the
dynamic analysis: an indirect branch or call is treated as a multi-way
branch which could target any of the instructions which that
instruction branched to during dynamic analysis.

It is worthwhile discussing briefly how this definition interacts with
compiler tail-call elimination optimisations.  There are three
interesting cases:

\begin{itemize}
\item
  If a function is ever called normally, using a call-type branch, the
  second rule will ensure that its first instruction is an entry
  point, which is as desired.
\item
  If a function is tail-called from precisely one place, and is never
  invoked with a normal call instruction, no entry point will be
  created for it and it will be merged into its calling function.
  This leads to a somewhat less precise points-to table, as the
  analysis cannot make any a-priori assumptions about the points-to
  table at the start of the function, but will not lead to any
  unsoundness.
\item
  If a function is tail-called from multiple different locations then
  the first rule will ensure that the join of the caller's CFGs is the
  entry point of a function.  This is correct for simple tail-call
  elimination, in which a call instruction followed by a return one is
  replaced with a simple jump.  This is the only form of tail call
  supported by gcc and by LLVM\editorial{Might be worth a cite for
    that?}.  Optimisations such as function epilogue
  sharing\needCite{} might cause the join to be further through the
  function, though, and hence cause the function entry point to be
  incorrectly place.  I am not aware of any widely-used compiler which
  implements such an optimisation and so this is a largely theoretical
  concern\editorial{Might be worth asking on some compiler mailing
    lists to make sure I haven't missed anything.}.
\end{itemize}

In other words, the function heads assigned by these rules will match
up with the actual first instructions of functions in the higher-level
language, for most common compilers, which makes it more likely that
the key assumption that function frames are created at entry points
actually holds.

The static analysis itself is a reasonably conventional points-to
analysis.  Memory is divided into two regions, one containing the
current stack frame and one containing everything else.  The analysis
maintains locations for each register, plus a single location for the
current frame and another for the rest of memory.  Each instruction is
assigned a label which is either a a special value indicating that the
instruction is unreachable or a mapping from locations to subsets of
\{points-at-frame, points-at-memory\}; this label gives the points-to
configuration of the locations at the start of the instruction.
Instructions are converted into a set of data flow rules and the
analysis then simply finds the least fixed point of those rules.

Define the following symbols:

\begin{itemize}
\item $before(i)$ is the start-of-instruction label for instruction
  $i$.
\item $after(i)$ is the end-of-instruction label for instruction $i$.
  This is a function of $before(i)$ and the type of instruction at
  $i$.
\item $pred(i)$ is the set of instructions in this function which
  might execute immediately before instruction $i$.
\item $\sqcup$ is the least upper bound operator on instruction
  labels, defined as:

  \begin{itemize}
  \item $unreached {\sqcup} l = l$ and $l {\sqcup} unreached = l$, for
    any instruction label $l$.
  \item $l {\sqcup} l' = l \cup l'$, for $l$ and $l'$ not equal to
    $unreached$, where $\cup$ is the element-wise union of all of the
    locations in $l$.
  \end{itemize}
\end{itemize}

Defining $before(i)$ in terms of $after(i)$ is then easy: $before(i) =
\sqcup_{i' \in pred(i)}after(i')$, so that the points-to configuration
at the start of an instruction is a conservative combination of the
points-to configurations of all of its reachable predecessor
instructions.  Defining $after(i)$ in terms of $before(i)$ is no more
difficult but requires more special cases.  I give only a few
interesting cases here; generalising to the full AMD64 instruction set
is straightforward but tedious and unenlightening.

\begin{itemize}
\item
  If $before(i)$ is unreached then $after(i)$ is also unreached.
\item
  Load instructions: LOAD ${\ast}reg \rightarrow reg'$ loads from the
  memory pointed to by register $reg$ into register $reg'$.
  $after(i)$ is will be unchanged except for the entry for $reg'$,
  which is set based on $before(i)(reg)$ as follows:

  \begin{itemize}
  \item $stack \rightarrow before(i)(stack)$
  \item $non-stack \rightarrow before(i)(nonstack)$
  \item ${stack,non-stack} \rightarrow before(i)(stack) {\cup} before(i)(nonstack)$
  \item $\varnothing \rightarrow \varnothing$ \todo{Do I need to
    explain that a bit more?}
  \end{itemize}

  \todo{That's kind of a clunky way of describing this, but it should
    at least be obvious what's going on.}
\item
  Store instructions: STORE $reg' \rightarrow reg$ stores the value of
  register $reg'$ into the memory location pointed to by register
  $reg$.  The points-to set for $reg$ and $reg'$ will be unchanged,
  but the points-to sets for the stack and non-stack pseudo locations
  may be updated.  If $before(i)(reg)$ includes $stack$ then
  $after(i)(stack) = before(i)(stack) \cup before(i)(reg')$; otherwise
  $after(i)(stack) = before(i)(stack)$.  Likewise, if $before(i)(reg)$
  includes $non-stack$ then $after(i)(non-stack) =
  before(i)(non-stack) \cup before(i)(reg')$, and otherwise
  $after(i)(non-stack) = before(i)(non-stack)$.
\item
  Set register to constant: SET $k \rightarrow reg$ sets the register
  $reg$ to constant value $k$.  This has already been discussed: if
  the constant refers to a location in the program's main binary then
  the points-to set for $reg$ will be set to $non-stack$; otherwise,
  it is set to $\varnothing$.
\item
  Register-register computation: SET $reg \oplus reg' \rightarrow
  reg''$ sets $reg''$ to some binary function of $reg$ and $reg'$.
  Conservatively, {\technique} sets $after(i)(reg'') = before(i)(reg)
  \cup before(i)(reg')$ in this case.  It might be possible to improve
  on this for some specific $\oplus$.

\item
  CALL $expr$ calls some function given by the expression $expr$.
  This is treated as a kind of escaping problem: if any of the
  function argument registers contains a pointer to the current stack
  frame, or if non-stack memory does, then the stack frame is assumed
  to escape into the called function, which is then assumed to store
  it into non-stack memory and into the function return value
  register.  The called function is assumed to have no other
  effects\editorial{Which isn't quite sound: could write a stack
    pointer into the current frame; whoops.}.  Indirect function calls
  are, as usual, handled by querying the dynamic analysis and taking
  the union of all functions which might possibly be called.  The
  identification of argument registers has already been discussed.
\end{itemize}

It now only remains to define the initial state for the fixed point
iteration.  This is simple:

\begin{itemize}
\item
  The label on a function entry point is as follows:

  \begin{itemize}
  \item The stack pointer points at the current stack frame.
  \item Argument registers cannot point at the current stack frame,
    but might point at memory outside of that frame.
  \item Other registers cannot point at anything.  This is not
    entirely true, in the sense that the caller could have loaded
    anything at all into one of these registers.  On the other hand,
    it is guaranteed that the function will never dereference one of
    these registers, if it conforms to the ABI, and so treating them
    as non-pointers is correct for the purposes of this analysis.
    Marking them as pointers would mean that the stack frame would be
    marked as containing pointers if the function ever saved them to
    the stack, which is common for call-preserved registers, reducing
    the accuracy of the analysis.
  \item Main memory can contain pointers to itself but not to the
    current stack frame.
  \item The current stack frame contains no pointers at all.
  \end{itemize}
\item
  The initial label for every other instruction is just $unreached$,
  reflecting the fact that we have not yet discovered any paths from
  the entry point to them.
\end{itemize}

The static analysis iterates until it finds a fixed point of these
rules.  The resulting labelling of instructions accurately models the
program's actual points-to behaviour, provided that the assumptions
discussed above all hold.

\todo{Need to eval how effective all of this actually is.}

\subsubsection{Encoding information into \StateMachines}

The information collected by the static analysis pass is
function-local, in the sense that it can tell whether a given
instruction accesses the current function's stack frame but cannot say
anything about other frames.  {\Technique}'s {\StateMachines} are,
however, cross-function, and so even the idea of a ``current
function'' is not entirely well-defined.  Solving this mismatch has
two main steps.  The first is to recover the function structure of the
program, and hence to assign identifiers to stack frames which might
be relevant.  The function-local information can then be extended to
say which frames a given pointer might refer to, rather than just
providing a simple does/does-not point at the current frame flag.
Both of these phases are performed as part of the process of compiling
CFGs to {\StateMachines}.

\todo{Frame ID assignment is done over the entire {\StateMachine},
  whereas static analysis incorporation is done independently for each
  entry point.  Not sure I make that terribly clear at the moment.}

The first step in the encoding is to assign identifiers to all of the
(dynamic) frames referenced by the program fragment being investigated
and to establish the stack layout at every point in the
{\StateMachine}, in terms of which frames are on the stack and where
the boundaries between them are.  This might, at first, appear to be
trivial.  To understand why it is not, it is helpful to consider a few
simple examples.  First, suppose that the behaviour being investigated
is in the second call to \verb|f| in \verb|g|:

\begin{verbatim}
g() {
l1:  f();
l2:  f();
}
\end{verbatim}

The analysis should assign different frame IDs to the two calls, and
so assigning the same frame ID to every instance of a given static
function would be incorrect.  At the same time, simply assigning a
different ID to every instance would also be incorrect.  For example:

\begin{verbatim}
g() {
    if (cond1)
       f();
}
h() {
    if (cond2)
       f();
}
\end{verbatim}

Suppose that the {\StateMachine} being generated has entry points for
both \verb|g| and \verb|h|.  Ideally, we would like to analyse the two
instances of \verb|f| only once, to avoid doing redundant work, and
this will only be possible if they are assigned the same frame ID.

The initial {\StateMachine} building process can identify the start
and end of functions by recognising \verb|call| and \verb|ret|
instructions.  These are initially represented by placeholder
\state{StartFunction($frame$, $rsp$)} and \state{EndFunction($frame$,
  $rsp$)} side-effects, where $rsp$ is the stack pointer at the
\verb|call| or \verb|ret| instruction and $frame$ is a placeholder for
a frame identifier which will be assigned later.  These side-effects
are similar to \state{Assert} ones in that they have no direct effect
on the execution of the machine, but instead serve to provide hints to
the analysis and to rule out certain uninteresting executions.  The
task of this pass is to replace these placeholders with actual frame
identifiers.

{\Technique} solves this problem by recasting it as a simple
constraint solving one.  A \state{StartFunction($frame$)} side-effect
produces a constraint that $stack(s1) = stack(s2) + frame$, where
$stack(s1)$ is the stack at the start of the side-effect, $stack(s2)$
is the stack at the end of it and $frame$ is the frame associated with
the side-effect, and conversely for \state{EndFunction} side-effects.
Ideally, {\Technique} would generate all of these constraints and
solve them, and hence directly determine the stack layout at every
point in the {\StateMachine}, but doing so is problematic because the
number of variables which must be solved for is not known initially.
The workaround for this is simply to split the constraint solver into
two passes: one which determines the depth of the stack at each point
in the \StateMachine, and hence how many variables are needed, and
another which solves to determine the values of those variables.  The
resulting stack layouts are then encoded into the {\StateMachine}
using special \state{StackLayout} side-effects at every entry point
and by attaching the relevant frame ID to each \state{StartFunction}
and \state{EndFunction} side effect\editorial{Be more careful about
  what entry point means.}.

Tracking the boundaries between stack frames is simple.  The initial
\state{StartFunction} and \state{EndFunction} side effects include a
copy of the stack pointer at the time when the matching instruction is
issued, and this is updated by all of {\StateMachine} simplification
passes in exactly the same way as any other side-effect input
expression would be, so that the boundaries can be trivially read out
of the side effects once the {\StateMachine} is converted to SSA form.

\todo{I could say quite a lot about how this works, and there are some
  moderately interesting subtle bits to it, but I don't think the
  interestingness justifies the amount of space needed to describe
  them properly.}

The next step, once frame IDs have been allocated and the stack layout
determined, is to incorporate the information from the static analysis
into the {\StateMachine}.  As discussed above, the necessary
information is, for each frame, when the {\StateMachine} starts:

\begin{itemize}
\item
  Whether the frame might contain a pointer back to itself;
\item
  Whether there might be any pointers to the frame from somewhere
  other than the frame itself; and
\item
  Which registers might contain pointers to the frame.
\end{itemize}

Suppose the call stack consists of functions $f$, $g$, and $h$, where
$h$ is the function executing when the {\StateMachine} starts.  The
static analysis then makes determining this information for $h$'s
frame trivial, as this is precisely the contents of $h$'s instruction
label\editorial{\emph{Almost} precisely, anyway}, but $f$ and $g$'s
frames present somewhat more difficulty.  One obvious approach would
be to determine the disposition of $g$'s frame directly from the
static analysis instruction label for the \verb|call| from $g$ to $h$,
but this is not sound because the prefix part of $h$ which is not
included in the {\StateMachine} might store $g$'s frame to somewhere
unexpected\editorial{Rephrase}.  Instead, {\technique} conservatively
assumes that if $g$'s frame ever reaches the start of $h$, either by
being stored to memory or by being placed in one of $h$'s argument
registers, the prefix of $h$ might copy the frame into anything which
it can reach: every register, non-stack memory, and every stack frame
which it can reach.  Likewise, $f$'s frame is assumed to have escaped
to everywhere if it can reach any of $g$'s arguments.  This, in
effect, flattens the static analysis instruction label for all
functions except the current one into a single stack-has-escaped flag,
which is in many cases overly conservative but is in all cases
safe\editorial{Or at least, it's safe whenever the static analysis
  itself is safe, which is as close as you're going to get here.}

Finally, the information obtained can be incorporated into the
{\StateMachine}.  It takes the form of a couple of special
side-effects.  The \state{StackLayout} side-effect is augmented with
additional flags saying, for each frame, whether that frame might
point at itself and whether anything else might point at it.  In
addition, a \state{PointsTo} side-effect is added for every register,
saying which initial frames the initial value of that register might
point at.

\todo{Good God I'm making a meal of explaining this.}

\todo{Mention that RSP's \state{PointsTo} says that it can point at
  any frame, not just the current one, because of the way
  cross-function constant propagation works.}

\todo{Maybe talk about interactions with cross-thread \StateMachines?
  They're not actually very interesting, but it's kind of an obvious
  omission.}

\subsubsection{The actual alias analysis}

\todo{Really need to look at some standard compiler alias analyses to
  figure out how novel this actually is.  It'll need some description
  regardless, because it's important and I'm pretty certain nobody
  else has tried it in this context, but the amount and type might
  change a bit.}

\todo{I should probably describe the use-initial-memory pass as an
  important special case here.}

\todo{Should probably define what I mean by ``satisfying'' a load
  about here.}

\todo{Important subtlety which I've completely failed to mention:
  distinction between initial memory, as returned by $LD$ expressions,
  and current memory, as returned by \state{Load} states.  Also the
  whole mapping of RSP-relative addresses into stack frames thing,
  which isn't subtle but is important.}

There is some standard terminology for describing compiler alias
analyses\editorial{Cite Hind 2001.}.  It does not apply perfectly in
this context, but here's an attempt to do so anyway:

\begin{itemize}
\item
  The analysis is partly flow-sensitive.  Two accesses are only
  considered to alias if the control flow of the {\StateMachine}
  allows them to occur in an appropriate order, but higher-order
  control flow dependencies are not tracked.  \todo{I did find a cite
    for another alias analysis which has the same kind of sensitivity,
    but I now seem to have lost it.  I should dig that up again.}
\item
  The analysis is not context-sensitive in the usual optimising
  compiler sense of examining a function's calling context, beyond
  that which has been incorporated into the {\StateMachine}.  On the
  other hand, the {\StateMachine}-level analysis does use context
  information from the static analysis phase.
\item
  The heap is not modelled at all.  Instead, the objects which are
  pointed at are frames on the stack, or a special value indicating
  that a value points at something other than a stack frame.
\item
  Aggregates, as such, do not really exist, and so aggregate modelling
  is not entirely meaningful.  To the extent that it does mean
  something, the analysis distinguishes aggregate fields.
\item
  The analysis operates on whole {\StateMachines}, and hence on small
  fragments of the original program.
\item
  The alias representation is a hybrid, including both an explicit
  alias table and a points-to table. \todo{Could actually build the
    aliasing table lazily, which might be a bit faster but would make
    everything much more confusing; ref demand driven aliasing
    analysis, maybe?}
\end{itemize}

\todo{Not sure that added a great deal, really.}

The two key data structures here are the points-to table, which
specifies which stack frames each frame or {\StateMachine}-level
variable might point at, and the aliasing table, which maps from
memory-accessing side effects to a set of side-effects with which they
might alias.  They are built inductively, starting with
conservative\editorial{Why?  Should be able to use aggressive tables
  safely, and the results would probably be better.} initial values
and then iteratively refining them until a fixed point is reached.
This fixed point then allows aliasing queries between accesses to the
stack to be resolved reasonably accurately\footnote{Remember that the
  dynamic analysis already provides a way of resolving queries between
  non-stack accesses.}, enabling a number of other useful
simplifications such as store-load forwarding and the elimination of
redundant store operations.

The points-to table contains an entry for every {\StateMachine}-level
variable which is ever modified by the {\StateMachine} giving the
points-to set of the value assigned to that variable.  A points-to set
in this context is simply the set of stack frames which the value
might point at plus a flag saying whether the variable might point at
something other than the stack.  The initial points-to sets allow any
variable to point at any location.

It is possibly slightly surprising that the points-to table does not
contain any entries for variables which are referenced but never
modified.  These correspond to the initial values of program
registers.  They are instead handled in a flow-sensitive way using the
{\StateMachine}'s internal control flow and the \state{PointsTo}
side-effects introduced earlier: whenever the points-to set of such a
register is needed, the analysis examines all paths from the start of
the {\StateMachine} to the point at which the set is needed, finds the
\state{PointsTo} for the register along the path, and then takes the
union of the sets over all paths.  If any path does not have such a
set then the result is a safe points-to set which can point at
anything.  This allows the analysis to correctly incorporate the
results of the static analysis even for {\StateMachines} with multiple
entry points with different initial points-to configurations.
\todo{The actual approach is a bit less stupid than this and avoids a
  lot of redundant work, but this is complicated enough already
  without need to drag in all that stuff.}  \todo{Might be worth
  talking about what actually happens when you have multiple contexts
  with conflicting initial points-to configurations?}

The alias table is a mapping from \state{Load} side-effects in the
{\StateMachine} to other memory-accessing side-effects in the
{\StateMachine}, plus flags indicating whether a given might load from
the initial state of memory or load the result of a store not present
in the {\StateMachine}\footnote{The latter is useful when simplifying
  single-threaded {\StateMachines} prior to considering their
  interactions with other threads.}.  Building the initial aliasing
table is somewhat more complex than the initial points-to table, as it
incorporates some control-flow information from the structure of the
{\StateMachine} and information from the dynamic analysis.

The first stage of building the initial aliasing table is an $O(n^2)$
possibly-reaching analysis.  This builds up a complete table saying,
for each state of the {\StateMachine}, which memory accesses might
occur before that state and which are not killed before reaching the
state.  The definition of killed is moderately interesting: an access
is killed by another access which definitely accesses the same
location, according to some simple arithmetic rules on the address
expression, regardless of whether the killing access is a store or a
load.  The point is that if you have a store followed by two loads to
the same location then it's safe to only consider forwarding from the
first load to the second, and not from the store to the second load,
and doing so gives you a smaller possibly-reaching set, which makes
the rest of the analysis a bit easier.

Once the possibly-reaching analysis is complete, the result is
winnowed back down to an actual aliasing table by removing the entries
for things other than loads and eliminating anything where the dynamic
analysis suggests that aliasing is impossible.  \todo{Need a two-step
  process because otherwise the data flow doesn't work; explain why.}
Slight oddity: we keep any store which might possibly alias, but loads
only if they definitely do; need to explain why that's sound wrt to
the loads-kill-accesses thing in the initial possibly-reaching
analysis.

\todo{Also need to talk about how you set the might-load-initial flag.
  The might-load-external is a bit more obvious, but might still need
  a sentence or two.}

Once the initial tables have been built, the analysis proceeds to
refine them until they accurately reflect the {\StateMachine}'s actual
behaviour.  Refining the alias table is straightforward: simply
iterate over every entry and compute the points-to set for the two
accesses.  If they do not overlap, remove the entry from the table.

Refining the points-to table is likewise simple: for each variable,
the analysis finds the side-effect which defines it (which must be
unique, because the {\StateMachine} is in SSA form) and calculate a
new points-to set for the value computed by that side-effect, and this
is used to update the entry in the table.  In the case of \state{Load}
side-effects this will involve examining the alias table and computing
points-to sets for every possibly-aliasing memory access and taking
the union.

Possibly slightly surprising: a load $l$ can never return a pointer to
a stack frame $s$ if $s$ is not on any thread's stack when $l$
executes, even if $l$ is satisfied by a store which might have stored
$s$.  This is because of the call-allocates-stack-frames rule: if $s$
is dead when $l$ executes then any access to $s$ after being loaded by
$l$ would be a use of freed stack memory, and we assume that that
never happens. \todo{Need to argue this a bit more carefully.}

Also need to talk about what happens wrt initial memory loads.  I
should probably have mentioned initial memory load expressions some
time before this, to be honest.

\subsubsection{Using the results of the analysis}

The result of this analysis is an aliasing table which says, for each
load, which other stores or loads might possibly satisfy it.  This
information can be used to simplify the {\StateMachine} in a number of
useful ways:

\begin{itemize}
\item
  Forwarding values from stores to loads.  If the load can only be
  satisfied by a single store, and there are no possible interfering
  external stores and no possibility of loading the initial contents
  of memory, the load can be replaced by a copy of the value written
  by the store.  Note that this is safe because the {\StateMachine} is
  in SSA form, and so this cannot involve moving an expression which
  references a value across a definition of that value, and because of
  the initial control-flow dependent aliasing table, which ensures
  that this cannot introduce any additional uses of uninitialised
  variables.
\item
  Likewise, if there are no satisfying stores and no external stores
  then the load can be replaced by a copy from an initial-memory
  expression.
\item
  If a \state{Load} has multiple potentially-satisfying stores, and no
  external stores, the load can sometimes be replaced with an MTBDD
  which selects amongst the various possible \state{Store}s and
  forwards from an appropriate one, using an algorithm essentially
  identical to that used when eliminating \state{$\Phi$} states.
\item
  If the same location is definitely loaded twice, with no intervening
  stores and not potential external stores, the second load can be
  replaced with a simple copy from the result of the first one.  This
  is essentially the same idea as the standard compiler available
  expression\needCite{} optimisation.
\item
  Stores which are definitely never loaded can also sometimes be
  eliminated.  This is, of course, only safe if the {\StateMachine}
  being optimised is not the write-side {\StateMachine}, or if the
  dynamic analysis can show that the store is definitely
  thread-private.
\item
  Finally, \state{StackLayout}, \state{StartFunction}, and
  \state{EndFunction} side-effects can be removed if it can be shown
  that no memory access ever accesses the relevant stack frames.
\end{itemize}

\subsection{Variable unification}
\label{sect:unification}

The various {\StateMachine} simplifications can sometimes lead to
there being multiple fragments in a single {\StateMachine} which differ
only in variable and register names.  The variable unification pass
attempts to unify these fragments together by renaming variables and
introducing additional copies, while maintaining SSA form.  This is
conceptually rather simple: find all of the places in the
{\StateMachine} where two states are identical except for variable
names, build a new fragment of {\StateMachine} which is equivalent to
both input fragments, and then replace the old fragments with the new
one.  The details are, however, moderately subtle, and I now discuss
them briefly.

First, the definition of ``identical except for variable names''
includes the successor pointers of the state but not the predecessor
pointers, so states do not have to be reachable from the same place
but must reach the same place after completing\editorial{This is kind
  of arbitrary; an almost identical analysis could use the converse
  constraint, but I've not bothered to implement that.}.

Building the unifying {\StateMachine} fragments requires a moderate
amount of care.  There are in general three components to building the
unifier:

\begin{itemize}
\item
  Unifying any inputs which the state might require.
\item
  Unifying any memory accesses issued by the state.
\item
  Unifying any output registers which the state might produce.
\end{itemize}

Unifying output registers is the simplest of these.  Suppose we have
two side effects which we wish to unify:
\verb|A: Copy reg1 = 5 then C| and \verb|B: Copy reg2 = 5 then C|.
The obvious unifier here is like this:

\begin{verbatim}
A': Copy reg1 = 5 then B'
B': Copy reg2 = reg1 then C
\end{verbatim}

And this is the one used by SLI.  While it is correct in almost all
cases, it is perhaps not obvious why it is correct.  In particular,
the \verb|A| state has gained an assignment to \verb|reg2| and the
\verb|B| state one to \verb|reg1|, and one might be concerned that
this might affect the \StateMachine's behaviour.  To see why this is
correct, first notice that there can be no assignments to \verb|reg2|
before \verb|A|: the {\StateMachine} is in static single assignment
form, and so there can be no assignments to \verb|reg2| except for
\verb|B|, and the {\StateMachine} is acyclic, so there can be no path
from \verb|C| to \verb|A| and hence none from \verb|B| to \verb|A|.
Likewise, \verb|reg1| is uninitialised at \verb|B|.  Therefore,
ignoring \verb|Phi| nodes, any path through \verb|C| starting at
\verb|A| cannot depend on the value of \verb|reg2|, and likewise any
path starting at \verb|B| cannot depend on the value of \verb|reg1|,
and so modifying their values is safe.

\verb|Phi| side effects complicate the situation somewhat, as they can
take uninitialised variables as input\footnote{Recall that Phi side
  effects select the most recently assigned input variable, and so
  will simply ignore any uninitialised variables in their inputs.}.
\verb|Phi| side effects which do not take either of the registers as
inputs are obviously unaffected by this transformation, as are those
which take both\footnote{The only possible effect of the
  transformation is that such a side-effect might take reg1 as input
  rather than reg2, or vice versa, but since their values will
  necessarily be equal that is not a problem.}, but any which take
only one of the registers as input will potentially produce a
different value.  Consider, for instance, this example program:

\begin{verbatim}
l1: a = 5;
l2: if (x)
l3:   b = 7;
l4: else
l5:   c = 7;
l6: d = Phi(a, b);
\end{verbatim}

The final value of \verb|d| will be \verb|7| if \verb|x| is true and
\verb|5| otherwise.  Attempting to unify \verb|l3| and \verb|l5| will,
however, produce a program fragment like this:

\begin{verbatim}
l1 : a = 5;
l2 : if (x)
l3 :   goto l3';
l4 : else
l5 :   goto l3';
l3': b = 7;
l5': c = 7;
l6 : d = Phi(a, b);
\end{verbatim}

In this case, the final value of \verb|d| is always \verb|7|.  SLI
therefore detects when there is some $\Phi$ side effect which consumes
some but not all of the output variables and will not perform the
simplification in that case.

Unifying inputs is more complicated.  The approach used by SLI is to
insert additional \verb|Phi| side-effects which select appropriate
register inputs into new freshly-allocated output registers and to
then use those new registers in the expression to be unified.  For
example, consider a program like this:

\begin{verbatim}
l1: if (x) {
l2:    a = 73;
l3:    Assert(a + 7 == 12);
l4: } else {
l5:    c = 92;
l6:    Assert(c + 7 == 12);
l7: }
\end{verbatim}

(This example could, of course, be reduced more easily using simple
copy propagation; assume for the purposes of exposition that that
cannot be done for some reason.)  We would now like to unify the
\verb|l3| and \verb|l6| statements.  One possible solution would be
this:

\begin{verbatim}
l1: if (x) {
l2:    a = 73;
l4: } else {
l5:    c = 92;
l7: }
l8: d = Phi(a, c)
l9: b = d + 7
\end{verbatim}

Building the unifier is simple when it is possible to do so: compare
the two expressions which are to be unified, building up a mapping
from registers in one expression to registers in the other as we do
so, then iterate over all of the registers in this mapping and compare
them to the \StateMachine's control flow to determine whether a
\verb|Phi| can select the right one (failing if not), and then
generate the unifier itself in the obvious way.

\todo{I'm a little worried here that this is really very similar to
  the theorem-prover or Prolog style unification algorithms, and the
  terminology is also very similar, but they're not *quite* the same,
  which might be a bit confusing.}

\todo{Should probably have a more realistic example, really; this one
  makes it look like this is something which won't happen very often,
  whereas actually it's quite useful.}

The final step of unifying side effects is unifying their memory
accesses, if they have any.  At this point, the extra level of
indirection between memory access identifiers and CFG nodes, discussed
in section \ref{sect:derive:description}, becomes useful, as unifying
two memory accesses becomes simply a matter of allocating a new memory
access identifier whose CFG set is the union of the two input
identifier's CFG sets\editorial{Need to either say more or move this
  to some place a bit less obvious.}.

\subsection{Other static analysis}

\subsubsection{Frame pointer elimination}

\todo{Not entirely convinced this needs to be here at all.}

One possibly surprising property of SLI is that it is sometimes more
effective on programs built with compiler optimisations enabled than
it is on unoptimised builds, as optimising compilers are generally
quite good at removing unimportant steps from the program.  The most
important compiler optimisation, from SLI's perspective, is frame
pointer elimination.  When frame pointers are in use the program
maintains two pointers into the current stack frame, the stack pointer
and the frame pointer and the compiler emits some stack accesses
relative to the stack pointer and some relative to the frame pointer.
This complicates alias analysis for accesses to function-local
variables.  SLI therefore uses a static analysis to determine how the
stack and frame pointer registers are related to each other throughout
the program and uses this to rewrite frame-pointer-relative accesses
into stack-pointer-relative ones wherever possible.

\begin{figure}
\begin{algorithmic}
  \For {Every instruction $i$ in the program}
     \State {$exitLabels[i] \gets \bot$}
  \EndFor
  \While {Not converged}
     \For {Every instruction $i$ in the program}
        \State {$entryLabels[i] \gets \bigsqcup\limits_{p \in predecessors(i)}exitLabels[p]$}
        \State {$exitLabels[i] \gets \textsc{exitConfig}(i, entry)$}
     \EndFor
  \EndWhile
  \State \Return $entryLabels$
\end{algorithmic}
\caption{Frame pointer offset algorithm}
\label{fig:derive:frame_pointer_alg}
\end{figure}

The static analysis is itself rather simple and is illustrated in
figure~\ref{fig:derive:frame_pointer_alg}.  This is a fairly
conventional iteration to a fixed point.  Each instruction has an exit
label which is initialised to $\bot$ and an entry label which is not
initialised at all, the label at entry to an instruction is the
$\sqcup$-union of all of its predecessors' labels, the label at the
end of the instruction is a function (denoted \textsc{exitConfig}) of
its entry label and the type of instruction, and the final result is
the map from instructions to exit labels.  A few things remain to
define:

\begin{itemize}
\item
  The label on an instruction has one of three forms: $\bot$,
  indicating that nothing at all is currently known about that
  instruction, $offset(k)$, indicating that at that point in the
  program the difference between the stack and frame pointers is the
  constant $k$, or $\top$, indicating that difference between the two
  pointers is known to not be a constant.
\item
  $l \sqcup \bot = l$ and $\top \sqcup l = l$, for any $l$.
\item
  $l \sqcup \top = \top$ and $\top \sqcup l = \top$, for any $l$.
\item
  $offset(k) \sqcup offset(k) = offset(k)$
\item
  $offset(k) \sqcup offset(k)' = \top$ for any $k \not= k'$.
\item
  The behaviour of $\textsc{exitConfig}(i, l)$ depends on the
  instruction $i$ being examined:

  \begin{itemize}
  \item If $i$ modifies neither the frame pointer nor the stack
    pointer then the result is $l$.
  \item If it sets the frame pointer to be equal to the stack pointer
    plus $k$ then the result is $const(k)$, and likewise if it sets
    the stack pointer to the frame pointer plus $k$ the result is
    $const(-k)$.
  \item If it adds $k$ to the frame pointer and $l$ is $const(k')$
    then the result is $const(k'+k)$.  For other values of $l$, the
    result is just $l$.  Conversely, if it adds $k$ to the stack
    pointer then the result is $const(k'-k)$.
  \item Otherwise, for any other modifications to the stack or frame
    pointers, the result is $\top$.
  \end{itemize}
\end{itemize}

Once these rules have converged any $const(k)$ entries in the
resulting table will correctly specify the offset between the stack
and frame pointers at that instruction.  The {\StateMachine} compiler,
described in section~\ref{sect:derive:compile_cfg}, can then use this
information to remove most references to the frame pointer.

\todo{Need to eval how often this works.}

