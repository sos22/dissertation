\chapter{Future work}
\label{sect:fw}

This dissertation leaves many potential refinements uninvestigated.
Some of these are straightforward improvements to {\implementation}:
implementing more {\StateMachine} library models, or considering
alternative architectures, or improving performance, or generalising
to more types of bugs.  Others involve more fundamental changes to
{\technique} itself.  I describe some possible ideas below.

\section{Finding and characterising bugs}

The initial analysis which builds \glspl{verificationcondition} is
probably the weakest part of {\technique}, as it is only capable of
considering instruction interleavings over fairly small windows of
instructions.  The obvious way of improving that would be to re-cast
the algorithm to take as input either source code or an intermediate
format such as LLVM bitcode~\cite{FFFLattner2013}.  These provide the
analysis with far more information than raw machine code and so might
reasonably be expected to expand the fragments of program which it can
consider.  Alternatively, the initial static analysis which builds the
\gls{programmodel} could be extended to include more
decompilation-style analyses, which would achieve much the same
effect.

\section{Reproducing bugs}

As described in \autoref{sect:reproduce:unenforcable}, {\technique}
cannot enforce every possible happens-before graph.  The fundamental
problem is that rather than enforcing a requirement that $A$ happens
before $B$, {\technique} enforces the stronger requirement that $A$
happen \emph{immediately} before $B$.  This introduces additional edges
into the happens-before graph, and if one of those edges closes a
cycle then the graph becomes unenforcible.  In other words, the
problem is that {\technique} uses synchronous messages to enforce what
should be an asynchronous ordering constraint.

Switching to asynchronous messages would eliminate this problem.
Rather than having threads synchronise at message operations, the
sending thread would instead generate and publish a message structure
which could be collected by the receiving thread.  Any side-condition
would be evaluated by the receiving thread, and any needed
cross-thread state would be included as a payload in the message
structure.  There would then be no need for the sending thread to wait
for the receiving one, and hence no risk of introducing cycles in the
happens-before graph.  The main disadvantage of this scheme would be
the greater implementation complexity in the enforcer itself.

\section{Fixing bugs}

I have described how {\technique} can fix bugs by introducing an
additional global lock.  This is effective but inefficient.  In
particular, it can eliminate large numbers of safe instruction
interleavings, leading to an unnecessary loss of concurrency.  This is
an especially irritating limitation because the prior analyses
characterise precisely which interleavings are safe and which
dangerous, in the form of the \glspl{verificationcondition}, and the
fix generation process simply ignores this information.  It would be
interesting to investigate alternative schemes which make better use
of it.

\begin{wrapfigure}{o}{4cm}
  \vspace{-12pt}
  \begin{figgure}
  \centerline{
  \begin{tikzpicture}
    \node (LD1) {Load 1};
    \node [right = 1.5 of LD1] (dummy) {};
    \node [below = of dummy] (ST1) {Store 1};
    \node [below = of ST1] (ST2) {Store 2};
    \node [below = 0.9 of ST2] (dummy2) {};
    \node at (dummy2 -| LD1) (LD2) {Load 2};
    \draw[->] (LD1) -- (LD2);
    \draw[->] (ST1) -- (ST2);
    \draw[->,happensBeforeEdge] (LD1) to node [above,sloped] {$i_1 = i_2$} (ST1);
    \draw[->,happensBeforeEdge] (ST2) -- (LD2);
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:concl:hb_graph}
  \end{figgure}
  \vspace{-12pt}
\end{wrapfigure}
One approach to doing so makes use of a kind of extended hazard
pointer~\cite{Michael2004}.  Consider, for example, the threads in
\autoref{fig:concl:hb_graph}.  This is intended to illustrate a
construction in which there is one thread which issues two loads and
another which issues two stores and in which the program will crash if
both stores occur in between the two loads and the variable $i$ has
the same value in both threads.  One way of preventing this bug would
be to have the first thread publish a ``hazard'', containing the value
of its $i$ variable, before running the first load and clear it after
the second one.  The second thread could then check for such hazards
before running the first store and, if it finds one with a matching
$i$, make sure to wait for the hazard to clear before running the
second one.  This would fix the bug and allow the two threads to run
in parallel for most of the critical section, or for all of it when
the $i$s differ.  The loss of concurrency, and hence loss of
performance, associated with the fix would therefore probably be much
lower.

\cleardoublepage
\chapter{Conclusions}
\label{sect:concl}

I have described {\technique}, a set of techniques for finding,
characterising, reproducing, and fixing atomicity violation bugs
starting from a runnable copy of the program, without access to the
program's source and with minimal user intervention.  I have also
described my prototype implementation, {\implementation}, and
discussed in detail its performance and its effects on a variety of
real and artificial test bugs and programs.  The result is that
{\technique} is highly effective at all of its goals when used on the
artificial test cases, and performs well enough to be useful for some
more realistic tests taken from very large existing pieces of
software.  I have also demonstrated some of the limitations of
{\technique} in its current form; primarily, that it struggles to
solve the aliasing problems necessary to handle more complicated bugs.
Finally, I have compared {\technique} to a variety of existing
systems, highlighting similarities and differences.

The most important contribution of this work is the crash enforcement
mechanism, which takes a description of a concurrency bug in a
particular format and converts it into a modified version of the
program which is far more likely to exhibit the bug.  This process can
take advantage of any data-dependent side conditions necessary for the
bug to reproduce, over and above the actual concurrency pattern
required, allowing it to reproduce the bug far more quickly and
reliably than simpler schedule enforcement schemes.  The additional
computational time necessary to generate these enforcers, given the
bug description, is modest, and easily outweighed by the reduction in
reproduction time.

A variant of the same technique can also be used to fix bugs.  These
fixes have very low overhead, comparable to that which could be
obtained by a hand-crafted fix for the same bug, and can be deployed
to protect existing programs very easily.  On the other hand, the set
of bugs which can be completely protected by these fixes is somewhat
smaller than the set which can be easily reproduced.  Nevertheless, I
have demonstrated that this technique can be used to generate
effective and low-overhead fixes for at least some real-world
concurrency errors.

I also outlined a new scheme for finding concurrency-related bugs in
binary programs.  The results of this part of the dissertation are
perhaps less convincing; it found very few bugs in real programs, and
is computationally very expensive.  I briefly discussed how combining
it with existing work might help to improve on these weaknesses.  Even
without those possible future refinements, the algorithm is
embarrassingly parallel, and so its performance is likely to improve
as hardware execution facilities become more parallel; precisely the
situation in which it would be most useful.
