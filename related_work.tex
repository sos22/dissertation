\chapter{Related work}
\label{chapter:related_work}

\todo{LOOM?}  \todo{Comparison to STM block inference?}  \todo{Could
  do with a cite to \cite{Saxena2009} in here somewhere i.e. LESE.}
\todo{Could also do with a cite of \cite{Khoo2010} i.e. the hybrid
  type checking/symbolic execution thing.}

\section{Automatically finding bugs}

Automatically detecting and characterising bugs in programs has been
an active area of research for many years.  Some of the more important
and relevant are summarised in \autoref{table:rw:find_char}.  They are
described in slightly more detail in the rest of this section.

\begin{sidewaystable}
  \begin{tabular}{l>{\raggedright}p{5.5cm}lp{11cm}}
    Technique family           & Type of error discovered                     & Static/dynamic  & Example systems \\
    \hline
    Locksets                   & Locking protocol violations                  & Static          & RacerX\cite{Engler2003},RELAY\cite{Voung2007} \\
                               &                                              & Dynamic         & Eraser\cite{Savage1997} \\
    \hdashline
    Happens-before             & Memory races                                 & Dynamic         & RaceTrack\cite{Yu2005}, FastTrack\cite{Flanagan2009}, Netzer\cite{Netzer1991} \\
    \hdashline
    Concurrent aliasing        & Memory races                                 & Static          & Chord\cite{Naik2006} \\
    \hdashline
    Schedule perturbation      & Timing dependencies                          & Dynamic         & DataCollider\cite{Erickson2010}, AtomRace\cite{Letko2008}, CTrigger\cite{Zhou} \\
                               &                                              & Mixed           & Chess\cite{Musuvathi2008} \\
    \hdashline
    Stereotyping               & Anomalous behaviour                          & Dynamic         & Anomaly-based IDSes\cite{Forrest1996a}, Pu and Wei\cite{Pu2006}, HeapMD\cite{Chilimbi2006}, DIDUCE\cite{Hangal2002}, MUVI\cite{Lu2007} \\
                               &                                              & Static          & Bugs as deviant behaviours\cite{Engler2001}\\
    \hdashline
    Extended assertions        & \multirow{2}{*}{\parbox{5.5cm}{Violations of programmer-identified properties}} & Dynamic        & TESLA\cite{Watson2013}\editorial{Need a better cite}, Uppuluri et al.\cite{Uppuluri2005} \\
                               &                                              & Static          & Meta-compilation\cite{Engler2000a}\\
    \hdashline
    Typestate systems\cite{Strom1986a}& \multirow{2}{*}{\parbox{5.5cm}{Object access protocol sequencing violations}} & Dynamic & Gradual typestates\cite{Wolff2011}, 2ndStrike\cite{Gao2011} \\
                               &                                                                             & Static  & Plaid\cite{Sunshine2011}\\
                               &                                                                             & Hybrid  & Clara\cite{Bodden2010}\\
    \hdashline
    Symbolic execution         & Most forms of bugs                           & Static          & SLAM\cite{Ball2011}, KLEE\cite{Cadar}, JPF\cite{Havelund2000} \\
                               & Most forms of bugs                           & Hybrid          & S2E\cite{Chipounov2011} \\
                               & Memory management errors                     & Static          & SLAyer\cite{Berdine2011} \\
                               & Infinite loops                               & Static          & Terminator\cite{Cook2006a} \\
    \hline
    Bug characterisation       & Value replacement                            & Dynamic online  & Value replacement\cite{Jeffrey2009} \\
                               & Delta debugging                              & Dynamic offline & Delta Debugging\cite{Cleve2005,Choi2002} \\
                               & Static program slicing                       & Static          & Weiser\cite{Weiser1981} \\
                               & Dynamic program slicing                      & Dynamic         & Dynamic program slicing\cite{Agrawal1990a} \\
                               & Assisted debugging                           & Interactive     & Model Inference System\cite{Shapiro1982} \\
  \end{tabular}
  \caption{Summary of some existing bug detection and characterisation
    techniques.}
  \label{table:rw:find_char}
\end{sidewaystable}

\subsection{Lockset analysis}

Most programmers designing a piece of concurrent software aim to
follow a particular concurrency protocol, and most concurrency bugs
are caused by failing to do so in part of the program.  By far the
most common such protocol associates a lock with each structure field
and requires that the lock be held whenever accessing the field.
Lockset-based tools attempt to infer a protocol of this form and then
flag an error whenever the program violates it.

Eraser\cite{Savage1997} was the earliest tool of this form.  It is a
dynamic analysis which attempts to build a mapping from memory
locations to the set of program locks which might be associated with
them by the protocol.  These sets start off containing every lock in
the program, and shrink whenever a program thread accesses a location
while not holding one of the associated locks.  If the set ever
becomes empty then there is no locking protocol which is compatible
with the program's behaviour, and so the tool flags a potential
concurrency error\footnote{There is an similarity here with
  {\technique}'s dynamic alias analysis
  (\autoref{sect:program_model:dynamic_alias}), in that both
  approaches justify their analysis with reference to the fields of
  high-level language structures but must perform the actual analysis
  on memory locations, and hence the two techniques make similar
  assumptions about memory type stability.}.

This type of analysis is both sound and complete, in the sense that if
it reports a protocol violation there has definitely been such a
violation, and conversely if it reports no errors then there must
definitely exist a locking protocol which is compatible with the
observed behaviour.  Unfortunately, that is not the most useful sense
of soundness.  In particular, a lockset analysis will report errors
for any lock-free algorithms used by the program, some of which might
be correctly implemented.  Avoiding these false positives generally
requires extending the tool with algorithm-specific special cases.

Lockset analysis can also be applied statically, as, for instance, in
the RacerX system proposed by Engler\cite{Engler2003}.  In RacerX, an
initial static analysis determines, for every line in the program,
which locks are held when that line executes, and the tool can then
use this information to determine which locks protect each field in a
compound structure.  It then reports a potential error whenever this
set is empty.  As with Eraser, lock-free algorithms and other more
complex concurrency protocols require some special cases in the tool.
Despite this limitation, was the tool able to find a number of
interesting bugs in large-scale software systems.

The main weakness of RacerX is its very simple aliasing model, which
assumes that there is precisely one instance of every data type; this
greatly simplifies scaling the analysis to very large programs, but
also greatly reduces its soundness.  RELAY\cite{Voung2007} attempted
to fix this problem whilst also further improving on RacerX's analysis
performance.  The key technique is to use symbolic execution to
construct procedure summaries\cite{Qadeer2004}, showing what locks
each procedure acquires and releases and what structure fields it
accesses.  These summaries then allow the tool to perform a sound
cross-function race analysis without needing to consider the full
bodies of each function every time, leading to useful improvements in
both scalability and soundness.

\todo{Proper comparison to {\technique}.}

\subsection{Happens-before}

Lockset analysis is effective for programs which follow a compatible
concurrency protocol, but not for general concurrent software.  There
is a class of more powerful tools, such as RaceTrack\cite{Yu2005} or
FastTrack\cite{Flanagan2009}, which can detect concurrency bugs whilst
imposing far weaker requirements on the structure of the program to be
analysed.  At their core, these tools consist of a mechanism to build
a partial order of the program's accesses to memory, to determine
which parts of that partial order might affect the program's
operation, and to flag a potential error if the program is dependent
on some unexpected ordering.  A tool might, for instance, declare that
a program is allowed to depend on the order in which threads are
granted a lock when they contend for it, but that it should not depend
on the order of concurrent accesses to the same memory location.  For
performance reasons, most tools based on this idea do not attempt to
maintain the complete partial order but instead model only the
relevant part of it using Lamport's vector clock
algorithm\cite{Lamport1978}.

Schemes based on this idea can, in principle, be made sound, in the
sense that they can report every place in a particular program's
execution which depends on the order in which concurrent operations
are executed.  Practical implementations, however, compromise
soundness in order to reduce the false positive rate.  Consider, for
instance, two programs $P$ and $P'$ where $P'$ is the same as $P$
except for the introduction of the sequence ``release($x$);
acquire($x$);'' in some part of the program which definitely holds
lock $x$.  It is clearly possible for this transformation to introduce
additional bugs, but the only edges introduced to the partial order
will be between lock operations.  A tool which reported all of the
lock edges would have a very high false positive rate for lock-based
concurrent software, but one which does not cannot distinguish $P$ and
$P'$ and so must have false negatives.

\todo{Why is this related?}

Ordering-based schemes can also be applied statically.  An example of
such a system is the Chord static race detector for
Java\cite{Naik2006}.  At a high level, Chord works by first building a
model of the program's existing synchronisation structure, in what
they refer to as the lock analysis phase, and to then perform a second
analysis to determine when two memory accesses in different threads
might alias, which they call the escaping pairs computation.  The
structure constructed the lock analysis phase describes the safe edges
of a happens-before partial ordering and the escaping pairs
computation describes the unsafe ones.  Any unsafe edges which are not
prevented by safe ones are reported as potential program bugs.  Their
would be some scope for combining their model of the program's safe
edges into {\technique}'s \gls{programmodel}, reducing the number of
concurrent interleavings which would have to be considered in the
initial analysis phase and hence the number of
\glspl{verificationcondition} requiring runtime verification, although
converting between their source-level analysis and {\technique}'s
machine code-level one might be challenging.

\subsection{Schedule perturbation}

One plausible definition of a concurrency bug is a bug whose behaviour
depends on the precise relative timing of parts of a program which run
concurrently.  This suggests an alternative scheme for finding such
bugs: simply alter the program's timing and see what happens.  This is
the approach taken by DataCollider\cite{Erickson2010}.  Rather than
trying to find races directly, it instead inserts delays into the
program's execution so as to make races more likely and detects them
when they occur.  This approach has a number of important advantages:
it has no false positives (any race reported will definitely have
happened); it has reasonably low overhead (so the program's behaviour
during analysis is likely to be at least broadly similar to that
during normal execution); and it requires relatively little in the way
of supporting machinery (so it can be used in constrained environments
such as kernel-mode drivers or embedded systems).  I have already
described the algorithm in \autoref{sect:eval:datacollider}.  The
parallels with {\technique} are obvious.  The important difference is
that {\technique} makes use of an initial analysis phase to discover
the best places to insert delays, allowing it to reproduce bugs far
faster than DataCollider.

Chess\cite{Musuvathi2008} can be thought of as a more systematic
approach to the same idea: rather than inserting delays at randomly
selected points in order to perturb the schedule, Chess systematically
enumerates all possible program schedules.  It can then flag errors
when some schedules exhibit interesting bugs.  This allows Chess to
reproduce a wide variety of bugs quickly and easily.  Chess makes two
major approximations in order to get good results.  The first is the
use of bounded preemptions\cite{Musuvathi2007}: Chess only considers
schedules with up to $k$ preemptions, where $k$ is some constant (by
default, two).  Bugs which require more complex schedules cannot be
reproduced.  The second is an unsafe handling of memory-mediated
thread interactions: Chess performs an initial run of the program
under a dynamic data race detector and then assumes that only accesses
flagged in that run will ever suffer data races.  This means that it
can sometimes miss interesting instruction interleavings if
instructions race in some schedules and not others.  Despite this,
Chess has been demonstrated to be an effective way of detecting many
interesting bugs in real-world programs.

CTrigger\cite{Zhou} is another take on this idea.  The approach taken
in that work is to perform an initial static analysis on a program so
as to discover some potential atomicity violations and to then
construct alternative schedules which attempt to trigger those
violations, in the hope that some of them correspond to a bug.  This
is, again, rather similar to {\technique}'s \gls{bugenforcer}
mechanism.  There are several important differences between the two
techniques: CTrigger operates on the program's source, whereas
{\technique} operates on machine code; CTrigger considers only simple
two-variable atomicity violations, whereas {\technique} considers a
much broader class of atomicity violations; CTrigger aims to reproduce
atomicity violations, whereas {\technique} attempts to reproduce
actual bugs; and CTrigger reproduces simple instruction interleavings,
whereas {\technique} can reproduce data-dependent ones.

\subsection{Stereotyping}

All of the approaches discussed so far rely on some pre-defined notion
of what it means for a program to have a bug.  An alternative approach
is to simply define a bug to be anything which the program doesn't do
very often.  Tools based on this idea start by building up a model of
how a stereotypical execution of the program behaves and then report
any deviation from the stereotype as a potential bug.

This is essentially the same problem as is solved by anomaly detecting
intrusion detection systems\cite{Forrest1996a}, and such systems can
be regarded as a kind of bug detector.  In practice, though, systems
designed for detecting intrusions are not usually well-suited to
detecting bugs, as they tend to operate at the wrong level of
abstraction, and I do not consider them in detail here.

The Daikon\cite{Ernst2007} was one of the earliest anomaly-based
bug-detecting systems.  Their basic approach is to collect one or more
traces of the program while it is operating normally, recording the
values of local variables and function parameters at various
interesting points in the program's execution.  These traces are then
mined to find conditions on those values which always hold, and these
conditions are converted into assertions which can be inserted into
the program.  Any failure of one of those assertions is reported as a
probable bug.  DIDUCE\cite{Hangal2002} works on a similar principle,
but builds its invariants ``the other way around'': rather than trying
to find conditions which are true for all known executions, it starts
with a condition that nothing at all is possible and then weakens it
until it allows all of the known behaviour.  This tends to lead to
sharper inferred invariants, in the sense of allowing less behaviour,
and so DIDUCE tends to find more bugs but with a higher false-positive
rate.

The model-building parts of these tools could potentially be combined
with {\technique} so as to expand its \gls{programmodel}, and hence
allow it to analyse aspects of the program's behaviour which depend on
instructions outside of the \gls{analysiswindow}.  That could
potentially lead to useful improvements in both analysis performance
and precision.  HeapMD\cite{Chilimbi2006}, which uses this kind of
method to build up a model of the heap, would be particularly
interesting to investigate in this respect.

The anomaly-detection approach has also been applied to cross-process
concurrency bugs such as filesystem TOCTOU
errors\cite[pages~44--45]{Apple2012SecureCoding}.  Pu and
Wei\cite{Pu2006}, which built on earlier work by Uppuluri et
al.\cite{Uppuluri2005}, produced a modified version of the Linux
kernel which protects applications against this kind of attack.  This
kind of technique is of less direct relevance to {\technique}, which
only considers intra-process bugs, but might still represent an
interesting way of handling calls to library functions.

These techniques can also be applied statically, as in, for example,
the system presented by Engler et al in SOSP
2001\cite{Engler2001}\editorial{ugg} or MUVI\cite{Lu2007}.  The idea
here is to perform a kind of statistical static analysis to identify
common patterns in a program's source code and to then flag a warning
whenever any of these patterns are violated.  It might be, for
instance, that a call to \texttt{spin\_lock} is almost always followed
by a call to \texttt{spin\_unlock}, so that the remaining calls to
\texttt{spin\_lock} are likely to be bugs.  Systems based on this
observation are able to discover interesting violations of
program-specific invariants without ever having to be told what those
invariants are.  MUVI\cite{Lu2007}, which discovers rules related to
when variables are likely to be accessed together, is particularly
relevant in the context of concurrency bugs, as many atomicity
violation bugs will violate at least one of those rules.

\subsection{Extended assertions}

All of the schemes discussed so far in this section are intended to
discover concurrency bugs automatically.  An alternative approach is
to instead make it easier for the programmer to find the errors
themselves.  Meta-level compilation\cite{Engler2000a}, which allows
programmers to extend the compiler with system-specific static
checking rules, is one example of such a system.

The TESLA project\cite{Watson2013}\editorial{still need a better
  cite} represents another approach to this idea.  In the TESLA model,
the programmer provides an approximation of the desired state machine
for some fragment of the program, expressed as a formula in a
linear-time temporal logic.  This is then compiled into a program
monitor which runs alongside the program and checks that it conforms
to this state machine.  There are close parallels here with
{\technique}'s \glspl{bugenforcer}.  It be interesting to try to
combine the two approaches by, for instance, using TESLA assertions as
input to the \gls{bugenforcer} building algorithm, rather than trying
to infer them directly from the binary.

\subsection{Typestate systems}

Type state systems\cite{Strom1986a} are an older approach to the
problem of allowing the programmer to express complex program
properties which are to be enforced.  In this model, the assertions
are described by adding state to program types, with a description of
how certain operations can move instances of a type from one state to
another, and then restricting the program to only allow certain
operations on objects which are in a particular state.  These
assertions are typically checked at run time\cite{Wolff2011}, whether
in an ad-hoc\cite[pages~305--314]{Gamma1995} or
systematic\cite{Aldrich2009} manner, and can also in some cases be
checked statically\cite{Lam2005}.  Some systems, such as
Clara\cite{Bodden2010}\editorial{Might also want to reference chain
  from there.}, attempt to split the difference, checking as much as
possible of the assertion statically and deferring the rest to run
time.  This gives them a great deal of flexibility to trade the
incompleteness of dynamic analysis off against the high computational
cost of sound static analysis.  There is potentially some scope for
combining this sort of technique with {\technique}'s enforcer
mechanism to evaluate some side-conditions statically, which might
reduce the enforcer overhead.

2ndStrike\cite{Gao2011} is a typestate checking system which is
particularly relevant to {\technique}.  In that work, the authors
compared traces of a concurrent program's execution to typestate-style
specifications of its intended actions so as to derive alternative
schedules which might plausibly have violated the typestate
properties.  At a high level, this is rather similar to {\technique}'s
approach of finding potential bugs using symbolic execution and then
trying to reproduce them by forcing the program to follow a particular
schedule.  There is even a useful parallel between 2ndStrike's
typestate protocols and {\technique}'s side-condition checking logic.
The key advantage of {\technique} is that it can infer the
side-condition for itself, whereas 2ndStrike relies on a programmer
manually specifying the typestate protocol.  This does, however, allow
2ndStrike to investigate more complex bugs than {\technique} can
handle.

\subsection{Symbolic execution}

There is a very large body of existing literature which investigates
ways of automatically detecting various classes of non-concurrency
bugs using symbolic execution.  These range from the very simple
systems integrated into most compilers\needCite{} all the way through
to whole-program model checking with symbolic
execution\cite{Ball2011}.  \todo{Say more here.}

The most influential recent example is probably KLEE\cite{Cadar}, a
symbolic execution engine for LLVM bitcode\cite{Lattner2013}.  The aim
behind KLEE is the automatic generation of test suites for
single-threaded programs which exercise the maximum proportion of the
program's functionality with the minimum amount of testing.  This is
conceptually very similar to {\technique}'s enforcer approach to
finding bugs: perform some static analysis on the program to find
places which might have a bug, and then convert those places into
tests which will show whether or not that bug is present.  The
difference is that {\technique}'s tests are expressed in terms of the
concurrency schedules which are to be exercised, whereas KLEE's are
expressed in terms of the inputs to the program\footnote{KLEE itself
  can be parallelised, as in, for instance, Cloud9\cite{Ciortea2010},
  but this is not the same as exploring parallel schedules in the
  program under test.}.  It would be possible to combine the two
techniques by, for instance, using KLEE to find program inputs which
allow {\technique} enforcer side-conditions to be satisfied.  This
would mitigate {\technique}'s most important weakness, that most of
the enforcers it generates require conditions which the program can
never actually reach, and increase KLEE's ability to find interesting
concurrency bugs.

SLAM\cite{Ball2011}, and the many systems based on it, such as
SLAyer\cite{Berdine2011} and Terminator\cite{Cook2006a}, is another
important example.  Unlike KLEE, it operates primarily at the level of
source code.  Most SLAM-based analyses are primarily thread-local,
considering only the sequential execution case; \cite{Cook2007} is one
important exception.  \todo{Say more.}

JPF, the Java Pathfinder\cite{Havelund2000}, is another similar system
which operates on Java Bytecode\cite{Lindholm2013}.  As such, it has
access to a great deal more detailed information about the program,
making the analysis task significantly easier, but still less than a
source-level analysis would have.  It has been used both as a research
platform for new approaches to the symbolic execution problem
itself\cite{D'Amorim2008,Gligoric2010} and to verify large-scale
systems in an industrial context\cite{PCZsCZreanu2008}.  As such, it
represents one of the most well-developed symbolic execution systems
in use today.  Operating at the bytecode level means that they avoid
most of the difficult issues faced by {\technique}'s symbolic
execution system.

Concollic execution systems such as CUTE\cite{Sen2005} or the more
recent S2E\cite{Chipounov2009} are a refinement of symbolic execution
which can more easily scale to very large software systems.  The main
weakness of symbolic execution is its high computational cost, which
is caused by the need to explore a very large number of paths.  This
can be mitigated by dividing the program into fragments which are
validated independently, so that the component which is currently
being checked is executed symbolically while the rest of the system is
executed concretely.  It might, for instance, be useful to execute a
program symbolically but all of its libraries concretely, so that the
high cost of symbolic execution is only paid for the code which is
most likely to contain interesting bugs.  Managing the interface
between the concrete and symbolic code is quite complex in this model,
especially if the tool is intended to be sound.  \todo{Say more}

\nocite{Andrews2004}

\section{Automatically characterising bugs}
\label{sect:rw:auto_characterise}

One of {\implementation}'s modes of operation takes a snapshot from a
program which crashed due to a bug and converts it to a verification
condition showing what must have happened for the bug to have
occurred; in other words, it provides a characterisation of the bug.
This is a field which has been extensively researched in the past.
The Model Inference System\cite{Shapiro1982} is one of the oldest
systems.  It was a system for localising bugs in Prolog programs by
asking the user a minimal number of questions\editorial{So what?}.

Delta Debugging\cite{Cleve2005} is a more recent example.  In this
approach, the tool compares traces taken from a program when it was
working to ones taken while it was exhibiting a bug in order to find
the parts of the traces which most effectively predict whether the
program will suffer the bug being investigated, and hence to
automatically locate the root cause of the bug.  The approach was
extended by \cite{Jeffrey2009}\editorial{ugg} to include actively
modifying a program's state as it runs and observing the effects.

The Delta Debugging paper which is most relevant to the current work
is probably \cite{Choi2002}, which explored how to apply the Delta
Debugging technique to concurrent programs.  Their basic approach is
to record a program's action in a deterministic replay system and to
then replay it under slight variants of that schedule, so as to
determine which parts of the schedule are necessary for the bug to
reproduce.  This allows them to produce the simplest possible
reproduction of the original concurrency bug.  \todo{Compare to
  {\technique} in a more than trivial way.}
Narayanasamy\cite{Narayanasamy2007} proposed a similar algorithm
using a different replay framework.

Program slicing is another approach to the bug characterisation
problem.  The starting point for this work is the observation that the
hardest part of understanding a bug is often figuring out which parts
of a large program are actually relevant to it, and so automating this
process would usefully simplify the bug-finding process.  There are
two basic approaches to program slicing: static\cite{Weiser1981} and
dynamic\cite{Agrawal1990a}.  Both of these rely on taking some
representation of the program and then filtering and transforming it
so as to remove aspects which are irrelevant to the bug under
investigation; the difference is that a static program slice starts
from the static program text, whereas a dynamic one starts from a
trace of the program's execution.  Program slices are conceptually
very similar to {\technique}'s {\StateMachines}, albeit derived in a
different way and for a different purpose.  The key difference between
the two representations is that a program slice is usually represented
in the same language as the original program, whereas a
{\StateMachines} are expressed in an analysis language.  This makes
them easier to analyse, but at the expense of also making them more
difficult to derive.

\section{Automatically fixing bugs}

There have been many previous systems which aimed to automatically fix
bugs in programs, and I now give a brief overview of some of the more
important ones.

\subsection{Description of the problem}

\todo{Could actually pull this up to the introduction.}

One way of thinking about automatically fixing bugs is as a
transformation from one program to another, preserving some properties
of the program while altering others.  This is a challenging problem
at the best of times, but it is particularly difficult in this case
because the properties are usually quite ill-defined.  It is extremely
rare for one of the programs to be fixed to have a complete,
machine-readable, specification, and only slightly more common for
their to be a precise specification of the behaviour to be avoided.
Consider, for instance, a program which has a data race and which
calls \texttt{abort()} when the race goes one way and \texttt{exit(0)}
when it goes the other way.  This would appear, at first, to be an
obvious example of a concurrency bug, and the obvious fix would be to
cause the race to always go the way which avoids the \texttt{abort()}
call.  On the other hand, if the program's intended behaviour were to
investigate the processor's memory model and to then report its
results by either raising or not raising the \texttt{ABRT} signal then
this ``fix'' might convert a correct program into an incorrect one.
This is obviously an unlikely specification for a program to have; it
is equally obviously a \emph{possible} one.  As another example,
consider a video player which sometimes displays frames with a few bad
pixels due to bad thread interleaving.  This program clearly has a
bug, but a player which never shows an incorrect frame but which can
only display them at a tenth the desired rate would probably be far
less useful, despite being in some sense ``more correct''.  The
difference between a correct fix and an incorrect one is not the fix
itself, or the program to be fixed, but the intent of the user, and
without knowing that the fix generation tool must necessarily rely on
heuristics.

A slight change of perspective makes the trade-offs easier to
understand.  Rather than viewing fix generation as a transformation of
the program, it is sometimes more useful to view it as a
transformation to the computing environment in which the program is
running, defined in terms of the operating system, processor,
libraries, etc.  Whenever a programmer writes a program, they will
have some (usually quite informal) model of the semantics of the
underlying computing environment, and they will have designed their
program against that semantic model.  This semantic model is extremely
unlikely to be exactly the same as the semantics which is actually
implemented by any physical computer (if nothing else, the physical
semantics differ markedly across computers, and most programs are
designed to run on more than one system), and that semantic gap gives
fix generation tools some room to manoeuvre.  In particular, the
programmer's semantic model will usually leave some parts unspecified,
and hence map to a large set of physical semantics such that any
property which is guaranteed by the programmer's model will be
guaranteed by any of the physical semantics.  This means that we can
safely select any physical semantics from this set, secure in the
knowledge that doing so will preserve whatever correctness the
original program might have had, and it is this flexibility which
potentially allows us to fix or mask errors.

Of course, this does not solve the problem, because we have no way of
knowing what semantics the original programmer had in mind when
writing the program.  We can make some intelligent guesses, however:

\begin{itemize}
\item Some parts of the physical semantics will differ from run to
  run.  One obvious example is the exact memory interleaving when two
  processors run in parallel.  Assuming the program is intended to
  work every time it runs, it is reasonable to assume that the
  programmer's semantics leave this behaviour undefined, and so it is
  safe for the automatic bug fixing tool to change it.

\item Likewise, language specifications and processor architecture
  manuals also often leave certain boundary cases unspecified,
  e.g. the effect of a use-after-free in C\cite{Kernighan1988}.  While
  it is possible for a program to depend on this unspecified
  behaviour, it is usually considered to be poor software engineering
  practise\cite{CWE758}, and so it is often safe to assume that it
  remains unspecified in the programmer's semantic model.

\item Sometimes, an operation is perfectly well defined, but indicates
  an error sufficiently often that it is sensible that it does so
  every time, and hence that it is safe to change its meaning.  For
  instance, one might reasonably require that a program never exit due
  to dereferencing a bad pointer, and hence allow the meaning of any
  operation which would normally do so to be changed.

\item Many systems leave the exact circumstances under which certain
  components fail undefined, and hence allow us to inject artificial
  errors safely.

\item More controversially, it would be possible to introduce a kind
  of extra memory or hysteresis into the program, such that once it
  has been observed to behave in a certain way a certain number of
  times, it is forced to keep behaving in that way from then on.  For
  instance, if a particular variable is found to be between five and
  ten in every training run, and is then found to be twelve in a
  subsequent one, it could be forcibly changed back to ten.  It is
  hard to imagine any programmer ever using this as their semantic
  model of the hardware, but it might sometimes capture part of their
  model of the \emph{program}, and hence allow a tool to produce fixes
  which are ``sympathetic'' to that model.
\end{itemize}

In order to make a useful bug fixing system, it is generally necessary
to change the semantics in two ways.  First, there must be some
indication that something has gone wrong: in the physical semantics,
every operation has a defined result, and so there is no way to tell
whether a given operation was desired, and so no way of triggering the
automatic fixing process.  This issue can be avoided by declaring
certain actions to be bad, so that we can assume that any such action
is considered to be a failure\footnote{Note that it is also possible
  to design always-on systems, which try to fix or ameliorate bugs in
  general without caring about any \emph{specific} instance, and in
  that case no trigger is needed.}.  Second, we must relax the
semantics enough to give us the necessary flexibility to avoid the bad
actions.  The choice of these two changes is one of the most critical
aspects of a program auto-fix system.  In many cases, they can be
changed independently of one another.

Interestingly, the new semantics is not always required to be causal.
It may, in some cases, be useful to respond to an error by rolling
back to an earlier checkpoint and taking a slightly different path.
From the point of view of the program, the error influenced the
behaviour at the checkpoint, even though the error happened strictly
after the checkpoint, and hence, from the program's perspective, this
is a non-causal semantics\footnote{Causality is, of course, maintained
  from the perspective of the bug fixer itself, and so the semantics
  is paradox-free and implementable.}.

\subsection{Software rejuvenation and micro-reboots}

One of the earliest attempts at fault remediation was software
rejuvenation\cite{Huang1995}, which attempted to ameliorate the
effects of resource leaks by periodically rebooting the affected
systems.  This lead to quite a lot of work which attempted to
calculate the optimum reboot schedule
\cite{Li2002,Vaidyanathan2001,Garg1998}, and also some attempts at
reducing the cost of reboots \cite{Candea2002,Patterson2002}.  This is
essentially current industry practice in dealing with faults in most
software: wait until it goes wrong and then turn it off and on again.
While undeniably effective, this is a rather inelegant approach to the
problem, and carries very high cost; most other work on automatic
fault remediation can be regarded as attempts to reduce the frequency
with which operators must fall back on these techniques.

\subsection{Failure obliviousness and related techniques}
More recently, Rinard et al\cite{Rinard2004} described failure
obliviousness, a technique for disguising certain classes of memory
faults in high-availability systems at the (possible) expense of
reduced integrity.  The core idea here is, essentially, to treat
hardware exceptions as warnings rather than errors, and to try to
execute through them as far as possible in the hope that the error
will self-cleanse rather than propagating further.  Some faults are
trivial to ignore (stores through bad pointers, for instance, can be
simply discard), while others require more sophistication (loads of
bad pointers, for instance, must somehow synthesis a loaded value).
The original paper simply used a manually pre-defined sequence of
plausible values; later work expanded upon this by looking at the
dynamic dataflow context\cite{Nagarajan2009} or by using a lookaside
table of recently discarded writes\cite{Rinard2005a}.

The idea behind the reactive immune system\cite{Sidiroglou2005} is
similar, except that rather than trying to discard memory operations
they instead try to convert errors from unhandleable memory errors
into whatever sort of errors the program can handle.  The initial
implementation did this by forcing functions to return immediately
with an error value, obtained by type analysis on the source code;
this was refined in the ASSURE system\cite{Sidiroglou2005} to instead
take snapshots at places where it is convenient to inject errors and
then roll back when an error is detected.  Of course, error handling
is often rather buggy itself, and so recovery to error handling is not
always useful.  This issue was investigated in detail by
S\"{u}\ss{}kraut et al.\cite{Susskraut2006}, who also propose some
techniques for automatically improving its robustness.  \todo{Could
  cite HEALERS here?}

One potentially interesting approach, proposed by Elkarablieh et
al.\cite{Elkarablieh2007} in a slightly different context, would be to
try to mine the program for information about the intended contents of
data structures.  This would then provide useful information when
deciding how to synthesise the results of wild reads so as to minimise
the potential for error propagation, or even, in a somewhat extreme
form, to proactively fix data structures which have suffered
corruption.  This potentially increases the effectiveness of error
hiding, but also potentially increases the potential for the program
to generate completely nonsense results.  In the original paper, these
data structure invariants were obtained from \verb|assert()|-like
statements in the program source, combined with some basic static
analysis.  A later version, proposed by Malik et al.\cite{Malik}, used
Daikon\cite{Ernst2007}-like detection of statistically justified
invariants during normal program operation, which allowed the
technique to be applied without source code, but is also utterly
terrifying\editorial{phraseology}.  ClearView\cite{Perkins} refines
this approach by combining it with an automated testing system to try
to reduce the risk of introducing new bugs.

DieHard\cite{Berger2006} is another application of the failure
oblivious concept to heap-related issues.  In this system, however, no
attempt is made to discover or to enforce data structure invariants;
instead, the heap is structured so as to minimise the probability of
certain common types of bugs causing user-visible errors.  The authors
use two main techniques to achieve this:

\begin{itemize}
\item First, the heap is expanded, such that there are likely to be
  large dead zones between any two allocations.  This makes buffer
  overflows much less dangerous.
\item Second, they avoid reusing heap locations quickly after they
  have been \texttt{free()}d, which reduces the risk of use-after-free
  bugs actually causing problems.
\end{itemize}

Neither of these techniques will eliminate bugs, but they can
dramatically reduce the probability of their causing user-visible
problems (at the expense of dramatically increasing memory
requirements and marginally increasing runtimes).
Exterminator\cite{Novark2007} further builds on this work by using
heuristics on the expanded heap to try to identify probable bugs, and
then modifying the allocator to only apply heap expansion to
allocations which are likely to benefit from it.  Assuming that all
such allocations are detected, this retains all of the bug-fixing
benefits of DieHard while noticeably reducing its overhead (at least
in the common case where only a small number of allocations actually
need padding).

The AutoPaG system\cite{Lin2007} tackled a related problem, that of
overflows of stack-based buffers.  In this work, the authors assume
that a buffer overflow has already been identified by some mechanism
(a CCured\cite{Necula2005}-like safe compiler in the paper, but others
are possible), and then apply static analysis to find its root cause
at the source code level.  They then generate a source-level patch
which redirects any out-of-bounds accesses to the array back to a safe
location, in what is essentially a variant of failure obliviousness.
This allows them to mask the bug until a true fix can be obtained,
with very low run-time overhead in both time and space.
Unfortunately, their static analysis is not complete, and so they
must occasionally fall back to a dynamic scheme.

\subsection{Input rectification}

Rather than trying to fix bugs in a program, another strategy is to
sanitise the program's inputs so that it never sees anything which
might upset it.  RX\cite{Qin2007} is one example of such a system.  RX
protects a program reactively.  When it notices that something has
gone wrong, it rolls the program to an earlier checkpoint and then
replays it in a slightly different environment, with slightly
different inputs and a slightly different thread schedule, in the hope
that doing so will avoid the bug.  When it finds a strategy which
works it caches the details of the modifications it made, so that next
time it encounters a similar failure it can simply repeat those
modifications.  In this way it is able to protect the program from the
dangerous inputs without needing any knowledge of the program
structure, and very little knowledge of its interactions with the
environment.  First-Aid\cite{Gao2009} extended RX to handle certain
kinds of heap-related errors using a similar strategy of rolling the
program back and then trying it again with slightly different heap
behaviour; it can be thought of as the union of RX and DieHard.
Bouncer\cite{Costa2007} and Vigilante\cite{Costa2008} used similar
techniques to quickly protect network-facing services from
newly-released worms.

RX's main weakness is that it has no knowledge of the structure of a
program's inputs.  This means that if a bug is caused by an input it
has no choice but to discard the entire input.  SOAP\cite{Long2012}
tries to rectify this weakness by finding safe approximations to
dangerous input files.  The idea here is to start with a model of the
rough structure, extend it to include safeness constraints by running
the program against a training set and using some dynamic analysis,
and to then sanitise any future inputs to the program so that they
satisfy these safeness constraints.  The intent here is to find a
closest safe approximation to the dangerous input.  This means that
programs protected by SOAP can still do something useful even on
inputs which would normally cause them to crash.

\subsection{Hardware-based fixes for data races}
Of course, memory errors are only one class of bugs.  Synchronisation
errors form another important class, and a number of projects have
investigated remediation strategies for these.  One of the earliest
was ReEnact\cite{Prvulovic2003a}, which used modified thread-level
speculation hardware to capture precisely what happened during a data
race or atomicity violation, and then to control instruction
scheduling during subsequent re-executions so as to avoid the bug in
future.  This is similar to the intent of the fixes generated by
{\technique}, with a few exceptions:

\begin{itemize}
\item The ReEnact scheme requires unusual hardware support, whereas
  {\technique} is purely software-based.
\item Their fixes are dynamic, in the sense that the program must be
  constantly monitored to ensure it does not follow a bad schedule,
  whereas the {\technique} fixes statically introduce required locking
  so that bad schedules become impossible.
\item They require some (quite modest) amount of programmer
  involvement in order to identify which races are critical to a
  particular bug and hence to direct the fixing process, except in a
  few unusual special cases.  {\Implementation}, by contrast, is
  entirely automated\footnote{Except for needing to identify
    malloc-like functions, as discussed in
    Section~\ref{sect:program_model:dynamic_alias}.}.
\end{itemize}

Despite these limitations, ReEnact was able to fix some bugs, and
shows reasonably low run-time overhead (mostly on the order of ten
percent, depending on the benchmark).

Atom-Aid\cite{Lucia2009} is another scheme which tries to use unusual
hardware features to protect against data race and atomicity violation
bugs, but using hardware transactional memory\cite{Herlihy1993} rather
than thread-level speculation.  This paper grew out of an earlier
observation that, in some cases, processor performance can be improved
by bundling sequences of memory accesses into transactions, and hence
batching interconnect operations and amortising their
costs\cite{Ceze2007}.  This has the interesting side effect of
eliminating a large number potential instruction interleavings, and
hence a large number of potential synchronisation bugs.  Atom-Aid
attempts to maximise this effect by carefully tweaking transaction
boundaries in response to the program's observed behaviour.  This
allows them to hide most atomicity violations with very low
overhead\footnote{The paper asserts that they have negligible
  overhead, but does not attempt to quantify or justify that
  statement, and so the true overhead is rather difficult to
  evaluate.}.  Unfortunately, the necessary hardware changes are
unlikely to be widely deployed in the near future, and it is hard to
see how to adapt the system into a software-only implementation, which
makes these techniques somewhat less interesting.

\subsection{Software-based fixes for data races}
ConTest\cite{Krena2007} provides one of the simplest approaches to
fixing data races using software rather than hardware.  In this work,
races are detected using the ERASER\cite{Savage1997} algorithm, and an
implicit assumption is made that all data races are bugs which need to
be eliminated.  Elimination is performed using a small number of
pre-defined synchronisation patterns\footnote{In the paper, the only
  race pattern considered is wrapping a load followed by a store in a
  lock, but others could be added easily.}.  It is unclear from the
paper whether this is actually a useful thing to do, because the
majority of races are benign, and the majority which are not are more
complicated than their patterns can describe.  Tallam et
al.\cite{Tallam2008} suggested essentially the same mechanism a year
later, but restricted themselves to uniprocessor execution (so the
only parallelism is the coarse-grained variant provided by the
operating system's thread abstraction); this allowed them to make some
simplifications to their implementation, but further reduced the
useful scope of the technique.

ToleRace\cite{Kirovski2007}, which attempts to provide toleration for
asymmetric races, provides another useful point in the design space.
An asymmetric race is defined by the authors to be a situation where
some thread correctly follows a locking discipline (which must be
manually specified by the programmer), but another thread does not,
and thus causes the correct thread to fail.  This presents a challenge
when debugging, as most na\"{i}ve approaches to postmortem analysis
will blame the wrong thread for the crash.  ToleRace ameliorates this
class of problems by arranging that when a thread acquires a
particular lock, a local snapshot of all of the values protected by
the lock is taken, with any accesses to protected variables made while
holding the lock redirected to the snapshot.  This prevents the
correct thread from seeing the effects of incorrect threads while it
is in the critical section, which makes the race much less likely to
cause serious problems, but does not guarantee to produce a consistent
snapshot of the protected data.  ISOLATOR\cite{Ramalingam2009} fixes
this defect by introducing implicit per-page locks, which are enforced
using virtual memory techniques.  However, it is still necessary for
the programmer to provide a manually-specified lock discipline, which
makes these techniques difficult to use in practise.

AVIO\cite{Lu} is another approach to automatically fixing
concurrency-related bugs.  The idea here is to observe the program
during normal operation, and hence build up a model of its expected
access interleavings, and to then ensure later on that only those
interleavings are possible.  The intuition here is that bugs are
unusual events, and so preventing anything unusual from happening will
prevent any bugs from occurring.  The weakness, of course, is that
sometimes something unusual does happen (if, for instance, the program
receives some input which was not adequately covered by the training
data), and AVIO will be unable to provide protection in this case.
AVIO also has the weakness that it requires unusual hardware support
in order to achieve good performance; while it can be implemented
purely in software, the software implementation causes a roughly
twenty-five-fold slow-down, making it impractical to use in production
systems.

Kivati\cite{Chew2010} can be thought of as a refinement to AVIO which
reduces these weaknesses.  In Kivati, the access interleaving
invariants are discovered by means of a static analysis conducted
before the program starts running.  This set is usually far smaller
than the set which is discovered by AVIO.  That then allows Kivati's
second refinement, which is to enforce the invariants using the
processor's watchpoint registers\cite[Chapter 16.2: Debug
  Registers]{Intel2009}, obviating the need for custom hardware.  The
result is a similar level of protection to AVIO but with far less
overhead, usually on the order of a few tens of percent.  Kivati's
main weakness is that it can only protect against single-variable
races, due to limitations in both the static analysis used.  Even if
the static analysis were extended, limitations on processor watchpoint
facilities would prevent it from considering large numbers of
variables at the same time.  {\Technique} does not share this
limitation, and the fixes it generates have noticeably lower overhead.
The downside, of course, is that {\technique}'s initial analysis phase
is far more expensive than Kivati's.

AFix\cite{Jin2011} is another similar piece of work in this area, and
was already discussed briefly in Section~\ref{sect:fixing:rw_afix}.
It works by patching additional lock operations into LLVM bitcode so
as to eliminate atomicity violation bugs.  It was later generalised to
CFix\cite{Jin2012}, which adds support for fixing certain ordering
violations by patching in condition variable operations.  This is
interesting because most other automated bug-fixing systems (including
{\technique}) largely ignore ordering violations.  \todo{Say more.}

ConAir\cite{Zhang2013} and ConMem\cite{Zhang2010} take a completely
different approach to fixing concurrency bugs.  Rather than trying to
prevent the bad interleavings from happening, they instead allows the
program to suffer the bug, and then try to roll the one buggy thread
back and retry it in isolation, in the hope that whatever it was that
caused it to go wrong will have resolved itself.  Until the failure is
detected, the ConAir does not make any modifications to the program's
behaviour.  This gives it very low overhead, but makes it problematic
to roll threads back after they have performed any non-idempotent
transformations of the program state, including most modifications to
shared memory.  The restriction to idempotent re-execution regions is
similar to the \gls{w-isolation} assumption: ConAir can only
re-execute regions which cannot influence another thread's behaviour,
and {\technique}, when the \gls{w-isolation} assumption is enabled,
can only analyse \glspl{crashingthread} which do not influence the
\gls{interferingthread}'s behaviour.  In fact, the restriction on
ConAir is somewhat stronger than the \gls{w-isolation} assumption, as
it cannot re-execute any code which \emph{might} influence another
thread, whereas \gls{w-isolation} is only violated if every path which
triggers the bug involves that kind of influence.

Aviso\cite{Lucia2013} presents another variant on this approach.
Like ConAir, Aviso starts by allowing the program to suffer the bug at
least once and then modifies the schedule so that it does not suffer
it again.  Unlike ConAir, though, Aviso does not attempt to roll-back
any threads, but instead records that the schedule is bad.  They then
observe multiple runs of the program, potentially across multiple
machines, so as to statistically infer a causal relationship between
properties of the schedule and the reproduction of the bug.  These can
then be used to avoid the bug in future runs.

\subsection{Deadlock bugs}
The final class of bugs considered here is deadlocks.  Techniques for
healing deadlocks in multithreaded applications have only been
investigated relatively recently (deadlock avoidance has been studied
in other contexts for much longer; see e.g. \cite{Viswanadham1990} or
\cite{Dijkstra2004}).  One of the earliest, proposed by Nir-Buchbinder
et al.\cite{Nir-Buchbinder2008}, was to build the dynamic lock graph
as the program runs, discover any strongly-connected components
(SCCs), and then reducing every SCC to a single lock.  This eliminates
the potential for any lock order reversal deadlock involving that
cluster of locks, and would, if a complete lock graph were available,
it would completely eliminate all LOR deadlocks in the program.
Unfortunately, dynamically collected lock graphs are inherently
potentially incomplete, and this means that the healing is incomplete,
and can in fact introduce new deadlocks in certain situations.  The
need to potentially combine large sets of locks into a single large
lock can also lead to excessive serialisation, and hence poor
performance.

Gadara\cite{Wang2008} tackled both of these problems.  First, they
used static analysis to build a conservative approximation of the lock
graph (rather than the optimistic one produced by a dynamic analysis).
Second, they use discrete control theory to derive a ``controller''
which inserts delays in the dynamic program execution in some minimal
set of places so as to avoid deadlocks without introducing unnecessary
serialisation.  Of course, the use of a conservative approximation
means that they will occasionally detect a deadlock where none is
actually possible, and this will lead to additional synchronisation
operations; it is unclear from the paper whether Gadara-protected
systems are more or less serialised than Nir-Buchbinder-protected
ones.  At a high-level this controller is conceptually similar to the
message-passing machines used in {\technique}'s crash enforcers,
although the details and purpose of the technique are significantly
different.

It is also possible to make progress on these problems in a purely
dynamic system.  For instance, Dimmunix\cite{Jula2008} waits until a
deadlock is observed at run time, then captures a signature for that
particular deadlock, and arranges that the signature never reappears
by delaying lock acquire operations.  The hope is that preventing the
signature will also prevent the deadlock, and hence that the program
will, over time, become immune to whatever deadlocks might be lurking
in it.  Because it only fix deadlocks which have actually been
observed, serialisation is kept low and the need for complete locking
information is side-stepped, although at the cost of having to suffer
every deadlock at least once in order to fix it.

Of course, this approach will only be successful if the deadlock
signatures accurately capture the cause of the deadlock, without
capturing too much extraneous information.  The suggestion in the
paper is to use the set of locks held by every thread, combined with a
(slightly summarised) backtrace captured when the lock was acquired.
Earlier versions of the paper (e.g. \cite{Jula2008b}) mentioned
alternative schemes; I assume the fact that the discussion was dropped
implies that the alternatives were investigated and found to be
unhelpful.  That would be consistent with the rest of the evaluation.

\subsection{Deterministic multi-threading}

Deterministic multi-threading systems, such as
Determinator\cite{Aviram2010} or dOS\cite{Bergan2010}, take a
different approach to concurrency bugs.  Rather than trying to detect
or fix such bugs, they instead design the runtime environment as to
make such bugs deterministic, and hence far easier to find and fix, in
much the same way that {\technique}'s \glspl{bugenforcer} make its
bugs easier for a programmer to deal with.

The most direct way of ensuring determinism, recommended by
Determinator, is to make the API presented to programmers
deterministic, so that there is simply no way for applications to do
anything which even appears to be non-deterministic.  In the case of
Determinator, this means that the main form of parallelism supported
is fork-join, with shared-memory parallelism implemented as a merge
operation on memory, analogous to the merge operation in a distributed
version control system\cite{Hamano2013}.  This is easy to understand
and has very respectable performance, and would arguably have been a
very good idea thirty years ago, but requires that most software be
re-implemented for the API.  This makes it quite impractical for any
but quite specialist environments.

\todo{Wow, this is a crappy paragraph.} Rather than re-designing the
API, it is also possible to retrofit determinism to an existing API
such as POSIX or Win32.  This is the approach recommended by
dOS\cite{Bergan2010} and DThreads\cite{Liu2011}.  The key
observation behind these systems is that even when the API is
notionally non-deterministic, most actual programs spend most of their
time running completely deterministic code.  In particular, even when
two threads are operating in the same address space, most of the time
they will be operating on different parts of the address space, and so
will be unable to influence each other's behaviour.  If there were an
efficient way of detecting transitions between deterministic and
non-deterministic modes of operation then it would be possible to
achieve efficient determinism by running the already-deterministic
components of the program in parallel and serialising the
non-deterministic component.  DThreads and dOS achieve this by
implementing a concurrent-read exclusive-write protocol in the
hardware's page tables\cite[Chapter 4: Paging]{Intel2009}, so that it
is never possible for one thread to read a memory location which is
being concurrently modified by another.  Any thread which attempts to
violate this protocol is delayed to an epoch point, at which the
threads are permitted to run serially.  Careful choice of epoch points
then allows the system to make software design for a non-deterministic
API behave deterministically with very little overhead.

\input{related_work/summary}

\section{Decompilation and other analysis of machine code}

{\Technique}'s approach to generating {\StateMachines} from machine
code can be viewed as a form of decompilation, in the sense that it
takes a low-level representation of a program and converts it to a
higher-level one.  The most important difference is that decompilation
aims to preserve all of the behaviours of the program, whereas
{\technique} seeks only to preserve those behaviours which are most
relevant to the bug which is being investigated.  Cifuentes'
dissertation\cite{Cifuentes1994} provides a thorough, if now quite
outdated, overview of the field.  Boomerang\cite{Emmerik2004} is a
more recent example.

CodeSurfer\cite{Balakrishnan2005a,Balakrishnan2008}, also known as
WYSINWYX, is a more recent example of a decompilation platform.  The
intent here is to provide a human user with assistance in determining
what a binary-only program does, rather than supporting extensive
automated analyses.  Like BAP, CodeSurfer has only limited support for
multi-threaded programs.  On the other hand, it does support an
advanced memory aliasing analysis, value-set
analysis\cite{Balakrishnan2004}.  The aliasing analysis is currently
one of {\technique}'s major weaknesses, and so combining these
techniques might produce interesting results.

Decompilation is not the only kind of analysis which has been applied
to machine code.  BAP, the Binary Analysis Platform\cite{Brumley2011},
is one of the more recent examples.  This is a set of tools used to
convert machine code into an intermediate language suitable for other
forms of analysis.  As such, it is very similar to {\technique}'s
{\StateMachine} building process.  The chief difference is that BAP
does not consider multi-threaded programs at all, whereas that is the
primary purpose of {\StateMachine}.  Along the same lines, the
BitBlaze\cite{Song2008} project designed a number of analyses on
machine code intended to uncover security problems given only a binary
program.

\section{Theorem proving and SAT solving techniques}

{\Technique} represents its \glspl{verificationcondition} and most
internal predicates as boolean BDDs, as discussed in
\autoref{sect:sm_expr_language}, augmented with some simple arithmetic
simplification and canonicalisation rules.  A
\gls{verificationcondition} is considered to be satisfiable if these
cannot reduce the BDD to \false.  This can be thought of as a very
simple SMT solver\cite{Barrett2009}: the BDD library implement the SAT
solver part and the arithmetic rules the theory part.  As such, it
might be interesting to investigate replacing these components with an
existing standard SMT solver such as Z3\cite{Moura2008} or
Yices\cite{Dutertre2006}.  This might eliminate some of the analysis
phase's false positives, and hence reduce the number of
\glspl{bugenforcer} which need to be generated and tested.

Some preliminary experiments suggested that the number of false
positives which could be eliminated in this way is actually quite
small: of a random sample of ten false positives generated from
mysqld, only one was caused by arithmetic incompleteness; the others
were caused by {\technique}'s weak models of the program's aliasing
and concurrency behaviour.  Also, in another experiment, I implemented
a much more powerful lazy DPLL-based satisfiability
checker\cite{Davis1962}, and this did not eliminate enough false
positives to justify its high computational cost.  On the other hand,
a more powerful SMT solver might allow some true negatives to be
eliminated more quickly, which might help to reduce the cost of the
main analysis phase.

SMT solvers might also be useful when factorising the side-condition
for placement on a thread CFG (see \autoref{sect:enforce:place_vcs}).
Side-condition placement is very similar to the standard predicate
abstraction problem\cite{Graf1997}, and it might be possible to adapt
a predicate abstraction algorithm such as \cite{Lahiri2006} to this
context.  This would potentially allow the factorisation algorithm to
take more advantage of the theory when determining which parts of the
side-condition are evaluatable, and hence allow it to evaluate some
components of the side-condition sooner.
