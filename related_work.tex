\chapter{Related work}
\label{chapter:related_work}

\section{Automatically finding bugs}

Automatically detecting and characterising bugs in programs has been
an active area of research for many years.
Table~\ref{table:rw:find_char} gives a brief summary of some of the
more interesting pieces of work; the next few sections give a more
detailed description.

\begin{sidewaystable}
  \begin{tabular}{l>{\raggedright}p{5.5cm}lp{11cm}}
    Technique family           & Type of error discovered                     & Static/dynamic  & Example systems \\
    \hline
    Locksets                   & Locking protocol violations                  & Static          & RacerX~\cite{Engler2003},RELAY~\cite{Voung2007} \\
                               &                                              & Dynamic         & Eraser~\cite{Savage1997} \\
    \hdashline
    Happens-before             & Memory races                                 & Dynamic         & RaceTrack~\cite{Yu2005}, FastTrack~\cite{Flanagan2009}, Netzer~\cite{Netzer1991} \\
    \hdashline
    Concurrent aliasing        & Memory races                                 & Static          & Chord~\cite{Naik2006} \\
    \hdashline
    Schedule perturbation      & Timing dependencies                          & Dynamic         & DataCollider~\cite{Erickson2010}, AtomRace~\cite{Letko2008} \\
                               &                                              & Mixed           & Chess~\cite{Musuvathi2008} \\
    \hdashline
    Stereotyping               & Anomalous behaviour                          & Dynamic         & Anomaly-based IDSes~\cite{Forrest1996a}, Pu and Wei~\cite{Pu2006}, HeapMD~\cite{Chilimbi2006}, DIDUCE~\cite{Hangal2002}, MUVI\needCite{} \\
                               &                                              & Static          & \todo{Bugs as deviant behaviours}\\
    \hdashline
    Extended assertions        & Violations of programmer-identified properties& Dynamic        & TESLA~\needCite{}, Uppuluri et al.~\cite{Uppuluri2005} \\
    \hdashline
    Typestate systems~\cite{Strom1986a}& \multirow{2}{*}{\parbox{5.5cm}{Object access protocol sequencing violations}} & Dynamic & Gradual typestates~\cite{Wolff2011}\\
                               &                                                                             & Static  & Plaid~\cite{Sunshine2011}\\
                               &                                                                             & Hybrid  & Clara~\cite{Bodden2010}\\
    \hdashline
    Symbolic execution         & Most forms of bugs                           & Static          & SLAM~\cite{Ball2011}, KLEE~\cite{Cadar}, JPF~\cite{Havelund2000}, Zing~\cite{Andrews2004} \\
    \hline
    Bug characterisation       & Value replacement                            & Dynamic online  & Value replacement~\cite{Jeffrey2009} \\
                               & Delta debugging                              & Dynamic offline & Delta Debugging~\cite{Cleve2005,Choi2002} \\
                               & Static program slicing                       & Static          & \todo{...} \\
                               & Dynamic program slicing                      & Dynamic         & \todo{...} \\
                               & Assisted debugging                           & Interactive     & Model Inference System~\cite{Shapiro1982} \\
  \end{tabular}
  \caption{Summary of some existing bug detection and characterisation
    techniques.}
  \label{table:rw:find_char}
\end{sidewaystable}

\todo{Proving Thread Termination 2007?}

\todo{Comparison to STM block inference?}

\subsection{Lockset analysis}

Lockset analysis is a family of techniques used to detect problems in
a program's locking protocol.  A locking protocol is a set of rules
specifying what locks a thread must hold in order to operate on a
particular structure field or shared variable.  A lockset analysis
works by inferring properties of the program's locking protocol and
then reporting any violations of the protocol as a potential bug.

Eraser~\cite{Savage1997} was the earliest tool to take this approach.
This is a dynamic tool which maintains, for each location in memory, a
set of locks which might possibly protect it.  Whenever a thread
accesses a location, the tool intersects the set of locks held by the
thread with the set associated with the location to form a new lockset
for that location\footnote{There is an similarity here with
  {\technique}'s dynamic alias analysis
  (\autoref{sect:program_model:dynamic_alias}), in that both
  approaches justify their analysis with reference to the fields of
  high-level language structures but must perform the actual analysis
  on memory locations, and hence the two techniques make similar
  assumptions about memory type stability.}.  If the set ever becomes
empty, the tool reports a potential bug.  This behaves reasonably for
the most common locking protocol, in which each source-level field or
compound structure is protected by some lock, and where the most
common type of bug is simply forgetting to acquire the necessary
locks.  On the other hand, this kind of approach requires extensive
special cases to handle other concurrency protocols.  A common example
of such a protocol is structure ownership: a given structure is
``owned'' by a particular thread, which can access the structure
without acquiring any locks while other threads can only access the
structure by first acquiring ownership of it\footnote{See, for
  instance, \cite{Spear2007}.}.  This is perfectly safe (provided that
ownership transfer is implemented correctly), but leaves most fields
in most structures with empty lock sets.  Extending lock-set based
schemes to handle these other protocols generally requires a large
number of special cases.

This kind of analysis can also be applied statically.
RacerX~\cite{Engler2003} is one of the earliest examples of such a
tool.  An initial static analysis determines, for every line in the
program, which locks are held when that line executes.  The tool can
then use this to determine which locks protect each field in a
compound structure and then flag an error if this set is
empty\footnote{The actual implementation includes a number of
  additional techniques to reduce false positives.}.  RacerX's
aliasing model is very simple, assuming that there is precisely one
instance of every data type; this is necessary to scale to non-trivial
programs, but significantly reduces the analysis' soundness.

RELAY\cite{Voung2007} attempted to improve the soundness of RacerX
while also improving its scalability.  The key technique is to use
symbolic execution to construct procedure summaries\cite{Qadeer2004},
showing what locks each procedure acquires and releases and what
structure fields it accesses.  These summaries then allow them to
perform a sound cross-function race analysis without needing to
consider the full bodies of each function every time, leading to a
significant improvement in scalability.

\subsection{Happens-before}

Happens-before is another approach to the dynamic race detection
problem.  The idea here is that certain types of happens-before edges
are usually safe (e.g. the ones imposed by operating system-provided
locking functions) while others are usually unsafe (e.g. the ones
introduced by data races).  The tool can then detect likely errors by
modelling the happens-before graph as the program runs and generating
a warning whenever it detects one of those unsafe edges.  For
efficiency reasons, most tools do not attempt to maintain the entire
graph at run time, but instead maintain an approximation of the
relevant part of it using Lamport's vector clock
algorithm~\cite{Lamport1978}.

Recent tools of this form include RaceTrack\cite{Yu2005} and
FastTrack\cite{Flanagan2009}; Netzer~\cite{Netzer1991} is one of the
few which manipulate the graph without using vector clocks.  These
tools are generally quite effective at finding places in the program's
execution where behaviour depends on the details of the interleaving
chosen by the hardware, and this does include most concurrency bugs.

Their main weakness, apart from their high performance overhead, is
the need to classify edges according to whether they are safe or
unsafe.  It is difficult to come up with any such classification which
does not suffer from false negatives and which does not also suffer
from an excessive number of false positives.  For instance, suppose
that a program is correctly synchronised\editorial{Could be more
  precise there} and is then modified to include the sequence
``release($x$); acquire($x$)'' at some point which holds the lock $x$.
It is clearly possible for this transformation to introduce a new bug
into the program.  At the same time, the only edges which can possibly
be introduced into the happens-before graph are from the release
operation to an acquire in another thread or from a release in another
thread to the acquire in this one.  The tool can either report these
edges, in which case it will also be forced to report all other
acquire and release edges and hence have a very high false positive
rate, or it can not report the edges, and suffer a false negative.

\subsection{Concurrent aliasing}

\todo{I'm having to stretch a bit here to make this fit with my
  framework.}

Chord~\cite{Naik2006} can be thought of as the static equivalent of a
happens-before race detector.  The idea here is to perform an initial
static analysis which models the safe edges of the happens-before
graph by discovering the program's existing synchronisation structure
(what they refer to as the lock analysis phase), and to then perform a
second static analysis to find all of the possibly unsafe ones (the
escaping pairs computation).  These two partial models of the
program's happens before graph can then be combined and compared so as
to find any unsafe edges which are not prevented by the safe ones;
these are reported as potential program bugs.  Their implementation is
for Java, and depends to some extent on Java's type safety properties,
but it would probably not be excessively difficult to generalise to
non-typesafe languages such as C, at the cost of some precision.
There would be some scope for combining their model of the program's
safe edges with {\technique}'s analysis, although translating their
source-level analysis to machine code-level behaviour might be
challenging.

\subsection{Schedule perturbation}

One plausible definition of a concurrency bug is a bug whose behaviour
depends on the precise relative timing of parts of a program which run
concurrently.  This suggests an alternative scheme for finding such
bugs: simply alter the program's timing and see what happens.  This is
the approach taken by DataCollider~\cite{Erickson2010}.  Rather than
trying to find races directly, it instead inserts delays into the
program's execution so as to make races more likely, detects them when
they occur, and then uses heuristics to determine how dangerous that
race is.  This approach has a number of important advantages: it has
no false positives (any race reported will definitely have happened);
and it has reasonably low overhead (so the program's behaviour during
analysis is likely to be at least broadly similar to that during
normal execution); and it requires relatively little in the way of
supporting machinery, so can be used in constrained environments such
as kernel-mode drivers or embedded systems.  I have already described
the algorithm in section~\ref{sect:eval:datacollider}.
AtomRace\cite{Letko2008} uses a very similar technique to DataCollider
to find bugs, and combines it with a set of simple rules for
automatically fixing the bugs which it finds.

The parallels with {\technique} are obvious.  The important
differences are that {\technique} makes use of an initial analysis
phase to discover the best places to insert delays, allowing it to
reproduce bugs far faster than DataCollider, and that {\technique}
is able to derive a far broader class of fixes than AtomAid.

Chess\cite{Musuvathi2008} can be thought of as a more systematic
approach to the same idea: rather than inserting delays at randomly
selected points in order to perturb the schedule, Chess systematically
enumerates all possible program schedules.  It can then flag errors
when some schedules exhibit interesting bugs.  This allows Chess to
reproduce a wide variety of bugs quickly and easily.  Chess makes two
major approximations in order to get good results.  The first is the
use of bounded preemptions\cite{Musuvathi2007}: Chess only considers
schedules with up to $k$ preemptions, where $k$ is some constant (by
default, two).  Bugs which require more complex schedules cannot be
reproduced.  The second is an unsafe handling of memory-mediated
thread interactions: Chess performs an initial run of the program
under a dynamic data race detector and then assumes that only accesses
flagged in that run will ever suffer data races.  This means that it
can sometimes miss interesting instruction interleavings if
instructions race in some schedules and not others.  Despite this,
Chess has been demonstrated to be an effective way of detecting many
interesting bugs in real-world programs.

\subsection{Stereotyping}

All of the approaches discussed so far rely on some pre-defined notion
of what it means for a program to have a bug.  An alternative approach
is to simply define a bug to be anything which the program doesn't do
very often.  Tools based on this idea start by building up a model of
how a stereotypical execution of the program behaves and then report
any deviation from the stereotype as a potential bug.

This is essentially the same problem as is solved by anomaly detecting
intrusion detection systems~\cite{Forrest1996a}, and such systems can
be regarded as a kind of bug detector.  In practice, though, systems
designed for detecting intrusions are not usually well-suited to
detecting bugs, as they tend to operate at the wrong level of
abstraction\editorial{Bollocks do they.}, and I do not consider them
in detail here.

The Daikon~\cite{Ernst2007} was one of the earliest anomaly-based
bug-detecting systems.  Their basic approach is to collect one or more
traces of the program while it is operating normally, recording the
values of local variables and function parameters at various
interesting points in the program's execution.  These traces are then
mined to find conditions on those values which always hold, and these
conditions are converted into assertions which can be inserted into
the program.  Any failure of one of those assertions is reported as a
probable bug.  DIDUCE~\cite{Hangal2002} works on a similar principle,
but builds its invariants ``the other way around'': rather than trying
to find conditions which are true for all known executions, it starts
with a condition that nothing at all is possible and then weakens it
until it allows all of the known behaviour.  This tends to lead to
sharper inferred invariants, in the sense of allowing less behaviour,
and so DIDUCE tends to find more bugs but with a higher false-positive
rate.

The model-building parts of these tools could potentially be combined
with {\technique} so as to expand its \gls{programmodel}, and hence
allow it to analyse aspects of the program's behaviour which depend on
instructions outside of the \gls{analysiswindow}.  That could
potentially lead to useful improvements in both analysis performance
and precision.  HeapMD~\cite{Chilimbi2006}, which uses this kind of
method to build up a model of the heap, would be particularly
interesting to investigate in this respect.

The anomaly-detection approach has also been applied to cross-process
concurrency bugs such as filesystem TOCTOU
errors\cite[pages~44--45]{Apple2012SecureCoding}.  Pu and
Wei~\cite{Pu2006}, which built on earlier work by Uppuluri et
al.~\cite{Uppuluri2005}, produced a modified version of the Linux
kernel which protects applications against this kind of attack.  This
kind of technique is of less direct relevance to {\technique}, which
only considers intra-process bugs, but might still represent an
interesting way of handling calls to library functions.


\todo{MUVI?}

\todo{Bugs as deviant behaviours.}

\subsection{Extended assertions}

\todo{Really want a bit more here.}

All of the schemes discussed so far in this section are intended to
discover concurrency bugs automatically.  An alternative approach,
advocated by the TESLA project\needCite{}, amongst others, is just to
make it easier for programmers to find bugs themselves.  The core idea
here is to provide programmers with a language for expressing temporal
and concurrent properties of the program and to then check these
properties as the program runs, reporting an error if they ever fail.
This is conceptually rather similar to {\technique}'s enforcers, with
the difference that a TESLA assertion simply checks whether a bug has
happened, whereas a {\technique} enforcer will adjust the program's
schedule so as to make the target bug more likely.  One potentially
interesting avenue for future work would be to combine the two
techniques: take a TESLA assertion and convert it to a
{\implementation}-style message passing automaton which would make it
easier to reproduce the bug detected by the assertion.

\subsection{Typestate systems}

Type state systems~\cite{Strom1986a} are an older approach to the
problem of allowing the programmer to express complex program
properties which are to be enforced.  In this model, the assertions
are described by adding state to program types, with a description of
how certain operations can move instances of a type from one state to
another, and then restricting the program to only allow certain
operations on objects which are in a particular state.  These
assertions are typically checked at run time\cite{Wolff2011}, whether
in an ad-hoc\cite[pages~305--314]{Gamma1995} or
systematic\cite{Aldrich2009} manner, and can also in some cases be
checked statically\cite{Lam2005}.  Some systems, such as
Clara\cite{Bodden2010}\editorial{Might also want to reference chain
  from there.}, attempt to split the difference, checking as much as
possible of the assertion statically and deferring the rest to run
time.  This gives them a great deal of flexibility to trade the
incompleteness of dynamic analysis off against the high computational
cost of sound static analysis.  There is potentially some scope for
combining this sort of technique with {\technique}'s enforcer
mechanism to evaluate some side-conditions statically, which might
reduce the enforcer overhead.

\todo{Could cite PSpec here (Perl, Weihl, 1993)?}

\subsection{Symbolic execution}

There is a very large body of existing literature which investigates
ways of automatically detecting various classes of non-concurrency
bugs.  These range from the very simple systems integrated into most
compilers\needCite{} all the way through to whole-program model
checking with symbolic execution\cite{Ball2011}.  \todo{Say more here.}

The most influential recent example is probably KLEE\cite{Cadar}, a
symbolic execution engine for LLVM bitcode\cite{Lattner2013}.  The aim
behind KLEE is the automatic generation of test suites for
single-threaded programs which exercise the maximum proportion of the
program's functionality with the minimum amount of testing.  This is
conceptually very similar to {\technique}'s enforcer approach to
finding bugs: perform some static analysis on the program to find
places which might have a bug, and then convert those places into
tests which will show whether or not that bug is present.  The
difference is that {\technique}'s tests are expressed in terms of the
concurrency schedules which are to be exercised, whereas KLEE's are
expressed in terms of the inputs to the program\footnote{KLEE itself
  can be parallelised, as in, for instance, Cloud9\cite{Ciortea2010},
  but this is not the same as exploring parallel schedules in the
  program under test.}.  It would be possible to combine the two
techniques by, for instance, using KLEE to find program inputs which
allow {\technique} enforcer side-conditions to be satisfied.  This
would mitigate {\technique}'s most important weakness, that most of
the enforcers it generates require conditions which the program can
never actually reach, and increase KLEE's ability to find interesting
concurrency bugs.

\todo{SLAM}

JPF, the Java Pathfinder\cite{Havelund2000}, is another similar system
which operates on Java Bytecode\cite{Lindholm2013}.  As such, it has
access to a great deal more detailed information about the program,
making the analysis task significantly easier, but still less than a
source-level analysis would have.  It has been used both as a research
platform for new approaches to the symbolic execution problem itself
\todo{Cite Amorim and Lauterburg 2008, Gligoric 2010} and to verify
large-scale systems in an industrial context \todo{Cite Mehlitz 2007}.
As such, it represents one of the most well-developed symbolic
execution systems in use today.  Operating at the bytecode level means
that they avoid most of the difficult issues faced by {\technique}'s
symbolic execution system.

\todo{Zing}

\todo{Mention S2E?}

\section{Automatically characterising bugs}

One of {\implementation}'s modes of operation takes a snapshot from a
program which crashed due to a bug and converts it to a verification
condition showing what must have happened for the bug to have
occurred; in other words, it provides a characterisation of the bug.
This is a field which has been extensively researched in the past.
The Model Inference System~\cite{Shapiro1982} is one of the oldest
systems.  It was a system for localising bugs in Prolog programs by
asking the user a minimal number of questions\editorial{So what?}.

Delta Debugging\cite{Cleve2005} is a more recent example.  In this
approach, the tool compares traces taken from a program when it was
working to ones taken while it was exhibiting a bug in order to find
the parts of the traces which most effectively predict whether the
program will suffer the bug being investigated, and hence to
automatically locate the root cause of the bug.  The approach was
extended by \cite{Jeffrey2009}\editorial{ugg} to include actively
modifying a program's state as it runs and observing the effects.  The
Delta Debugging paper which is most relevant to the current work is
probably \cite{Choi2002}, which explored how to apply the Delta
Debugging technique to concurrent programs.  Their basic approach is
to record a program's action in a deterministic replay system and to
then replay it under slight variants of that schedule, so as to
determine which parts of the schedule are necessary for the bug to
reproduce.  This allows them to produce the simplest possible
reproduction of the original concurrency bug.  \todo{Compare to
  {\technique} in a more than trivial way.}

\section{Automatically fixing bugs}

\todo{Might be worth talking about schedule memoisation here.}

There have been many previous systems which aimed to automatically fix
bugs in programs, and I now give a brief overview of some of the more
important ones.

\subsection{Description of meta-problem}

\todo{Could actually pull this up to the introduction.}

One way of thinking about automatically fixing bugs is as a
transformation from one program to another, preserving some properties
of the program while altering others.  This is a challenging problem
at the best of times, but it is particularly difficult in this case
because the properties are usually quite ill-defined.  It is extremely
rare for one of the programs to be fixed to have a complete,
machine-readable, specification, and only slightly more common for
their to be a precise specification of the behaviour to be avoided.
Consider, for instance, a program which has a data race and which
calls \texttt{abort()} when the race goes one way and \texttt{exit(0)}
when it goes the other way.  This would appear, at first, to be an
obvious example of a concurrency bug, and the obvious fix would be to
cause the race to always go the way which avoids the \texttt{abort()}
call.  On the other hand, if the program's intended behaviour were to
investigate the processor's memory model and to then report its
results by either raising or not raising the \texttt{ABRT} signal then
this ``fix'' might convert a correct program into an incorrect one.
This is obviously an unlikely specification for a program to have; it
is equally obviously a \emph{possible} one.  As another example,
consider a video player which sometimes displays frames with a few bad
pixels due to bad thread interleaving.  This program clearly has a
bug, but a player which never shows an incorrect frame but which can
only display them at a tenth the desired rate would probably be far
less useful, despite being in some sense ``more correct''.  The
difference between a correct fix and an incorrect one is not the fix
itself, or the program to be fixed, but the intent of the user, and
without knowing that the fix generation tool must necessarily rely on
heuristics.

A slight change of perspective makes the trade-offs slightly easier to
understand.  Rather than viewing fix generation as a transformation of
the program, it is sometimes more useful to view it as a
transformation to the computing environment in which the program is
running, defined in terms of the operating system, processor,
libraries, etc.  Whenever a programmer writes a program, they will
have some (usually quite informal) model of the semantics of the
underlying computing environment, and they will have designed their
program against that semantic model.  This semantic model is extremely
unlikely to be exactly the same as the semantics which is actually
implemented by any physical computer (if nothing else, the physical
semantics differ markedly across computers, and most programs are
designed to run on more than one system), and that semantic gap gives
fix generation tools some room to manoeuvre.  In particular, the
programmer's semantic model will usually leave some parts unspecified,
and hence map to a large set of physical semantics such that any
property which is guaranteed by the programmer's model will be
guaranteed by any of the physical semantics.  This means that we can
safely select any physical semantics from this set, secure in the
knowledge that doing so will preserve whatever correctness the
original program might have had, and it is this flexibility which
potentially allows us to fix or mask errors.

Of course, this does not solve the problem, because we have no way of
knowing what semantics the original programmer had in mind when
writing the program.  We can make some intelligent guesses, however:

\begin{itemize}
\item Some parts of the physical semantics will differ from run to
  run.  One obvious example is the exact memory interleaving when two
  processors run in parallel.  Assuming the program is intended to
  work every time it runs, it is reasonable to assume that the
  programmer's semantics leave this behaviour undefined, and so it is
  safe for the automatic bug fixing tool to change it.

\item Likewise, language specifications and processor architecture
  manuals also often leave certain boundary cases unspecified,
  e.g. the effect of a use-after-free in C\cite{Kernighan1988}.  While
  it is possible for a program to depend on this unspecified
  behaviour, it is usually considered to be poor software engineering
  practise\cite{CWE758}, and so it is often safe to assume that it
  remains unspecified in the programmer's semantic model.

\item Sometimes, an operation is perfectly well defined, but indicates
  an error sufficiently often that it is sensible that it does so
  every time, and hence that it is safe to change its meaning.  For
  instance, one might reasonably require that a program never exit due
  to dereferencing a bad pointer, and hence allow the meaning of any
  operation which would normally do so to be changed.

\item Many systems leave the exact circumstances under which certain
  components fail undefined, and hence allow us to inject artificial
  errors safely.

\item More controversially, it would be possible to introduce a kind
  of extra memory or hysteresis into the program, such that once it
  has been observed to behave in a certain way a certain number of
  times, it is forced to keep behaving in that way from then on.  For
  instance, if a particular variable is found to be between five and
  ten in every training run, and is then found to be twelve in a
  subsequent one, it could be forcibly changed back to ten.  It is
  hard to imagine any programmer ever using this as their semantic
  model of the hardware, but it might sometimes capture part of their
  model of the \emph{program}, and hence allow a tool to produce fixes
  which are ``sympathetic'' to that model.
\end{itemize}

In order to make a useful bug fixing system, it is generally necessary
to change the semantics in two ways.  First, there must be some
indication that something has gone wrong: in the physical semantics,
every operation has a defined result, and so there is no way to tell
whether a given operation was desired, and so no way of triggering the
automatic fixing process.  This issue can be avoided by declaring
certain actions to be bad, so that we can assume that any such action
is considered to be a failure\footnote{Note that it is also possible
  to design always-on systems, which try to fix or ameliorate bugs in
  general without caring about any \emph{specific} instance, and in
  that case no trigger is needed. \todo{I have no idea what I meant
    when I wrote that.} }.  Second, we must relax the semantics enough
to give us the necessary flexibility to avoid the bad actions.  The
choice of these two changes is one of the most critical aspects of a
program auto-fix system.  In many cases, they can be changed
independently of one another.

Interestingly, the new semantics is not always required to be causal.
It may, in some cases, be useful to respond to an error by rolling
back to an earlier checkpoint and taking a slightly different path.
From the point of view of the program, the error influenced the
behaviour at the checkpoint, even though the error happened strictly
after the checkpoint, and hence, from the program's perspective, this
is a non-causal semantics\footnote{Causality is, of course, maintained
  from the perspective of the bug fixer itself, and so the semantics
  is paradox-free and implementable.}.

\subsection{Software rejuvenation and micro-reboots}

\todo{Too bitchy.}

One of the earliest attempts at fault remediation was software
rejuvenation\cite{Huang1995}, which attempted to ameliorate the
effects of resource leaks by periodically rebooting the affected
systems.  This spawned a surprising amount of work on calculating the
optimum reboot schedule
\cite{Li2002,Vaidyanathan1999,Vaidyanathan2001,Trivedi2000,Garg1998,Garg1995,Garg1998a,Castelli2001}\editorial{Shrink
  list} and, somewhat more usefully, some attempts at reducing the
cost of reboots \cite{Candea2002,Candea2001,Candea,Patterson2002}.
While historically interesting, these are unlikely to be applicable to
the current problem, and so are not discussed further here.

\subsection{Failure obliviousness and related techniques}
More recently, Rinard et al\cite{Rinard2004} described failure
obliviousness, a technique for disguising certain classes of memory
faults in high-availability systems at the (possible) expense of
reduced integrity.  The core idea here is, essentially, to treat
hardware exceptions as warnings rather than errors, and to try to
execute through them as far as possible in the hope that the error
will self-cleanse rather than propagating further.  Some faults are
trivial to ignore (stores through bad pointers, for instance, can be
simply discard), while others require more sophistication (loads of
bad pointers, for instance, must somehow synthesis a loaded value).
The original paper simply used a manually pre-defined sequence of
plausible values; later work expanded upon this by looking at the
dynamic dataflow context\cite{Nagarajan2009} or by using a lookaside
table of recently discarded writes\cite{Rinard2005a}.  \todo{Say
  more.}

The idea behind the reactive immune system\cite{Sidiroglou2005} is
similar, except that rather than trying to discard memory operations
they instead try to convert errors from unhandleable memory errors
into whatever sort of errors the program can handle.  The initial
implementation did this by forcing functions to return immediately
with an error value, obtained by type analysis on the source code;
this was refined in the ASSURE system\cite{Sidiroglou2005} to instead
take snapshots at places where it is convenient to inject errors and
then roll back when an error is detected.  Of course, error handling
is often rather buggy itself, and so recovery to error handling is not
always useful.  This issue was investigated in detail by
S\"{u}\ss{}kraut et al.\cite{Susskraut2006}, who also propose some
techniques for automatically improving its robustness.  \todo{Could
  cite HEALERS here?}

One potentially interesting approach, proposed by Elkarablieh et
al.\cite{Elkarablieh2007} in a slightly different context, would be to
try to mine the program for information about the intended contents of
data structures.  This would then provide useful information when
deciding how to synthesise the results of wild reads so as to minimise
the potential for error propagation, or even, in a somewhat extreme
form, to proactively fix data structures which have suffered
corruption.  This potentially increases the effectiveness of error
hiding, but also potentially increases the potential for the program
to generate completely nonsense results.  In the original paper, these
data structure invariants were obtained from \verb|assert()|-like
statements in the program source, combined with some basic static
analysis.  A later version, proposed by Malik et al.\cite{Malik}, used
Daikon\cite{Ernst2007}-like detection of statistically justified
invariants during normal program operation, which allowed the
technique to be applied without source code, but is also utterly
terrifying\editorial{phraseology}.  ClearView\cite{Perkins} refines
this approach by combining it with an automated testing system to try
to reduce the risk of introducing new bugs.

DieHard\cite{Berger2006} is another application of the failure
oblivious concept to heap-related issues.  In this system, however, no
attempt is made to discover or to enforce data structure invariants;
instead, the heap is structured so as to minimise the probability of
certain common types of bugs causing user-visible errors.  The authors
use two main techniques to achieve this:

\begin{itemize}
\item First, the heap is expanded, such that there are likely to be
  large dead zones between any two allocations.  This makes buffer
  overflows much less dangerous.
\item Second, they avoid reusing heap locations quickly after they
  have been \texttt{free()}d, which reduces the risk of use-after-free
  bugs actually causing problems.
\end{itemize}

Neither of these techniques will eliminate bugs, but they can
dramatically reduce the probability of their causing user-visible
problems (at the expense of dramatically increasing memory
requirements and marginally increasing runtimes).
Exterminator\cite{Novark2007} further builds on this work by using
heuristics on the expanded heap to try to identify probable bugs, and
then modifying the allocator to only apply heap expansion to
allocations which are likely to benefit from it.  Assuming that all
such allocations are detected, this retains all of the bug-fixing
benefits of DieHard while noticeably reducing its overhead (at least
in the common case where only a small number of allocations actually
need padding).

The AutoPaG system\cite{Lin2007} tackled a related problem, that of
overflows of stack-based buffers.  In this work, the authors assume
that a buffer overflow has already been identified by some mechanism
(a CCured\cite{Necula2005}-like safe compiler in the paper, but others
are possible), and then apply static analysis to find its root cause
at the source code level.  They then generate a source-level patch
which redirects any out-of-bounds accesses to the array back to a safe
location, in what is essentially a variant of failure obliviousness.
This allows them to mask the bug until a true fix can be obtained,
with very low run-time overhead in both time and space.
Unfortunately, their static analysis is not complete, and so they
must occasionally fall back to a dynamic scheme.

\subsection{Input rectification}

Rather than trying to fix bugs in a program, another strategy is to
sanitise the program's inputs so that it never sees anything which
might upset it.  RX\cite{Qin2007} is one example of such a system.  RX
protects a program reactively.  When it notices that something has
gone wrong, it rolls the program to an earlier checkpoint and then
replays it in a slightly different environment, with slightly
different inputs and a slightly different thread schedule, in the hope
that doing so will avoid the bug.  When it finds a strategy which
works it caches the details of the modifications it made, so that next
time it encounters a similar failure it can simply repeat those
modifications.  In this way it is able to protect the program from the
dangerous inputs without needing any knowledge of the program
structure, and very little knowledge of its interactions with the
environment.  First-Aid\cite{Gao2009} extended RX to handle certain
kinds of heap-related errors using a similar strategy of rolling the
program back and then trying it again with slightly different heap
behaviour; it can be thought of as the union of RX and DieHard.
Bouncer\cite{Costa2007} and Vigilante\cite{Costa2008} used similar
techniques to quickly protect network-facing services from
newly-released worms.

RX's main weakness is that it has no knowledge of the structure of a
program's inputs.  This means that if a bug is caused by an input it
has no choice but to discard the entire input.  SOAP\cite{Long2012}
tries to rectify this weakness by finding safe approximations to
dangerous input files.  The idea here is to start with a model of the
rough structure, extend it to include safeness constraints by running
the program against a training set and using some dynamic analysis,
and to then sanitise any future inputs to the program so that they
satisfy these safeness constraints.  The intent here is to find a
closest safe approximation to the dangerous input.  This means that
programs protected by SOAP can still do something useful even on
inputs which would normally cause them to crash.

\subsection{Hardware-based fixes for data races}
Of course, memory errors are only one class of bugs.  Synchronisation
errors form another important class, and a number of projects have
investigated remediation strategies for these.  One of the earliest
was ReEnact\cite{Prvulovic2003a}, which used modified thread-level
speculation hardware to capture precisely what happened during a data
race or atomicity violation, and then to control instruction
scheduling during subsequent re-executions so as to avoid the bug in
future.  This is similar to the intent of the fixes generated by
{\technique}, with a few exceptions:

\begin{itemize}
\item The ReEnact scheme requires unusual hardware support, whereas
  {\technique} is purely software-based.
\item Their fixes are dynamic, in the sense that the program must be
  constantly monitored to ensure it does not follow a bad schedule,
  whereas the {\technique} fixes statically introduce required locking
  so that bad schedules become impossible.
\item They require some (quite modest) amount of programmer
  involvement in order to identify which races are critical to a
  particular bug and hence to direct the fixing process, except in a
  few unusual special cases.  {\Implementation}, by contrast, is
  entirely automated\footnote{Except for needing to identify
    malloc-like functions, as discussed in
    Section~\ref{sect:program_model:dynamic_alias}.}.
\end{itemize}

Despite these limitations, ReEnact was able to fix some bugs, and
shows reasonably low run-time overhead (mostly on the order of ten
percent, depending on the benchmark).

Atom-Aid\cite{Lucia2009} is another scheme which tries to use unusual
hardware features to protect against data race and atomicity violation
bugs, but using hardware transactional memory\cite{Herlihy1993} rather
than thread-level speculation.  This paper grew out of an earlier
observation that, in some cases, processor performance can be improved
by bundling sequences of memory accesses into transactions, and hence
batching interconnect operations and amortising their
costs\cite{Ceze2007}.  This has the interesting side effect of
eliminating a large number potential instruction interleavings, and
hence a large number of potential synchronisation bugs.  Atom-Aid
attempts to maximise this effect by carefully tweaking transaction
boundaries in response to the program's observed behaviour.  This
allows them to hide most atomicity violations with very low
overhead\footnote{The paper asserts that they have negligible
  overhead, but does not attempt to quantify or justify that
  statement, and so the true overhead is rather difficult to
  evaluate.}.  Unfortunately, the necessary hardware changes are
unlikely to be widely deployed in the near future, and it is hard to
see how to adapt the system into a software-only implementation, which
makes these techniques somewhat less interesting.

\subsection{Software-based fixes for data races}
ConTest\cite{Krena2007} provides one of the simplest approaches to
fixing data races using software rather than hardware.  In this work,
races are detected using the ERASER\cite{Savage1997} algorithm, and an
implicit assumption is made that all data races are bugs which need to
be eliminated.  Elimination is performed using a small number of
pre-defined synchronisation patterns\footnote{In the paper, the only
  race pattern considered is wrapping a load followed by a store in a
  lock, but others could be added easily.}.  It is unclear from the
paper whether this is actually a useful thing to do, because the
majority of races are benign, and the majority which are not are more
complicated than their patterns can describe.  Tallam et
al.\cite{Tallam2008} suggested essentially the same mechanism a year
later, but restricted themselves to uniprocessor execution (so the
only parallelism is the coarse-grained variant provided by the
operating system's thread abstraction); this allowed them to make some
simplifications to their implementation, but further reduced the
useful scope of the technique.

ToleRace\cite{Kirovski2007}, which attempts to provide toleration for
asymmetric races, provides another useful point in the design space.
An asymmetric race is defined by the authors to be a situation where
some thread correctly follows a locking discipline (which must be
manually specified by the programmer), but another thread does not,
and thus causes the correct thread to fail.  This presents a challenge
when debugging, as most na\"{i}ve approaches to postmortem analysis
will blame the wrong thread for the crash.  ToleRace ameliorates this
class of problems by arranging that when a thread acquires a
particular lock, a local snapshot of all of the values protected by
the lock is taken, with any accesses to protected variables made while
holding the lock redirected to the snapshot.  This prevents the
correct thread from seeing the effects of incorrect threads while it
is in the critical section, which makes the race much less likely to
cause serious problems, but does not guarantee to produce a consistent
snapshot of the protected data.  ISOLATOR\cite{Ramalingam2009} fixes
this defect by introducing implicit per-page locks, which are enforced
using virtual memory techniques.  However, it is still necessary for
the programmer to provide a manually-specified lock discipline, which
makes these techniques difficult to use in practise.

AVIO\cite{Lu} is another approach to automatically fixing
concurrency-related bugs.  The idea here is to observe the program
during normal operation, and hence build up a model of its expected
access interleavings, and to then ensure later on that only those
interleavings are possible.  The intuition here is that bugs are
unusual events, and so preventing anything unusual from happening will
prevent any bugs from occurring.  The weakness, of course, is that
sometimes something unusual does happen (if, for instance, the program
receives some input which was not adequately covered by the training
data), and AVIO will be unable to provide protection in this case.
AVIO also has the weakness that it requires unusual hardware support
in order to achieve good performance; while it can be implemented
purely in software, the software implementation causes a roughly
twenty-five-fold slow-down, making it impractical to use in production
systems.

Kivati\cite{Chew2010a} can be thought of as a refinement to AVIO which
reduces these weaknesses.  In Kivati, the access interleaving
invariants are discovered by means of a static analysis conducted
before the program starts running.  This set is usually far smaller
than the set which is discovered by AVIO.  That then allows Kivati's
second refinement, which is to enforce the invariants using the
processor's watchpoint registers\needCite{}, obviating the need for
custom hardware.  The result is a similar level of protection to AVIO
but with far less overhead, usually on the order of a few tens of
percent.  Kivati's main weakness is that it can only protect against
single-variable races, due to limitations in both the static analysis
used.  Even if the static analysis were extended, limitations on
processor watchpoint facilities would prevent it from considering
large numbers of variables at the same time.  {\Technique} does not
share this limitation, and the fixes it generates have noticeably
lower overhead.  The downside, of course, is that {\technique}'s
initial analysis phase is far more expensive than Kivati's.

\subsection{Deadlock bugs}
The final class of bugs considered here is deadlocks.  Techniques for
healing deadlocks in multithreaded applications have only been
investigated relatively recently (deadlock avoidance has been studied
in other contexts for much longer; see e.g. \cite{Viswanadham1990} or
\cite{Dijkstra2004}).  One of the earliest, proposed by Nir-Buchbinder
et al.\cite{Nir-Buchbinder2008}, was to build the dynamic lock graph
as the program runs, discover any strongly-connected components
(SCCs), and then reducing every SCC to a single lock.  This eliminates
the potential for any lock order reversal deadlock involving that
cluster of locks, and would, if a complete lock graph were available,
it would completely eliminate all LOR deadlocks in the program.
Unfortunately, dynamically collected lock graphs are inherently
potentially incomplete, and this means that the healing is incomplete,
and can in fact introduce new deadlocks in certain situations.  The
need to potentially combine large sets of locks into a single large
lock can also lead to excessive serialisation, and hence poor
performance.

Gadara\cite{Wang2008} tackled both of these problems.  First, they
used static analysis to build a conservative approximation of the lock
graph (rather than the optimistic one produced by a dynamic analysis).
Second, they use discrete control theory to derive a ``controller''
which inserts delays in the dynamic program execution in some minimal
set of places so as to avoid deadlocks without introducing unnecessary
serialisation.  Of course, the use of a conservative approximation
means that they will occasionally detect a deadlock where none is
actually possible, and this will lead to additional synchronisation
operations; it is unclear from the paper whether Gadara-protected
systems are more or less serialised than Nir-Buchbinder-protected
ones.  At a high-level this controller is conceptually similar to the
message-passing machines used in {\technique}'s crash enforcers,
although the details and purpose of the technique are significantly
different.

It is also possible to make progress on these problems in a purely
dynamic system.  For instance, Dimmunix\cite{Jula2008} waits until a
deadlock is observed at run time, then captures a signature for that
particular deadlock, and arranges that the signature never reappears
by delaying lock acquire operations.  The hope is that preventing the
signature will also prevent the deadlock, and hence that the program
will, over time, become immune to whatever deadlocks might be lurking
in it.  Because it only fix deadlocks which have actually been
observed, serialisation is kept low and the need for complete locking
information is side-stepped, although at the cost of having to suffer
every deadlock at least once in order to fix it.

Of course, this approach will only be successful if the deadlock
signatures accurately capture the cause of the deadlock, without
capturing too much extraneous information.  The suggestion in the
paper is to use the set of locks held by every thread, combined with a
(slightly summarised) backtrace captured when the lock was acquired.
Earlier versions of the paper (e.g. \cite{Jula2008b}) mentioned
alternative schemes; I assume the fact that the discussion was dropped
implies that the alternatives were investigated and found to be
unhelpful.  That would be consistent with the rest of the evaluation.

\input{related_work/summary}

\section{Decompilation and other analysis of machine code}

{\Technique}'s approach to generating {\StateMachines} from machine
code can be viewed as a form of decompilation, in the sense that it
takes a low-level representation of a program and converts it to a
higher-level one.  The most important difference is that decompilation
aims to preserve all of the behaviours of the program, whereas
{\technique} seeks only to preserve those behaviours which are most
relevant to the bug which is being investigated.  Cifuentes'
dissertation\cite{Cifuentes1994} provides a thorough, if now quite
outdated, overview of the field.  Boomerang\cite{Emmerik2004} is a
more recent example.

CodeSurfer\cite{Balakrishnan2005a,Balakrishnan2008} is a more recent
example of a decompilation platform.  The intent here is to provide a
human user with assistance in determining what a binary-only program
does, rather than supporting extensive automated analyses.  Like BAP,
CodeSurfer has only limited support for multi-threaded programs.  On
the other hand, it does support an advanced memory aliasing analysis,
value-set analysis\cite{Balakrishnan2004}.  The aliasing analysis is
currently one of {\technique}'s major weaknesses, and so combining
these techniques might produce interesting results.  \todo{Maybe cite
  the WYSINWYX paper here as well?  It's basically the same as
  CodeSurfer, but I guess it wouldn't hurt to mention it explicitly.}

Decompilation is not the only kind of analysis which has been applied
to machine code.  BAP, the Binary Analysis Platform\cite{Brumley2011},
is one of the more recent examples.  This is a set of tools used to
convert machine code into an intermediate language suitable for other
forms of analysis.  As such, it is very similar to {\technique}'s
{\StateMachine} building process.  The chief difference is that BAP
does not consider multi-threaded programs at all, whereas that is the
primary purpose of {\StateMachine}.  Along the same lines, the
BitBlaze~\cite{Song2008} project designed a number of analyses on
machine code intended to uncover security problems given only a binary
program.

\section{Theorem proving and SAT solving techniques}

{\Technique} represents its \glspl{verificationcondition} and most
internal predicates as boolean BDDs, as discussed in
\autoref{sect:sm_expr_language}, augmented with some simple arithmetic
simplification and canonicalisation rules.  A
\gls{verificationcondition} is considered to be satisfiable if these
cannot reduce the BDD to \false.  This can be thought of as a very
simple SMT solver\needCite{}: the BDD library implement the SAT solver
part and the arithmetic rules the theory part.  As such, it might be
interesting to investigate replacing these components with an existing
standard SMT solver such as Z3\needCite{} or Yices\needCite{}.  This
might eliminate some of the analysis phase's false positives, and
hence reduce the number of \glspl{bugenforcer} which need to be
generated and tested.

Some preliminary experiments suggested that the number of false
positives which could be eliminated in this way is actually quite
small: of a random sample of ten false positives generated from
mysqld, only one was caused by arithmetic incompleteness; the others
were caused by {\technique}'s weak models of the program's aliasing
and concurrency behaviour.  Also, in another experiment, I implemented
a much more powerful lazy DPLL-based satisfiability
checker\needCite{}, and this did not eliminate enough false positives
to justify its high computational cost.  On the other hand, a more
powerful SMT solver might allow some true negatives to be eliminated
more quickly, which might help to reduce the cost of the main analysis
phase.

SMT solvers might also be useful when factorising the side-condition
for placement on a thread CFG (see \autoref{sect:enforce:place_vcs}).
Side-condition placement is very similar to the standard predicate
abstraction problem\needCite{}, and it might be possible to adapt a
predicate abstraction algorithm such as \cite{Lahiri2006} to this
context.  This would potentially allow the factorisation algorithm to
take more advantage of the theory when determining which parts of the
side-condition are evaluatable, and hence allow it to evaluate some
components of the side-condition sooner.
