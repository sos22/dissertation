\chapter{Evaluation}
\label{chapter:eval}

\section{Stuff I still need to do}

\begin{itemize}
\item Consider doing an ordering violation artificial bug, just to
  show the bug hiding problem?
\item Re-do unoptimised MySQL.  Time static and main analysis phases.
  Check number of candidates generated.  Run candidates.
  \todo{Running on hogun now.}
\item Re-do optimised MySQL.  Time static and main analysis phases.
  Check number of candidates generated.  Run candidates.
\end{itemize}

\section{Eval}

Previous chapters have described the basic {\technique} technique.  I
will now evaluate its effectiveness and the performance of my
implementation {\implementation}.  This evaluation will consist of the
following parts:

\begin{itemize}
\item \autoref{sect:eval:artificial} explores the behaviour of the
  tool and the technique on a number of artificial bugs in simple test
  programs.  This includes a comparison to a
  DataCollider\needCite{}-like tool which I implemented for the
  purpose of this evaluation.
\item \autoref{sect:eval:semiartificial} investigates some
  slightly more realistic bugs.  These include a simplified version of
  a real bug in glibc\needCite{} and some bugs which I deliberately
  introduced into the STAMP benchmark suite\needCite{}.
\item \autoref{sect:eval:real} applies the tool to some larger
  programs: pbzip2\needCite{}, MySQL\needCite{}, and
  Thunderbird\needCite{}.  I show that the analysis completes in a
  tolerable amount of time even for some very large programs, and
  demonstrate that it can both reproduce and fix a (small) number of
  real bugs.
\item \autoref{sect:eval:time_details} then investigates the
  tool's performance on large programs in slightly more detail,
  showing how the time taken breaks down across the various phases of
  the analysis and the effects of some of the input parameters.
\item \autoref{sect:eval:dynamic_analysis} investigates the
  performance and effectiveness of the dynamic aliasing analysis.
  This includes how much it benefits the main analysis, how long it
  needs to run for to achieve reasonable coverage, and the performance
  overhead incurred by the program under investigation while it is
  running.
\end{itemize}

\subsection{Statistical note}

Many of the distributions involved in this section are statistically
awkward due to having high kurtosis.  It would be quite deceptive to
quote a standard deviation for these distributions, and so I avoid
doing so, preferring instead to give either the full CDF or some
quantiles of the distribution.  In other cases I will write either $x
\pm_\mu y$ to give the mean and standard deviation of the mean,
calculated using the central limit theorem, for some distribution, or
$x \pm_p y$ to give the mean and population standard deviation of some
distribution.  The decision of which measure to give will be a largely
subjective one based on which I consider to be the most useful
characterisation of the data.

\section{Artificial bugs}
\label{sect:eval:artificial}

I now present the results of running {\implementation} on a number of
artificial bugs, showing that it assists in both reproducing the bugs
and fixing them, and that the analysis phases complete very quickly.

All of these experiments in this section were run on a four-core Intel
Q6600 2.40GHz with 8GiB of memory running Ubuntu Lucid Lynx.
\gls{alpha} was set to 50 (although the use of \texttt{STOP\_ANALYSIS}
markers reduced the size of effective \gls{analysiswindow} to far less
than this; see detailed test descriptions below).  None of the
experiments suffered any timeouts or out-of-memory errors.  The system
compiler, used to build the \glspl{bugenforcer} and fixes, as well as
{\implementation} itself, was gcc version 4.4.3.

\subsection{Simple time-of-check, time-of-use (TOCTOU) bug (simple\_toctou)}
\label{sect:eval:simple_toctou}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&*global\_ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }\hfill %
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }\hfill
  \caption{The two sides of the simple\_toctou bug.}
  \label{fig:eval:simple_toctou}
\end{figure}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
    \begin{tabular}{rlll}
              & \multicolumn{3}{l}{crashing\_thread:} \\
      400694: & movq  & global\_ptr, &\!\!\!\%rax\\
      40069b: & testq & \%rax,       &\!\!\!\%rax \\
      40069e: & je    & \multicolumn{2}{l}{4006ad}\\
      4006a0: & movq  & global\_ptr, &\!\!\!\%rax\\
      4006a7: & movl  & \$0x5,       &\!\!\!(\%rax)\\
    \end{tabular}
    }
  }%
  \hspace{-5mm}\subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{rlll}
        & \multicolumn{3}{l}{interfering\_thread:} \\
        400816: & lea  & c(\%rsp), &\!\!\!\%rbp \\
        ...\\
        400884: & movq & \%rbp, &\!\!\!global\_ptr\\
        ...\\
        4008fb: & movq & \$0x0, &\!\!\!global\_ptr\\
      \end{tabular}
      }
    }
  \caption{Disassembly of the program fragments in
    \autoref{fig:eval:simple_toctou}.}
  \label{fig:eval:simple_toctou:compiled}
\end{figure}

This is the simplest possible kind of concurrency bug: a
single-variable time-of-check, time-of-use race\footnote{This bug was
  previously discussed in slightly less detail in
  \autoref{sect:derive:simple_toctou_example}.}.  The two threads
involved are shown in \autoref{fig:eval:simple_toctou}.  The intent is
to model a very simple structure which is accessed frequently but
updated rarely.  The bug is, of course, that the interfering thread
might set \texttt{global\_ptr} to \texttt{NULL} in between the two
reads of it in the \gls{crashingthread}, causing the
\gls{crashingthread} to crash when it dereferences the pointer it
loaded.  \texttt{STOP\_ANALYSIS()} is a marker which prevents
{\technique}'s analysis from exploring past that point when building
the CFGs from which {\StateMachines} are constructed.  It is used here
to avoid complicating the presentation with extraneous details;
{\implementation} is able to reproduce and fix the bug even without
those markers.

\subsubsection{Generating \glsentryplural{verificationcondition}}

The first step in the analysis is to build the CFG for the
\gls{crashingthread} (see \autoref{sect:derive:build_static_cfg}), and
this is shown in \autoref{fig:eval:simple_toctou:cfg}.  This is then
compiled and simplified to produce the {\StateMachine} shown in
\autoref{fig:eval:simple_toctou:sm}.

\begin{figure}
  \centerline{
  \subfigure[][Control flow graph]{
    \centering
    \begin{tikzpicture}
      \node (cfg1) [CfgInstr] {\texttt{400694}: cfg1};
      \node (cfg2) [CfgInstr,below=of cfg1] {\texttt{40069b}: cfg2};
      \node (cfg3) [CfgInstr,below=of cfg2] {\texttt{40069e}: cfg3};
      \node (cfg3b) [right = of cfg3] {$\varnothing$};
      \node (cfg4) [CfgInstr,below=of cfg3] {\texttt{4006a0}: cfg4};
      \node (cfg4b) [below = of cfg4] {$\varnothing$};
      \draw[->] (cfg1) -- (cfg2);
      \draw[->] (cfg2) -- (cfg3);
      \draw[->,ifTrue] (cfg3) -- (cfg3b);
      \draw[->,ifFalse] (cfg3) -- (cfg4);
      \draw[->] (cfg4) -- (cfg4b);
    \end{tikzpicture}
    \label{fig:eval:simple_toctou:cfg}
  }
  {\hfill}
  \subfigure[][{\STateMachine}]{
    \begin{tikzpicture}
      \node (l1) at (0,2) [stateSideEffect] {\stLoad{1}{\texttt{global\_ptr}} @ cfg1 };
      \node (l2) [stateIf, below=of l1] {\stIf{\smTmp{1} = 0}};
      \node (l4) [stateSideEffect, below=of l2] {\stLoad{2}{\texttt{global\_ptr}} @ cfg4 };
      \node (l3) [stateTerminal, right=of l4] {\stSurvive };
      \node (l5) [stateIf, below=of l4] {\stIf{\smBadPtr{\smTmp{2}}}};
      \node (l6) [stateTerminal, below=of l5] {\stCrash};
      \draw[->] (l1) -- (l2);
      \draw[->,ifTrue] (l2) -- (l3);
      \draw[->,ifFalse] (l2) -- (l4);
      \draw[->] (l4) -- (l5);
      \draw[->,ifFalse] (l5) -- (l3);
      \draw[->,ifTrue] (l5) -- (l6);
    \end{tikzpicture}
    \label{fig:eval:simple_toctou:sm}
  }
  }
  \caption{\Gls{cfg} and {\STateMachine} for the crashing thread in
    \autoref{fig:eval:simple_toctou:compiled}.  $\varnothing$
    indicates that the thread has left the \gls{cfg}. Dotted lines
    indicate the false successor of a conditional.}
\end{figure}

The next step is to build the $\beta$ set (see
\autoref{sect:derive:write_side}).  The crashing {\StateMachine}
contains two loads, at \texttt{400694} and \texttt{4006a0}, and so the
analysis will query the \gls{programmodel} to find what stores might
interact with them, and the \gls{programmodel} will use the results of
the dynamic alias analysis (see
\autoref{sect:program_model:dynamic_alias}) to return the set
\{\texttt{400884}, \texttt{4008fb}\}.  The \texttt{STOP\_ANALYSIS()}
markers prevent these from being clustered together, and these are the
only members of $\beta$, and so there will be two
\glspl{interferingthread} \glspl{cfg}, shown in
figures~\ref{fig:eval:simple_toctou:interfering_cfg1}
and~\ref{fig:eval:simple_toctou:interfering_cfg2}.

\begin{figure}
  \begin{tabular}{b{0.5\textwidth}b{0.5\textwidth}}
    \subfigure[][CFG for interfering store \texttt{400884}]{
      \centerline{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\texttt{400884}: cfg5};
        \node (b) [below = of a] {$\varnothing$};
        \draw[->] (a) -- (b);
      \end{tikzpicture}
      }
      \label{fig:eval:simple_toctou:interfering_cfg1}
    } &
    \subfigure[][{\STateMachine} for interfering store \texttt{400884}, without \gls{programmodel}]{
      \centerline{
      \begin{tikzpicture}
        \node [stateSideEffect] {\stStore{\smReg{rbp}{2}}{\texttt{global\_ptr}} @ cfg5};
      \end{tikzpicture}
      }
      \label{fig:eval:simple_toctou:interfering_sm1}
    } \\
    \subfigure[][CFG for interfering store \texttt{4008fb}]{
      \centerline{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\texttt{4008fb}: cfg6};
        \node (b) [below = of a] {$\varnothing$};
        \draw[->] (a) -- (b);
      \end{tikzpicture}
      }
      \label{fig:eval:simple_toctou:interfering_cfg2}
    } &
    \subfigure[][{\STateMachine} for interfering store \texttt{4008fb}]{
      \centerline{
      \begin{tikzpicture}
        \node [stateSideEffect] {\stStore{0}{\texttt{global\_ptr}} @ cfg6};
      \end{tikzpicture}
      }
      \label{fig:eval:simple_toctou:interfering_sm2}
    }
  \end{tabular}
  \caption{Interfering CFGs and {\StateMachines}.}
\end{figure}

Consider the interfering store at \texttt{400884} first.  Without the
\gls{programmodel}, this would produce the {\StateMachine} shown in
\autoref{fig:eval:simple_toctou:interfering_sm1}.  This is a false
positive: there is no way for the program to crash due to interleaving
this {\StateMachine} with the crashing one, as $\smReg{rbp}{2}$ is
guaranteed to be a valid pointer, but the information needed to show
that lies outside of the \gls{analysiswindow}, and so will not be
included in the {\StateMachine}.  The \gls{programmodel}'s static
analyses, by contrast, analyse entire functions, rather than
restricting themselves to a fixed \gls{analysiswindow}, and are
capable of showing that $\smReg{rbp}{2}$ is a valid pointer, allowing
{\implementation} to eliminate this potential bug without using a
run-time \gls{bugenforcer}.  The other interfering store, at
\texttt{4008fb}, cannot be eliminated in this way, and produces the
\gls{verificationcondition} shown in
\autoref{fig:eval:simple_toctou:inferred_assumption2}.

\begin{figure}
  \begin{tabular}{lll}
    \gls{ci-atomic}: & $\smLoad{\texttt{global\_ptr}} = 0$ &\!\!\!$\vee\,\, \neg\smBadPtr{\smLoad{\texttt{global\_ptr}}}$ \\
    \gls{ic-atomic}: & $\smReg{rbp}{2} = 0$                &\!\!\!$\vee\,\, \neg\smBadPtr{\smReg{rbp}{2}}$\\
    \Gls{verificationcondition}: & \multicolumn{2}{l}{$\happensBefore{\mai{cfg1}{1}}{\mai{cfg5}{2}} \wedge \happensBefore{\mai{cfg5}{2}}{\mai{cfg4}{2}} \wedge \smBadPtr{\smReg{rbp}{2}}  \wedge$}\\
                                      & $\smLoad{\texttt{global\_ptr}} \not= 0$\\
  \end{tabular}
  \caption{\Gls{inferredassumption} and \gls{verificationcondition}
    produced using the crashing {\StateMachine} in
    \autoref{fig:eval:simple_toctou:sm} and the interfering
    {\StateMachine} in
    \autoref{fig:eval:simple_toctou:interfering_sm1}.}
  \label{fig:eval:simple_toctou:inferred_assumption1}
\end{figure}

\begin{figure}
  \begin{tabular}{lll}
    \gls{ci-atomic}: & $\smLoad{\texttt{global\_ptr}} = 0$ &\!\!\!$\vee\,\, \neg\smBadPtr{\smLoad{\texttt{global\_ptr}}}$ \\
    \gls{ic-atomic}: & \true\\
    \Gls{verificationcondition}: & \multicolumn{2}{l}{$\happensBefore{\mai{cfg1}{1}}{\mai{cfg6}{2}} \wedge \happensBefore{\mai{cfg6}{2}}{\mai{cfg4}{2}} \wedge \smLoad{\texttt{global\_ptr}} \not= 0$}\\
  \end{tabular}
  \caption{\Gls{inferredassumption} and \gls{verificationcondition}
    produced using the crashing {\StateMachine} in
    \autoref{fig:eval:simple_toctou:sm} and the interfering
    {\StateMachine} in
    \autoref{fig:eval:simple_toctou:interfering_sm2}.}
  \label{fig:eval:simple_toctou:inferred_assumption2}
\end{figure}

The time taken to perform this analysis is quite modest: $0.52 \pm_p
0.04$ seconds for the static analysis phase and $0.18 \pm_p 0.01$
seconds for the {\StateMachine} analysis (mean and population standard
deviation for ten runs).  The dynamic analysis phase achieved complete
coverage essentially as soon as the program started.  These results
are best thought of as lower bounds on the time needed to process a
bug; any realistic program will take far longer.

\subsubsection{Reproducing the bug}
This \gls{verificationcondition} can now be turned into a
\gls{bugenforcer}.  The only happens-before graph will be the one
shown in \autoref{fig:eval:simple_toctou:hb_graph}, and it will have
the side condition $\smLoad{\texttt{global\_ptr}} \not= 0$.  That can
be evaluated completely at either $\mai{cfg1}{1}$ or $\mai{cfg6}{2}$,
so {\technique} will produce the crash enforcement plan shown in
\autoref{fig:eval:simple_toctou:enforce_plan}.  In other words,
whenever a program thread reaches \texttt{400694} and
\texttt{global\_ptr} is non-zero, the enforcer will wait for an
interfering thread to reach \texttt{4008fb}, after loading
\texttt{global\_ptr}.  If one does arrive, the enforcer will make the
crashing thread wait for the interfering thread to complete its store
before proceeding to the load at \texttt{4006a0}.  Similarly, any
thread reaching \texttt{4008fb} will check \texttt{global\_ptr} and, if
it is non-zero, wait for some other thread to reach \texttt{400694}.

\begin{wrapfigure}{r}{7cm}
  \begin{tikzpicture}
    \node[draw] (l1) {\texttt{400694}: $\mai{cfg1}{1}$};
    \node[draw, below right = of l1] (l2) {\texttt{4008fb}: $\mai{cfg6}{2}$ };
    \node[draw, below left = of l2] (l3) {\texttt{4006a0}: $\mai{cfg4}{1}$ };
    \draw[->] (l1) -- (l3);
    \draw[->, happensBeforeEdge] (l1) -- (l2);
    \draw[->, happensBeforeEdge] (l2) -- (l3);
  \end{tikzpicture}
  \caption{Happens-before graph to be enforced for simple\_toctou}
  \label{fig:eval:simple_toctou:hb_graph}
\end{wrapfigure}
This enforcer is effective at reproducing the bug.  Without the
enforcer, the mean time taken to reproduce the bug was $4.3 \pm_\mu
0.4$ seconds, for 100 runs; with it, the mean time was $1.158 \pm_\mu
0.003$.  Most of this reduction is caused by eliminating outliers from
the distribution: the median time to reproduce the bug decreased from
3.0 seconds to 1.2, a 2.5-fold reduction, while the $95^{th}$
percentile was reduced from 12 seconds to 1.2, a ten-fold reduction.
\autoref{fig:eval:crash_cdf:simple_toctou} shows the complete CDFs
for both configurations.

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (l1) {\texttt{400694}: $\mai{cfg1}{1}$};
    \node[left = 0 of l1] {$\smLoad{\mathrm{global\_ptr}} \not= 0$};
    \node[draw, below right = of l1] (l2) {\texttt{4008fb}: $\mai{cfg6}{2}$ };
    \node[right = 0 of l2] {$\smLoad{\mathrm{global\_ptr}} \not= 0$};
    \node[draw, below left = of l2] (l3) {\texttt{4006a0}: $\mai{cfg4}{1}$ };
    \draw[->] (l1) -- (l3);
    \draw[->, happensBeforeEdge] (l1) -- (l2);
    \draw[->, happensBeforeEdge] (l2) -- (l3);
  \end{tikzpicture}
  \caption{Crash enforcement plan for simple\_toctou}
  \label{fig:eval:simple_toctou:enforce_plan}
\end{figure}

The time taken to build the enforcer is again modest: $0.13 \pm_p
0.01$ seconds, mean and population standard deviation of ten runs.  As
before, this is best thought of as a lower bound on the time taken to
build an enforcer; more realistic tests will usually take longer.

\subsubsection{Fixing the bug}
{\Implementation} can also generate a fix for this bug.  In this case,
the CFG fragments to be protected will be the complete CFGs shown in
figures~\ref{fig:eval:simple_toctou:cfg}
and~\ref{fig:eval:simple_toctou:interfering_cfg2}, as the first and
last CFG nodes in both threads are involved in happens-before edges.
This corresponds to modifying the program as shown in
\autoref{fig:eval:simple_toctou:fix}.  This correctly fixes the
bug.

\begin{figure}
  \centerline{
    {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{acquire\_lock();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&t = global\_ptr;\\
        &&release\_lock();\\
        &&*t = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  {\hfill}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &acquire\_lock();\\
        &global\_ptr = NULL;\\
        &release\_lock();\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }\hfill
  }
  \caption{The fix generated by {\implementation} for the simple\_toctou bug.}
  \label{fig:eval:simple_toctou:fix}
\end{figure}

It does, however, have a rather high performance overhead: without a
fix, the crashing thread completes $352.5 \pm_p 0.2 {\times} 10^6$
iterations of the loop per second; with one, it completes $95.9 \pm_p
0.2 {\times} 10^6$ (mean and standard deviation of ten runs each of
ten seconds, discarding any runs in which the unfixed program
crashed).  That gives an overhead of roughly a factor of 3.7.  This is
obviously rather large, but is probably close to {\technique}'s worst
case\footnote{Ignoring the loss of concurrency inherent lock-based
  fixes.}: the read-side critical section is very small and runs with
very high frequency, and so the patch must acquire and release the
lock with similarly high frequency and these lock operations dominate
the time taken.  Any realistic test would usually have much lower
overhead, assuming lock contention does not become a factor, simply
because the critical sections would run less frequently and the
overhead could be more effectively amortised.  Even in this case, a
factor of four overhead is not completely unreasonable when the
alternative is a program which crashes frequently.

For comparison, I also produced a version of the patch which does
everything except for acquiring and releasing the lock.  This version
completed $349.2 \pm_p 0.3 {\times} 10^6$ loops per second, and so in
this case the slow down caused by the patch was a little less than
1\%.  This strongly suggests that the overhead in this case is mostly
caused by the lock operations themselves, rather than the {\technique}
infrastructure.

\begin{table}
  \centerline{
  \begin{tabular}{|l|l|l|}
    \hline
                     & Gain control with breakpoints & Gain control with branches \\
    \hline
    Locking enabled  & $6.08 \pm_p 0.01$               & $95.9 \pm_p 0.2$\\
    Locking disabled & $6.38 \pm_p 0.01$               & $349.2 \pm_p 0.3$\\
    \hline
  \end{tabular}
  }
  \caption{Performance of some variants of the fix, in millions of
    loop iterations completed per second.  The original program
    completed $352.5 \pm_p 0.2$ million iterations per second.  All
    measurements mean and population standard deviation of ten runs,
    each for ten seconds, with any run which crashed repeated until it
    did not crash.}
  \label{table:eval:simple_toctou:other_fixes}
\end{table}

As a further point of comparison, I also produced a version of this
fix which used debug breakpoints to gain control of the program rather
than branches.  This completed $6379000 \pm_p 7000$ loop iterations
per second, even with the actual locking disabled, giving it an
overhead of roughly a factor of 55.  This might be problematic in a
production environment.

\subsection{Indexed TOCTOU bug (indexed\_toctou)}

\label{sect:eval:indexed_toctou}

In this variant of a TOCTOU bug, shown in
\autoref{fig:eval:indexed_toctou}, there are multiple instances of the
structure which is being raced on and the bug will only manifest if
the reading and writing threads happen to coincide\footnote{This bug
  was used as an example in \autoref{sect:reproducing_bugs}.}.  This
bug exercises the side condition-checking part of the
\gls{bugenforcer} mechanism.  Except where otherwise noted,
\verb|NR_PTRS| is set to 100.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{idx = random() \% NR\_PTRS;}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptrs[idx] != NULL) \{}\\
        &&*(global\_ptrs[idx]) = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \}\\
      \end{tabular}
    }
  }%
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        & idx = random() \% NR\_PTRS;\\
        & STOP\_ANALYSIS();\\
        & global\_ptrs[idx] = NULL;\\
        & STOP\_ANALYSIS();\\
        & global\_ptrs[idx] = \&t;\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The two sides of the indexed\_toctou bug.}
  \label{fig:eval:indexed_toctou}
\end{figure}

As with the simple\_toctou test, this test produces a single
\gls{verificationcondition}, with a similar enforcer and fix.  The
only important difference is that the enforcer includes a side
condition $\mathtt{idx}_1 = \mathtt{idx}_2$, where $\mathtt{idx}_1$ is
an expression for \texttt{idx} in the crashing thread and
$\mathtt{idx}_2$ that in the interfering thread, which is checked on
the first happens-before edge.

The enforcer was effective at making this bug reproduce more easily.
With no enforcer loaded, the mean time to reproduce the bug was $1.2
\pm_\mu 0.2$ seconds, mean and standard deviation of mean for 100
runs; with an enforcer, the mean time to reproduce was $0.24 \pm_\mu
0.01$ seconds.  As with the simple\_toctou test, most of this
reduction was due to removing the long tail: the $95^{th}$ percentile
reproduction time was reduced from 5.8 seconds to 0.48 seconds,
whereas the median actually increased slightly, from 0.16 seconds to
0.21 seconds.  \todo{Investigate how that changes when you change the
  delay parameter.}

I also investigated the behaviour of this test with an enforcer loaded
but no side condition checking performed.  In that case, the mean time
taken to reproduce the bug was $18 \pm_\mu 2$ seconds.  This reduced
enforcer not only fails to make the bug reproduce more quickly; it
actually makes it \emph{less} likely to be triggered, per unit time!
This is because an enforcer without side-condition checking will often
slow the program down in order to impose the happens-before graph even
in situations where doing so is unlikely to trigger the bug, and this
causes the buggy code to run far less frequently than it otherwise
would.  The full CDFs are shown in
\autoref{fig:eval:indexed_toctou:no_scs}.

\begin{figure}
  \input{eval/artificial_bugs/special/indexed_toctou_no_scs.tex}
  \caption{Effect of side-condition checking on the time taken to
    reproduce the indexed\_toctou bug.}
  \label{fig:eval:indexed_toctou:no_scs}
\end{figure}

\begin{figure}
  \subfigure[][Without enforcer]{ \input{eval/artificial_bugs/special/indexed_toctou_vary_nr_ptrs_no_enforcer.tex} }
  \subfigure[][With enforcer]{ \input{eval/artificial_bugs/special/indexed_toctou_vary_nr_ptrs_enforcer.tex} }
  \caption{Reproduction times with and without an enforcer loaded, for
    varying values of \texttt{NR\_PTRS}.  Note that the two graphs use
    different scales, and that both use a log scale.  Error bars on
    mean line are plus or minus one standard deviation of mean.}
  \label{fig:eval:indexed_toctou:nr_ptrs}
\end{figure}

I also investigated the effect changing the \texttt{NR\_PTRS}
parameter.  The results are shown in
\autoref{fig:eval:indexed_toctou:nr_ptrs}.  I ran each configuration
100 times at each of the sampled abscissae and timed how long it took
to reproduce the bug; these charts show the $25^{th}$, $50^{th}$, and
$75^{th}$ percentiles and the mean and standard deviation of the mean.

The most obvious property of these charts is that the behaviour of the
case without the enforcer is quite ``noisy''.  Some of this noise is
the usual experimental error; the distributions being measured have a
very long positive tail, and 100 samples is barely sufficient for the
$75^{th}$ percentile to become meaningful.  Much of it, though,
reflects the actual behaviour of the program.  The dips in
reproduction time around $\texttt{NR\_PTRS} = 200$ and
$\texttt{NR\_PTRS} = 400$, for instance, are both
reproducible\footnote{This means that plotting a regression line on
  the chart is something of an abuse, but I believe that this is the
  clearest way of presenting this data.}.  I am unable to explain this
precise behaviour, beyond speculating that it might be due to some
aliasing effect between those values of \texttt{NR\_PTRS} and some
periodic structure as processor cache lines, or possibly even some
peculiarity the random number generator.  The complex behaviour of an
apparently simple test is reflective of the general complexity of
concurrency-related bugs, and is one of the reasons why they are often
difficult for programmers to correctly diagnose\needCite{}.  The chart
with the enforcer, by contrast, shows much simpler behaviour.  This
would make the bug far easier for a programmer to fix, even without
the significant reduction in the time take to reproduce it.

The automatic fix generator works well with this bug, and produces
roughly the same fix as it did in the simple\_toctou bug: one critical
section which covers the two critical loads in the read thread and one
which covers the critical store in the write thread.  The overhead in
this case was, however, proportionally much smaller, at around 15\%
rather than a factor of four (see \autoref{table:eval:fix_overheads}
for full details).  The emphasis must be on ``proportionally'': the
cost of the patch and lock operations in microseconds remains
unchanged, but the benchmark loop now includes a call to
\texttt{random}, which is itself quite expensive.  The patch overheads
therefore account for a smaller fraction of the test program's run
time and their performance impact is lower.

\subsection{Biassed indexed TOCTOU bugs (crash\_indexed\_toctou, interfering\_indexed\_toctou)}

These bugs are similar to the indexed\_toctou with $\texttt{NR\_PTRS}
= 100$, except with an additional one second delay in either the
interfering or crashing thread's loops, such that either the
interfering thread (for interfering\_indexed\_toctou) or the crashing
thread (for crash\_indexed\_toctou) runs far more often than the
other.  These tests are intended to illustrate the importance of
placing delays at appropriate points in the happens-before graph.  The
results are shown in \autoref{fig:biassed_indexed_toctou:times}.
These results show that, while the delay placement mechanism is not
always guaranteed to find the best possible placement, it does avoid
some very poor ones.

\begin{figure}
  \input{eval/artificial_bugs/special/delay_positioning.tex}
  \caption{CDF of time taken to reproduce the crash\_indexed\_toctou
    and interfering\_indexed\_toctou bugs in different configurations.
    All configurations were repeated one hundred times.}
  \label{fig:biassed_indexed_toctou:times}
\end{figure}

\subsection{Multi-variable consistency constraint (multi\_variable)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        1 & \multicolumn{2}{l}{while (1) \{} \\
        2 & & STOP\_ANALYSIS();\\
        3 & & v1 = global1;\\
        4 & & v2 = global2;\\
        5 & & assert(v1 == v2);\\
        6 & & STOP\_ANALYSIS();\\
        7 & & sleep(10 milliseconds);\\
        8 & \multicolumn{2}{l}{\}}\\
        \\
        \\
        \\
        \\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        9 & \multicolumn{2}{l}{while (1) \{}\\
        10 & & STOP\_ANALYSIS();\\
        11 & & global1 = 5;\\
        12 & & STOP\_ANALYSIS();\\
        13 & & global2 = 5;\\
        14 & & STOP\_ANALYSIS();\\
        15 & & sleep(100 milliseconds);\\
        16 & & STOP\_ANALYSIS();\\
        17 & & global1 = 7;\\
        18 & & global2 = 7;\\
        19 & & STOP\_ANALYSIS();\\
        20 & \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The two sides of the multi\_variable bug. The delays were
    chosen so that the program crashed in a reasonable amount of time
    when run unmodified.}
  \label{fig:eval:multi_variable}
\end{figure}

This bug is intended to explore {\technique}'s effects on
multi-variable atomicity violations.  The two sides of the bug are
shown in \autoref{fig:eval:multi_variable}.  Note that in this case
the race leads to an assertion failure, whereas previous bugs lead to
a bad pointer dereference.  {\Technique} reports a single candidate
bug in this program, corresponding to interleaving the crashing thread
with lines \texttt{17} and \texttt{18} in the interfering thread.  This
enforcer causes the bug to reproduce quickly (in an average of $322
\pm_\mu 4$ms, mean and standard deviation of mean for 100 runs), and
in all cases within the three minute timeout, whereas without an
enforcer the bug failed to reproduce before the timeout in 73\% of
cases.

It is perhaps surprising that {\technique} only discovered a single
possible bug and did not detect that interleaving the
\gls{crashingthread} with lines \texttt{11} and \texttt{13} in the
\gls{interferingthread} might lead to a crash.  The reason is the
\texttt{STOP\_ANALYSIS()} marker on line \texttt{12} which prevents
the two stores from being clustered together, forcing {\technique} to
treat them as completely independent \glspl{interferingthread},
neither can cause an atomicity violation in the \gls{crashingthread}.
For instance, consider the thread generated from line \texttt{11} (the
one generated from line \texttt{13} is similar).  As discussed in
\autoref{sect:derive:inferred_assumption}, an \gls{interferingthread}
can only cause an atomicity violation if concatenating it with the
\gls{crashingthread}, in either order, produces a program which will
not crash, which implies that the \gls{inferredassumption} shown in
\autoref{fig:eval:multi_variable:other_bug} must hold.  The initial
value of \texttt{global1} must therefore be \texttt{5} and line
\texttt{11} cannot possibly have any effect on the behaviour of the
\gls{crashingthread}.  There is no need to investigate the bug at
run-time and no \gls{bugenforcer} is generated.

\begin{figure}
  \hfill
  \subfigure[][CI atomic]{
    \begin{tabular}{ll}
      \tt 3 & \tt v1 = global1;\\
      \tt 4 & \tt v2 = global2;\\
      \tt 5 & \tt assert(v1 == v2);\\
      \tt 11 & \tt v1 = 5;\\
      \\
      \multicolumn{2}{l}{$\smLoad{\texttt{global1}} = \smLoad{\texttt{global2}}$}\\
    \end{tabular}
  }
  \hfill
  \subfigure[][IC atomic]{
    \begin{tabular}{ll}
      \tt 11 & \tt v1 = 5;\\
      \tt 3 & \tt v1 = global1;\\
      \tt 4 & \tt v2 = global2;\\
      \tt 5 & \tt assert(v1 == v2);\\
      \\
      $\smLoad{\texttt{global2}} = 5$\\
    \end{tabular}
  }
  \hfill
  \vspace{12pt}
  \centerline{Inferred assumption: $\smLoad{\texttt{global2}} = 5 \wedge \smLoad{\texttt{global1}} = 5$}
  \caption{Deriving the \gls{inferredassumption} for the other bug in
    the multi\_variable test program.}
  \label{fig:eval:multi_variable:other_bug}
\end{figure}

The bug which is reported can be converted to a fix.  This fix
correctly fixes the reported bug, but does not, of course, fix the one
which is not reported.  As such, the program might still crash.  In
this case, the fix actually causes the program to crash more
frequently, increasing the reproduction rate from 27\% within three
minutes to 81\%, from one hundred runs, because of the way in which it
alters the timing of the test program.  If the
\texttt{STOP\_ANALYSIS()} marker is removed then both bugs are
discovered and the resulting fix prevents the program from crashing.

\subsection{Write-to-read hazard (write\_to\_read)}

This test, shown in \autoref{fig:eval:write_to_read}, investigates a
form of write-write-read race, whereas all of the previous ones have
considered only write-read ones.  The crashing thread loops setting a
global variable to point at some location and then proceeds to use
that global variable, while at the same time the interfering thread
loops setting that global variable to \texttt{NULL}.  In other words,
there is a write-to-read hazard in the crashing thread which might be
interrupted by the interfering thread, leading to a crash.

This test program is surprisingly reliable, given that it runs both
sides of the race in a tight loop with no delays or synchronisation,
and can often run for several minutes without encountering an error on
an otherwise idle system, during which time the buggy code might run
billions of times.  A more realistic test would run the two sections
far less frequently, and so might easily require hundreds of years of
CPU time to reproduce the bug.  I suspect that this is because the
store and load instructions in the crashing thread are close enough
together that the load is always satisfied from the processor's write
buffer, and so only returns \texttt{NULL} when the processor receives
an interrupt in precisely the wrong place.  The {\technique}-generated
bug enforcer, by contrast, can reliably reproduce the bug in a few
hundred milliseconds.

\begin{figure}
  \centerline{
    {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = \&t;\\
        &*global\_ptr = 5;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
        \\
      \end{tabular}
    }
  }
    {\hfill}
    }
  \caption{The write\_to\_read test case.}
  \label{fig:eval:write_to_read}
\end{figure}

{\Technique} can also generate a fix for this bug.  The difficulty of
reproducing the bug makes it difficult to validate experimentally that
the generated fix is correct, but manual inspection suggested that it
is.

\subsection{Multiple bugs (multi\_bugs)}

This test combines simple\_toctou and write\_to\_read into a single
test, demonstrating {\technique}'s ability to exercise several bugs
using a single enforcer.  The test program is shown in
\autoref{fig:eval:multi_bugs}.  The behaviour of
\texttt{select\_test} is configurable at run time, and will either
always select simple\_toctou, always select write\_to\_read, or select
randomly.  The dynamic analysis phases were run with it configured to
select randomly.

{\Implementation} is able to generate \glspl{verificationcondition}
for both of the component bugs in the test, and these can both be
instantiated into \glspl{bugenforcer}.  These enforcers both succeed
in reproducing their respective bugs, but neither is able to reproduce
both bugs.  {\Implementation} can also generate a combined
\gls{bugenforcer} using both of the \glspl{verificationcondition}, and
this one is able to reproduce both bugs.  When the test is configured
to exercise both bugs the enforcer consistently reproduces the
write\_to\_read bug, simply because the enforcer reproduces the
write\_to\_read bug before the simple\_toctou interfering critical
section ever runs.

In the same way, both candidate bugs can be instantiated into
individual fixes, both of which correctly fix their respective bug
while leaving the other bug in place.  Alternatively, a combined fix
can be generated, and this successfully fixes both bugs.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        & \multicolumn{2}{l}{r = select\_test();}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        & \multicolumn{2}{l}{if (r) \{}\\
        && simple\_toctou\_crashing();\\
        & \multicolumn{2}{l}{\} else \{}\\
        && write\_to\_read\_crashing();\\
        & \multicolumn{2}{l}{\}}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        & \multicolumn{2}{l}{for (i = 0; i < 2000000; i++) \{}\\
        && STOP\_ANALYSIS(); \\
        && write\_to\_read\_interfering();\\
        && STOP\_ANALYSIS(); \\
        & \multicolumn{2}{l}{\}}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        & \multicolumn{2}{l}{simple\_toctou\_interfering();}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The multi\_bugs test.  The constant \texttt{2000000} was
    chosen so that the two bugs reproduce with roughly equal
    probability.}
  \label{fig:eval:multi_bugs}
\end{figure}

\begin{figure}
  \input{eval/artificial_bugs/special/multi_bugs.tex}
  \caption{Reproduction times CDFs for the multi\_bug test.}
\end{figure}

\subsection{Multiple crashing and interfering threads (multi\_threads)}

This test investigates {\implementation}'s behaviour in programs with
a very large number of threads.  The test program is the same as
indexed\_toctou (\autoref{fig:eval:indexed_toctou}) with
$\texttt{NR\_PTRS} = 100$, except that rather than having a single
thread running the crashing and interfering critical sections, this
test has 32 threads running each\footnote{Note that these tests were
  conducted on a system with only four hardware threads, and so the
  concurrency here is primarily the coarse grained kind provided by
  the operating system's time division multiplexing rather than the
  fine grained kind provided by true hardware parallelism.}.
{\Technique} behaves roughly as expected here: it is able to generate
an enforcer and a fix, with the enforcer making the bug happen more
quickly and the fix preventing it from happening at all.

\todo{Not sure how interesting this is.} This test illustrates one
subtlety in the implementation of the enforcer: it must be (at least
reasonably) fair.  The {\implementation} crash enforcement interpreter
(see \autoref{sect:enforce:interpreting}) uses a single global lock to
protect all of its internal data structures.  Most threads in this
test spend most of their time either in the interpreter or waiting for
this lock; with an unfair lock, this lead to such severe starvation
issues that the bug could not reproduce (see
\autoref{fig:eval:multi_threads}).  Switching to a fair lock
implementation avoided the issue.

\begin{figure}
  \input{eval/artificial_bugs/special/multi_threads.tex}
  \caption{CDF of reproduction times for the multi\_threads test with
    different enforcer lock implementations.}
  \label{fig:eval:multi_threads}
\end{figure}

\subsection{Complicated happens-before graphs ($\textrm{complex\_hb}_{\{5,11,17\}}$)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &x1 = global;\\
        &x2 = global;\\
        &x3 = global;\\
        &assert(!(x1 == 0 \&\& x2 == 1 \&\& x3 == 2));\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global = 0;\\
        &global = 1;\\
        &global = 2;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
        \\
      \end{tabular}
    }
  }
  \caption{Crashing and interfering threads for the
    $\textrm{complex\_hb}_5$ test.  The $\textrm{complex\_hb}_{11}$
    and $\textrm{complex\_hb}_{17}$ tests are generated by extending
    this pattern to require additional happens-before edges.}
  \label{fig:eval:complex_hb}
\end{figure}

\begin{figure}
  \subfigure[][]{
    \begin{tabular}{c}
      \begin{tikzpicture}
        \node (dummy) {};
        \node (ld1) [CfgInstr, below = of dummy] {\texttt{x1 = global;}};
        \node (ld2) [CfgInstr, below = 2 of ld1] {\texttt{x2 = global;}};
        \node (ld3) [CfgInstr, below = 2 of ld2] {\texttt{x3 = global;}};
        \node (st1) [CfgInstr, right = of dummy] {\texttt{global = 0;}};
        \node (st2) [CfgInstr, below = 2 of st1] {\texttt{global = 1;}};
        \node (st3) [CfgInstr, below = 2 of st2] {\texttt{global = 2;}};
        \draw[->] (ld1) -- (ld2);
        \draw[->] (ld2) -- (ld3);
        \draw[->] (st1) -- (st2);
        \draw[->] (st2) -- (st3);
        \draw[->,happensBeforeEdge] (st1) -- (ld1);
        \draw[->,happensBeforeEdge] (ld1) -- (st2);
        \draw[->,happensBeforeEdge] (st2) -- (ld2);
        \draw[->,happensBeforeEdge] (ld2) -- (st3);
        \draw[->,happensBeforeEdge] (st3) -- (ld3);
      \end{tikzpicture}\\
      Side condition: \true
    \end{tabular}
  }
  \hfill
  \subfigure[][]{
    \begin{tabular}{c}
      \begin{tikzpicture}
        \node (ld1) [CfgInstr] {\texttt{x1 = global;}};
        \node (dummy) [right = of ld1] {};
        \node (ld2) [CfgInstr, below = 3 of ld1] {\texttt{x2 = global;}};
        \node (ld3) [CfgInstr, below = 2 of ld2] {\texttt{x3 = global;}};
        \node (st1) [CfgInstr, below = of dummy] {\texttt{global = 0;}};
        \node (st2) [CfgInstr, below = 0.5 of st1] {\texttt{global = 1;}};
        \node (st3) [CfgInstr, below = 1.7 of st2] {\texttt{global = 2;}};
        \draw[->] (ld1) -- (ld2);
        \draw[->] (ld2) -- (ld3);
        \draw[->] (st1) -- (st2);
        \draw[->] (st2) -- (st3);
        \draw[->,happensBeforeEdge] (ld1) -- (st1);
        \draw[->,happensBeforeEdge] (st2) -- (ld2);
        \draw[->,happensBeforeEdge] (ld2) -- (st3);
        \draw[->,happensBeforeEdge] (st3) -- (ld3);
      \end{tikzpicture}\\
      Side condition: $\smLoad{\mathrm{global}} = 0$
    \end{tabular}
  }
  \caption{Happens-before graphs generated for the $\textrm{complex\_hb}_5$ test.}
  \label{fig:eval:complex_hb:hb}
\end{figure}

This test, shown in \autoref{fig:eval:complex_hb} is intended to
evaluate {\technique}'s ability to handle more complicated
happens-before graphs which require more than two context switches.
As expected, {\implementation} is able to generate both an enforcer
and a fix for this bug, which can either make the bug reproduce easily
or not at all.

The happens-before graphs and side conditions generated for this test
are shown in \autoref{fig:eval:complex_hb:hb}.  The one on the left
is the expected graph, and will trigger the bug.  The one on the right
is not.  It describes the case in which the first store in the
interfering thread happens after the first load in the crashing
thread, but the initial contents of memory happens to contain the
right value.  It is impossible to enforce the desired happens-before
graph when \texttt{global} is initially 0, as the program structure
means that first store will only run when \texttt{global} is 2, but
the {\technique} analysis is not powerful enough to show that.
Without the side condition, the enforcer would have to try to enforce
both graphs at run-time, and so it would take longer to reproduce the
bug; with it, the enforcer can easily discard the spurious
happens-before graph at run time, partially compensating for the
incompleteness of the main analysis.

\subsection{A simple double-free bug (double\_free)}

\begin{figure}
  \subfigure[][Active threads]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{t = global\_ptr;}\\
        &\multicolumn{2}{l}{if (t != NULL) \{}\\
        &&free(t);\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{global\_ptr = NULL;}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{sleep(1 millisecond);}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Environmental thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr == NULL) \{}\\
        &&global\_ptr = malloc(64);\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
        \\
        \\
        \\
      \end{tabular}
    }
  }
  \vspace{-12pt}
  \caption{Threads involved in the double\_free bug.}
  \label{fig:eval:double_free}
\end{figure}

This test demonstrates {\technique}'s ability to handle some simple
double-free bugs.  The test program is shown in
\autoref{fig:eval:double_free}.  Note that in this test, the crashing
and interfering threads (collectively, the active threads) both run
the same code, shown on the left of the figure, while a third
environmental thread modifies the environment in which they are
operating.  The two active threads loop reading \texttt{global\_ptr}
and, if it is non-\texttt{NULL}, releasing it and setting it to
\texttt{NULL}.  The environment thread is meanwhile undoing their work
by examining \texttt{global\_ptr} and, when it is \texttt{NULL},
setting it to a newly-allocated block.  Every time it does so, the two
active threads will race trying to release the block and reset
\texttt{global\_ptr}.  In some interleavings, both active threads will
try to release the same block, leading to a double-free bug.  Note
that the program threads do not map directly onto the threads in the
\gls{verificationcondition}: the interfering thread is whichever of
the two active threads wins the race and releases the block first; the
crashing thread is the other active thread, which \texttt{free}s a
block which has already been released; and the environmental thread
does not appear in the \gls{verificationcondition} at all, despite
being necessary for the bug to reproduce.  {\Implementation} is able
to build an enforcer and a fix for this bug, and they behave as
expected.

The critical sections for the fix are moderately interesting.  One
runs from immediately before the load of \texttt{global\_ptr} to
immediately after the \texttt{free}, while the other runs from
immediately before the \texttt{free} to immediately after the store of
\texttt{global\_ptr}.  These are sufficient to prevent the
\gls{crashingthread} from releasing something which has already been
released by the \gls{interferingthread}, but not vice versa, and so
might appear to constitute an incomplete fix.  This is not the case.
As discussed in \autoref{sect:fix_global_lock}, {\technique} fixes
track for each program thread a set of protected \gls{cfg} nodes which
that thread might currently be executing, which could potentially
include nodes from both the crashing and the interfering analysis
threads, and the fix acquires the lock whenever any of the potential
nodes require protection.  This means that even though the fix is
asymmetric when expressed in terms of the analysis threads it is fully
symmetric when expressed in terms of the program threads, and so does
actually fix the bug.

Note that the critical region includes the call to \texttt{free}, and
so the generated patch will call into libc while holding the patch
lock.  In this case, that is safe (and in fact necessary to fix the
bug), but might carry a risk of deadlock for some other library
functions.  As discussed previously, {\technique}'s fixes detect such
deadlocks using a timeout and recover by simply allowing the critical
sections to proceed unprotected, avoiding the deadlock at the risk of
potentially re-introducing the fixed bug.

\subsection{A program with existing synchronisation (existing\_sync\_visible, existing\_sync\_invisible)}

\begin{figure}
  \subfigure[][Crashing thread with {\technique}-visible synchronisation]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while(1)\{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{lock();}\\
        &\multicolumn{2}{l}{if (ptr != NULL)}\\
        &&*ptr = 5;\\
        &\multicolumn{2}{l}{unlock();}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
        \\
      \end{tabular}
    }
    \hspace{3mm}
    \label{fig:eval:existing_sync:visible}
  }
  \hspace{-2mm}
  \hfill
  \subfigure[][Crashing thread with {\technique}-invisible synchronisation]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while(1)\{}\\
        &\multicolumn{2}{l}{lock();}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (ptr != NULL)}\\
        &&*ptr = 5;\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{unlock();}\\
        \multicolumn{3}{l}{\}}\\
        \\
      \end{tabular}
    }
    \hspace{3mm}
    \label{fig:eval:existing_sync:invisible}
  }
  \hspace{-3mm}
  \hfill
  \hspace{-3mm}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while(1)\{}\\
        &ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &lock();\\
        &ptr = 0;\\
        &unlock();\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
    \hspace{-3mm}
  }
  \vspace{-12pt}
  \caption{Threads for the existing\_sync\_visible and existing\_sync\_invisible tests.}
  \label{fig:eval:existing_sync}
\end{figure}

These tests explore {\technique}'s interactions with the program's
existing synchronisation.  They are the same as simple\_toctou except
for the addition of calls to \texttt{pthread\_mutex\_lock} and
\texttt{pthread\_mutex\_unlock} which prevent the bug from ever
reproducing.  As discussed previously\editorial{ref?}, {\technique}
has no global model of the program's synchronisation, and so can only
analyse synchronisation within the \gls{analysiswindow}.  The analysis
will therefore be aware of the synchronisation in the
existing\_sync\_visible crashing thread,
\autoref{fig:eval:existing_sync:visible}, but not that in the
existing\_sync\_invisible crashing thread,
\autoref{fig:eval:existing_sync:invisible}.  As such, it generates a
\gls{verificationcondition} for the existing\_sync\_invisible test but
not for the existing\_sync\_visible one.

\begin{figure}
  \centerline{
    \begin{tikzpicture}
      \node (LD1) {First load};
      \path (node cs:name=LD1,anchor=west) -- ++(0,-2) node (LD2) [right] {Second load};
      \node (dummy) [right = 2 of LD1] {};
      \node (ST) [below = 0.6 of dummy] {Store};
      \draw (node cs:name=LD1,anchor=north west) -- ++(-0.2,0) |- (node cs:name=LD2,anchor=south west);
      \draw (node cs:name=ST,anchor=north east) -- ++(0.2,0) |- (node cs:name=ST,anchor=south east);
      \draw[->, happensBeforeEdge] (LD1) -- (ST);
      \draw[->, happensBeforeEdge] (ST) -- (LD2);
      \draw[->] (LD1) -- ++(0,-1.8);
      \path (-1.5,-2) to node [sloped] {locked region} (-1.5,0);
      \node at (5.3,-1.05) {locked region};
    \end{tikzpicture}
  }
  \caption{Happens-before graph which must be enforced for the
    existing\_sync\_invisible test, along with the program's existing
    locked regions.  This bug cannot be reproduced.}
  \label{fig:eval:existing_sync:hb}
\end{figure}

This \gls{verificationcondition} can be instantiated into a bug
enforcer, but, as might be expected, this enforcer cannot cause any
bugs to reproduce.  The happens-before graph for the bug is shown in
\autoref{fig:eval:existing_sync:hb}; this clearly contradicts the
program's existing synchronisation strategy, and so trying to enforce
it will lead to a deadlock.  The unbound message operations in the
crash enforcement plan therefore all time out, preventing successful
plan completion.

The \gls{verificationcondition} can similarly be converted to a fix.
This fix does not fix any actual bugs, as there are none, but does not
otherwise harm the program's execution.

\subsection{A bug which lacks the W isolation property (w\_isolation)}

\begin{figure}
  \centerline{
    {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &s = malloc();\\
        &s->v = 7;\\
        &global\_ptr = s;\\
        &assert(s->v == 7);\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
    {\hfill}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{p = global\_ptr;}\\
        &\multicolumn{2}{l}{if (p != NULL) \{}\\
        &&p->v = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
    {\hfill}
  }
  \caption{Racing threads for the w\_isolation test.  Garbage
    collection-related code is not shown.}
  \label{fig:w_isolation}
\end{figure}

This test illustrates a bug which lacks the W isolation property (see
\autoref{sect:derive:w_isolation}).  In this test, the crashing thread
allocates a fresh data structure, initialises the field \texttt{v},
publishes it via a global pointer, and then asserts that \texttt{v} is
unchanged.  Meanwhile, the interfering thread loops checking for any
published structures and, if it finds one, changing the value of
\texttt{v}.  This bug can only be reproduced when the interfering
thread is able to access the structure which was stored by the
crashing one, and hence lacks the W isolation property.
{\Implementation} is able to correctly analyse this bug, producing an
enforcer and a fix, when configured to not assume the W isolation
property, but cannot otherwise.

\subsection{Comparison to DataCollider}
\label{sect:eval:datacollider}

As a point of comparison, I implemented a tool, inspired by
DataCollider\cite{Erickson2010}, which explores alternative thread
schedules at random without first analysing the program to obtain
{\StateMachines} and verification conditions.  When the program
starts, this tool places breakpoints at a randomly selected subset of
the program's memory accesses.  When one of the breakpoints is hit, it
determines what memory location the instruction is accessing and sets
a processor watch point\needCite{} on that location.  If the watch
point is hit by any other threads, the tool has discovered a race, and
it selects one of the two racing threads to go first at random.  In
this way, the tool encourages the program to explore its available
schedules much more quickly than it otherwise would.

The effectiveness of this tool is obviously highly dependent on the
fraction of instructions which are covered by breakpoints and the
length of the delays inserted.  I used the following parameters in all
experiments:

\begin{itemize}
\item
  At any given point, half of the program's store instructions will
  have an instruction breakpoint, excluding stack accesses.
\item
  When a breakpoint is hit, the tool will wait for up to 100$\mu$s for
  a matching read or write to arrive.  When a thread does arrive, it
  will select which to resume first at random, with equal probability.
  Note that this is a factor of a thousand smaller than the timeout
  used for {\technique} enforcers in most of this evaluation.
\item
  Every 100ms, the tool discards its current instruction breakpoint
  set and generates a new one.
\end{itemize}

These parameters were chosen because they minimise the median
reproduction time for the simple\_toctou test\editorial{I should maybe
  give some actual experiments I did to confirm that.}.

It is instructive to compare these parameters to those used in the
DataCollider paper.  DataCollider uses timeouts of between one and
fifteen milliseconds, depending on the type of instruction, and so the
100$\mu$s timeout is shorter but not dramatically so.  Setting
breakpoints on half of instructions, on the other hand, is many orders
of magnitude different from the settings recommended by the
DataCollider paper.  The actual DataCollider implementation adjusts
the breakpoint density dynamically so as to achieve a particular
breakpoint rate, and their paper does not specify a numerical
breakpoint density, which makes a direct comparison difficult.  It is,
however, possible to estimate the breakpoint density from the
information which they do give.  Their evaluation shows breakpoint
rates of up to 1500 breakpoints per second in a virtual machine with
two processors running at 2.4GHz, so $4.8 \times 10^9$ cycles per
second.  If one assumes roughly one non-stack memory access every
hundred cycles that translates to 4.8 million memory accessing
instructions per second, and so they must trap roughly 0.03\% of
memory accessing instructions.  Even allowing for the fact that their
implementation preferentially places breakpoints on instructions which
execute infrequently this is still likely to translate to a breakpoint
ratio several orders of magnitude lower than that used in this
evaluation.

Compared to {\technique}, and to the original DataCollider, these
parameters have two interesting effects: the tool intervenes in the
program's execution much more frequently, because of the high
breakpoint ratio, but makes relatively small changes to the execution
each time, because of the short timeout.  In other words, these
parameters mean that the DataCollider-like tool gets a very large
number of opportunities to reproduce the bug, but has a low
probability of reproduction at each opportunity.  This is a sensible
strategy for these test bugs, as the test harness will ensure that the
buggy code is run very frequently, but would perhaps be slightly less
effective in more realistic programs where the buggy code runs less
often.  It is therefore quite unlikely that DataCollider would perform
as well on more realistic tests as it does on these artificial bugs.

In another respect, of course, this is unfair to DataCollider, as the
original DataCollider was only intended to discover races, and not as
a tool for exploring alternative program schedules.  It is therefore
not entirely surprising that the results of applying it in this
context are often somewhat poor.  Nevertheless, it represents the
approach which is conceptually close to {\technique}'s in the existing
literature, and so I consider it to be an interesting reference
point.

The results of these experiments are shown in
\autoref{fig:eval:summary_cdfs}.  For the majority of experiments,
the DataCollider-like tool gives a modest reduction in the time taken
to reproduce the bug, but is less effective than {\technique}, as
expected.  There are a few exceptions, however:

\begin{itemize}
\item The cross\_function bug actually reproduces less frequently with
  the DataCollider-like tool than it does with no reproduction-aiding
  tools.  This is because the tool, in this test, often places a delay
  in the \gls{crashingthread} at a place which is irrelevant to the
  bug under investigation.  This slows down the \gls{crashingthread},
  and so there are fewer opportunities to reproduce the bug, but,
  because it is mispositioned, does not increase the probability of
  each opportunity actually succeeding.  The end result is that it
  takes slightly longer to reproduce the bug.
\item The double\_free test reproduces more quickly with the
  DataCollider-like tool than with {\technique}.  This is in large
  part an artifact of the test harness.  This test is implemented such
  that the program will crash when the two racing threads start their
  critical section at the same time, and so the harness includes a
  small delay between starting the two threads so as to avoid crashing
  on the very first iteration.  This is effective for the {\technique}
  enforcers and for the baseline case, but the way the
  DataCollider-like tool is implemented tends to resynchronise the
  threads so that they crash very quickly.  In fact, more than 90\% of
  the time this test program crashes before the tool has detected any
  races or inserted any delays.
\item The multi\_variable test also reproduces more quickly under the
  DataCollider-like tool than under the {\technique} enforcer.  With
  the DataCollider-like tool, the mean time to reproduction is 75ms,
  the median time 69ms, and the $95^{th}$ percentile 172ms; with a
  {\technique} enforcer, the mean time is 322ms, the median 330ms, and
  the $95^{th}$ percentile 386ms.  This is primarily because
  {\technique}'s timeouts are not well-suited to this test; reducing
  the timeout from 100ms to 1ms reduced the mean time to 50ms, the
  median to 50ms, and the $95^{th}$ percentile to 58ms, which would
  represent a useful improvement on the DataCollider-like tool.
  Setting a timeout that short would probably harm reproduction
  performance on more complex tests (although not, of course, as
  setting the timeout to match the 100$\mu$s used by the
  DataCollider-like tool).
\item Finally, the multi\_threads test reproduces more quickly under
  the DataCollider-like tool than under the {\technique} enforcer.
  The problem here is simply that the instruction interpreter used in
  {\technique} enforcers is rather slow, taking more than 1.5 seconds
  to step every thread far enough through their critical section to
  actually try to perform message operations, and then several hundred
  milliseconds more to reach a point where they can reproduce the bug.
  The DataCollider-like tool, on the other hand, runs most of the
  program unmodified, and so does not suffer from this problem.  In a
  more realistic program the critical sections would probably cover a
  smaller proportion of the entire program, and so the program would
  spend less time in the interpreter, which would somewhat mitigate
  this effect.
\end{itemize}

Overall, the {\technique} enforcers are usually able to reproduce most
of the target bugs more quickly than the DataCollider-like tool, and
the cases where the DataCollider-like tool is more effective are
mostly caused by the tests being quite small relative to realistic
programs.

\subsection{Summary tables}

I now give a quantitative characterisation of {\technique}'s
performance on these test programs.  This includes CDFs of the time
taken to reproduce the various bugs, in
\autoref{fig:eval:summary_cdfs}, the time taken to perform the various
analysis steps, in \autoref{table:eval:summary_analysis_times}, and
the performance effects of the fixes, in
\autoref{table:eval:fix_overheads}.  The important points here are:

\begin{itemize}
\item {\Technique}'s enforcers make these bugs reproduce more quickly,
  often much more quickly.
\item The various analysis passes are generally very fast, usually
  taking a few hundred milliseconds.  The most time-consuming program
  to analyse is complex\_hb\_17, which takes 1.711 seconds to analyse.
\item The fixes which {\technique} generates for these bugs have
  tolerable overhead, generally from a few tens of percent to a small
  factor.  More realistic tests would probably show lower overheads,
  as the cost of the fix could be more easily amortised over a larger
  program.
\end{itemize}

In other words, {\technique} is an effective and efficient technique
for both finding and fixing bugs in small programs.  The next few
sections will explore how well it scales up to more complex programs.

The performance table shows a measure of the performance impact of the
various fixes generated by {\technique}.  Performance here is measured
in the number of loop iterations completed per second in the two
threads, excluding any threads for which the test harness deliberately
inserts delays.  Each test program was run 20 times for ten seconds,
with any crashing runs repeated.  I report the number of iterations
per second with and without a fix, and the ratio of those two
quantities.  The table reports each quantity as $[a; b; c]$, where $b$
is an estimate of the quantity of interest (the mean number of
iterations per second for the raw performance numbers, or the ratio of
those two means for the ratio column), and $[a;b]$ forms a 95\%
confidence interval for that quantity.  The confidence intervals for
the raw performance measures were calculated using the central limit
theorem (and simply assuming that 20 samples are sufficient for that
to be valid).  The confidence interval for the ratio was calculated
using Luxburg's approximation\needCite{}.  Ratios greater than one
indicate that the program ran more slowly with the fix applied; those
less than one indicate that it ran more quickly.  In the case of the
multi\_threads test, the performance metric given is summed across all
threads.

\input{eval/artificial_bugs/crash_time_cdfs}

\begin{sidewaystable}
  \centerline{
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    Test name                      & Static analysis & Generating verification conditions & Building the enforcers & Building the fixes \\
    \hline
    simple\_toctou                 & $0.571 \pm_p 0.060$ &  $0.195 \pm_p 0.018$ &  $0.126 \pm_p 0.006$ &  $0.142 \pm_p 0.005$\\
    indexed\_toctou                & $0.604 \pm_p 0.089$ &  $0.275 \pm_p 0.004$ &  $0.146 \pm_p 0.005$ &  $0.142 \pm_p 0.004$\\
    crash\_indexed\_toctou         & $0.566 \pm_p 0.068$ &  $0.289 \pm_p 0.009$ &  $0.149 \pm_p 0.007$ &  $0.138 \pm_p 0.004$\\
    interfering\_indexed\_toctou   & $0.624 \pm_p 0.055$ &  $0.288 \pm_p 0.008$ &  $0.149 \pm_p 0.007$ &  $0.142 \pm_p 0.010$\\
    context                        & $0.682 \pm_p 0.058$ &  $0.233 \pm_p 0.005$ &  $0.132 \pm_p 0.007$ &  $0.139 \pm_p 0.005$\\
    cross\_function                & $0.669 \pm_p 0.036$ &  $0.186 \pm_p 0.004$ &  $0.132 \pm_p 0.005$ &  $0.138 \pm_p 0.006$\\
    double\_free                   & $0.447 \pm_p 0.029$ &  $0.186 \pm_p 0.005$ &  $0.130 \pm_p 0.002$ &  $0.135 \pm_p 0.003$\\
    multi\_variable                & $0.792 \pm_p 0.115$ &  $0.228 \pm_p 0.006$ &  $0.157 \pm_p 0.004$ &  $0.135 \pm_p 0.004$\\
    write\_to\_read                & $0.439 \pm_p 0.035$ &  $0.182 \pm_p 0.003$ &  $0.125 \pm_p 0.004$ &  $0.135 \pm_p 0.003$\\
    broken\_publish                & $0.521 \pm_p 0.037$ &  $0.181 \pm_p 0.004$ &  $0.123 \pm_p 0.005$ &  $0.139 \pm_p 0.004$\\
    complex\_hb\_5                 & $0.426 \pm_p 0.011$ &  $0.212 \pm_p 0.006$ &  $0.149 \pm_p 0.007$ &  $0.141 \pm_p 0.008$\\
    complex\_hb\_11                & $0.457 \pm_p 0.021$ &  $0.343 \pm_p 0.005$ &  $0.162 \pm_p 0.004$ &  $0.141 \pm_p 0.005$\\
    complex\_hb\_17                & $0.451 \pm_p 0.025$ &  $1.711 \pm_p 0.012$ &  $0.200 \pm_p 0.003$ &  $0.140 \pm_p 0.004$\\
    existing\_sync\_visible        & $0.566 \pm_p 0.020$ &  $0.156 \pm_p 0.005$ & \multicolumn{2}{c|}{\emph{Nothing generated}} \\
    existing\_sync\_invisible      & $0.572 \pm_p 0.023$ &  $0.206 \pm_p 0.006$ &  $0.136 \pm_p 0.003$ &  $0.140 \pm_p 0.004$\\
    multi\_bugs                    & $0.625 \pm_p 0.024$ &  $0.288 \pm_p 0.005$ & &  \\
    \hspace{5mm}Bug 1 & & &  $0.161 \pm_p 0.006$ &  $0.136 \pm_p 0.003$\\
    \hspace{5mm}Bug 2 & & &  $0.143 \pm_p 0.006$ &  $0.140 \pm_p 0.005$\\
    multi\_threads                 & $0.562 \pm_p 0.025$ &  $0.232 \pm_p 0.002$ &  $0.160 \pm_p 0.022$ &  $0.142 \pm_p 0.009$\\
    w\_isolation                   & $0.509 \pm_p 0.031$ &  $0.143 \pm_p 0.004$ &  $0.117 \pm_p 0.003$ &  $0.137 \pm_p 0.005$\\
    glibc                          & $0.337 \pm_p 0.030$ &  $0.193 \pm_p 0.004$ &  $0.128 \pm_p 0.004$ &  $0.134 \pm_p 0.003$\\
    \hline
  \end{tabular}
  }
  \caption{Time taken in the various analysis phases for the
    artificial bugs.  Times are given as mean and standard deviation
    of ten runs.  The W isolation assumption was enabled for all tests
    except w\_isolation.}
  \label{table:eval:summary_analysis_times}
\end{sidewaystable}

\begin{sidewaystable}
  \centerline{
  \begin{tabular}{|ll|l|l|l|}
    \hline
    \multicolumn{2}{|l|}{Test name} & Performance without fix & Performance with fix & Ratio\\
    \hline
    \multicolumn{2}{|l|}{simple\_toctou                }  & $[352,\!540,\!000; 352,\!630,\!000; 352,\!730,\!000]$  &  $[95,\!430,\!000; 95,\!600,\!000; 95,\!770,\!000]$  &  $[3.681; 3.689; 3.696]$ \\
    \multicolumn{2}{|l|}{indexed\_toctou               } & & & \\
    & \multicolumn{1}{l|}{Crashing thread} & $[9,\!280,\!000; 9,\!540,\!000; 9,\!800,\!000]$  &  $[9,\!100,\!000; 9,\!600,\!000; 10,\!100,\!000]$  &  $[0.92; 0.99; 1.08]$ \\
    & \multicolumn{1}{l|}{Interfering thread} & $[8,\!140,\!000; 8,\!340,\!000; 8,\!540,\!000]$  &  $[8,\!300,\!000; 8,\!760,\!000; 9,\!220,\!000]$  &  $[0.88; 0.95; 1.03]$ \\
    \multicolumn{2}{|l|}{crash\_indexed\_toctou        }  & $[156,\!726,\!000; 156,\!735,\!000; 156,\!744,\!000]$  &  $[70,\!650,\!000; 70,\!710,\!000; 70,\!770,\!000]$  &  $[2.2147; 2.2166; 2.2185]$ \\
    \multicolumn{2}{|l|}{interfering\_indexed\_toctou  }  & $[69,\!770,\!000; 70,\!000,\!000; 70,\!230,\!000]$  &  $[42,\!660,\!000; 43,\!040,\!000; 43,\!420,\!000]$  &  $[1.607; 1.626; 1.646]$ \\
    \multicolumn{2}{|l|}{double\_free                  }  & $[17,\!800; 18,\!110; 18,\!420]$  &  $[18,\!770; 18,\!860; 18,\!940]$  &  $[0.940; 0.960; 0.981]$ \\
    \multicolumn{2}{|l|}{write\_to\_read               }  & $[487,\!800,\!000; 489,\!700,\!000; 491,\!600,\!000]$  &  $[91,\!400,\!000; 94,\!600,\!000; 97,\!800,\!000]$  &  $[4.99; 5.18; 5.38]$ \\
    \multicolumn{2}{|l|}{broken\_publish               } & & & \\
    & \multicolumn{1}{l|}{Crashing thread} & $[1,\!840,\!000; 1,\!980,\!000; 2,\!120,\!000]$  &  $[1,\!910,\!000; 2,\!070,\!000; 2,\!230,\!000]$  &  $[0.82; 0.96; 1.11]$ \\
    & \multicolumn{1}{l|}{Interfering thread} & $[1,\!840,\!000; 1,\!980,\!000; 2,\!120,\!000]$  &  $[1,\!910,\!000; 2,\!070,\!000; 2,\!230,\!000]$  &  $[0.82; 0.96; 1.11]$ \\
    \multicolumn{2}{|l|}{complex\_hb\_5                } & & & \\
    & \multicolumn{1}{l|}{Crashing thread} & $[50,\!000,\!000; 56,\!000,\!000; 62,\!000,\!000]$  &  $[7,\!950,\!000; 8,\!420,\!000; 8,\!890,\!000]$  &  $[5.6; 6.6; 7.8]$ \\
    & \multicolumn{1}{l|}{Interfering thread} & $[50,\!900,\!000; 52,\!800,\!000; 54,\!700,\!000]$  &  $[13,\!470,\!000; 13,\!890,\!000; 14,\!300,\!000]$  &  $[3.56; 3.80; 4.06]$ \\
    \multicolumn{2}{|l|}{complex\_hb\_11               } & & & \\
    & \multicolumn{1}{l|}{Crashing thread} & $[41,\!000,\!000; 46,\!000,\!000; 51,\!000,\!000]$  &  $[7,\!700,\!000; 8,\!300,\!000; 9,\!000,\!000]$  &  $[4.5; 5.5; 6.6]$ \\
    & \multicolumn{1}{l|}{Interfering thread} & $[41,\!300,\!000; 43,\!200,\!000; 45,\!000,\!000]$  &  $[13,\!420,\!000; 13,\!840,\!000; 14,\!250,\!000]$  &  $[2.89; 3.12; 3.36]$ \\
    \multicolumn{2}{|l|}{complex\_hb\_17               } & & & \\
    & \multicolumn{1}{l|}{Crashing thread} & $[40,\!800,\!000; 45,\!100,\!000; 49,\!300,\!000]$  &  $[8,\!100,\!000; 8,\!800,\!000; 9,\!400,\!000]$  &  $[4.3; 5.1; 6.1]$ \\
    & \multicolumn{1}{l|}{Interfering thread} & $[41,\!600,\!000; 43,\!000,\!000; 44,\!400,\!000]$  &  $[13,\!500,\!000; 14,\!100,\!000; 14,\!800,\!000]$  &  $[2.82; 3.04; 3.28]$ \\
    \multicolumn{2}{|l|}{existing\_sync\_invisible     }  & $[147,\!041,\!000; 147,\!064,\!000; 147,\!087,\!000]$  &  $[68,\!305,\!000; 68,\!339,\!000; 68,\!374,\!000]$  &  $[2.1505; 2.1520; 2.1534]$ \\
    \multicolumn{2}{|l|}{multi\_threads                } & & & \\
    & \multicolumn{1}{l|}{Crashing thread} & $[7,\!320,\!000; 7,\!390,\!000; 7,\!470,\!000]$  &  $[7,\!300,\!000; 7,\!370,\!000; 7,\!430,\!000]$  &  $[0.985; 1.004; 1.023]$ \\
    & \multicolumn{1}{l|}{Interfering thread} & $[6,\!970,\!000; 7,\!040,\!000; 7,\!110,\!000]$  &  $[7,\!130,\!000; 7,\!200,\!000; 7,\!260,\!000]$  &  $[0.959; 0.978; 0.997]$ \\
    \multicolumn{2}{|l|}{multi\_bugs} & $[145,\!000,\!000; 148,\!400,\!000; 151,\!900,\!000]$ & & \\
    & simple\_toctou fix only & & $[91,\!000,\!000; 93,\!200,\!000; 95,\!300,\!000]$  &  $[1.52; 1.59; 1.67]$ \\
    & write\_to\_read fix only & & $[52,\!600,\!000; 54,\!500,\!000; 56,\!500,\!000]$  &  $[2.57; 2.72; 2.89]$\\
    & Both fixes & &  $[59,\!900,\!000; 63,\!400,\!000; 66,\!800,\!000]$  &  $[2.17; 2.34; 2.54]$ \\
    \hline
  \end{tabular}
  }
  \caption{Performance overheads of automatically-generated fixes,
    measured in loop iterations per second.  Further details of the
    experiments are given in the text.}
  \label{table:eval:fix_overheads}
\end{sidewaystable}

\section{Semi-artificial bugs}
\label{sect:eval:semiartificial}

I now present the results of running the tool on some bugs which are
partly artificial and partly real.  This includes the kernel of a real
bug and two cases produced by deliberately introducing bugs into
programs from the STAMP benchmark suite\needCite{}.

\subsection{A kernel of a real bug (glibc)}
\label{sect:eval:glibc}

\todo{I'm tempted to treat this as an artificial bug instead of a
  semi-artificial one.}

glibc is a kernel of glibc bug 2644 \cite{Cambell2006}, which
affected versions of glibc up to 2.5 and could lead to a crash if
multiple threads were shut down at the same time.  A simplified
version of the code involved is shown in \autoref{fig:eval:glibc},
where \texttt{forcedunwind} and \texttt{done\_init} are global
variables.  The program will crash if the load of
\texttt{forcedunwind} on line 1 loads a \texttt{NULL} pointer and the
load of \texttt{done\_init} on line 3 loads 1.  This is clearly
possible due to interleaving with the stores on lines 5 and 6.

Note that the bug here depends on the compiler's optimiser, and is not
apparent at the source-code level\footnote{Unfortunately, only the
  32-bit x86 version of gcc optimises the function like this, and my
  {\implementation} only supports 64-bit programs, which prevented me
  from testing with the real bug.}.  {\Technique} operates entirely at
the machine-code level and so this does not present any additional
complexity.  {\Technique} is able to produce both an enforcer and a
fix for this bug and they behave as expected.  Performance metrics
have already been given in tables
\ref{table:eval:summary_analysis_times} and
\ref{table:eval:fix_overheads} and in
\autoref{fig:eval:crash_cdf:glibc}.

\begin{figure}
  \subfigure[][Before optimisation]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{\_Unwind\_ForcedUnwind() \{}\\
        & \multicolumn{2}{l}{if (forcedunwind == NULL) \{} \\
        &&pthread\_cancel\_init();\\
        & \multicolumn{2}{l}{\}} \\
        & \multicolumn{2}{l}{forcedunwind();}\\
        \multicolumn{3}{l}{\}}\\
        \multicolumn{3}{l}{pthread\_cancel\_init() \{}\\
        & \multicolumn{2}{l}{if (done\_init) return;}\\
        & \multicolumn{2}{l}{forcedunwind = \_forcedunwind\_impl;}\\
        & \multicolumn{2}{l}{done\_init = 1;}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][After optimisation.  Blue indicates the crashing fragment and red the interfering one.]{
    \texttt{
      \begin{tikzpicture}
        \path [use as bounding box] (0,0) rectangle (0,1);
        \draw [fill, color=blue!20] (1.3,0.4) rectangle (7,2.8);
        \path [pattern color=red, pattern=diagonal hatch] (1.3,1) rectangle (7,0.4);
        \draw [fill, color=red!20] (1.3,0.4) rectangle (7,-0.8);
        \draw [fill, color=blue!20] (1.3,-2.1) rectangle (2.5,-1.4);
      \end{tikzpicture}
      \begin{tabular}{lllll}
          & \multicolumn{4}{l}{\_Unwind\_ForcedUnwind() \{}\\
        1 & & \multicolumn{3}{l}{l = forcedunwind;}\\
        2 & & \multicolumn{2}{l}{if} & (l == NULL \&\&\\
        3 & & & &\hspace{3mm}\!done\_init) \{\\
        4 & & & \multicolumn{2}{l}{l = \_forcedunwind\_impl;} \\
        5 & & & \multicolumn{2}{l}{forcedunwind = l;} \\
        6 & & & \multicolumn{2}{l}{done\_init = 1;}\\
        7 & & \multicolumn{3}{l}{\}}\\
        8 & & \multicolumn{3}{l}{l();}\\
          & \multicolumn{4}{l}{\}}\\
        \\
      \end{tabular}
    }
  }
  \caption{Source code for the glibc test case.}
  \label{fig:eval:glibc}
\end{figure}


\subsection{labyrinth}

This test consists of the labyrinth component of the STAMP benchmark
suite\needCite{}, converted to use locks rather than transactional
memory and with one of its critical sections removed.  It is shown in
\autoref{fig:eval:labyrinth}.  This program is structured as a
read-modify-writeback operation: the \texttt{memcpy} on line 8 takes a
local copy of a shared structure, which is then worked on by
\texttt{PdoExpansion} and \texttt{PdoTraceback}, generating results
which can be written back by \texttt{TMgrid\_addPath}.  Before
performing the writeback, \texttt{TMgrid\_addPath} first checks that
no other threads have generated the path which it is adding, crashing
if they have.  Removing the lock operations on lines 7 and 15
introduces a race which can trigger this crash.

\begin{figure}
  \texttt{
    \begin{tabular}{lllll}
        & \multicolumn{4}{l}{TMgrid\_addPath(grid\_t *gridPtr, vector\_t *pointVectorPtr) \{}\\
      1 & & \multicolumn{3}{l}{for (i = 1; i < pointVectorPtr->size() - 1; i++) \{}\\
      2 & & & \multicolumn{2}{l}{long *gridPointPtr = pointVectorPtr->get(i);}\\
      3 & & & \multicolumn{2}{l}{assert(*gridPointPtr == GRID\_POINT\_EMPTY);}\\
      4 & & & \multicolumn{2}{l}{*gridPointPtr = GRID\_POINT\_PTR;}\\
      5 & & \multicolumn{3}{l}{\}}\\
      6 & \multicolumn{4}{l}{\}}\\
      \\
      7  & \multicolumn{4}{l}{acquire\_lock();}\\
      8  & \multicolumn{4}{l}{memcpy(myGridPtr, gridPtr);}\\
      9  & \multicolumn{4}{l}{if (PdoExpansion(myGridPtr)) \{}\\
      10 & & \multicolumn{3}{l}{pointVectorPtr = PdoTraceback(myGridPtr);}\\
      11 & & \multicolumn{3}{l}{if (pointVectorPtr) \{}\\
      12 & & & \multicolumn{2}{l}{TMgrid\_addPath(gridPtr, pointVectorPtr);}\\
      13 & & \multicolumn{3}{l}{\}}\\
      14 & \multicolumn{4}{l}{\}}\\
      15 & \multicolumn{4}{l}{release\_lock();}\\
    \end{tabular}
  }
  \caption{The labyrinth test. \texttt{PdoExpansion} and
    \texttt{PdoTraceback} are thread-local functions.}
  \label{fig:eval:labyrinth}
\end{figure}

{\Implementation} is not able to generate a
\gls{verificationcondition} for this test, and is therefore unable to
generate an enforcer or a fix for it.  The reason is simple:
\texttt{PdoExpansion} and \texttt{PdoTraceback} are both large
functions, and {\implementation} takes an unreasonable amount of time
and memory to analyse them.  It is therefore unable to link the load
of \texttt{*gridPointPtr} on line 3 to the store in the
\texttt{memcpy} on line 8, preventing it from discovering the bug's
concurrency behaviour.

Manual inspection of the machine code suggested in order to correctly
analyse this bug, \gls{alpha} would need to be set to at least 570
instructions, of which 265 correspond to \texttt{PdoExpansion} and 195
to \texttt{PdoTraceback}.  As will be discussed in
\autoref{sect:eval:alpha}, it is difficult to push \gls{alpha} above a
few dozen with the current implementation, and so analysing the entire
path is unlikely to be feasible.  On the other hand, if {\technique}
could use procedure summaries\needCite{} to represent
\texttt{PdoExpansion} and \texttt{PdoTraceback}, it would only have to
analyse 76 instructions, which is far more plausible.  I have not
investigated this possibility in any detail.

\subsection{bayes}

This test is, like labyrinth, formed by taking one of the STAMP
benchmark programs and removing one of the critical sections.  A
simplified version of the original program is shown in
\autoref{fig:eval:bayes}.  It implements a task queue as a
singly-linked list with a C++-like iterator protocol\needCite{}.  When
a worker thread is ready to start a new task, it gets the first
element from the list (line 12), checks whether the list was empty
(line 13), and, if it was not, removes the element from the list (line
14) and proceeds to use it.  The \texttt{list\_remove} operation works
by scanning the list (\texttt{findPrevious}, line 1) to find the
element prior to the one to be removed (which in this case will be a
special dummy head element), using that to check whether the element
to be removed is present (line 3) and, if it is, removing the target
element from the list (lines 4 to 6).

\begin{figure}
  \texttt{
    \begin{tabular}{llll}
      & \multicolumn{3}{l}{bool list\_remove(list, what) \{}\\
      1 & & \multicolumn{2}{l}{prev = findPrevious(list, what);}\\
      2 & & \multicolumn{2}{l}{node = prev->next;}\\
      3 & & \multicolumn{2}{l}{if ((node != NULL) \&\& list->compare(node->data, what) == 0) \{} \\
      4 & & & prev->next = node->next;\\
      5 & & & node->next = NULL;\\
      6 & & & free(node);\\
      7 & & & return TRUE;\\
      8 & & \multicolumn{2}{l}{\}}\\
      9 & & \multicolumn{2}{l}{return FALSE;}\\
      10 & \multicolumn{3}{l}{\}}\\
      \\
      11 & \multicolumn{3}{l}{acquire\_lock();}\\
      12 & \multicolumn{3}{l}{it = taskList->begin();}\\
      13 & \multicolumn{3}{l}{if (it != taskList->end()) \{}\\
      14 & & \multicolumn{2}{l}{status = list\_remove(taskList, it->get());}\\
      15 & & \multicolumn{2}{l}{assert(status);}\\
      16 & \multicolumn{3}{l}{\}}\\
      17 & \multicolumn{3}{l}{release\_lock();}\\
    \end{tabular}
  }
  \caption{The bayes test program.}
  \label{fig:eval:bayes}
\end{figure}

For this test, I removed the lock operations on lines 11 and 17.  This
introduced several possible crashing bugs.  I ran the program 100
times without any modifications so as to establish a baseline:

\begin{itemize}
\item The assertion on line 15 fired 12 times.
\item The \texttt{free} on line 6 caused a double-free crash in the C
  library 12 times.
\item All other runs completed without apparent errors.
\end{itemize}

The Bayes program was invoked with the parameters \texttt{-v32 -r1024
  -n2 -p20 -i2 -e2 -s1 -t 2}.

Both of these bugs are in the correct form to be investigated by
{\technique}, and so I produced \glspl{bugenforcer} for both of them.
I investigated the assertion failure first.  The race here is
essentially a time-of-check, time-of-use one, where the list element
might be removed by another thread in between the check on lines 12
and 13 in the caller and the call to \texttt{findPrevious} on line 1.
The path from line 12 to the crash on line 15 is 48 instructions long,
and so I analysed this bug with an \gls{analysiswindow} of 50
instructions.  This produced a single \gls{verificationcondition},
which I converted to a \gls{bugenforcer}.  The enforcer was moderately
effective: of 100 runs of the program, 26 suffered the bug under
investigation, while 74 suffered the double-free bug.  This is not
quite the desired result, which would have been for the program to
suffer the bug under investigation every time, but would still
probably be useful to a programmer investigating the bug.

I next investigated the double-free bug on line 6.  The race in this
case is that another thread might release the node in between the load
on line 2 and the free on line 6.  The path in this case is 28
instructions, and so I analysed this bug with \gls{alpha} set to 30
instructions\footnote{Unfortunately, attempting to analyse this bug
  with a \gls{analysiswindow} of 50 instructions ran out of memory on
  my 8GiB test machine without producing any
  \glspl{verificationcondition}.}.  This, again, produced a single
\gls{bugenforcer}.  I ran the program 100 times with this enforcer
and all of these runs suffered the double free bug, precisely as
desired.

{\Technique} can generate fixes for both of these bugs.  I ran the
program 100 times with each of the fixes applied.  With only the
double-free fix applied, the program completed successfully 90 times,
suffered the assertion failure five times, and suffered five other
crashes elsewhere in the program.  With the assertion-only fix
applied, the program completed 95 runs successfully but failed an
assertion elsewhere in the program in the remaining 5 cases.  As
expected, both partial fixes eliminate the bug which they are designed
for, and marginally improve the program's reliability over the
baseline case, but do not fix any of the other bugs in the program.

{\Technique} can also generate a combined fix which fixes both bugs.
I ran the program 100 times with that combined fix applied.  In that
case, every run completed successfully.  This is because the union of
the critical sections for the two bugs is the same as the critical
section which I removed in order to produce the test program, and so
the combined fix restores the program to its correct functionality.

\todo{Could maybe give some more details here?  e.g. perf numbers,
  details of fixes and enforcers?}

\section{Experiments with real programs}
\label{sect:eval:real}

\subsection{pbzip2}

\begin{table}
  \centerline{
  \begin{tabular}{|l|l|}
    \hline
    Build \gls{programmodel} & \\
    \hspace{5mm}Time taken & $4.29 \pm_\mu 0.02$s \\
    \hline
    Generating \glspl{verificationcondition} & \\
    \hspace{5mm}Time taken & $333.1 \pm_\mu 0.1$s \\
    \hspace{5mm}Potentially-crashing instructions examined & 356 \\
    \hspace{5mm}Instructions dismissed without generating a {\StateMachine} & 203 \\
    \hspace{5mm}Timeouts without generating interfering \glspl{cfg} & 5 \\
    \hspace{5mm}Interfering \glspl{cfg} generated & 184 \\
    \hspace{5mm}Timeouts processing interfering \glspl{cfg} & 0 \\
    \hspace{5mm}\Glspl{verificationcondition} generated & 14 \\
    \hline
    Build \glspl{bugenforcer} & \\
    \hspace{5mm}Time taken & $1.476 \pm_\mu 0.008$s \\
    \hspace{5mm}Timeouts & 0 \\
    \hspace{5mm}\Glspl{bugenforcer} built & 14 \\
    \hline
  \end{tabular}
  }
  \caption{Statistics related to finding bad pointer dereference bugs
    in pbzip2.  All results following twenty repeats.  Values given
    without an error indication were precisely the same on every run.
    There were no out of memory errors on any run.  All timeouts were
    set to one minute.}
  \label{tab:eval:pbzip2:phase_times}
\end{table}

The first real program I investigated was pbzip2 version 1.1.6, a
widely-used compression tool.  I used {\implementation} to find all
potential bad pointer dereference bugs in the program, using an
\gls{analysiswindow} of 20 instructions.  This generated 14
\glspl{verificationcondition} which required run time checking, and so
I converted all of these into \glspl{bugenforcer}.  I then used pbzip2
to compress ten randomly-generated 100MiB files using each enforcer in
turn.  This did not find any bugs in pbzip2.

\subsection{Thunderbird}

\begin{wraptable}{O}{8.5cm}
  \begin{centering}
    \begin{tabular}{|l|l|}
      \hline
      Build \gls{programmodel} & $1240 \pm_p 10$s \\
      Build \glspl{verificationcondition} & $390 \pm_p 10$ms \\
      Build individual \glspl{bugenforcer} & $640 \pm_p 10$ms \\
      Build combined \glspl{bugenforcer} & $710 \pm_p 20$ms\\
      \hline
    \end{tabular}
  \end{centering}
  \vspace{-6pt}
  \caption{Time taken to process the Thunderbird bug, mean and
    standard deviation of ten runs.}
  \vspace{-12pt}
  \label{tab:eval:thunderbird:times}
\end{wraptable}
\verb|thunderbird| is Mozilla bug number 391259\cite{Mery2007}, a
simple time-of-check, time-of-use race in the IMAP client component of
Thunderbird, a popular open-source e-mail client.  The relevant parts
of the program are shown in \autoref{fig:eval:thunderbird}.  If
\verb|m_transport| is set to \verb|NULL| by \verb|CloseStreams()| in
between the two accesses in \verb|ProcessCurrentURL| then the program
will crash.  The simplest way to trigger this behaviour is for the
user to click on an IMAP folder and to then immediately close
Thunderbird.  For these experiments, I assumed that the crashing
instruction had already been identified, and so I had
{\implementation} investigate that one instruction rather than
generating every possible \glspl{verificationcondition} and
\glspl{bugenforcer}.

This is essentially the same bug as simple\_toctou, but embedded in a
much large program.  As such, {\implementation} produces a very
similar \gls{verificationcondition}, \gls{bugenforcer}, and fix.  The
enforcer is, however, much more difficult to use in this case.
Triggering this bug requires user interaction, and it is difficult to
perform the necessary operations in {\implementation}'s default 200ms
timeout.  Increasing the timeout to five seconds made it trivial to
trigger the desired behaviour: of ten attempts with the enforcer
loaded and using a five second timeout, every one reproduced the bug.
Without the enforcer, or using the 200ms timeout, ten attempts to
reproduce the bug all failed.  The code involved in the bug runs quite
rarely, and in a background thread, and so the user interface was
perfectly usable even with this very long delay.

{\Implementation} produced two \glspl{verificationcondition} for this
bug: one representing the expected race, and one representing a race
with another part of Thunderbird which can also set
\texttt{m\_transport} to \texttt{NULL}.  The second
\gls{verificationcondition} is a false positive: that part of
Thunderbird is correctly synchronised, and so cannot reproduce the
bug.  For all other stores to \texttt{m\_transport}, the
\gls{programmodel} is able to show that the stored value is a valid
pointer, and so no \glspl{verificationcondition} are generated.
{\Implementation} can generated \glspl{bugenforcer} for both
\glspl{verificationcondition}, or it can generate a combined
\gls{bugenforcer} which tries to check both bugs.  All of these
\glspl{bugenforcer} have the expected effect: the false positive one
has no apparent effect, whereas the true positive and combined ones
both successfully reproduce the bug.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{llll}
        & \multicolumn{3}{l}{ProcessCurrentURL() \{}\\
        & & \multicolumn{2}{l}{if (m\_transport) \{}\\
        & & & m\_transport->SetTimeout();\\
        & & \}\\
        & \}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        & \multicolumn{2}{l}{CloseStreams() \{}\\
        & & m\_transport = NULL;\\
        & \multicolumn{2}{l}{\}}\\
        \\
        \\
      \end{tabular}
    }
  }
  \vspace{-6pt}
  \caption{The thunderbird bug.}
  \label{fig:eval:thunderbird}
\end{figure}

The times taken to perform the various phases of the analysis are
shown in \autoref{tab:eval:thunderbird:times}.  The time taken
building the \gls{programmodel} clearly completely dominates the other
phases.  This is unsurprising; it must analyse the entire program,
whereas the other phases analyse only the small window which is
relevant to the bug.  Twenty minutes is still a perfectly reasonable
analysis time for a program as complicated as Thunderbird.

This bug does illustrate one weakness of the {\technique} approach to
finding bugs: it is difficult to apply to programs which do not have
good automated test suites.  {\Technique} generates a very large
number of false positives in its main analysis phase and relies on the
enforcers to distinguish them from real bugs, but this is only
effective if the enforcers can be run automatically, which is in turn
only possible when it is possible to exercise some reasonable
proportion of the program automatically.  Using {\technique}'s
bug-finding mode on Thunderbird would be completely infeasible for
this reason: it produces several thousand candidate bugs, which is
tolerable if each can be dismissed with a few seconds of further
computation, but not if each requires several minutes of user
interaction.

\subsection{MySQL}
\label{sect:eval:mysql}

This is MySQL bug 56324\needCite{}.  A simplified version of the buggy
code is shown in \autoref{fig:eval:mysqld}.  This is, again, a
variant of the simple\_toctou bug, embedded in a much larger program.
As expected, {\implementation} is able to find the bug, generate an
enforcer, and generate a fix, and they all behave as expected.  For
this experiment, I selected a test out of the MySQL test suite which
exercises the buggy code (rpl\_change\_master) and ran it under the
dynamic analysis.  I then used the results of that dynamic analysis to
build a \gls{programmodel}, and used the resulting \gls{programmodel}
to analyse the crashing instruction, with \gls{alpha} set to 20.  This
produced two \glspl{verificationcondition}: the expected one
representing the dangerous interleaving, and a spurious one
representing an interleaving with the code which initialises
\texttt{PSI\_server} when the program starts up.  I converted these to
a single combined \gls{bugenforcer} and ran the rpl\_change\_master
test ten times under that test, and every run reproduced the bug.  By
comparison, ten runs without an enforcer failed to reproduce the bug
at all.

\todo{Put in some performance numbers here.}

{\Technique} can also generate a fix for this bug.  In this case, the
fix consists of two critical sections, one which covers the store in
the \gls{interferingthread} and another which covers the two loads of
\texttt{PSI\_server} in the \gls{crashingthread}.  This is sufficient
to eliminate the identified bad interleaving, but is probably not the
fix which a human programmer would have produced, as it does not cover
the actual call to \texttt{delete\_current\_thread}.  As such, it is
possible for the fixed program to call
\texttt{delete\_current\_thread} after \texttt{PSI\_server} is set to
\texttt{NULL}, or to call \texttt{delete\_current\_thread} multiple
times.  Fortunately, \texttt{delete\_current\_thread} itself only
accesses thread-local variables and is idempotent, and so the fix is
still effective.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{if (PSI\_server) \{}\\
        & PSI\_server->delete\_current\_thread();\\
        \}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{l}
        \\
        PSI\_server = NULL;\\
        \\
      \end{tabular}
    }
  }
  \vspace{-6pt}
  \caption{The MySQL bug.}
  \label{fig:eval:mysqld}
\end{figure}

{\Implementation} can also be used in bug-finding mode on MySQL, as it
has an extensive automated test suite.  In this mode it was able to
locate both the bug given here, and was also able to locate several
other, similar, races on \texttt{PSI\_server} which were unknown to
the author before writing the tool.  This suggests that
{\implementation} can sometimes be used to find previously-unknown
bugs, as desired.

\todo{Complication: this bug is only present in builds of MySQL which
  don't have compiler optimisations, because otherwise the compiler
  caches \texttt{PSI\_server} in a register and avoids the crash.}

\section{Performance exploration}
\label{sect:eval:time_details}

\subsection{Finding bugs}
This section looks at how the time taken by the analysis breaks down
across the various phases and components of {\implementation}.  I look
first at the component which must be run for each potentially-crashing
instruction.  For this experiment, I selected 1000 memory-accessing
instructions at random from the optimised build of MySQL, analysed
them with {\implementation}, and timed how long the various components
of the analysis took.  The results are shown in
\autoref{fig:eval:time_breakdown:crashing}.  This chart shows
several important properties of the analysis:

\begin{figure}
  \input{eval/bubble_charts/bubble1.tex}
  \caption{Time taken by the per-\gls{crashingthread} phases.  See
    text for detailed description.}
  \label{fig:eval:time_breakdown:crashing}
\end{figure}

\begin{itemize}
\item Each phase of the analysis is represented by a distinct blob.
  Each blob is formed from a number of horizontal lines, with each
  line representing a single potentially-crashing instruction which
  required non-trivial work at that phase of the analysis.  The width
  of these lines gives the amount of time which that instruction spent
  in that phase, using the scale immediately below the chart.  This means
  that several useful properties can be read off from the blobs:

  \begin{itemize}
  \item The area of a blob is proportional to the total amount of time
    spent in the corresponding phase, across all potentially-crashing
    instructions.
  \item The width of a blob is proportional to the maximum time spent
    in that phase by any potentially-crashing instruction.
  \item The shape of the blob shows the level of variability in the
    time spent in the phase.
  \item The height of the blob shows how many instructions required
    analysis in that phase.
  \end{itemize}

  The lines within a blob are ordered so that they increase
  monotonically and then decrease monotonically but the ordering is
  otherwise arbitrary.  In particular, the height of a blob's centre
  is not meaningful.

\item The horizontal position of a blob gives the average time at
  which that phase finished, for instructions which required
  non-trivial work in that phase, using the scale given on the x axis.
  Note that this is different to the scale used for the length of
  horizontal lines within a blob; this is necessary because the mean
  and maximum for some of the distributions displayed differ by two
  orders of magnitude.

\item Not every instruction requires analysis, and the solid line
  shows how many instructions can be dismissed early.  For instance,
  roughly 19\% of the instructions analysed were accessing either the
  stack or a fixed address which is guaranteed to be mapped, and there
  is no need to perform any analysis on these instructions.
  Similarly, for 48\% of instructions the {\StateMachine} simplifiers
  were able to reduce the {\StateMachine} to the point where it
  produced an empty $\beta$ set, and so there were no interfering
  \glspl{cfg} for these potentially-crashing instructions.  It is
  arguably something of an abuse to plot a regression line here, as
  there the number of dismissals is not really a continuous function
  of time, but I feel that this is the clearest way of presenting the
  data.

\item Similarly, some instructions do not start later phases because
  of timeouts in an earlier phase.  The number of timeouts before a
  particular phase is given by the difference between the solid and
  dashed lines.  In this case, the only timeout occurred while
  simplifying the crashing {\StateMachine}.  The timeout was set to
  one minute, and runs from the start of the analysis to the start of
  the ``Process interfering CFGs'' phase.

\item The chart only shows time spent in the most important phases.
  Any remaining time is shown in an ``Other'' blob on the right of the
  chart.
\end{itemize}

The chart shows three features which are worth noting:

\begin{itemize}
\item Most of the time is spent in the ``Process interfering
  \glspl{cfg}'' phase.  This is discussed in more detail later.
\item The ``Derive C-atomic'' phase is tiny.  The crashing
  {\StateMachines} are individually quite small, and so symbolically
  executing them usually completes quickly.
\item Most of the distributions have large outliers, indicating that
  most potential bugs can be processed very easily but the remaining
  ones require a significant amount of analysis.  This is
  unsurprising: many of the algorithms involved have very bad but very
  rare worst-case complexity; steps which avoid those worst cases are
  processed quickly, but those which hit them perform very badly.
\end{itemize}

I now investigate the time taken processing each interfering
\gls{cfg}.  \autoref{fig:eval:time_breakdown:interfering} shows the
time taken by the various sub-phases of this, in the same style as the
previous figure.  The data for this chart was collected in the same
run of {\implementation} as that for the previous chart.  There were
2251 interfering \glspl{cfg} in total.

The third phase of this chart, ``rederive crashing {\StateMachine}'',
perhaps requires further explanation.  Fixing the interfering
\gls{cfg} fixes the set of instructions which the \gls{crashingthread}
might race with, which can often allow further simplifications in the
crashing {\StateMachine} and in C-atomic.  This phase is responsible
for discovering such simplifications.  In this case, those
simplifications were sufficient to completely dismiss 56\% of the
interfering \glspl{cfg}.

\begin{figure}
  \input{eval/bubble_charts/bubble2.tex}
  \caption{Time taken by the per-interfering \gls{cfg} phases.  See
    text for detailed description.}
  \label{fig:eval:time_breakdown:interfering}
\end{figure}

Perhaps the most surprising property of this chart is that
symbolically executing the cross-product {\StateMachine} is less
expensive than executing the \gls{ic-atomic} one, despite having to
handle far more complex instruction schedules.  There are two reasons
for this.  First, the cross-product construction converts the
program's concurrency behaviour into control flow operations, and the
{\StateMachine} simplifiers are particularly good at simplifying
control flow.  The symbolic execution phase therefore never sees most
of the interleaving-related complexity (although the cross-product
simplification phase does, and therefore takes modestly longer than
the other simplification phases).  Second, the cross-product symbolic
execution has more information than the \gls{ic-atomic} one.  The
entire reason for deriving \gls{ic-atomic} is to constraint the set of
environments in which the threads might execute, and the cross-product
symbolic execution can take advantage of that to eliminate some
potential paths from consideration.

The phase suffered several failures:

\begin{itemize}
\item There was one timeout symbolically executing the cross-product
  {\StateMachine}.
\item There were three timeouts symbolically executing the \gls{ic-atomic}
  {\StateMachine}, and two out-of-memory failures.
\item There was one timeout simplifying the interfering
  {\StateMachine}.
\end{itemize}

The timeout was set to sixty seconds, covering the entire analysis
from the start of the ``compile interfering {\StateMachine}'' phase to
the end of the ``final satisfiability check'' one, and the machine the
analysis was running on had 8GiB of memory.

This represents a total failure rate of 0.3\%, or 0.4\% including the
failure in the previous phase.  \todo{Say more about the causes of the
  failures.}

\subsubsection{Effects of {\StateMachine} simplification}

For comparison, I also performed this experiment with the
{\StateMachine} simplifiers disabled.  The results are shown in
figures~\ref{fig:eval:time_breakdown:crashing_no_simple}
and~\ref{fig:eval:time_breakdown:interfering_no_simple}.  These are
roughly as expected: the entire process is much slower, with the
average time per potentially-crashing instruction rising from 3.4 to
22.5 seconds and the time per interfering \gls{cfg} rising from 0.82
to 4.2 seconds, and the failure rate increases from 0.4\% to 3.2\%.

Also, although not shown on the charts, the number of interfering
\glspl{cfg} increased from 2251 to 4154.  These correspond to cases
where the simplifiers were able to show that a particular memory load
in the crashing {\StateMachine} could not possible influence whether
the {\StateMachine} ultimately predicts a crash, so that the
configuration with the simplifiers enabled did not need to consider
races with that location.  The number of \glspl{verificationcondition}
needing run-time validation, by contrast, fell when simplification was
disabled.  Disabling simplification never causes additional
\glspl{verificationcondition} to be found, as the simplifiers are all
sound.  The \glspl{verificationcondition} which were lost correspond
to cases where the analysis succeeded with the simplifiers enabled and
failed with them disabled.

\begin{figure}
  \input{eval/bubble_charts/bubble5.tex}
  \caption{Time taken by the per-\gls{crashingthread} phases with
    {\StateMachine} simplification disabled.  Note the change in
    scale.}
  \label{fig:eval:time_breakdown:crashing_no_simple}
\end{figure}

\begin{figure}
  \input{eval/bubble_charts/bubble6.tex}
  \caption{Time taken by the per-interfering \gls{cfg} phases with
    {\StateMachine} simplification disabled.  Note the change in
    scale.}
  \label{fig:eval:time_breakdown:interfering_no_simple}
\end{figure}

\subsubsection{Effect of the W isolation assumption}
\label{sect:eval:w_isolation}

\todo{Might need a slightly bigger data set here; the effect is kind
  of marginal with this one.}

I now investigate the effect of the \gls{w-isolation} on the analysis,
in terms of the time taken and the set of
\glspl{verificationcondition} generated.  I took the same 1000
instructions from the optimised build of MySQL and analysed them with
and without the \gls{w-isolation} assumption, with \gls{alpha} set to
20 in both cases.  The results are shown in
\autoref{table:eval:w-isolation}.  As expected, the \gls{w-isolation}
assumption modestly decreases both the number of
\glspl{verificationcondition} which must be checked by run-time
\glspl{bugenforcer} and the time taken to generate them.

\begin{table}
  \begin{tabular}{|l|l|l|}
    \hline
    & \multicolumn{2}{c|}{\gls{w-isolation}} \\
    \hline
                                                                & Enabled & Disabled \\
    \hline
    Potentially-crashing instructions examined                  & 1000    & 1000 \\
    Instructions dismissed without generating a {\StateMachine} & $20 \pm_b 1$\% & $20 \pm_b 1$\% \\
    Timeouts building crashing {\StateMachines}                 & $0.1 \pm_b 0.1$\% & $0.4 \pm_b 0.2$\% \\
    Out of memory building crashing {\StateMachines}            & 0                & 0 \\
    Time to build crashing {\StateMachine} *                    & $0.14 \pm_\mu 0.02$s & $0.16 \pm_\mu 0.03$s \\
    \Glspl{crashingthread} generating no interfering \glspl{cfg}& $48 \pm_b 1$\% & $43 \pm 1$\%\\

    Time to process each interfering \gls{cfg}                  & $0.24 \pm_\mu 0.04$s & $0.37 \pm_\mu 0.04$s \\
    Interfering \glspl{cfg} running out of memory               & $0.09 \pm_b 0.06$\% & $0.11 \pm_b 0.06$\% \\
    Interfering \glspl{cfg} running out of time                 & $0.2 \pm_b 0.1$\% & $0.6 \pm_b 0.1$\% \\

    Interfering \glspl{cfg} generating \glspl{verificationcondition}   & $12.7 \pm_b 0.7$\% & $12.6 \pm_b 0.6$\% \\
    \Glspl{verificationcondition} per potentially-crashing instruction & $0.29 \pm_{10000} 0.04$ & $0.34 \pm_{10000} 0.05$\\
    \hline
  \end{tabular}
  \caption{Effects of the \gls{w-isolation} assumption. *: excluding
    timeouts and instructions dismissed without building a
    {\StateMachine}.}
  \label{table:eval:w-isolation}
\end{table}

The table includes three different types of error indicator:

\begin{itemize}
\item $x \pm_\mu y$ shows the mean and standard deviation of mean of
  some quantity, as usual.
\item $x \pm_b y$\% is used to show the frequency of some event.  $x$
  is an estimate of the frequency itself, calculated in the obvious
  way.  $y$ is an estimate of $x$'s standard deviation, calculated as
  $\sqrt{x(1-x)/n}$, where $n$ is the number of samples.  This is
  the standard deviation which would be observed in $x$ if the
  frequency really were $x$ and if the observations were all
  identically independently distributed.
\item $x \pm_{10000} y$ is used to show the number of
  \glspl{verificationcondition} generated for each
  potentially-crashing instruction.  $x$ is a simple average over all
  of the potentially-crashing instructions.  For this error
  indication, I took the set of 1000 results generated by the
  experiment and resampled them with replacement to produce a further
  10,000 sample sets each of size 1000.  I then used these 10,000
  sample sets to compute 10,000 estimates of the ratio.  I report the
  population standard deviation of those estimates as $y$.  I also
  manually inspected the generated estimates to confirm that there
  were not an excessive number of outliers in either sense.
\end{itemize}

\subsubsection{Effects of the $\alpha$ parameter}
\label{sect:eval:alpha}

\todo{Need to regenerate the graphs with the 300 second timeout rather
  than the 60 second one, once that's finished running.}

\begin{figure}
  \centerline{\input{eval/alpha/unopt/bpm.tex}}
  \caption{Effect of the $\alpha$ parameter on the time taken to build
    the \gls{crashingthread} {\StateMachines}.  See text for detailed
    description of this chart.}
  \label{fig:perf:alpha:bpm:unopt}
\end{figure}

The value of the $\alpha$ parameter has a large effect on the
behaviour of the \gls{verificationcondition} generating process.  I
now investigate these effects in slightly more detail.  For these
experiments, I selected 1000 instructions at random in an unoptimised
build of MySQL and ran the analysis on each of them in turn.  I then
repeated the analysis for a number of different values of \gls{alpha}.
All timeouts were set to five minutes, rather than the usual sixty
seconds, so as to reduce truncation effects at high values of
\gls{alpha}.

The first thing I investigated was the effect of \gls{alpha} on the
time taken to derive the \gls{crashingthread} {\StateMachines},
including deriving the \gls{cfg}, compiling it to a {\StateMachine},
performing the initial simplification, and deriving C-atomic.  The
results are shown in the violin plot in
\autoref{fig:perf:alpha:bpm:unopt}.  This chart shows an estimate of
the probability density function of the time taken to build the
crashing {\StateMachines} for each value of \gls{alpha}, including
building the initial \gls{cfg}, compiling to a {\StateMachine}, and
performing the initial simplifications.  The density function was
estimated in log space using a truncated gaussian kernel density
estimator with the threshold set to the timeout time and a bandwidth
of $\sigma.n^{-0.2}$, where $\sigma$ is the log space population
standard deviation and $n$ the number of samples, in accordance with
Silverman's Rule of Thumb\needCite{}.  The chart also shows the
arithmetic mean excluding timeouts, plus or minus one standard
deviation of that mean, as a hollow gray rectangle.  The bandwidth
used for each value of \gls{alpha} is indicated by a small vertical
line below and to the right of the main violin plot; any feature which
is vertically smaller than this is highly likely to be an analysis
artifact and should be ignored.

The chart also shows an estimated standard deviation of the density as
pale blue limning around the main violins; the width of the limn gives
the standard deviation of the density at that point, on the same scale
as the width of the violin.  These estimates were produced using 1000
full-size bootstrap resamplings of the original data.

Note that time is shown on a log scale, which is necessary as the data
shows interesting behaviour across many orders of magnitude.  This
means that it is difficult to simultaneously assign convenient
meanings to the width and the area of the regions on the chart.  I
have chosen a KDE which ensures that a fixed area represents a fixed
amount of probability across the chart, but does not give any
particularly convenient property to the width of a region, as I felt
that gave a less deceptive impression of the data.

The chart also shows the proportion of crashing instructions which
could be dismissed without generating a {\StateMachine} (the
pre-dismissed box), the number which suffered a timeout (the timeout
box), and the number which ran out of memory.  These are shown on the
same scale as the rest of the probabilities.

The configuration used for this experiment was slightly different from
that used for most of the other experiments:
\begin{itemize}
\item The timeout was increased from sixty seconds to five minutes, as
  that made the data easier to interpret for high values of
  \gls{alpha}.
\item The machine used to run this experiment was a 1.9GHz Opteron
  6168 with 16GiB of memory.
\item {\Implementation} was initially configured to analyse ten
  potentially-crashing instructions in parallel, whereas the previous
  experiments analysed one at a time.  Any instructions which suffered
  an out-of-memory error when run in parallel were re-run in series
  after the experiment completed; an operation was only treated as
  running out of memory if it failed on this final run.  Timed-out
  phases of the analysis were not re-run.  The distributions obtained
  without performing any re-runs are shown on the chart with dashed
  grey lines.
\end{itemize}
The results of this experiment are therefore not directly comparable
with the results of the other experiments.

The overall shape of this chart is roughly what one would expect: the
time taken to analyse a particular potentially-crashing instruction
rises roughly exponentially with the value of \gls{alpha}, and the
proportion of timeouts rises rapidly once \gls{alpha} exceeds a
certain value, which in this case is about 50.

The next thing I investigated was the effect of \gls{alpha} on the
size of the \glspl{crashingthread}.  The results are shown in
\autoref{fig:eval:time_breakdown:crashing_size}\footnote{This is a box
  plot rather than a density estimate, as the density estimator
  assumes continuous data and the quantities shown here are
  discreet.}.  This chart shows the sizes of the
\glspl{crashingthread}, in terms of the number of (dynamic) program
instructions represented in the \gls{cfg}, the number of states in the
{\StateMachine} before simplification, and the number of states in the
{\StateMachine} after simplification.  These results are again easy to
explain:

\begin{itemize}
\item The number of instructions in the thread is always at least
  \gls{alpha}, and the average number grows roughly exponentially with
  \gls{alpha}.
\item The number of states in the initial {\StateMachine} is generally
  larger than the number of instructions.  There are two reasons for
  this.  The first is that the machine code being analysed is in a
  CISC instruction set, whereas the {\StateMachine} language is very
  RISC-like, and so each instruction usually requires multiple states.
  The second is that {\implementation} uses some initial special
  states, such as \state{ImportRegister} and some not described here,
  to set up an initial environment for the {\StateMachine} and to
  import properties from the \gls{programmodel}.  The number of these
  special states is usually around 30, and so they represent an
  important source of states when \gls{alpha} is small but not when it
  becomes larger.
\item The number of states in the simplified {\StateMachine} follows
  roughly the same trend as the other measures, but is usually about a
  factor of ten smaller.  This simply indicates that the simplifier is
  working as desired.
\end{itemize}

Note that the distributions shown in all three charts exclude any
\gls{crashingthread} which suffered a timeout before generating the
relevant object (so, for instance, the ``Simplified states'' chart
excludes any \glspl{crashingthread} which did not complete
simplification).  This will have caused some selection bias, as the
larger \glspl{crashingthread} are most likely to suffer a timeout.
This will be particularly important for the high quantiles of the high
\gls{alpha} distributions.

\begin{figure}
  \centerline{\input{eval/alpha/unopt/crashing_size.tex}}
  \caption{Effect of the $\alpha$ parameter on the size of the
    \gls{crashingthread}.  Note that this chart completely ignores any
    potentially-crashing instruction which suffered a failure, whether
    due to running out of memory or time.}
  \label{fig:eval:time_breakdown:crashing_size}
\end{figure}

\begin{figure}
  \input{eval/alpha/unopt/gsc.tex}
  \caption{Effect of the $\alpha$ parameter on the derivation of the
    \gls{interferingthread} \glspl{cfg}.}
  \label{fig:perf:alpha:gsc:unopt}
\end{figure}

Once the \gls{crashingthread} {\StateMachine} has been generated, the
next step is to derive \glspl{cfg} for the \glspl{interferingthread}.
The response of this process to the $\alpha$ parameter is shown in
\autoref{fig:perf:alpha:gsc:unopt}.  In this chart, the
pre-dismissed box includes anything which was dismissed by the
previous stage or where the crashing {\StateMachine} was sufficient to
show that no bug of the desired class could exist without reference to
the interfering \glspl{cfg}.  The pre-failed box gives the number of
times that no interfering \glspl{cfg} could be derived due to a
failure in a previous phase of the analysis.  As before, increasing
$\alpha$ leads to an increase in the both the time taken to perform
the analysis and the number of timeouts.  This behaviour needs little
explanation.

The lower part of the chart, showing the number of interfering
\glspl{cfg} per \gls{crashingthread}, is more surprising.  Many of
\glspl{crashingthread} do not produce any interfering \gls{cfg} at
all.  This indicates that the potential crash could be dismissed based
on contents of the crashing {\StateMachine} and the dynamic aliasing
model, with no need for any further analysis.  There are several ways
that this could happen:

\begin{itemize}
\item It might be that none of the \gls{alpha} instructions leading up
  to the crashing instruction loaded from thread-shared memory, in
  which case they trivially cannot suffer a concurrency bug of this
  form.
\item Similarly, the simplifiers can sometimes show that a the value
  of a load from shared memory cannot possibly influence whether the
  thread will crash, and if so the \gls{crashingthread} can be
  dismissed without needing to derive the interfering \glspl{cfg}.
\item If the \gls{crashingthread} makes only a single load from memory
  then it is safe to dismiss any interfering \glspl{cfg} which make
  only a single store, as in that case there are no interleavings
  which are not equivalent to running the two threads atomically.
\end{itemize}

All of these special cases are clearly more likely when \gls{alpha} is
small, and indeed we observe that many of the small \gls{alpha} tests
do not generate any interfering \glspl{cfg}.  This does not, however,
explain why the large \gls{alpha} tests also show a large number of
\glspl{crashingthread} which do not generate any interfering
\glspl{cfg}.  The explanation here is selection bias: only simple
\glspl{crashingthread} avoid timing out before generating the
interfering \glspl{cfg}, and these tend to generate fewer interfering
\glspl{cfg}.

The next step of the analysis is to take these pairs of crashing
{\StateMachines} and interfering \glspl{cfg} and convert them into
\glspl{verificationcondition}.  This is shown in
\autoref{fig:perf:alpha:gvc:unopt}.  Note that the denominator for
the various proportions has changed: previously, it was the total
number of potentially-crashing instructions, whereas now it is the
number of pairs generated by the previous analysis stages.

The effect of $\alpha$ on the number of \glspl{verificationcondition}
generated is easiest to understand if it is expressed as the fraction
of \glspl{cfg} which generate a
\gls{verificationcondition}\footnote{Recall that there is at most one
  \gls{verificationcondition} for each interfering \gls{cfg}.}  This
initially increases with $\alpha$ and then begins to fall off again
when $\alpha$ increases further.  Increasing $\alpha$ allows
{\technique} to consider interleaving the program's threads over a
larger window of instructions, and so to consider more complicated
concurrency behaviour, and this leads to a modest increase in the
fraction of interfering \glspl{cfg} which might cause undesirable
behaviour in the \gls{crashingthread}, accounting for the initial
increase.  The subsequent decrease is, again, a selection effect, as
only threads with simple aliasing and concurrency behaviour will reach
this stage of the analysis.  The time taken by the analysis follows
broadly the same pattern, for essentially similar reasons.

\begin{figure}
  \centerline{\input{eval/alpha/unopt/gvc.tex}}
  \caption{Effect of the $\alpha$ parameter on the number of
    \glspl{verificationcondition} generated and the time taken to do
    so.  All timeouts in earlier stages of the analysis are excluded.
    The time taken includes building the interfering {\StateMachine},
    rederiving crashing {\StateMachine}, deriving \gls{ic-atomic}
    constraints, building the \gls{verificationcondition}, and the
    final satisfiability check.}
  \label{fig:perf:alpha:gvc:unopt}
\end{figure}

\begin{figure}
  \input{eval/alpha/opt/bpm.tex}
  \caption{Effect of the $\alpha$ parameter on the time taken to build
    the \gls{crashingthread} {\StateMachines} for an optimised build
    of MySQL.}
  \label{fig:perf:alpha:bpm:opt}
\end{figure}

\begin{figure}
  \input{eval/alpha/opt/gsc.tex}
  \caption{Effect of the $\alpha$ parameter on the number of
    \gls{interferingthread} \glspl{cfg} per \gls{crashingthread}
        {\StateMachine} for an optimised build of MySQL.  \todo{Urk}}
  \label{fig:perf:alpha:gsc:opt}
\end{figure}

\begin{figure}
  \input{eval/alpha/opt/gvc.tex}
  \caption{Effect of the $\alpha$ parameter on the number of
    \glspl{verificationcondition} generated and the time taken to do
    so for an optimised build of MySQL.}
  \label{fig:perf:alpha:gvc:opt}
\end{figure}

The results with an optimised version of MySQL are broadly similar,
and are shown in figures~\ref{fig:perf:alpha:bpm:opt},
\ref{fig:perf:alpha:gsc:opt}, and \ref{fig:perf:alpha:gvc:opt}.  The
most obvious difference between the optimised and unoptimised graphs
is that the optimised program takes longer to analyse, and suffers a
higher timeout rate.  This is not particular surprising: the optimised
program fits more complicated behaviour into the \gls{analysiswindow}
and so takes longer to process.  \todo{Not sure there's much to say
  about that.}

\subsection{Building bug enforcers}

The next thing to look at is how long it takes to convert
\glspl{verificationcondition} to \glspl{bugenforcer}.  For this
experiment, I chose 5000 \glspl{verificationcondition} at random from
those generated by analysing the whole of MySQL, converted each one to
an enforcer, and timed how long each phase took; the results are shown
in \autoref{fig:eval:time_breakdown:convert_to_enforcer}\footnote{I
  also performed a similar experiment using the 297
  \glspl{verificationcondition} generated by the previous phase.  The
  results were broadly similar, but the smaller sample set meant that
  some low-probability outcomes were completely unrepresented in the
  results.}.

Several of the \glspl{verificationcondition} could not be converted to
\glspl{bugenforcer} due to running out of memory; 34 (0.7\%) while
slicing the \gls{verificationcondition} by the happens-before graph
and 4 (0.1\%) while placing the side conditions.  In addition, one
\gls{verificationcondition} reached the 60 second timeout during
slicing.  Re-running that one condition without a timeout showed that
it would eventually have run out of memory.  The system used to run
the test had 8GiB of memory.  To some extent, these out-of-memory
errors reflect a problem with my implementation {\implementation}
rather than the technique {\technique}: {\implementation} manages most
of the data structures involved using a garbage collector which only
runs in between the phases, which simplified the implementation but
will have significantly increased the system's memory consumption.  It
would therefore be reasonable to suppose that a more refined
implementation could usefully reduce the number of
\glspl{verificationcondition} which fail.  It is unlikely, however,
that it would be possible to eliminate all failures completely, as
both phases make heavy use of BDD reordering, which has an exponential
worst-case memory consumption.  The algorithm is designed such that
most of the time the BDDs are in nearly the correct order to begin
with, and so this worst case can usually be avoided, but the minority
of cases which do hit it will always require a very large amount of
memory.

The only other phase which requires a non-trivial amount of time is
the final one, compiling the enforcer.  This involves converting the
generated enforcer to C and passing it off to the system compiler (in
this case, gcc version 4.4.3) to convert it into an ELF library.  The
time taken by this phase is completely dominated by the time taken by
the external compiler.

\begin{figure}
  \input{eval/bubble_charts/bubble4.tex}
  \caption{Time taken by the different phases involved in converting a
    \gls{verificationcondition} to a \gls{bugenforcer}.}
  \label{fig:eval:time_breakdown:convert_to_enforcer}
\end{figure}

\subsection{Generating fixes}
\label{sect:eval:genfix}

The final component of {\technique} which I investigate here is fix
generation.  The performance behaviour here is far simpler than for
the other phases.  It is summarised in
\autoref{tab:eval:gen_fix_perf}.  The time taken is clearly completely
dominated by the time taken by the system compiler, and is perfectly
reasonable for these patches.

\begin{table}
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    Phase & Mean & $5^{th}$\% & $95^{th}$\% & Population standard deviation \\
    \hline
    Find critical sections & $0.50 \pm_\mu 0.02$ & 0.1 & 1.6 & 1.1 \\
    Identify patch points & $0.805 \pm_\mu 0.008$ & 0.3 & 1.4 & 0.6 \\
    Partial evaluation & $0.75 \pm_\mu 0.06$ & 0.2 & 1.5 & 4.4 \\
    System compiler & $105.1 \pm_\mu 0.2$ & 85 & 128 & 14 \\
    \hline
    Total & $107.2 \pm_\mu 0.2$ & 87 & 131 & 15 \\
    \hline
  \end{tabular}
  \caption{Time taken to convert 5000 \gls{verificationcondition}
    generated from MySQL to fixes.  All times in milliseconds.}
  \label{tab:eval:gen_fix_perf}
\end{table}

\autoref{tab:eval:gen_fix_perf:props} gives some statistics related to
the complexity of the patches generated.  The properties examined are:

\begin{itemize}
\item The number of program instructions in the protected region.
\item The number of lock and unlock operations present in the
  generated machine code fragment.
\item The size the generated machine code fragment, in terms of both
  the number of instructions and the number of bytes.  This is further
  broken down into the number of instructions needed to acquire the
  lock, the number to release it, and the number in other forms of
  patching overhead.
\item The number of instructions in the original program which are
  patched to gain control of the program.
\item The number of instructions in the $\mathit{Cont}$ set (see
  \autoref{sect:enforce:gain_control}).
\item The number of relocations.  This is the number of places in the
  generated machine code which must be modified at run-time when the
  patch is loaded, to account for the patch being loaded at an address
  which is unknown at compile time.
\end{itemize}

The table shows the mean and central limit theorem standard deviation
of the mean, the $5^{th}$ and $95^{th}$ percentiles, and the
population standard deviation ($\sigma$) of the parameter.  As can be
seen, the generated patches usually consist of a few dozen
instructions or a few hundred bytes of machine code.  The size is
therefore perfectly manageable.  Note that most of the distributions
shown in the table are non-Gaussian and so, for instance, the $\sigma$
column should be treated with caution.

\begin{table}
  \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
                           & \multicolumn{4}{c|}{Distribution summary} & \multicolumn{3}{c|}{Effect on time taken} \\
    \hline
    Property of fix        & Mean & $5^{th}$\% & $95^{th}$\% & $\sigma$ & $\beta$, ms & $\gamma$ & R\\
    \hline
    Protected instructions & $17.9 \pm_\mu 0.4$  & 3   & 40  & 30  & 0.2  & 0.04 & 0.28 \\
    Lock operations        & $2.58 \pm_\mu 0.04$ & 1   & 4   & 2.7 & 1.6  & 0.009& 0.13 \\
    Unlock operations      & $2.88 \pm_\mu 0.02$ & 1   & 5   & 1.4 & 0.12 & 0.00001& 0.005 \\
    Patch instructions     & $70 \pm_\mu 1$      & 27  & 127 & 72  & 0.06 & 0.009& 0.13\\
    \hspace{5mm}Acquiring lock&$15.5 \pm_\mu 0.2$& 6   & 24  & 16  &$\ast$&$\ast$& $\ast$\\
    \hspace{5mm}Releasing lock&$17.3 \pm_\mu 0.1$& 6   & 30  & 8   &$\ast$&$\ast$& $\ast$\\
    \hspace{5mm}Overhead   & $19.1 \pm_\mu 0.5$  & 0   & 50  & 32  &$\ast$&$\ast$& $\ast$\\
    Patch bytes            & $326 \pm_\mu 5$     & 126 & 594 & 346 & 0.01 & 0.009& 0.13\\
    Patch points           & $2.26 \pm_\mu 0.01$ & 2   & 3   & 0.88& 0.8  & 0.0008& 0.04\\
    $\mathit{Cont}$        &$0.034 \pm_\mu 0.003$& 0   & 0   & 0.2 &$\ast$&$\ast$& $\ast$\\
    Relocations            & $11.0 \pm_\mu 0.1$  & 6   & 18  & 8.0 & 0.5  & 0.03 & 0.24\\
    \hline
  \end{tabular}
  \caption{Summaries of some gross properties of the generated fixes,
    and their effects on the time taken by all phases of fix
    generation.  See text for details. $\ast$: Not meaningful.}
  \label{tab:eval:gen_fix_perf:props}
\end{table}

The table also shows the dependence of the time taken to generate the
patch on some of these measurements.  For these calculations, I
performed a simple linear regression of the total time taken on the
parameter.  The table shows $\beta$, the gradient of the resulting
line, $\gamma$, the proportion of the standard deviation of time which
can be explained by that linear regression, and R, the Pearson
product-moment correlation coefficient.  As can be seen, the number of
protected instructions and the number of relocations are modestly
positively correlated with the total time taken, while the other
parameters show weaker correlations, or none at all.  This gives an
indication of how the patch process would scale to more complex
critical sections.  These results should be treated with some caution,
though: none of the correlations are very strong, and many of the
error distributions are non-Gaussian, the sample size is only modest,
and there are strong correlations between the different properties of
the fix in addition to between the properties and the time taken.

\section{Dynamic analysis}
\label{sect:eval:dynamic_analysis}

\subsection{Effects on analysis performance and correctness}

\todo{This is another one which is lacking error bars.}

Similarly to the investigation of the effects of the \gls{w-isolation}
assumption in \autoref{sect:eval:w_isolation}, I now investigate the
effects of the dynamic aliasing analysis on the generation of
\glspl{verificationcondition}\editorial{gurk}.  It is difficult to
completely avoid using the information from the dynamic analysis while
generating \glspl{verificationcondition}, as it is needed to generate
the \gls{interferingthread} \glspl{cfg}, but it is possible to disable
the parts of the {\StateMachine} simplifiers and the symbolic
execution engine which use it, and that is what I did in this test.
The results are shown in \autoref{table:eval:effect_of_dyn}.  Note
that the aliasing enabled column of this table was produced from the
same experiment as produced the \gls{w-isolation} enabled column in
\autoref{table:eval:w-isolation}.

The results are shown in \autoref{table:eval:effect_of_dyn}, using the
same error indicators as \autoref{table:eval:w-isolation}.  The main
conclusion to draw from this table is that discarding the information
from the dynamic analysis makes {\technique} perform worse at every
step: the time to derive C-atomic and the interfering \glspl{cfg}
increases, the number of \glspl{cfg} increases, the time to process
each \gls{cfg} increases, and the number of
\glspl{verificationcondition} requiring run-timing checking increases.
These results strongly suggest that the dynamic analysis is absolutely
necessary for {\technique} to be useful at all.

\begin{table}
  \begin{tabular}{|l|l|l|}
    \hline
    & \multicolumn{2}{c|}{Dynamic aliasing information} \\
    \hline
                                                                & Enabled & Disabled \\
    \hline
    Potentially-crashing instructions examined                  & 1000    & 1000 \\
    Instructions dismissed without generating a {\StateMachine} & $20 \pm_b 1$\% & $20 \pm_b 1$\% \\

    Time to build crashing {\StateMachine} *                    & $0.14 \pm_\mu 0.02$s & $0.31 \pm_\mu 0.03$s \\
    Timeouts building crashing {\StateMachines}                 & $0.1 \pm_b 0.1$\% & $0.1 \pm_b 0.1$\% \\
    Out of memory building crashing {\StateMachines}            & 0                & 0 \\

    Time to build C-atomic                                      & $0.004 \pm_\mu 0.001s$ & $0.3 \pm_\mu 0.1s$ \\
    Timeouts building C-atomic                                  & 0                   & $0.4 \pm_b 0.2$\% \\
    Out of memory building C-atomic                             & 0                   & $0.1 \pm_b 0.1$\% \\

    \Glspl{crashingthread} generating no interfering \glspl{cfg}& $48 \pm_b 1$\% & $41 \pm_b 1$\%\\

    Time to build interfering \glspl{cfg}                       & $0.4 \pm_\mu 0.1$s & $1.6 \pm_\mu 0.3$s \\
    Interfering \glspl{cfg} per crashing \gls{cfg}              & $6.9 \pm_\mu 0.4$ & $7.5 \pm_\mu 0.4$ \\

    Time to process each interfering \gls{cfg}                  & $0.24 \pm_\mu 0.04$s & $3.5 \pm_\mu 0.2$ \\
    Interfering \glspl{cfg} running out of memory               & $0.09 \pm_b 0.06$\% & $4.8 \pm_b 0.4$\% \\
    Interfering \glspl{cfg} running out of time                 & $0.2 \pm_b 0.1$\% & $32.4 \pm_b 0.9$\% \\

    Interfering \glspl{cfg} generating \glspl{verificationcondition}   & $12.7 \pm_b 0.7$\% & $73 \pm_b 1$\% \\
    \Glspl{verificationcondition} per checked instruction & $0.29 \pm_{10000} 0.04$ & $1.3 \pm_{10000} 0.1$\\
    \hline
  \end{tabular}
  \caption{Effects of dynamic aliasing information on the bug-finding
    analysis. *: excluding timeouts and instructions dismissed without
    building a {\StateMachine}.}
  \label{table:eval:effect_of_dyn}
\end{table}

\subsection{Convergence times}

One obvious question with a dynamic analysis is how long it takes to
achieve reasonable coverage.  This experiment aims to investigate
that.  For this experiment, I modified the dynamic analysis tool so
that it periodically recorded a snapshot of the aliasing table, and I
then examined these snapshots to quantify the rate at which aliasing
pairs were added to the table.  The results for mysql, Thunderbird,
and pbzip2 are shown in figures~\ref{fig:eval:dyn_convergence:mysqld},
\ref{fig:eval:dyn_convergence:thunderbird}, and
\ref{fig:eval:dyn_convergence:pbzip2}, respectively.  These graphs
show the number of aliasing pairs in the table at time $t$, as a
percentage of the number of pairs in the final table.  These tests
involved starting and restarting the program several times; such
restarts are shown as dashed vertical lines.  The most important
result here is that the dynamic aliasing analysis achieves reasonable
coverage reasonably quickly, usually within ten minutes to quarter of
an hour.  Asking users to run this dynamic analysis before using
{\technique} is therefore not an unreasonable burden.

For all tests, the dynamic analysis tool was configured to take one
snapshot every 1,000,000 Valgrind basic blocks, which usually
corresponds to a couple of hundred milliseconds of execution
time. This is arguably a somewhat unfair test: taking a snapshot can
itself take several hundred milliseconds, depending on the size of
aliasing table is at the time, and so this experiment actually spent
most of its time taking snapshots rather than running the test
program.  A more realistic test would probably show faster
convergence.

The precise behaviour of the dynamic analysis depends, in part, on the
workload which is being used to exercise the program.  For these
experiments, I used the following workloads:

\begin{itemize}
\item For MySQL, I used four tests out of the MySQL test suite. The
  first of these, rpl\_change\_master, was chosen to match the bug
  discussed in \autoref{sect:eval:mysql}.  The other three were chosen
  at random.
\item For Thunderbird, I tried to use the program in roughly the way
  an ordinary user would for a few minutes.  It is difficult to
  specify workloads precisely in a highly interactive GUI
  program\editorial{Excuses...}.
\item For pbzip2, I compressed three randomly generated 10MiB files in
  series.
\end{itemize}

\begin{figure}
  \subfigure[][rpl\_change\_master]{
    \input{eval/dyn_convergence/rpl_change_master.tex}
  }
  \subfigure[][innodb\_multi\_update]{
    \input{eval/dyn_convergence/innodb_multi_update.tex}
  }
  \subfigure[][binlog\_stm\_drop\_tbl]{
    \input{eval/dyn_convergence/binlog_stm_drop_tbl.tex}
  }
  \subfigure[][timestamp\_basic]{
    \input{eval/dyn_convergence/timestamp_basic.tex}
  }
  \caption{Dynamic aliasing coverage against time for MySQL, using
    some tests out of the test suite.}
  \label{fig:eval:dyn_convergence:mysqld}
\end{figure}

\begin{figure}
  \input{eval/dyn_convergence/thunderbird.tex}
  \caption{Dynamic aliasing coverage against time for Thunderbird.}
  \label{fig:eval:dyn_convergence:thunderbird}
\end{figure}

\begin{figure}
  \input{eval/dyn_convergence/pbzip2.tex}
  \caption{Dynamic aliasing coverage against time for pbzip2.}
  \label{fig:eval:dyn_convergence:pbzip2}
\end{figure}

\subsection{Dynamic analysis performance}

I also briefly investigated the performance cost of the analysis.  The
time taken by pbzip2 to compress a 100MiB file without the dynamic
analysis was $7.8 \pm_\mu 0.1$ seconds on this machine, mean and standard
deviation of mean for ten runs with independent randomly-generated
files; with the dynamic analysis, this increased to $274 \pm_\mu 2$
seconds, a factor of nearly thirty-five.  This is a rather large
overhead, and would be completely infeasible in a production
environment, but is probably tolerable for something which needs to
run for a few tens of minutes in a development one.

For comparison, a null Valgrind skin completed this test in $226.6
\pm_\mu 0.5$ seconds, an overhead of a factor of twenty-nine.  That
suggests that most of the overhead of the dynamic analysis tool comes
simply from the fact that it is implemented as a Valgrind skin, and
that re-implementing it in a faster analysis framework such as
PIN\cite{Luk2005} might provide a useful speed-up.

\todo{Do I need perf numbers for any other test programs?  It'd make
  it much more convincing, but they'd be a nuisance to produce.}
