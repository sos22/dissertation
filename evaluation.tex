%\newcommand{\biggraph}[1]{\input{#1}}
\newcommand{\biggraph}[1]{}
%\draftonly{\renewcommand{\biggraph}[1]{}}

\chapter{Evaluation}
\label{chapter:eval}

Experiments still to run:

\begin{itemize}
\item Run the mysql enforcers on ise, count the reproes.
\item Measure the effectiveness of the complex\_hb enforcers.
\item Need to re-do the no-simplifiers experiments.
\item Need to re-run the I isolation experiments.
\item Need to do the fix overhead experiments.
\item Finish re-doing the alpha experiments.
\item Need to re-do the no \gls{ic-atomic} tests.
\end{itemize}

\begin{sanefig}
  \begin{tabular}{lp{12.7cm}}
    $x \pm^n_p y$ & The mean, $x$, and sample standard deviation, $y$, of $n$ samples of some distribution with appropriately low skew and kurtosis.\\
    & \\
    $x \pm^n_\mu y$ & The mean, $x$, of some distribution and the standard deviation of the mean, $y$, calculated by applying the central limit theorem to $n$ samples of the distribution.\editorial{I'm at least half convinced that \emph{none} of the distributions here are well-behaved enough for the CLT to apply, at least not with the sample sizes I have, but it's pretty much a convention to just assume that it always does, so meh.}\\
    & \\
    $x \pm_\mu y$ & The mean, $x$, of some distribution and the standard deviation of the mean, $y$, calculated from distributions derived elsewhere rather than directly from sampled data.\\
    & \\
    $[x,y]^n_b$ & $[x,y]$ form a 90\% confidence interval for some statistic of a distribution, derived by a bootstrap with $b$ replicates over $n$ samples of the distribution. \\
    & \\
    $z \in [x,y]^n_\infty$ & $z$ is a maximum likelihood estimator of some statistic of a distribution and $[x,y]$ form a 90\% confidence interval for it, calculated by performing a bootstrap over $n$ samples of the distribution and taking the limit as the number of replicates goes to infinity.\\
    & \\
    $0/n$ & A Bernoulli process was sampled $n$ times and did not succeed. \\
    &\\
    $n/n$ & A Bernoulli process was sampled $n$ times and succeeded every time. \\
  \end{tabular}
  \caption{Summary of statistical notation used in this chapter.}
\end{sanefig}

Previous chapters have described {\technique} and how it can, under
appropriate circumstances, automate every stage of the debugging
process, from discovering bugs through characterising and reproducing
them to ultimately generating fixes.  This chapter provides an
experimental evaluation of the effectiveness and performance of my
implementation, {\implementation}.  This evaluation will consist of
four parts:
\begin{itemize}
\item \autoref{sect:eval:does_it_work} investigates whether
  {\technique} works at all, looking at the time taken to produce
  \glspl{verificationcondition}, \glspl{bugenforcer}, and fixes; the
  effects of \glspl{bugenforcer} and fixes on the frequency with which
  bugs reproduce; and the effects of the fixes on the program's
  behaviour.
\item \autoref{sect:eval:how_does_it_work} then explores some aspects
  of {\technique}'s performance in more detail, showing how the
  analysis time breaks down across the various phases.
\item Next, \autoref{sect:eval:why_does_it_work} examines the reasons
  for the properties observed in the previous sections and relates
  them back to the design features of {\technique}.
\item Finally, \autoref{sect:eval:does_it_scale} investigates
  {\technique}'s scalability with respect to bug complexity, using a
  variety of metrics, so as to determine when it is likely to fail.
\end{itemize}
Unless otherwise stated, all experiments were run on a four-core Intel
Q6600 2.40GHz with 8GiB of memory running Ubuntu Lucid Lynx.  The
\gls{w-isolation} assumption was enabled, \gls{alpha} was set to 20,
both analysis timeouts were set to five minutes, and the timeout used
by \glspl{bugenforcer} was randomly uniformly distributed between 100
and 200ms.  The system compiler, used to build the \glspl{bugenforcer}
and fixes, as well as {\implementation} itself, was gcc version 4.4.3.

\section{Does it work?}
\label{sect:eval:does_it_work}

This part of the evaluation aims to determine whether {\technique}
works at all.  In other words, is it able to reproduce and fix
concurrency bugs?  I investigate this using both small artificial
bugs, in \autoref{sect:eval:artificial_bugs}, and some real bugs taken
from large existing pieces of software, in
\autoref{sect:eval:does:real}.

\subsection{Artificial bugs}
\label{sect:eval:artificial_bugs}

\begin{sanefig}
  \begin{tabular}{p{8cm}p{6.5cm}}
    \multicolumn{2}{c}{\texttt{\#define NR\_PTRS 100}}\\
    \subfigure[][\RaggedRight {\rm \bugname{toctou}\!} crashing thread]{
      \begin{minipage}{7.2cm}
        \begin{literalC}
          while (1) \clbrace
            idx = random() \% NR\_PTRS;\\
            analysis\_window() \clbrace
              if (global\_ptrs[idx] != NULL) \clbrace
                *(global\_ptrs[idx]) = 5;
              \crbrace
            \crbrace
          \crbrace
        \end{literalC}
      \end{minipage}
      \label{fig:eval:artificial_bugs:programs:toctou:crashing}
    }
    &
    \subfigure[][{\rm \bugname{toctou}\!} interfering thread]{
      \begin{minipage}{6.2cm}
        \begin{literalC}
          while (1) \clbrace
            idx = random() \% NR\_PTRS; \\
            analysis\_window \clbrace
              global\_ptrs[idx] = NULL;
            \crbrace \\
            global\_ptrs[idx] = \&t;
          \crbrace
          \\
        \end{literalC}
      \end{minipage}
      \label{fig:eval:artificial_bugs:programs:toctou:interfering}
    } \\
    \subfigure[][{\rm \bugname{multi\_variable}\!} crashing thread]{
      \begin{minipage}{7.2cm}
        \begin{literalC}
          while (1) \clbrace
            analysis\_window \clbrace
              v1 = global1;\\
              v2 = global2;\\
              assert(v1 == v2);
            \crbrace \\
            sleep(500$\mu$s);
          \crbrace
          \\
        \end{literalC}
      \end{minipage}
      \label{fig:eval:artificial_bugs:programs:multi_variable:crashing}
    }
    &
    \subfigure[][\RaggedRight {\rm \bugname{multi\_variable}\!} interfering thread]{
      \begin{minipage}{6.2cm}
        \begin{literalC}
          while (1) \clbrace
            global1 = 5;\\
            global2 = 5;\\
            sleep(500$\mu$s);\\
            analysis\_window \clbrace
              global1 = 7;\\
              global2 = 7;
            \crbrace
          \crbrace
        \end{literalC}
      \end{minipage}
      \label{fig:eval:artificial_bugs:programs:multi_variable:interfering}
    } \\
    \subfigure[][{\rm \bugname{double\_free}\!} active threads]{
      \begin{minipage}{7.2cm}
        \begin{literalC}
          while (1) \clbrace
            analysis\_window \clbrace
              t = global\_ptr; \\
              if (t != NULL) \clbrace
                free(t);
              \crbrace \\
              global\_ptr = NULL;
            \crbrace \\
            sleep(1ms);
          \crbrace
        \end{literalC}
      \end{minipage}
      \label{fig:eval:artificial_bugs:programs:double_free:active}
    }
    &
    \subfigure[][{\rm \bugname{double\_free}\!} environmental thread]{
      \begin{minipage}{6.2cm}
        \begin{literalC}
          \\
          \\
          while (1) \clbrace
            if (global\_ptr == NULL) \clbrace
              global\_ptr = malloc(64);
            \crbrace
          \crbrace
          \\
          \\
          \\
        \end{literalC}
      \end{minipage}
      \label{fig:eval:artificial_bugs:programs:double_free:environmental}
    }
  \end{tabular}
  \caption{Artificial test programs.  {\tt analysis\_window}\hspace{-1pt} shows
    the extent of the \gls{analysiswindow}, which was specified
    manually for these tests.  The various delays were chosen so that
    the bug reproduced in a reasonable time when the program was run
    with neither an enforcer nor a fix applied.}
  \label{fig:eval:artificial_bugs:programs}
\end{sanefig}

\noindent
I first consider {\technique}'s behaviour when applied to bugs in
three artificial test programs, shown in
\autoref{fig:eval:artificial_bugs:programs}.  These programs
illustrate several important features of {\technique}:

\begin{itemize}
  \item The \bugname{toctou} test,
    figures~\ref{fig:eval:artificial_bugs:programs:toctou:crashing}
    and~\ref{fig:eval:artificial_bugs:programs:toctou:interfering},
    investigates {\technique}'s ability to reproduce data-dependent
    bad pointer dereferencing bugs, and in particular the importance
    of the side condition checking mechanism.
  \item The \bugname{multi\_variable} test,
    figures~\ref{fig:eval:artificial_bugs:programs:multi_variable:crashing}
    and~\ref{fig:eval:artificial_bugs:programs:multi_variable:interfering},
    shows a multi-variable atomicity violation.  Some previous
    approaches, such as Kivati\needCite{}, were not able to handle
    this kind of bug.
  \item The final test, \bugname{double\_free}, demonstrates
    {\technique}'s ability to handle double free-type bugs.  It
    consists of three threads: two ``active'' threads, shown in
    \autoref{fig:eval:artificial_bugs:programs:double_free:active},
    and one ``environmental'' one, shown in
    \autoref{fig:eval:artificial_bugs:programs:double_free:environmental}.
    It is possible for both active threads to release the same memory
    allocation, causing a double-free bug.  Note that the crashing and
    interfering {\StateMachines} will both represent the active thread
    and the environmental thread will not be represented by a
    {\StateMachine} at all.
\end{itemize}

\todo{Some kind of linkage text goes here.}

\subsubsection{Computational costs of building \glsentrytext{verificationcondition}s, \glsentrytext{bugenforcer}s, and fixes}

\noindent
\autoref{tab:eval:artificial_bugs:analysis_time} shows the time taken
to analyse these bugs and build either \glspl{bugenforcer} or fixes
for them.  As might be expected, these very simple bugs are processed
very quickly, at every stage of the process.  These figures provide a
reasonable lower bound on the time which {\technique} might take to
analyse a bug; realistic ones would, of course, take much longer.

\begin{sanetab}
  \begin{tabbular}{|p{2.4cm}|p{2.83cm}|p{2.83cm}|p{2.83cm}|p{2.83cm}|}
    \hline
    Test program              & \Gls{programmodel}      & \Gls{verificationcondition} & \Gls{bugenforcer} & Fix \\
    \hline
    \bugname{toctou}          & $0.541 \pm^{10}_p 0.056$ & $0.527 \pm^{10}_p 0.008$ & $0.206 \pm^{10}_p 0.008$ & $0.142 \pm^{10}_p 0.004$\\
    \bugname{multi\_variable} & $0.662 \pm^{10}_p 0.067$ & $0.370 \pm^{10}_p 0.007$ & $0.207 \pm^{10}_p 0.006$ & $0.135 \pm^{10}_p 0.004$\\
    \bugname{double\_free}    & $0.434 \pm^{10}_p 0.055$ & $0.342 \pm^{10}_p 0.009$ & $0.199 \pm^{10}_p 0.009$ & $0.135 \pm^{10}_p 0.003$\\
    \hline
  \end{tabbular}
  \caption{Time taken, in seconds, to build the \gls{programmodel},
    \gls{verificationcondition}, \gls{bugenforcer}, and fix for the
    artificial bugs.  Each configuration was run eleven times and the
    results of the first run discarded. \todo{Should be after the
      figure which defines the tests.}}
  \label{tab:eval:artificial_bugs:analysis_time}
\end{sanetab}

\subsubsection{Reproducing the bugs}

\begin{sanefig}
  \biggraph{eval/artificial_bugs/cdf1.tex}
  \caption{CDF of time taken to reproduce the bugs in the artificial
    test programs, with and without \glspl{bugenforcer}, and some
    summary statistics.  Note log scale.  All times in seconds.  Means
    calculated ignoring timeouts.  Grey region gives a 90\% confidence
    interval, computed using the Dvoretsky-Kiefer-Wolfowitz-Massart
    (DKWM) inequality\cite{Massart1990}.  Note that DKWM confidence
    intervals are curve-wise rather than point-wise i.e. there is a
    90\% confidence that the entire curve is within the shaded region,
    rather than that any given point is. \todo{Need more abscissae.
      Experimental protocol?}}
  \label{fig:eval:artificial_bugs:cdf1}
\end{sanefig}

\noindent
\autoref{fig:eval:artificial_bugs:cdf1} shows how effective the
\glspl{bugenforcer} generated from these bugs are, giving cumulative
distribution functions (CDFs) of the reproduction times and some
summary statistics.  This figure shows that the enforcers are able to
reduce the mean time taken to reproduce these bugs, often by a large
amount.  They are particularly effective at eliminating large outliers
in which the bug takes a very long time to reproduce.  This is a
useful property: with fewer outliers, the behaviour of a bug will be,
qualitatively, more predictable, which is likely to make it easier for
a programmer to understand the bug, even without a reduction in
reproduction time.

\subsubsection{Fixing the bugs}

\todo{I'm undecided as to how much this actually adds.}

\begin{sanetab}
  \begin{tabbular}{|p{5cm}|p{3cm}|p{3cm}|p{3cm}|}
    \hline
                           & \multicolumn{3}{c|}{Time to run main loop, microseconds} \\
    \cline{2-4}
    Test program           & Unfixed & Fixed & Increase \\
    \hline
    \bugname{toctou}       & & &\\
    \hspace{1em}Crashing thread         & $0.62 \pm_{\mu}^{10} 0.03$   & $0.69 \pm_{\mu}^{10} 0.07$ & $0.07 \pm_\mu 0.08$ \\
    \hspace{1em}Interfering thread      & $0.64 \pm_{\mu}^{10} 0.06$   & $0.84 \pm_{\mu}^{10} 0.06$ & $0.20 \pm_\mu 0.08$ \\
    \hline
    \bugname{multi\_variable} & & &\\
    \hspace{1em}Crashing thread         & $561 \pm_{\mu}^{10} 5$       & $556.7 \pm_\mu^{10} 0.3$ & $-4 \pm_\mu 5$\\
    \hspace{1em}Interfering thread      & $561 \pm_{\mu}^{10} 5$       & $556.8 \pm_\mu^{10} 0.3$ & $-4 \pm_\mu 5$\\
    \hline
    \bugname{double\_free}    & & &\\
    \hspace{1em}Active threads          & $1086 \pm_{\mu}^{10} 1$      & $1063.8 \pm_\mu^{10} 0.6$ & $-22 \pm_\mu 1$\\
    \hspace{1em}Environmental thread    & $0.0878 \pm_{\mu}^{10} 0.0008$ & $0.0861 \pm_\mu^{10} 0.0007$ & $0.002 \pm_\mu 0.001$\\
    \hline
  \end{tabbular}
  \caption{Time taken to run a single iteration of the main loop of
    each test, with and without a fix applied.  This experiment was
    structured as eleven batches, with each configuration tested once
    in each batch in random order and the results of the first batch
    discarded.  A configuration was tested by running it for ten
    seconds, restarting whenever the test program crashed, and
    counting the number of times the loop ran during that time.  I
    then calculated the time per iteration as
    $\frac{10\mathrm{s}}{n}$, where $n$ is the number of iterations,
    and present summary statistics for that distribution.}
  \label{tab:eval:artificial_bugs:fixes}
\end{sanetab}

\noindent
{\Technique} is able to generate fixes for all three of these
artificial bugs, and these fixes correctly eliminate the bugs and
prevent the programs from crashing.
\autoref{tab:eval:artificial_bugs:fixes} shows the effect these fixes
have on the program's performance, expressed in terms of time taken to
complete the two loops.  These results are difficult to interpret, as
all of the loops contain calls to either \texttt{sleep}, which leads
to obvious distortions in the data, or complex library functions such
as \texttt{random} or \texttt{malloc}, which interact with the
additional synchronisation in unintuitive ways.  Nevertheless, they
suggest that the cost of a {\technique} critical section is small even
on the scale of microseconds, and hence that {\technique} will be able
to fix bugs in code which runs millions of times per second without
the program as a whole suffering crippling performance degradation.  I
cover this theme in more detail in
\autoref{sect:eval:why:fix_overhead}.

\subsection{Bugs in real programs}
\label{sect:eval:does:real}

The previous section showed that {\technique} can be used to reproduce
and fix bugs in some simple artificial test programs.  This section
repeats the experiments using two bugs taken from real programs:
\begin{itemize}
\item The first, \bugname{thunderbird}, is Mozilla bug number
  391259\cite{Mery2007}, a time-of-check, time-of-use race in the IMAP
  client component of Thunderbird, a popular open-source e-mail
  client.  The relevant parts of the program are shown in
  figures~\ref{fig:eval:real_bugs:programs:thunderbird:crashing}
  and~\ref{fig:eval:real_bugs:programs:thunderbird:interfering}.  If
  \verb|m_transport| is set to \verb|NULL| by the
  \gls{interferingthread} in between the two accesses in the crashing
  one then the program will crash.
\item The second, \bugname{mysql}, is MySQL bug number
  56324\needCite{}.  A simplified version of the buggy code is shown
  in figures~\ref{fig:eval:real_bugs:programs:mysql:crashing}
  and~\ref{fig:eval:real_bugs:programs:mysql:interfering}.  The
  program will crash if the interfering thread sets
  \texttt{PSI\_server} to \texttt{NULL} in between the two accesses to
  that variable in the crashing thread.
\end{itemize}
{\Technique} is able to reproduce and fix both of these bugs.  For
these tests, unless otherwise noted, I assumed that the crashing
instruction had already been identified, rather than attempting to
discover it automatically using {\technique}.

\begin{sanefig}
  \begin{tabular}{p{8.9cm}p{5.8cm}}
    \subfigure[][{\rm \bugname{thunderbird\hspace{-1pt}}} crashing thread]{
      \begin{minipage}{8cm}
        \begin{literalC}
          if (this->m\_transport) \clbrace
            this->m\_transport->SetTimeout();
          \crbrace
        \end{literalC}
      \end{minipage}
      \label{fig:eval:real_bugs:programs:thunderbird:crashing}
    }
    &
    \subfigure[][{\rm \bugname{thunderbird\hspace{-1pt}}} interfering thread]{
      \begin{minipage}{5.5cm}
        \begin{literalC}
          \\
          this->m\_transport = NULL;\\
        \end{literalC}
      \end{minipage}
      \label{fig:eval:real_bugs:programs:thunderbird:interfering}
    }\\
    \subfigure[][{\rm \bugname{mysql\hspace{-1pt}}} crashing thread]{
      \begin{minipage}{8cm}
        \begin{literalC}
          if (PSI\_server) \clbrace
          PSI\_server->delete\_current\_thread();
          \crbrace
        \end{literalC}
      \end{minipage}
      \label{fig:eval:real_bugs:programs:mysql:crashing}
    }
    &
    \subfigure[][{\rm \bugname{mysql\hspace{-1pt}}} interfering thread]{
      \begin{minipage}{5.5cm}
        \begin{literalC}
          \\
          PSI\_server = NULL;\\
        \end{literalC}
      \end{minipage}
      \label{fig:eval:real_bugs:programs:mysql:interfering}
    }
  \end{tabular}
  \caption{Test bugs in real programs.}
  \label{fig:eval:real_bugs:programs}
\end{sanefig}

\subsubsection{Characterising the bugs}

\autoref{tab:eval:real_bugs:analysis_time} shows how long it takes to
build the \gls{programmodel} and \glspl{verificationcondition} for
these bugs.  Note that the time to build the \gls{programmodel} is
dramatically larger for real programs than it was for the artificial
ones considered earlier, but the time to generate the
\glspl{verificationcondition} is largely unchanged.  This is because
      {\technique} must examine the entire program in order to build
      the \gls{programmodel}, but only needs to examine the
      \gls{analysiswindow} to build the \glspl{verificationcondition}.
      The high cost of building the model is somewhat mitigated by the
      fact that it is built for the program rather than for any
      particular bug, and so the cost could be amortised if there were
      several bugs in the same program.

\begin{sanetab}
  \begin{tabbular}{|p{2.72cm}|p{5.8cm}|p{5.8cm}|}
    \hline
    Test bug                  & \Gls{programmodel} & \Glspl{verificationcondition} \\
    \hline
    \bugname{thunderbird}     & $1240 \pm^{10}_p 10$s              & $0.39 \pm^{10}_p 0.01$s \\
    \bugname{mysql}           & $1088 \pm^{10}_p 7$s               & $1.13 \pm^{10}_p 0.02$s\\
    \hline
  \end{tabbular}
  \caption{Time taken to build the \gls{programmodel} and
    \glspl{verificationcondition} for the bugs taken from real
    programs.  \todo{Experimental protocol?}}
  \label{tab:eval:real_bugs:analysis_time}
\end{sanetab}

{\Technique} generated one false positive \gls{verificationcondition}
for each bug, in addition to the desired true positive, both of which
were caused by incompleteness in the {\technique}'s model of the
program's behaviour.  In the case of \bugname{thunderbird}, the
problem was the lack of knowledge of the program's existing
synchronisation structure: {\technique} located another place in the
program which set \texttt{m\_transport} to \texttt{NULL}, and
correctly identified that interleaving that store with the
\gls{crashingthread} might lead to a crash, but failed to notice that
an existing lock prevented the interleaving from happening.  The
\bugname{mysql} false positive, by contrast, was caused by an
incomplete model of the program's data structure\editorial{Looks like
  a typo; isn't; is actually an important distinction.  Think harder
  about how to make that clear.}.  {\Technique} located another
instruction which could set \texttt{PSI\_server} and which could
potentially race with the \gls{crashingthread}, but in that case the
value stored was always a valid pointer and so the interleaving could
not lead to a crash.

\subsubsection{Reproducing the bugs}

\begin{sanetab}
  \begin{tabbular}{|p{2.72cm}|l|l|l|}
    \hline
    Bug                   & True positive enforcer & False positive enforcer & Combined enforcer \\
    \hline
    \bugname{thunderbird} & $0.640 \pm^{10}_p 0.010$s     & $0.640 \pm^{10}_p 0.010$s     & $0.710 \pm^{10}_p 0.020$s\\
    \bugname{mysql}       & $0.566 \pm^{10}_p 0.011$s     & $0.553 \pm^{10}_p 0.010$s     & $0.678 \pm^{10}_p 0.010$s\\
    \hline    
  \end{tabbular}
  \caption{Time taken building the various \glspl{bugenforcer}}
  \label{tab:eval:real_bugs:build_enforcer_times}
\end{sanetab}

\begin{sanetab}
  \begin{tabbular}{|l|p{2.46cm}|p{2.46cm}|p{2.46cm}|p{2.46cm}|}
    \hline
                              & \multicolumn{4}{c|}{Enforcer} \\
    \cline{2-5}
    Bug                       & None & True positive & False positive & Combined \\
    \hline
    \bugname{thunderbird}     & 0/10 &    &   &    \\
    \hspace{1em}100--200ms timeout &   & 0/10  & 0/10 & 0/10  \\
    \hspace{1em}5s timeout    &   & 10/10 & 0/10 & 10/10 \\
    \bugname{mysql}           &   &    &   &    \\
    \hline
  \end{tabbular}
  \caption{Reproduction counts for the different bugs and
    configurations.  \todo{Need more data.}}
  \label{tab:eval:real_bugs:repro_effectiveness}
\end{sanetab}

\noindent
All four \glspl{verificationcondition} can be converted to
\glspl{bugenforcer}.
\autoref{tab:eval:real_bugs:build_enforcer_times} shows how long it
took to do so; as can be seen, this step was very fast.

I then attempted to evaluate the efficacy of the generated enforcers.
The results are shown in
\autoref{tab:eval:real_bugs:repro_effectiveness}.  For the
\bugname{mysql} bug, I used the \texttt{rpl\_change\_master} test out
of the MySQL test suite, as it runs quickly without manual
intervention and exercises the buggy code, and ran it one hundred
times in each configuration.  As can be seen, the true positive and
combined enforcers were effective at reproducing this bug, and it did
not reproduce at all without an enforcer or with the false positive
enforcer.

For the \bugname{thunderbird} bug, no convenient automated test was
available, and so I exercised the buggy code via manual interaction
with the Thunderbird GUI by clicking on an IMAP folder and then
quickly clicking the close button.  I repeated this ten times in each
configuration, restarting Thunderbird between each attempt.  The IMAP
folder was the only folder in an account on a local Dovecot 1.2.9 IMAP
server which had no other users, and no other IMAP accounts were
configured in Thunderbird.  The IMAP folder itself was empty.  None of
the {\technique} \glspl{bugenforcer} were able to reproduce this bug
using their default 100--200ms timeout, but increasing the timeout to
five seconds caused the bug to reproduce easily.  The program was
still quite usable even with this long delay, as the bug is in code
which executes infrequently, and only in a background thread.

This illustrates an important weakness of the {\technique} approach:
the timeout must be tailored to the application being tested, and in
some cases the bug as well.  Too small a timeout will prevent the
threads from properly rendezvousing, preventing the bug from
reproducing at all, while too long a timeout will cause a large probe
effect, also reducing the likelihood of reproduction.  Very roughly,
the timeout must be of the same general scale as the process which
triggers the bug.  In this case, that process involves user
interaction, and so the timeout must be broadly the same scale as
those interactions; five seconds is on that scale, whereas 200ms is
not.

\subsubsection{Fixing the bugs}

\begin{sanetab}
  \begin{tabbular}{|p{3.4cm}|p{11.25cm}|}
    \hline
    Bug                  & Time building fix \\
    \hline
    \bugname{mysql}      & $287 \pm_p^{10} 6$ms \\
    \bugname{thunderbird} & $373 \pm_p^{10} 6$ms \\
    \hline
  \end{tabbular}
  \caption{Time taken to convert \glspl{verificationcondition} to fixes.}
  \label{tab:eval:real_bugs:time_building_fixes}
\end{sanetab}

\noindent
These \glspl{verificationcondition} can also be converted to fixes;
the time taken to do so is shown in
\autoref{tab:eval:real_bugs:time_building_fixes}.  The fixes generated
are similar for both bugs: one critical section covering the two loads
in the \gls{crashingthread}, and another covering the store in the
\gls{interferingthread}\editorial{Diagram, maybe?}.  It is hard to
validate experimentally that these fixes are correct, as the bugs
reproduce quite rarely even without the fix\footnote{Due to
  implementation limitations, it is not possible to load an
  {\technique} fix and an {\technique} \gls{bugenforcer} into the same
  program.}\!\!, but manual inspection suggested that they had correctly
eliminated the dangerous interleaving.  It is also hard to
experimentally evaluate the performance impact of these fixes, as, in
the case of \bugname{thunderbird}, there is no convenient metric of
performance, beyond noting that performance was qualitatively
unaffected, and, in the case of \bugname{mysql}, the fix is to code
which executes sufficiently rarely that the system-level overhead was
immeasurably small.

These are not the fixes which a human programmer would make.  In
particular, the call to \texttt{delete\_current\_thread}, in
\bugname{mysql}, and \texttt{SetTimeout}, in \bugname{thunderbird},
are not protected in any way.  This means that the
\gls{crashingthread} might continue to execute a method in
\texttt{m\_transport} or \texttt{PSI\_server} after those variables
have been cleared.  In this case, the implementations of those methods
means that this is safe, but doing so does not concord with common
programming practice, and changes in the implementation could render
it unsafe without being visible to {\technique}.  This might cause
{\technique} to generate a fix which is qualitatively incomplete,
despite correctly eliminating all of the identified dangerous
interleavings.

The problem here lies in {\technique}'s definition of a ``bug'': an
atomicity violation which might lead to a crash on a particular
instruction.  Once {\technique} has ensured that that specific
instruction cannot crash, it considers the bug to be fixed; if the
program then crashes five instructions later, {\technique} defines
that to be a different bug, requiring a different fix.  This will not
necessarily agree closely with a programmer's or user's idea of what
it means to fix a bug.

\subsubsection{Finding unknown bugs}
\label{sect:how:finding_unknown}

\todo{Change in test machine here, but that's not as apparent as it
  should be from the text.}

\begin{sanetab}
  \begin{tabbular}{|l|p{4.35cm}|p{4.35cm}|}
    \hline
    Phase & Time taken & Cores used \\
    \hline
    Building \gls{programmodel} & $571 \pm_{\mu}^{10} 1$s & 1\\
    Deriving \glspl{verificationcondition} & $8500 \pm_{\mu}^{10} 100$s & 10 \\
    Converting to \glspl{bugenforcer} & $204 \pm_{\mu}^{10} 3$s & 10 \\
    \hdashline
    Total & $9300 \pm_{\mu} 100$s & NA \\
    \hline
  \end{tabbular}
  \caption{Time taken to generate a full suite of \glspl{bugenforcer}
    for MySQL on an AMD Opteron 6168 with 16GiB of memory.  The
    complete analysis was run eleven times and the results of the
    first run discarded; the results here are the average of the
    remaining ten runs.  Operating system disk caches were discarded
    in between each run.  The last two phases were parallelised; the
    first was not.}
  \label{tab:eval:does:building_all_enforcers_times}
\end{sanetab}

\noindent
MySQL has an extensive automated test suite, and so {\technique} can
be used to look for previously unknown bugs by generating every
possible \gls{bugenforcer} and running the test suite under each of
them.  {\Implementation} took two and a half hours to generate the
10173 \glspl{bugenforcer} for MySQL (see
\autoref{tab:eval:does:building_all_enforcers_times}).  To test the
effectiveness of these enforcers, I ran the
\texttt{rpl\_change\_master} test repeatedly with each\footnote{This
  experiment was structured as a series of eleven rounds, with each
  enforcer used once in each round.  Apart from stragglers at the end
  of each round, I tested ten enforcers in parallel.  The order of
  \glspl{bugenforcer}, and hence which \glspl{bugenforcer} ran in
  parallel with which other \glspl{bugenforcer}, was chosen randomly
  in each round.  The system used for this test was the same Opteron
  6168 as was used to generate the \glspl{bugenforcer}.  I discarded
  the results of the first round of testing; the results here are for
  the remaining ten rounds.  The results for the no-enforcer case were
  produced by running the test 10,000 times without an enforcer, again
  running ten instances of the test in parallel.}\!\!\!.  The results are
shown in \autoref{tab:eval:does:finding_unknown}.  Three of the
enforcers were able to reproduce the bugs for which they were
designed.  The first, hereafter referred to as enforcer A, reproduced
the \bugname{mysql} bug, as desired.  The other two, enforcers B and
C, reproduced essentially similar races elsewhere in MySQL.  I was
unaware of these bugs before running this experiment.  All three bugs
were very rare without an enforcer, or with the wrong enforcer, but
reproduced very easily when an appropriate enforcer was used.

It is perhaps surprising that the reproduction rates are lower with an
inappropriate enforcer than they are with no enforcer at all.  This
reflects the fact that randomly adding delays to a program's execution
is not an effective way of reproducing bugs: delaying a thread reduces
the program's effective concurrency, and unless the delays are
carefully positioned this will outweigh the benefits of encouraging it
to explore less-common schedules.

\begin{sanetab}
\begin{tabbular}{|l|l|l|l|}
\hline
         & \multicolumn{3}{c|}{Reproduction rate} \\\cline{2-4}
Enforcer\, & Bug A                         & Bug B                                    & Bug C \\
\hline
None     & 0/10000                       & $0.03 \in [0.02,0.06]^{10000}_{\infty}\%$     & $0.29 \in [0.21,0.38]^{10000}_{\infty}\%$\\
\hdashline
A        & $90 \in [80,100]^{10}_{\infty}\%$ & 0/10                                     & 0/10 \\
B        & 0/10                          & 10/10                                    & 0/10 \\
C        & 0/10                          & 0/10                                     & 10/10 \\
\hdashline
Other    & 0/101700                      & $0.014 \in [0.009,0.020]^{101700}_{\infty}\%$ & $0.10 \in [0.08,0.11]^{101700}_{\infty}\%$ \\
\hline
\end{tabbular}
\caption{Effectiveness of {\technique} at finding unknown bugs.}
\label{tab:eval:does:finding_unknown}
\end{sanetab}

\section{How does it work?}
\label{sect:eval:how_does_it_work}

The previous section demonstrated that {\technique} works, at a basic
level, for at least some real and artificial bugs.  This section aims
to expand upon this by giving more details of the way in which it
works, by breaking the time and memory usage down into the different
phases of the analysis and determining which phases represent the most
important bottlenecks.  For these experiments, I selected 10,000
memory-accessing instructions at random from MySQL and produced
\glspl{bugenforcer} and fixes for each in turn, recording the time
spent in each of the various steps of the analysis.

The analysis of a single potentially crashing instruction can be
roughly divided into four phases:
\begin{itemize}
\item Per-\gls{crashingthread} analysis work.  {\Technique} starts
  analysing a potentially-crashing instruction by constructing the
  crashing {\StateMachine}, and from that it builds the interfering
  \glspl{cfg}.  This work is done once for each potentially-crashing
  instruction.
\item Per-\gls{interferingthread} analysis work.  Each crashing
  {\StateMachine} will generate zero or more
  \glspl{interferingthread}, each of which is analysed independently.
\item Building the \gls{bugenforcer}.  Each \gls{interferingthread} in
  turn generates zero or one \glspl{verificationcondition}.  Each
  \gls{verificationcondition} is processed in isolation to produce a
  single \gls{bugenforcer}.
\item Building the fixes.  The \glspl{verificationcondition} can
  instead be converted into fixes.  Again, each
  \gls{verificationcondition} is processed in isolation to produce a
  single fix.
\end{itemize}
I consider each phase in turn\editorial{Orphan}.

\subsection{Per-\gls{crashingthread} analysis}

\begin{sanefig}
  \biggraph{eval/phase_breakdown/per_crashing.tex}
  \caption{Distributions of time taken by the per-crashing instruction
    steps of the analysis.  \todo{Explain the bulbs.  Make a gap
      between derive interfering \protect\glspl{cfg} skip box and
      kernel.}}
  \label{fig:eval:how:per_crashing_times}
\end{sanefig}

\noindent
The initial part of the analysis is performed once for each
potentially-crashing instruction.  For each instruction, {\technique}
derives the crashing \gls{cfg}
(\autoref{sect:derive:build_crashing_cfg}), decompiles it to a
{\StateMachine} (\autoref{sect:derive:compile_cfg}), simplifies it
(\autoref{sect:derive:simplify_sm}), builds the interfering
\glspl{cfg} (\autoref{sect:derive:write_side}), and then derives
C-atomic (\autoref{sect:derive:inferred_assumption} and
\autoref{sect:derive:w_isolation}).
\autoref{fig:eval:how:per_crashing_times} shows how much time is spent
in each of these steps.  This figure shows several useful pieces of
information:
\begin{itemize}
\item The main part of the figure shows the probability density
  function of the time spent in each step.  Note that this is shown on
  a log scale, and that the density is with respect to log time.  The
  kernel used in estimating the probability density function is shown
  below the PDF itself; this is the contribution which a single sample
  makes to the PDF\footnote{For these charts, I used a rectangular
    kernel of width $2.75Rn^{-0.2}$, where $R$ is the log
    inter-quartile range and $n$ the number of samples.  This
    bandwidth is reasonably robust to outliers and minimises the
    expected mean square error for log-Gaussian data.}\!\!.
\item The median of the distribution is shown as a horizontal line
  across the PDF.
\item The grey areas give 90\% confidence intervals for the PDF and
  median, computed using a bootstrap with 1000 replicates.
\item The (arithmetic) mean, plus or minus one standard deviation of
  the mean, is shown as a cross with bars.  This mean is calculated
  ignoring all failures.  \todo{Why is this pm an SD rather than a
    confidence interval?}
\item The ``Total'' PDF gives the distribution of the total time taken
  for each potentially-crashing instruction, measured from the start
  of processing to the end.  The ``Defect'' PDF, to its left, shows
  the difference between the total time taken and the sum of all of
  the measured steps.  As can be seen, it is small relative to the
  other quantities measured, which is necessary for the other
  measurements to be meaningful.
\item \todo{Blurg} The boxes above and below the PDFs give the number of
  potentially-crashing instructions which failed to complete that step
  and the number which did not start it at all, respectively, sized
  such that a constant area represents a constant probability across
  the figure.  Instructions skip a step if either a previous step
  failed or if a previous step was able to show that no bug is
  possible.  For instance, instructions which access a fixed location
  which is mapped by the program's ELF metadata are eliminated before
  building the crashing \gls{cfg} and those for which the
  \gls{programmodel} cannot find any racing instructions are
  eliminated after deriving the interfering \glspl{cfg}.  As can be
  seen, the majority of steps are skipped for the majority of
  instructions, which is helpful when speculatively analysing a large
  number of instructions.
  \autoref{tab:eval:how:failures_per_crashing} gives more details of
  the failures encountered during this phase.
  \begin{sanetab}
    \begin{tabbular}{|p{5.3cm}|p{2.9cm}|p{2.9cm}|p{2.9cm}|}
      \hline
      Step & Instructions starting step& Out of time & Out of memory \\
      \hline
      \RaggedRight Compile crashing {\StateMachine} & 8031 & 1 & 0 \\
      \RaggedRight Simplifying crashing {\StateMachine} & 8030 & 2 & 0 \\
      Derive C-atomic & 3678 & 1 & 4 \\
      \hline
    \end{tabbular}
    \caption{Causes of failures during per-crashing instruction
      processing.  Note that the timeout runs from the start of the
      per-crashing instruction phase, rather than being restarted for
      each step. \todo{Error bars?}}
    \label{tab:eval:how:failures_per_crashing}
  \end{sanetab}
\end{itemize}
The most important observation to draw from this figure, aside from
the gross summary of how long each step takes, is that most of the
distributions are dominated by their tails, in the sense that the mean
is often more than an order of magnitude greater than the median.
This reflects the fact that many of the algorithms have worst-case
running time far worse than their common case.  Instructions which
happen to hit one of the slow cases take a very long time to complete,
while those which avoid the slow cases complete very quickly.  The
time spent deriving C-atomic, for instance, has a median of a little
over 1ms and a mean of 120ms, while the total time taken has a median
of 30ms and a mean of 1500ms\editorial{No it doesn't!}.

\subsection{Per-\gls{interferingthread} analysis}
\label{sect:eval:how:per_interfering}

The per-crashing instruction analysis generates a large number of
pairs of crashing {\StateMachine} and interfering \gls{cfg} which are
analysed in the per-\gls{interferingthread} analysis phase.  The time
taken to do so is illustrated in
\autoref{fig:eval:how:per_interfering}, in the same style as
\autoref{fig:eval:how:per_crashing_times}.  Failures encountered
during this phase are detailed in
\autoref{tab:eval:how:failures_per_interfering}.

\begin{sanefig}
  \biggraph{eval/phase_breakdown/per_interfering}
  \caption{Time taken by per-\gls{interferingthread} analysis steps,
    as distributions over the 27535 interfering \glspl{cfg} generated
    by the per-crashing instruction phase.  In this figure, the
    {\StateMachine}-building steps include {\StateMachine}
    simplification.  The second step, ``Rederive crashing
    {\StateMachine}'', performs various additional simplifications to
    the crashing {\StateMachine} which become possible once
    {\technique} has identified the interfering {\StateMachine}.}
  \label{fig:eval:how:per_interfering}
\end{sanefig}

\begin{sanetab}
  \begin{tabbular}{|p{5.3cm}|p{2.9cm}|p{2.9cm}|p{2.9cm}|}
    \hline
    Step & Instructions starting step& Out of time & Out of memory \\
    \hline
    Build interfering {\StateMachine} & 27535 & 21 & 1 \\
    Rederive crashing {\StateMachine} & 23513 &  24 &  1 \\
    Run \gls{ic-atomic} {\StateMachine} & 13055 & 47 & 35 \\
    Build cross-product {\StateMachine} & 12935 & 1 & 0 \\
    Run cross-product {\StateMachine} & 12934 & 47 & 0 \\
    \hdashline
    Total & & 140 & 37\\
    \hline
  \end{tabbular}
  \caption{Causes of failures during per-\gls{interferingthread}
    processing.  Note that the timeout runs from the start of the
    per-\gls{interferingthread} phase, rather than being restarted for
    each step.}
  \label{tab:eval:how:failures_per_interfering}
\end{sanetab}
As with \autoref{fig:eval:how:per_crashing_times}, this figure shows
that most of the distributions involved are dominated by their tails,
indicating that most of the analysis time is spent processing a small
minority of unusually difficult \glspl{cfg}.  The two symbolic
execution phases are particularly prone to these outliers, because
they must consider every path through the relevant {\StateMachine} and
the number of such paths rises exponentially with the size of the
{\StateMachine}.  The ``Derive C-atomic'' per-\gls{crashingthread}
analysis step likewise shows a particularly long tail in which it is
particularly expensive.

More surprisingly, symbolically executing the \gls{ic-atomic}
{\StateMachine} is more expensive than executing the cross-product
{\StateMachine}, despite having to consider far fewer instruction
orderings.  This is because it has less information about the
configurations in which the {\StateMachines} might start: the
cross-product execution assumes that the \gls{ic-atomic} constraint
holds when the {\StateMachines} start, often eliminating a large
number of possible initial states, whereas the \gls{ic-atomic}
execution can only assume the weaker C-atomic constraint.

\begin{sanefig}
  \biggraph{eval/phase_breakdown/per_interfering_no_w_atomic}
  \caption{Time taken by per-\gls{interferingthread} analysis steps,
    with the \gls{ic-atomic} steps disabled.}
  \label{fig:eval:how:per_interfering:no_ic_atomic}
\end{sanefig}

\todo{REDO WITH NEW NUMBERS} This is further illustrated in
\autoref{fig:eval:how:per_interfering:no_ic_atomic}, which shows how
long the various steps take when {\technique} assumes \gls{ic-atomic}
is the constant \true, rather than attempting to derive it.  This
eliminates the steps involved in deriving \gls{ic-atomic}, which take
300ms, but increases the cost of the later steps so that the total
reduction in time taken is only 17ms\editorial{Check numbers}.
\editorial{odd linkage} It also increased the number of
\glspl{verificationcondition} generated, from 3501 to 5998.  All of
the extra conditions are false positives which would have needed to be
investigated by run-time enforcers, easily outweighing the reduced
cost of this phase of the analysis.

\subsection{Costs of building \gls{bugenforcer}}

\todo{REDO THIS SECTION WITH NEW DATA}

\begin{sanefig}
  \biggraph{eval/phase_breakdown/build_enforcer}
  \caption{Time taken to convert the 3501
    \glspl{verificationcondition} generated by the experiments in
    \autoref{sect:eval:how:per_interfering} into \glspl{bugenforcer}.
    \autoref{fig:eval:how:build_enforcer_failures} gives more details
    of the failures encountered. \todo{REDO} }
  \label{fig:eval:how:build_enforcer}
\end{sanefig}

\begin{sanetab}
  \begin{tabbular}{|p{5.6cm}|p{2.8cm}|p{2.8cm}|p{2.8cm}|}
    \hline
    Step & \Glspl{verificationcondition} starting step & Out of time & Out of memory \\
    \hline
    Extract happens-before graph & 3501 & 28 & 20 \\
    Place side conditions & 3453 & 0 & 2 \\
    \hline
  \end{tabbular}
  \caption{Causes of failures converting \glspl{verificationcondition}
    to \glspl{bugenforcer}.  Note that the timeout runs from the start
    of processing, rather than being restarted for each
    step. \todo{REDO WITH NEW NUMBERS}}
  \label{fig:eval:how:build_enforcer_failures}
\end{sanetab}

\noindent
\autoref{fig:eval:how:build_enforcer} shows how long it takes to
convert \glspl{verificationcondition} into \glspl{bugenforcer}.  The
figure divides the time taken into five steps:
\begin{itemize}
\item Extracting the happens-before graph, as discussed in
  \autoref{sect:enforce:slice_hb_graph}.  This step is generally
  reasonably quick, with a median time of 16ms, but occasionally takes
  a very long time, reaching the five minute timeout in 28 of the 3501
  runs.  This reflects the nature of the algorithm used: BDD
  reordering has a good common-case cost but a very poor worst-case
  one.  The \glspl{verificationcondition} which avoid the worst-case
  performance complete very quickly and those which do not form the
  long tail.
\item Determining when the verification condition's input expressions
  become available, as discussed in \autoref{sect:enforce:place_vcs}.
  This is, unsurprisingly, a very quick step, with relatively few
  outliers, as the rules governing when inputs become available are
  simple and easily applied.
\item Deciding how to evaluate the non-happens before component of the
  \gls{verificationcondition} by placing side conditions on the
  happens-before and control flow graphs.  Like the first step, this
  one is implemented using BDD reordering operations, and so is
  generally fast with a long tail of slow operations.  The small bulb
  of \glspl{verificationcondition} which complete in around 20$\mu$s
  consists of the \glspl{verificationcondition} which had only the
  trivial side condition \true, indicating that the happens-before
  graph is sufficient to trigger the bug regardless of the program's
  other state.
\item Building the patch strategy, expressed as the $\mathit{Cont}$
  and $\mathit{Patch}$ sets, as discussed in
  \autoref{sect:enforce:gain_control}.  \todo{Explain shape a little?}
\item Compiling the resulting enforcer into an ELF shared object.
  {\Implementation} performs this step by generating a C source file
  and passing it off to the system compiler and linker, with the bulk
  of the time spent in those external programs.  This makes it
  difficult to provide any useful analysis on why this step takes as
  long as it does.
\end{itemize}

\noindent
This phase suffered a total of fifty failures, giving a failure rate
of 1.4\%.  The per-\gls{crashingthread} and
per-\gls{interferingthread} suffered failure rates of 0.1\% and 0.4\%,
respectively; collectively, these suggest that {\technique}, with
these settings, will be unable to generate \glspl{bugenforcer} for
roughly 2\% of potential bugs of the targeted form.  While obviously
worse than a 0\% failure rate, a 2\% one is still reasonably low, and
is unlikely to be a crippling limitation.

\subsection{Costs of building fixes}

\Glspl{verificationcondition} can also be converted into fixes.
\autoref{tab:eval:gen_fix_perf} shows how long that takes, using the
4355 \glspl{verificationcondition} generated in the previous
experiments.  As can be seen, the time taken is completely dominated
by the system compiler.  This makes it difficult to explain why the
process takes as long as it does in any meaningful way.  Nevertheless,
these results confirm that building fixes from
\glspl{verificationcondition} is itself a very fast operation,
compared to building the \glspl{verificationcondition} themselves, and
so this step is unlikely to be the most important barrier to practical
use of {\technique}.

\begin{sanetab}
  \begin{tabbular}{|l|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
    \hline
          &      & \multicolumn{3}{c|}{Percentiles} \\
    \cline{3-5}
    Phase & Mean & $5^{th}$\% & $50^{th}$\% & $95^{th}$\%  \\
    \hline
    Find critical sections & $0.84 \pm^{4355}_\mu 0.05$ & $[0.116,0.121]$ & $[0.308,0.323]$ & $[2.10,2.44]$ \\
    Build patch strategy   & $7.8 \pm^{4355}_\mu 0.3$ & $[0.373,0.403]$ & $[0.940,0.962]$ & $[38.7,43.8]$ \\
    Graph expansion        & $2.2 \pm^{4355}_\mu 0.1$ & $[0.232,0.242]$ & $[0.554,0.576]$ & $[10,13]$ \\
    System compiler        & $120.8 \pm^{4355}_\mu 0.4$ & $[88.0,88.9]$ & $[120,122]$ & $[146,148]$ \\
    \hdashline
    Total & $131.6 \pm^{4355}_\mu 0.5$ & $[95.8,97.4]$ & $[130.0,131.2]$ & $[168.3,174.1]$  \\
    \hline
  \end{tabbular}
  \caption{Time taken to convert 4355 \glspl{verificationcondition}
    generated from MySQL to fixes.  All times in milliseconds.
    Distribution percentiles are given as 90\% confidence intervals,
    derived using a bootstrap with 1000 replicates.}
  \label{tab:eval:gen_fix_perf}
\end{sanetab}

\subsection{Dynamic aliasing analysis}

{\Technique} relies on an initial dynamic analysis
(\autoref{sect:program_model}) to build a model of the program's
aliasing behaviour, and if this is incomplete then it will not be able
to find all bugs in the program.
Figures~\ref{fig:eval:dyn_convergence:mysqld},
\ref{fig:eval:dyn_convergence:thunderbird},
and~\ref{fig:eval:dyn_convergence:pbzip2} show the number of edges in
the aliasing table, expressed as a proportion of the edges in the
final table, as a function of time, for a variety of test programs and
workloads.  These figures show that the aliasing table had in all
cases effectively converged on its final value within a few minutes,
suggesting that, for these workloads, it would only be necessary to
run the program to be analysed under the dynamic analysis for a few
minutes to achieve adequate coverage.  This is not an unreasonable
burden to place on the user.

\begin{sanefig}
  \begin{tabular}{cc}
    \subfigure[][rpl\_change\_master]{
      \biggraph{eval/dyn_convergence/rpl_change_master.tex}
    } &
    \subfigure[][innodb\_multi\_update]{
      \biggraph{eval/dyn_convergence/innodb_multi_update.tex}
    } \\
    \subfigure[][binlog\_stm\_drop\_tbl]{
      \biggraph{eval/dyn_convergence/binlog_stm_drop_tbl.tex}
    } &
    \subfigure[][timestamp\_basic]{
      \biggraph{eval/dyn_convergence/timestamp_basic.tex}
    }
  \end{tabular}
  \vspace{-12pt}
  \caption{Dynamic aliasing coverage against time for MySQL, using
    some tests out of the test suite.  Dashed vertical lines show where the
    program was restarted.}
  \label{fig:eval:dyn_convergence:mysqld}
\end{sanefig}

\begin{sanefig}
  \biggraph{eval/dyn_convergence/thunderbird}
  \vspace{-24pt}
  \caption{Dynamic aliasing coverage against time for Thunderbird
    during normal usage.  Dashed vertical lines show where the program was
    restarted.}
  \label{fig:eval:dyn_convergence:thunderbird}
\end{sanefig}

\begin{sanefig}
  \biggraph{eval/dyn_convergence/pbzip2}
  \vspace{-24pt}
  \caption{Dynamic aliasing coverage against time for pbzip2 version
    1.1.6 while compressing three randomly-generated 10MiB files.
    Dashed vertical lines show where the program was restarted.}
  \label{fig:eval:dyn_convergence:pbzip2}
\end{sanefig}

I also briefly investigated the performance of the analysis tool
itself, using pbzip2 as a test program.  For these experiments, I
compressed ten randomly-generated 100MiB files with and without the
dynamic analysis.  Without the dynamic analysis, compressing one file
took $7.8 \pm_\mu^{10} 0.1$ seconds; with the dynamic analysis, this
increased to $274 \pm_\mu^{10} 2$ seconds, a factor of roughly
thirty-five.  This is a rather large overhead, and would be infeasible
in a production environment, but is tolerable for something which
needs to run for a few tens of minutes in a development one.  For
comparison, a null Valgrind skin completed this test in $226.6
\pm_\mu^{10} 0.5$ seconds, an overhead of a factor of twenty-nine,
suggesting that most of the overhead of the dynamic analysis tool
comes simply from the fact that it is implemented as a Valgrind skin.
Re-implementing it in a faster analysis framework, such as
PIN\cite{Luk2005}, might therefore provide a useful speed-up.
\todo{What happens with other test programs?}

\section{Why does it work?}
\label{sect:eval:why_does_it_work}

Previous sections have established that {\technique} works at a basic
level and given some details of its operation.  This section aims to
expand on that by providing some explanations for the properties
observed and showing how they follow from the {\technique} design.

%% Several experiments in this section include comparisons to
%% {\randsched}, a random schedule exploration tool based on
%% DataCollider\cite{Erickson2010} which I implemented for the purposes
%% of this evaluation.  The tool works by setting instruction breakpoints
%% on a selection of the program's memory-accessing instructions and
%% then, when a thread reaches one of those instructions, setting memory
%% watchpoints on the accessed locations and waiting for some other
%% thread to trigger the watchpoint.  When that happens, it selects one
%% of the threads at random to go first.  This causes the program to
%% explore its possible schedules more quickly than it would if run
%% normally, without needing {\technique}'s expensive initial analysis
%% phase.

%% The effectiveness of this kind of tool depends on two parameters: the
%% placement of breakpoints, and how long the tool waits for another
%% thread to arrive when one of the watchpoints is triggered.  For this
%% evaluation, I configured {\randsched} to place breakpoints on half of
%% the program's memory-accessing instructions, excluding instructions
%% which access the stack, selecting a new set of instructions every
%% 100ms, and to wait 100$\mu$s after a watchpoint is triggered.  These
%% parameters minimised the median reproduction time for the
%% \bugname{toctou} test with \texttt{NR\_PTRS} set to 100.

%% Note that the 100$\mu$s timeout is much shorter than the 100-200ms
%% timeout used by {\technique} \glspl{bugenforcer}, and so {\randsched}
%% will make smaller modifications to the program's behaviour, but that
%% setting a breakpoint on every other instruction means that it will
%% make those modifications far more frequently.

\subsection{Importance of side-condition checking}

The most distinctive feature of {\technique}'s \gls{bugenforcer}
mechanism, compared to previous work such as Kivati\cite{Chew2010} or
MUVI\cite{Lu2007}, is side-condition checking, which enables it to
avoid spending time enforcing a particular concurrent ordering if some
aspect of the program's state means that doing so would be
unproductive.  \autoref{fig:eval:indexed_toctou:no_scs} shows the
reproduction time for the \bugname{toctou} test with a full enforcer
and with one which does not perform any side-condition checking.  This
clearly shows not only that reproduction performance without side
condition checking is far worse than with a full enforcer, but also
that it is worse than not using an enforcer at all.  Without
side-condition checking, the enforcer enforces the happens-before
graph every time the buggy code runs, causing the program to run far
more slowly, so the buggy code runs far less frequently.  In this
case, the happens-before graph is quite simple but the side-condition
has a low probability of succeeding, and increasing the likelihood of
reproducing the happens-before graph is insufficient to outweigh the
reduced number of chances to satisfy the
side-condition\editorial{Blah.  I have some maths which makes this a
  bit more formal, but doesn't add enough clarity to justify the space
  it'd take up.}.

\begin{sanefig}
  \biggraph{eval/artificial_bugs/special/indexed_toctou_no_scs.tex}
  \caption{Effect of side-condition checking on the time taken to
    reproduce the indexed\_toctou bug. \todo{Meaning of grey bits?
      Protocol?}}
  \label{fig:eval:indexed_toctou:no_scs}
\end{sanefig}

The importance of side-condition checking depends on the probability
of satisfying the condition, and hence, for the \bugname{toctou} test,
on \texttt{NR\_PTRS}.  \autoref{fig:eval:indexed_toctou:nr_ptrs} shows
this dependency.  Reproduction times rise as \texttt{NR\_PTRS},
whether an enforcer is used or not, but do so far more rapidly without
one, indicating that {\technique} enforcers become relatively more
effective as the probability of a side condition passing falls.  It is
also worth noting that the behaviour with an enforcer is qualitatively
simpler, with smaller inter-quartile ranges, means generally near to
the medians, and a generally smoother shape (the dips at
\texttt{NR\_PTRS} = 400 and \texttt{NR\_PTRS} = 200, for instance, are
much less pronounced when using an enforcer).  It seems likely that a
programmer tasked with fixing this bug would find the behaviour
illustrated in the second chart easier to understand than that in the
first.

\begin{sanefig}
  \todo{Regenerate, pull up by a page.}
  \subfigure[][Without enforcer]{ \biggraph{eval/artificial_bugs/special/indexed_toctou_vary_nr_ptrs_no_enforcer.tex} }
  \subfigure[][With enforcer]{ \biggraph{eval/artificial_bugs/special/indexed_toctou_vary_nr_ptrs_enforcer.tex} }
  \caption{Reproduction times with and without an enforcer loaded, for
    varying values of \texttt{NR\_PTRS}.  Note the log scales.  Each
    abscissa was sampled 110 times, discarding the first ten results
    and with the order of tests randomised.  Boxes show interquartile
    range and median with 90\% confidence interval for quantiles in
    grey.  Cross and bars give arithmetic mean and 90\% confidence
    interval for mean.  Confidence intervals computed by a bootstrap
    with 1000 replicates.}
  \label{fig:eval:indexed_toctou:nr_ptrs}
\end{sanefig}

\subsection{Interaction with the program's existing synchronisation}

\todo{Not convinced this belongs here.}

\begin{sanefig}
  \subfigure[][\RaggedRight Crashing thread with {\technique}-visible\\synchronisation]{
    \begin{minipage}{5.1cm}
      \begin{literalC}
        while (1) \clbrace
          analysis\_window \clbrace
            lock(); \\
            if (ptr != 0) \clbrace
              *ptr = 5;
            \crbrace \\
            unlock();
          \crbrace
        \crbrace
      \end{literalC}
    \end{minipage}
    \label{fig:eval:existing_sync:visible}
  }
  \subfigure[][\RaggedRight Crashing thread with {\technique}-invisible\\synchronisation]{
    \begin{minipage}{5.1cm}
      \begin{literalC}
        while (1) \clbrace
          lock();\\
          analysis\_window \clbrace
            if (ptr != 0) \clbrace
              *ptr = 5;
            \crbrace
          \crbrace \\
          unlock();
        \crbrace
      \end{literalC}
    \end{minipage}
    \label{fig:eval:existing_sync:invisible}
  }
  \subfigure[][Interfering thread]{
    \begin{minipage}{3cm}
      \begin{literalC}
        while (1) \clbrace
          ptr = \&t;\\
          analysis\_window \clbrace
            lock();\\
            ptr = 0;\\
            unlock();
          \crbrace
        \crbrace
        \\
      \end{literalC}
    \end{minipage}
    \label{fig:eval:existing_sync:interfering}
  }
  \vspace{-12pt}
  \caption{A correctly synchronised program.  \texttt{lock()} and
    \texttt{unlock()} acquire and release a global lock,
    respectively.}
  \label{fig:eval:existing_sync}
\end{sanefig}

\noindent
This section briefly explores the effects of any existing
synchronisation on {\technique}'s behaviour, using the test program
shown in \autoref{fig:eval:existing_sync}.  In this (correctly
synchronised) program, the interfering thread modifies a global
variable while the two crashing threads simultaneously make use of it.
The crashing threads differ only in the placement of the
synchronisation operations: the first, in
\autoref{fig:eval:existing_sync:visible}, places the synchronisation
within the \gls{analysiswindow}, making it visible to the {\technique}
analysis, whereas the second, in
\autoref{fig:eval:existing_sync:invisible}, moves it outside of the
window, so {\technique} will be unaware of it.

As expected, {\technique} produces a \gls{verificationcondition}, and
hence a \gls{bugenforcer}, for the second thread but not for the
first.  When loaded into the program, this enforcer attempts to
enforce a happens-before graph which contradicts the program's
existing synchronisation and therefore deadlocks.  This causes the
enforcer's message operations to time out, and so the enforcer exits
and allows the program to run normally (beyond suffering reduced
performance).

The \gls{verificationcondition} can also be converted to a ``fix''.
This fix does not fix any actual bugs, as there are none, but does not
otherwise harm the program's execution, beyond a slight loss of
performance.

\subsection{Effect of {\StateMachine} simplification}

{\Technique} applies various simplifications to {\StateMachines}
before attempting to symbolically execute them, and, as previously
indicated, these simplifications often account for a significant
proportion of the total analysis time.  {\Technique} is nevertheless
faster because of them, as the cost of the simplification is
outweighed by the reduction in the cost of the symbolic execution
steps.  To quantify this, I re-ran the experiments of
\autoref{sect:eval:how_does_it_work} with {\StateMachine}
simplification disabled; the results are shown in
\autoref{tab:eval:why:effects_of_simplification}.  As expected, the
simplifiers reduce the total time taken by the analysis and the number
of failures experienced.  Perhaps surprisingly, they also increased
the number of \glspl{verificationcondition} generated.  This is due to
a survival effect: disabling the simplifiers excludes the most complex
{\StateMachines}, as those tend to fail without simplification, and
those are generally the most likely to generate
\glspl{verificationcondition}.  All of the additional
\glspl{verificationcondition} observed with simplification enabled
corresponded to cases which simply failed with simplification
disabled.

\begin{sanetab}
  \begin{tabbular}{|l|l|l|}
    \hline
    & \multicolumn{2}{c|}{{\STateMachine} simplification} \\
    \cline{2-3}
    & Enabled & Disabled \\
    \hline
    Time to process crashing instruction & $x \pm_\mu^n y$ \\
    Interfering \glspl{cfg} per crashing instruction* & $x \in [a,b]^n_{1000}$ \\
    Interfering \glspl{cfg} generating \glspl{verificationcondition}$\dagger$ & $x \in [a,b]^n_\infty$\% \\
    Errors in per-crashing instruction phase & $x \in [a,b]^n_\infty$\% \\
    Errors in per-\gls{interferingthread} phase & $x \in [a,b]^n_{1000}$\% \\
    \hline
  \end{tabbular}
  \caption{Effect of {\StateMachine} simplification on analysis
    effectiveness. *: Excluding failures in the per-crashing
    instruction phase. $\dagger$: Excluding failures in either phase.
    \todo{Need data}}
  \label{tab:eval:why:effects_of_simplification}
\end{sanetab}

\subsection{Effect of the \glsentrytext{w-isolation} assumption}
\label{sect:eval:w_isolation}

{\Technique} can optionally make use of the \gls{w-isolation}
assumption to constrain the aliasing problem, which can sometimes
usefully improve analysis performance at the expense of discovering a
smaller class of bugs.  I evaluated the impact of this assumption by
repeating the experiments of \autoref{sect:eval:how_does_it_work} with
\gls{w-isolation} disabled.  The results are shown in
\autoref{tab:eval:why:w_isolation}.  These are broadly as expected:
the \gls{w-isolation} assumption causes the analysis to complete
slightly more quickly, with a slight reduction in the number of
failures, and causes {\technique} to generate a slightly smaller set
of \glspl{verificationcondition}.  In this instance, all of the
eliminated \glspl{verificationcondition} were false
positives\footnote{To confirm this, I converted them all into
  \glspl{bugenforcer} and applied them all to the
  \texttt{rpl\_change\_master} test, in the style of
  \autoref{sect:how:finding_unknown}; none were able to reproduce the
  hypothesised bugs.}\!\!, and so this is a very reasonable trade-off.

\begin{sanetab}
  \begin{tabbular}{|l|l|l|}
    \hline
    & \multicolumn{2}{c|}{\Gls{w-isolation} assumption} \\
    \cline{2-3}
    & Enabled & Disabled \\
    \hline
    Time to process crashing instruction & $x \pm_\mu^n y$ \\
    Interfering \glspl{cfg} per crashing instruction* & $x \in [a,b]^n_{1000}$ \\
    Interfering \glspl{cfg} generating \glspl{verificationcondition}$\dagger$ & $x \in [a,b]^n_\infty$\% \\
    Errors in per-crashing instruction phase & $x \in [a,b]^n_\infty$\% \\
    Errors in per-\gls{interferingthread} phase & $x \in [a,b]^n_{1000}$\% \\
    \hline
  \end{tabbular}
  \caption{Effect the \gls{w-isolation} assumption on analysis
    effectiveness. *: Excluding failures in the per-crashing
    instruction phase. $\dagger$: Excluding failures in either phase.
    \todo{Need data}}
  \label{tab:eval:why:w_isolation}
\end{sanetab}

\subsection{Effect of the dynamic \glsentrytext{programmodel} analysis}

\todo{What goes here?}

\subsection{Effect of the static \glsentrytext{programmodel} analysis}

\todo{What goes here?}

\subsection{Sources of fix overhead}
\label{sect:eval:why:fix_overhead}

\todo{Ugly writing.} As previously discussed, {\technique}-generated
fixes generally have low overheads, usually on the order of a few
microseconds per critical section.  This section gives a more detailed
break-down of the sources of that overhead by applying a variety of
fixes and partial fixes to a simple test program and comparing their
performance.  The test program, \bugname{nobug}, is shown in
\autoref{fig:eval:why:nobug}.  {\Technique} correctly identifies that
the program might suffer an atomicity violation crash if
\texttt{global} is ever 2, but, lacking a global model of the
program's structure, is unable to show that that is impossible, and so
generates a \gls{verificationcondition} and ``fix'' for this program.
Since this bug is a false positive, the fix has no effect on the
program's behaviour, beyond introducing a modest amount of additional
overhead.  

\begin{sanefig}
  {\hfill}
  \subfigure[][\Gls{crashingthread}]{
    \begin{minipage}{1cm}
      \begin{literalC}
        while (1) \clbrace
          analysis\_window \clbrace
            x$_0$ = global;\\
            x$_1$ = global;\\
            assert(x$_0$ + x$_1$ != 3);
          \crbrace
        \crbrace
      \end{literalC}
    \end{minipage}
  }
  {\hfill}
  \subfigure[][\Gls{interferingthread}]{
    \begin{minipage}{1cm}
      \begin{literalC}
        \\
        while (1) \clbrace
          analysis\_window \clbrace
            global = 1;
          \crbrace
        \crbrace
        \\
      \end{literalC}
    \end{minipage}
  }
  {\hfill}
  \caption{The {\!\rm \bugname{nobug}\!} test program.}
  \label{fig:eval:why:nobug}
\end{sanefig}

\begin{sanetab}
  \begin{tabbular}{|l|l|l|l|l|}
    \hline
                                       & \multicolumn{2}{c|}{Thread iteration time} & \multicolumn{2}{c|}{Thread overhead} \\
    \cline{2-5}
    Type of fix                        & Crashing & Interfering & Crashing & Interfering \\
    \hline
    \textit{None}                      & & & & \\
    \textit{Gain control only}         & & & & \\
    \textit{No synchronisation}        & & & & \\
    \textit{Synchronise only crashing} & & & & \\
    \textit{Normal}                    & & & & \\
    \hdashline
    \textit{Debug registers}           & & & & \\
    \hline
  \end{tabbular}
  \caption{Number of loop iterations completed by {\!\rm
      \bugname{nobug}\!} per second with a selection of different
    partial fixes applied.  See text for descriptions of the type of
    fix. \todo{Need data} }
  \label{tab:eval:why:nobugperf}
\end{sanetab}

\autoref{tab:eval:why:nobugperf} evaluates these overheads, for
several variants of the fix:
\begin{itemize}
\item \textit{None} --- No fix is applied at all.
\item \textit{Gain control only} --- The fix is generated by the usual
  algorithm, except that the $\mathit{newNode}$ function in
  \autoref{fig:fix:graph_grammar} is replaced with the constant
  $\varnothing$.  The resulting fix will gain control of the program
  in the usual way but will return control as soon as the program
  leaves the $\mathit{Cont}$ set.  This configuration measures the
  overheads introduced by simply gaining control of the program at the
  necessary points.
\item \textit{No synchronisation} --- The fix is generated by the
  usual algorithm, except that the ``Acquire lock'' and ``Release
  lock'' sequences in \autoref{fig:fix:graph_grammar} are replaced
  with no-ops.  The resulting patch will gain control of the program
  in the usual way, and will retain control for as long as a full
  patch will, but will never perform any synchronisation operations.
  This configuration measures the overheads introduced by duplicating
  and rearranging the program code in the critical sections.
\item \textit{Synchronise only crashing} --- This fix omits the
  \gls{interferingthread} synchronisation operations from the patch
  but retains the \gls{crashingthread} ones, and so makes it possible
  to evaluate the overhead of synchronisation operations in the
  absence of any contention.
\item \textit{Normal} --- A complete fix is generated, using the
  complete algorithms from \autoref{sect:enforce:gain_control} and
  \autoref{fig:fix:graph_grammar}.
\item \textit{Debug registers} --- A complete fix is generated, except
  that rather than gaining control using the patch strategy algorithm
  in \autoref{sect:enforce:gain_control}, it gains control using the
  processor's debug registers.
\end{itemize}
\todo{Need more once I have data.} These results show that the
synchronisation operations themselves are the most expensive part of
the generated fixes.  This is reassuring, and suggests that the
patching mechanism used by {\technique} is close to the optimal way of
introducing these synchronisation operations into this program.  As
expected, the fix which used debug registers was far slower than the
one which used the patch strategy mechanism.

\section{When does it work?}
\label{sect:eval:does_it_scale}

Previous sections have investigated whether {\technique} works, how it
works, and why it works.  This section builds upon these by looking at
how {\technique}'s behaviour depends on the complexity of the bug to
be investigated, quantifying its scalability, and hence places some
constraints on the situations in which {\technique} is likely to
behave well.

\subsection{Scalability to complex happens-before graphs}

\begin{sanefig}
  {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{} \\
        & \multicolumn{2}{l}{analysis\_window \{} \\
        & & $\texttt{x}_1$ = global;\\
        & & $\texttt{x}_2$ = global;\\
        & & $\vdots$ \\
        & & $\texttt{x}_N$ = global;\\
        & & \hspace{-2mm}\begin{tabular}{ll}
          assert(!(&\hspace{-2.5mm}$\texttt{x}_1$ == 1 \&\& \\
          &\hspace{-2.5mm}$\texttt{x}_2$ == 2 \&\& \\
          &$\vdots$ \\
          &\hspace{-2.5mm}$\texttt{x}_N$ == N)); \\
        \end{tabular}\\
        & \multicolumn{2}{l}{\}} \\
        \multicolumn{3}{l}{\}} \\
      \end{tabular}
    }
  }
  {\hfill}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \\
        \\
        \multicolumn{3}{l}{while (1) \{} \\
        & \multicolumn{2}{l}{analysis\_window \{} \\
        & & global = 1;\\
        & & global = 2;\\
        & & $\vdots$ \\
        & & global = N;\\
        & \multicolumn{2}{l}{\}} \\
        \multicolumn{3}{l}{\}} \\
        \\
        \\
      \end{tabular}
    }
  }
  {\hfill}
  \caption{The \bugname{complex\_hb}$_{N}$ test.  This bug will only
    be triggered if $2N-1$ happens-before edges are all correctly
    enforced.}
  \label{fig:eval:why:complex_hb}
\end{sanefig}

\noindent
This section explores {\technique}'s behaviour as the complexity of
the happens-before graph increases, using the test program used is
shown in \autoref{fig:eval:why:complex_hb}.  The two threads each
consist of a series of $N$ memory accesses arranged such that the
program crashes if it alternates accesses between the two threads.
The happens-before graph for this bug therefore contains $2N$ edges.

\begin{sanefig}
  \todo{Regenerate with more replicates.  Also, what sort of
    exponential did I fit?}
  \biggraph{eval/complex_hb/complex_hb_build_summaries}
  \caption{Time taken to analyse the \bugname{complex\_hb}$_N$ test,
    for varying values of $N$.  Note log scale.  Each abscissae was
    sampled eleven times, discarding the first, in random order.
    Crosses and bars give the mean and 90\% confidence interval of the
    mean, calculated using the central limit theorem.  The solid line
    shows a least-squares exponential regression over $7 \leq N \leq
    20$ extrapolated to the full range of $N$; the dashed one shows a
    quartic regression over the same data.  Grey regions give 90\%
    confidence intervals for the regression lines, computed using a
    1,000 replicate bootstrap.  Note that the regressions minimise the
    sum of squares loss, but are plotted on a logarithmic scale.}
  \label{fig:eval:complex_hb:analysis_time}
\end{sanefig}

\autoref{fig:eval:complex_hb:analysis_time} shows how the time taken
when generating \glspl{verificationcondition} varies with
$N$\footnote{Memory consumption on this test was 4MiB on the smallest
  test and 8MiB on the largest one, but these numbers are more
  functions of implementation details of {\implementation} than
  meaningful properties of {\technique}.}\!\!\!.  ~The time taken clearly
increases rapidly with $N$, and this will limit the complexity of bugs
which can be analysed with {\technique}.  This is not, however, likely
to be the most important limitation to {\technique}'s scalability:
most bugs in real programs require only two or three edges to be
enforced\cite{Musuvathi2008}, and {\technique} was able to analyse a
79-edge one in under ten minutes.

\todo{Not sure this adds all that much.} It is perhaps interesting to
note that, although very rapid, the rate of increase is less than
exponential.  The solid line in the figure shows the least-squares
exponential regression on the data for $7 \leq N \leq 20$\footnote{The
  results reported appear to be robust to small changes in the
  boundaries of this region.}, extrapolated over the complete range of
$N$.  It significantly over-estimates the rate at which the time taken
increases.  A quartic regression over the same data, shown as a dotted
line in the figure, gives a much closer fit in the extrapolated
region.  This is because, in this particular test, the {\StateMachine}
simplification step is able to make quite dramatic simplifications to
the structure of the {\StateMachines}, and so there is no need to ever
consider all $2^N$ interleavings of the memory accesses.

\todo{Do the same thing for memory usage.}

\todo{Also put in some numbers on how effective the enforcers are.
  The answer is that they're very effective, and how effective they
  are is basically independent of $N$, but I should probably say that.}

\subsection{Scalability with respect to memory access complexity}

\begin{sanefig}
  {\hfill} \subfigure[][Crashing thread]{ \texttt{
    \begin{tabular}{ll}
      \multicolumn{2}{l}{$\texttt{s}_1$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{s}_2$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{\vdots}\\
      \multicolumn{2}{l}{$\texttt{s}_S$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{l}_1$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{l}_2$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{\vdots}\\
      \multicolumn{2}{l}{$\texttt{l}_L$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{analysis\_window \{}\\
      &$\texttt{slots}[\texttt{s}_1]$ = 1;\\
      &$\texttt{slots}[\texttt{s}_2]$ = 2;\\
      &\vdots\\
      &$\texttt{slots}[\texttt{s}_S]$ = $S$;\\
      &\hspace{-2.3mm}\begin{tabular}{lll}
         assert(~(&\hspace{-2.5mm}$\texttt{slots}[\texttt{l}_1]$ &\hspace{-3mm} + \\
         &\hspace{-2.5mm}$\texttt{slots}[\texttt{l}_2]$ &\hspace{-3mm} + \\
         & \vdots \\
         &\hspace{-2.5mm}$\texttt{slots}[\texttt{l}_L]$ &\hspace{-4.5mm} ) \\
         &\multicolumn{2}{l}{\hspace{1em}!= $L.(S+1) + 1$);}\\
       \end{tabular}\\
      \multicolumn{2}{l}{\}}
    \end{tabular}
    }
  }
  {\hfill}
  \raisebox{-6pt}{
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \\
        \\
        \\
        \\
        \\
        \\
        \\
        \multicolumn{2}{l}{idx = random() \% NR\_PTRS;}\\
        \multicolumn{2}{l}{analysis\_window \{} \\
        & slots[idx] = S+1; \\
        \multicolumn{2}{l}{\}}\\
        \\
        \\
        \\
        \\
        \\
        \\
        \\
        \\
      \end{tabular}
    }
  }
  }
  {\hfill}
  \caption{The \bugname{complex\_alias}$_{L,S}$ test.
    \texttt{NR\_PTRS} is the constant 100.  \todo{Kind of ugly.}}
  \label{fig:eval:why:complex_aliasing}
\end{sanefig}

\begin{sanefig}
  \todo{Why is 2GB contour special?}
  \biggraph{eval/complex_alias/hard}
  \caption{Memory and time used to analyse the
    \bugname{complex\_alias}$_{L,S}$ test.  Configurations which timed
    out are shown with a cross; those which ran out of memory are
    shown with a circle.  Configurations which timed out on some
    repeats and ran out of memory on others are shown with both.  For
    the purposes of drawing the contours, experiments which failed
    were treated as if they had completed precisely at the timeout and
    used precisely the maximum memory; cells in which I made that
    assumption are shown in red.}
  \label{fig:eval:why:complex_aliasing:result1}
\end{sanefig}

\noindent
The previous section showed that {\technique} scales well to
complicated happens-before graphs\editorial{null}; this one
investigates how well it scales to complicated memory access patterns.
This first test program used is shown in
\autoref{fig:eval:why:complex_aliasing}.  The \gls{crashingthread}
\gls{analysiswindow} consists of $S$ stores and $L$ loads, with all of
the addresses unpredictable, followed by a final test on the sum of
the loaded values.  This test always fails, and so the program never
crashes, but {\technique} is unable to prove that without considering
every possible way of evaluating the condition.

\todo{Word salad}

\autoref{fig:eval:why:complex_aliasing:result1}\footnote{This
  experiment was structured as eleven rounds with each pair of values
  of $L$ and $S$ tested once in each round.  I discarded all results
  from the first round and, in addition, the highest and lowest values
  obtained for each pair.  The lines on the chart show contours of the
  averages of the remaining eight samples and the grey regions show
  the ranges observed.} shows the cost of analysing this program, in
terms of time taken and memory consumed, as functions of $L$ and $S$.
Both functions clearly increase rapidly with both $L$ and $S$, with a
much stronger response to $L$ than to $S$.  In order to analyse this
program, {\technique} must consider every possible value of each of
the $L$ loads.  There are $S+1$ such values, corresponding to the $S$
stores and the initial contents of memory, so {\technique} must
consider $(S+1)^L$ possible aliasing patterns.  This is a rapidly
growing function of both $L$ and $S$, and, for most relevant values of
its inputs, grows more rapidly with $L$ than $S$, conveniently
explaining the graph's most obvious qualitative features\footnote{This
  model is, however, a poor quantitative fit for the data, as the cost
  of analysing each aliasing pattern is also a function of $L$ and
  $S$.}\!\!.

\begin{sanefig}
  {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
    \begin{tabular}{ll}
      \multicolumn{2}{l}{$\texttt{s}_1$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{s}_2$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{\vdots}\\
      \multicolumn{2}{l}{$\texttt{s}_S$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{l}_1$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{l}_2$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{\vdots}\\
      \multicolumn{2}{l}{$\texttt{l}_L$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{analysis\_window \{}\\
      &$\texttt{slots}[\texttt{s}_1]$ = 1;\\
      &$\texttt{slots}[\texttt{s}_2]$ = 2;\\
      &\vdots\\
      &$\texttt{slots}[\texttt{s}_S]$ = $S$;\\
      &\hspace{-2.3mm}\begin{tabular}{lll}
         assert(&\hspace{-2.5mm}$\texttt{slots}[\texttt{l}_1]$ &\hspace{-3mm} != $S+1$ \&\& \\
         &\hspace{-2.5mm}$\texttt{slots}[\texttt{l}_2]$ &\hspace{-3mm} != $S+1$ \&\& \\
         & \vdots \\
         &\hspace{-2.5mm}$\texttt{slots}[\texttt{l}_L]$ &\hspace{-3mm} != $S+1$);\\
       \end{tabular}\\
      \multicolumn{2}{l}{\}}
    \end{tabular}
    }
  }
  {\hfill}
  \raisebox{-6pt}{
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \\
        \\
        \\
        \\
        \\
        \\
        \\
        \multicolumn{2}{l}{idx = random() \% NR\_PTRS;}\\
        \multicolumn{2}{l}{analysis\_window \{} \\
        & slots[idx] = 0; \\
        \multicolumn{2}{l}{\}}\\
        \\
        \\
        \\
        \\
        \\
        \\
        \\
      \end{tabular}
    }
  }
  }
  {\hfill}
  \caption{The \bugname{complex\_alias\_easy}$_{L,S}$ test.
    \texttt{NR\_PTRS} is the constant 100.  \todo{Kind of ugly.}}
  \label{fig:eval:why:complex_aliasing:easy}
\end{sanefig}

\begin{sanefig}
  \biggraph{eval/complex_alias/easy}
  \caption{Memory and time used to analyse the
    \bugname{complex\_alias\_easy}$_{L,S}$ test.  Configurations which
    timed out are shown with a cross; those which ran out of memory
    are shown with a circle.  Configurations which timed out on some
    repeats and ran out of memory on others are shown with both.  When
    drawing the contours, I assumed that failed tests would have
    completed after double the timeout time using double the available
    memory.  Cells in which I made that assumption are shown in red.
    Note that {\technique}'s garbage collector was configured to only
    run when the heap size exceeded 2GB, and so the behaviour is
    likely to be different on either side of the 2GB
    contour. \todo{Repeats previous caption.}}
  \label{fig:eval:why:complex_aliasing:result2}
\end{sanefig}

\autoref{fig:eval:why:complex_aliasing:easy} shows another variant of
this test program.  This version breaks the final test into $L$
independent predicates each on a single loaded value, as opposed to
the previous test's single predicate over $L$ values.  {\Technique}'s
lazy aliasing resolution (see \autoref{sect:derive:symbolic_execute})
naturally factorises the resulting aliasing problem into $L$
single-load ones, rather than the previous test program's single
$L$-load one, and so {\technique} now need only consider $(S+1).L$
aliasing configurations where it had previously to consider $(S+1)^L$.

The cost of doing so is shown in
\autoref{fig:eval:why:complex_aliasing:result2}.  As expected,
        {\technique} is able to analyse this variant of the test far
        more quickly and with far less memory, allowing it to scale to
        far larger values of $L$\footnote{The largest successful value
          of $S$ is unchanged, except for different quantisations, as
          it is in both experiments achieved when $L=1$ and $(S+1)^1 =
          (S+1).1$.}.  This behaviour is clearly more useful than
        always exhibiting the prior, poor, performance, but is also
        difficult to characterise crisply: sometimes {\technique} will
        struggle with bugs which depend on even a dozen memory loads;
        at other times, it will be able to process hundred-load bugs
        with little difficulty.  \todo{Need something more here.}

\subsection{Scalability with respect to \gls{alpha}}

\todo{This section is a mess.}

\begin{sanefig}
  \todo{Regenerate, killing off the pre-dismissed field, using the
    right kernel, in style of previous figures.}
  \biggraph{eval/alpha/unopt/bpm.tex}
  \caption{Effect of the \gls{alpha} parameter on the time taken to
    perform per-\gls{crashingthread} analysis.  A censoring correction
    was applied to the kernel density estimator near the 300 second
    timeout.}
  \label{fig:perf:alpha:bpm:unopt}
\end{sanefig}

\begin{sanefig}
  \todo{Kill second graph, merge first one with previous figure.
    Explain meaning of dashed boxes.}
  \biggraph{eval/alpha/unopt/gsc.tex}
  \caption{Effect of the $\alpha$ parameter on the time taken to
    perform per-\gls{interferingthread} analysis. A censoring
    correction was applied to the kernel density estimator near the
    300 second timeout.}
  \label{fig:perf:alpha:gsc:unopt}
\end{sanefig}

\noindent
Previous sections have investigated scalability with respect to some
measures of the complexity of the \gls{analysiswindow} using
artificial test programs.  This section investigates scalability with
respect to the raw size of the \gls{analysiswindow} using real
programs.  To do so, I selected 1000 instructions at random from MySQL
and analysed each at varying values of \gls{alpha}, recording the time
taken by the various analysis phases.  The results are shown in
figures \ref{fig:perf:alpha:bpm:unopt},
\ref{fig:eval:time_breakdown:crashing_size}, and
\ref{fig:perf:alpha:gsc:unopt}, in the same style as
\autoref{fig:eval:how:per_crashing_times}.

\todo{Analysis?}

The configuration used for this experiment was slightly different from
that used for most of the other experiments:
\begin{itemize}
\item The machine used to run this experiment was a 1.9GHz Opteron
  6168 with 16GiB of memory.
\item {\Implementation} was initially configured to analyse ten
  potentially-crashing instructions in parallel, whereas the previous
  experiments analysed one at a time.  Any instructions which suffered
  an out-of-memory error when run in parallel were re-run in series
  after the experiment completed; an operation was only treated as
  running out of memory if it failed on this final run.  Timed-out
  phases of the analysis were not re-run.  The distributions obtained
  without performing any re-runs are shown on the chart with dashed
  grey lines.
\end{itemize}
The results of this experiment are therefore not directly comparable
with the results of the other experiments.

\section{Discussion}

This evaluation has demonstrated several important properties of
{\technique}:
\begin{itemize}
\item It is able to quickly reproduce and fix bugs in both artificial
  and real test programs.  The generated fixes themselves have low
  enough overhead to be useful in practice.
\item It is able to find at least some previously-unknown bugs in real
  programs.
\item It exhibits reasonable scalability as the complexity of the
  program to be analysed increases.
\end{itemize}
It therefore satisfies the original design goals discussed in
\autoref{sect:intro:overview}.

The chief weakness of the evaluation is that it considers only a small
number of bugs in real programs, and those bugs were themselves
selected because they interact well with {\technique}.  It is hard to
say how these results would generalise to other programs and bugs.
The problem here is fundamentally that the class of bugs considered by
{\technique} is quite small: not only must the program crash in a
detectable way, but it must do so within a few dozen instructions.
Such bugs are a small subset of the already-small set of all
concurrency bugs, and I have been unable to find enough in real
programs to form the basis of a truly convincing evaluation.

Nevertheless, this evaluation has shown that it is possible to find,
reproduce, and fix concurrency bugs in large-scale multithreaded
software given only the program binary and some minimal information on
its memory allocation pattern, and that the generated fixes are
themselves reasonably efficient.  This represents a useful advance
over the state of the art; previous systems either required far more
information about the program (such as AutoPag\needCite{} and
ConTest\needCite{}), or had very high overhead without exotic hardware
(such as Atom-Aid\needCite{} or AVIO\needCite{}), or targeted a more
restricted class of bugs (such as Kivati\needCite{} or
Tolerace\needCite{}).  I expand on these comparisons in the next
chapter.
