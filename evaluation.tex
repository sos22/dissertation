\chapter{Evaluation}
\label{chapter:eval}

Previous chapters have described the basic {\technique} technique.  I
will now evaluate its effectiveness, and the performance of my
implementation {\implementation}.  This evaluation will consist of the
following parts:

\begin{itemize}
\item Section~\ref{sect:eval:artificial} explores the behaviour of the
  tool and the technique on a number of artificial bugs in simple test
  programs.  This includes a comparison to a
  DataCollider\needCite{}-like tool which I implemented for the
  purpose of the evaluation.
\item Section~\ref{sect:eval:semiartificial} investigates some
  slightly more realistic bugs.  These include a simplified version of
  a real bug in glibc\needCite{} and some bugs which I deliberately
  introduced into the STAMP benchmark suite\needCite{}.
\item Section~\ref{sect:eval:real} applies the tool to some large
  programs: pbzip2\needCite{}, MySQL\needCite{}, and
  Thunderbird\needCite{}.  I show that the analysis completes in a
  tolerable amount of time even for some very large programs,
  and demonstrate that it can both reproduce and fix a (small)
  number of real bugs.
\item Section~\ref{sect:eval:time_details} then investigates the
  tool's performance on large programs in slightly more detail,
  showing how the time taken breaks down across the various phases
  and the effects of some of {\implementation} optimisations.
\item The final section, \ref{sect:eval:validation}, gives the results
  of some experiments intended to validate that {\implementation} is
  itself correctly implemented, and hence to give confidence that the
  other results are reasonable.
\end{itemize}

\section{DataCollider-like implementation}
\label{sect:eval:datacollider}

\todo{Rewrite}

As a point of comparison, I implemented a tool which explores
alternative thread schedules at random, without first analysing the
program to obtain {\StateMachines} and verification conditions,
inspired by DataCollider\needCite{}.  The idea here is to select a
random subset of memory-accessing instructions in the program and
place breakpoint instructions at them.  When these breakpoints fire
the tool examines the program state to determine which memory
locations they access and sets a processor watch point\needCite{} on
that location.  It then waits a short while to see if the watch point
is triggered by any other threads.  If it is, the two instructions
race and the tool selects which to execute first at random, then
allows both threads to proceed unmolested.  Otherwise, the breakpoint
and watch point are cleared and another breakpoint set elsewhere in
the program.  The result is that the program explores its possible
schedules more quickly than it would if run without these
manipulations, which should lead to bugs being discovered more
quickly.

The effectiveness of this tool is obviously highly dependent on the
fraction of instructions which are covered by breakpoints and the
length of the delays inserted.  Some initial experiments\smh{By you?
  or by them?  what workloads?} suggested that these parameters lead
to the most rapid bug reproduction\editorial{Should have done this in
  a more systematic way.}:

\begin{itemize}
\item
  At any given point, half of the program's memory accessing
  instructions will have an instruction breakpoint, excluding stack
  accesses\editorial{Putting a breakpoint on every instruction lead to
    a deadlock; should probably figure out why.}.
\item
  When a breakpoint is hit, the tool will wait for up to 10ms for a
  matching read or write to arrive.  When a thread does arrive, it
  will select which to resume first at random, with equal probability.
\item
  Once a second, the tool discards its current instruction breakpoint
  set and generates a new one.
\end{itemize}

It is instructive to compare these parameters to those used in the
DataCollider paper.  The 10ms timeout is broadly similar, as
DataCollider uses between one and fifteen millisecond timeouts,
depending on the type of instruction.  Setting breakpoints on half of
instructions, on the other hand, is not.  The actual DataCollider
implementation adjusts the breakpoint density dynamically so as to
achieve a particular breakpoint rate, and their paper does not specify
a numerical breakpoint density, which makes a direct comparison
difficult.  It is, however, possible to estimate the breakpoint
density from the information which they do give.  Their evaluation
shows breakpoint rates of up to 1500 breakpoints per second in a
virtual machine with two processors running at 2.4GHz, so $4.8 \times
10^9$ cycles per second.  If one assumes roughly one non-stack memory
access every hundred cycles that translates to 4.8 million memory
accessing instructions per second, and so they must trap roughly
0.03\% of memory accessing instructions.  Even allowing for the fact
that their implementation preferentially places breakpoints on
instructions which execute infrequently this is still likely to
translate to a breakpoint ratio several orders of magnitude lower than
that used in this evaluation.

Setting such a high breakpoint ratio has two main effects: bugs are
detected more quickly, but the overheads of the tool are much higher.
In particular, the high overheads mean that this tool is not an
entirely practical approach to investigating concurrency bugs.  I
present it merely as a baseline against which to evaluate
{\implementation}.  \todo{Should really eval the overheads, rather
  than just asserting that they're massive.  I mean, they are, but I
  should have some evidence to back that up.}\smh{+maybe look at would
  they would be if you uses 0.03\% like the original?}

More fundamentally, using DataCollider to explore alternative
schedules is itself somewhat unfair, as DataCollider was originally
intended to discover races rather than to permute schedules, and,
while it is trivial to extend it to perform schedule exploration, it
is not entirely surprising that the results are somewhat poor.
Nevertheless, it represents the approach which is conceptually close
to {\technique}'s in the existing literature, and so I consider it to
be an interesting reference point.

\todo{I'd really like to do a comparison with CHESS, as well, but
  that's only implemented for 32-bit Windows programs, and
  {\technique} is only implemented for 64-bit Linux ones, which makes
  a bit of a mess of direct comparisons.  It'd be kind of fun to
  re-implement it for Linux, but I don't really have time to do that
  right now.}

\section{Artificial bugs}
\label{sect:eval:artificial}

I now present the results of running {\implementation} on a number of
artificial bugs, showing that it assists in both reproducing the bugs
and fixing them, and that the analysis phases complete very quickly.

\subsection{Simple time-of-check, time-of-use (TOCTOU) bug (simple\_toctou)}\footnote{This bug was previously discussed in
  Section~\ref{sect:derive:simple_toctou_example}.}
\label{sect:eval:simple_toctou}


\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&*global\_ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }\hfill %
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }\hfill
  \caption{The two sides of the simple\_toctou bug.}
  \label{fig:eval:simple_toctou}
\end{figure}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
    \begin{tabular}{rlll}
              & \multicolumn{3}{l}{crashing\_thread:} \\
      400694: & movq  & global\_ptr, &\!\!\!\%rax\\
      40069b: & testq & \%rax,       &\!\!\!\%rax \\
      40069e: & je    & \multicolumn{2}{l}{4006ad}\\
      4006a0: & movq  & global\_ptr, &\!\!\!\%rax\\
      4006a7: & movl  & \$0x5,       &\!\!\!(\%rax)\\
    \end{tabular}
    }
  }%
  \hspace{-5mm}\subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{rlll}
        & \multicolumn{3}{l}{interfering\_thread:} \\
        400816: & lea  & c(\%rsp), &\!\!\!\%rbp \\
        ...\\
        400884: & movq & \%rbp, &\!\!\!global\_ptr\\
        ...\\
        4008fb: & movq & \$0x0, &\!\!\!global\_ptr\\
      \end{tabular}
      }
    }
  \caption{Disassembly of the program fragments in Figure~\ref{fig:eval:simple_toctou}.}
  \label{fig:eval:simple_toctou:compiled}
\end{figure}

This is the simplest possible kind of concurrency bug: a
single-variable time-of-check, time-of-use race.  The two threads
involved are shown in Figure~\ref{fig:eval:simple_toctou}.  The intent
here is to model a very simple structure which is accessed frequently
but updated rarely.  The bug is, of course, that the interfering
thread might set \texttt{global\_ptr} to \texttt{NULL} in between the
two reads of it in the crashing thread, causing the crashing thread to
crash when it dereferences the pointer it loaded.
\texttt{STOP\_ANALYSIS()} is a marker which prevents {\technique}'s
analysis from exploring past that point when building the CFGs from
which {\StateMachines} are constructed.  It is used here to help keep
the crash summaries generated simple and easily explained;
{\implementation} is able to reproduce and fix the bug even without
those markers\editorial{Could last time I tried it, probably worth
  checking that again with the current version.}.

\subsubsection{Generating candidate bugs}

The first step in the analysis is to build the CFG for the
\backref{crashing thread} (see
Section~\ref{sect:derive:build_static_cfg}), which in this case is
straightforward; the result is shown in
Figure~\ref{fig:eval:simple_toctou:cfg}.  This can then be compiled to
produce the {\StateMachine} shown in
Figure~\ref{fig:eval:simple_toctou:sm}.

\begin{figure}
  \subfigure[][Control flow graph.  $\varnothing$ indicates that the
    thread has left the CFG.]{
    \begin{tikzpicture}
      \node (cfg1) [CfgInstr] {\texttt{400694}: cfg1};
      \node (cfg2) [CfgInstr,below=of cfg1] {\texttt{40069b}: cfg2};
      \node (cfg3) [CfgInstr,below=of cfg2] {\texttt{40069e}: cfg3};
      \node (cfg3b) [right = of cfg3] {$\varnothing$};
      \node (cfg4) [CfgInstr,below=of cfg3] {\texttt{4006a0}: cfg4};
      \node (cfg4b) [below = of cfg4] {$\varnothing$};
      \draw[->] (cfg1) -- (cfg2);
      \draw[->] (cfg2) -- (cfg3);
      \draw[->,ifTrue] (cfg3) -- (cfg3b);
      \draw[->,ifFalse] (cfg3) -- (cfg4);
      \draw[->] (cfg4) -- (cfg4b);
    \end{tikzpicture}
    \label{fig:eval:simple_toctou:cfg}
  }
  \subfigure[][{\STateMachine}]{
    \begin{tikzpicture}
      \node (l1) at (0,2) [stateSideEffect] {\stLoad{1}{\mathrm{global\_ptr}} @ cfg1 };
      \node (l2) [stateIf, below=of l1] {\stIf{\smTmp{1} = 0}};
      \node (l4) [stateSideEffect, below=of l2] {\stLoad{2}{\mathrm{global\_ptr}} @ cfg4 };
      \node (l3) [stateTerminal, right=of l4] {\stSurvive };
      \node (l5) [stateIf, below=of l4] {\stIf{\smBadPtr{\smTmp{2}}}};
      \node (l6) [stateTerminal, below=of l5] {\stCrash};
      \draw[->] (l1) -- (l2);
      \draw[->,ifTrue] (l2) -- (l3);
      \draw[->,ifFalse] (l2) -- (l4);
      \draw[->] (l4) -- (l5);
      \draw[->,ifFalse] (l5) -- (l3);
      \draw[->,ifTrue] (l5) -- (l6);
    \end{tikzpicture}
    \label{fig:eval:simple_toctou:sm}
  }
  \caption{\backref{CFG} and {\STateMachine} for the crashing thread
    in Figure~\ref{fig:eval:simple_toctou:compiled}.}
\end{figure}

The next step is to build the \backref{interfering stores} set (see
Section~\ref{sect:derive:write_side}).  The crashing {\StateMachine}
contains two loads, at \texttt{400694} and \texttt{4006a0}, and so the
analysis will ask\editorial{ick} the \backref{program model} what
stores might interact with them, and the \backref{program model} will
use the results of the \backref{dynamic alias analysis} (see
Section~\ref{sect:program_model:dynamic_alias}) to return the set
\{\texttt{400884}, \texttt{4008fb}\}.  The \texttt{STOP\_ANALYSIS()}
markers prevent these from being clustered together, and there are no
other \backref{communicating instructions}, and so there will be two
\backref{interfering thread} CFGs, shown in
Figures~\ref{fig:eval:simple_toctou:interfering_cfg1}
and~\ref{fig:eval:simple_toctou:interfering_cfg2}.

\begin{figure}
  \begin{tabular}{cc}
    \subfigure[][CFG for interfering store \texttt{400884}]{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\texttt{400884}: cfg5};
        \node (b) [below = of a] {$\varnothing$};
        \draw[->] (a) -- (b);
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_cfg1}
    } &
    \subfigure[][{\STateMachine} for interfering store \texttt{400884}, without static analysis]{
      \begin{tikzpicture}
        \node [stateSideEffect] {\stStore{\smReg{rbp}{2}}{\mathrm{global\_ptr}} @ cfg5};
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_sm1}
    } \\
    \subfigure[][CFG for interfering store \texttt{4008fb}]{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\texttt{4008fb}: cfg6};
        \node (b) [below = of a] {$\varnothing$};
        \draw[->] (a) -- (b);
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_cfg2}
    } &
    \subfigure[][{\STateMachine} for interfering store \texttt{4008fb}]{
      \begin{tikzpicture}
        \node [stateSideEffect] {\stStore{0}{\mathrm{global\_ptr}} @ cfg6};
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_sm2}
    }
  \end{tabular}
  \caption{Interfering CFGs and {\StateMachines}.}
\end{figure}

Consider the interfering store at \texttt{400884} first.  Without the
static analysis phases, this would produce the {\StateMachine} shown
in Figure~\ref{fig:eval:simple_toctou:interfering_sm1}.  There is no
way for the program to crash due to interleaving this {\StateMachine}
with the crashing thread, as $\smReg{rbp}{2}$ is never a bad pointer,
but the {\StateMachines} do not contain enough information to show
that.  The analysis will produce the verification condition shown in
Figure~\ref{fig:eval:simple_toctou:bad_summary}. The fixed register
and static aliasing analyses will both eliminate this candidate bug,
as either is capable of showing that $\smReg{rbp}{2}$ is a pointer
into thread 2's stack frame, and hence that it is a valid pointer.
The \backref{interfering store} at \texttt{4008fb}, on the other hand,
does represent a valid bug, as interleaving it with the
\backref{crashing thread} could lead to a crash, and it produces the
\backref{verification condition} shown in
Figure~\ref{fig:eval:simple_toctou:interfering_assumption2}.

\begin{figure}
  \begin{tabular}{lll}
    \backref{CI atomic}: & $\smLoad{\mathrm{global\_ptr}} = 0$ &\!\!\!$\vee\,\, \neg\smBadPtr{\smLoad{\mathrm{global\_ptr}}}$ \\
    \backref{IC atomic}: & $\smReg{rbp}{2} = 0$                &\!\!\!$\vee\,\, \neg\smBadPtr{\smReg{rbp}{2}}$\\
    \backref{Verification condition}: & \multicolumn{2}{l}{$\happensBefore{\mai{cfg1}{1}}{\mai{cfg5}{2}} \wedge \happensBefore{\mai{cfg5}{2}}{\mai{cfg4}{2}} \wedge \smBadPtr{\smReg{rbp}{2}}  \wedge$}\\
                                      & $\smLoad{\mathrm{global\_ptr}} \not= 0$\\
  \end{tabular}
  \caption{\backref{Inferred assumption} and \backref{verification
      condition} produced using the crashing {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:sm} and the interfering
    {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:interfering_sm1}.}
  \label{fig:eval:simple_toctou:inferred_assumption1}
\end{figure}

The time taken to perform this analysis is quite modest: $0.52 \pm
0.04$ seconds for the static analysis phase and $0.18 \pm 0.01$
seconds for the {\StateMachine} analysis (mean and standard deviation
of mean for ten runs in both cases).  The dynamic analysis phase
achieved complete coverage essentially as soon as the program started.
This is a reasonable lower bound on the time which {\technique} will
take to process a very simple bug; any realistic program will take far
longer than this to process.

\subsubsection{Reproducing the bug}
This \backref{verification condition} can now be turned into a
\backref{crash enforcer}.  The only happens-before graph will be the
one shown in Figure~\ref{fig:eval:simple_toctou:hb_graph}, and it will
have the side condition that $\smLoad{\mathrm{global\_ptr}} \not= 0$.
This side condition can be evaluated completely at either
$\mai{cfg1}{1}$ or $\mai{cfg6}{2}$, and so the crash enforcement plan
will be as shown in Figure~\ref{fig:eval:simple_toctou:enforce_plan}.
In other words, whenever a program thread reaches \texttt{400694} and
global\_ptr is non-zero, the enforcer will wait for an interfering
thread to reach \texttt{4008fb}, after loading global\_ptr.  If one
does arrive, the enforcer will make the crashing thread wait for the
interfering thread to complete its store before proceeding to the load
at \texttt{4006a0}.  This will be sufficient to reproduce the bug.

\begin{figure}
  \begin{tabular}{lll}
    \backref{CI atomic}: & $\smLoad{\mathrm{global\_ptr}} = 0$ &\!\!\!$\vee\,\, \neg\smBadPtr{\smLoad{\mathrm{global\_ptr}}}$ \\
    \backref{IC atomic}: & \true\\
    \backref{Verification condition}: & \multicolumn{2}{l}{$\happensBefore{\mai{cfg1}{1}}{\mai{cfg6}{2}} \wedge \happensBefore{\mai{cfg6}{2}}{\mai{cfg4}{2}} \wedge \smLoad{\mathrm{global\_ptr}} \not= 0$}\\
  \end{tabular}
  \caption{\backref{Inferred assumption} and \backref{verification
      condition} produced using the crashing {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:sm} and the interfering
    {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:interfering_sm2}.}
  \label{fig:eval:simple_toctou:inferred_assumption2}
\end{figure}

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (l1) {\texttt{400694}: $\mai{cfg1}{1}$};
    \node[draw, below right = of l1] (l2) {\texttt{4008fb}: $\mai{cfg6}{2}$ };
    \node[draw, below left = of l2] (l3) {\texttt{4006a0}: $\mai{cfg4}{1}$ };
    \draw[->] (l1) -- (l3);
    \draw[->, happensBeforeEdge] (l1) -- (l2);
    \draw[->, happensBeforeEdge] (l2) -- (l3);
  \end{tikzpicture}
  \caption{Happens-before graph to be enforced for simple\_toctou}
  \label{fig:eval:simple_toctou:hb_graph}
\end{figure}

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (l1) {\texttt{400694}: $\mai{cfg1}{1}$};
    \node[left = 0 of l1] {$\smLoad{\mathrm{global\_ptr}} \not= 0$};
    \node[draw, below right = of l1] (l2) {\texttt{4008fb}: $\mai{cfg6}{2}$ };
    \node[right = 0 of l2] {$\smLoad{\mathrm{global\_ptr}} \not= 0$};
    \node[draw, below left = of l2] (l3) {\texttt{4006a0}: $\mai{cfg4}{1}$ };
    \draw[->] (l1) -- (l3);
    \draw[->, happensBeforeEdge] (l1) -- (l2);
    \draw[->, happensBeforeEdge] (l2) -- (l3);
  \end{tikzpicture}
  \caption{Crash enforcement plan for simple\_toctou}
  \label{fig:eval:simple_toctou:enforce_plan}
\end{figure}

This enforcer is effective at reproducing the bug.  Without the
enforcer, the time taken to reproduce the bug is roughly exponentially
distributed with $\lambda = 0.20$Hz; with the enforcer, it is roughly
normally distributed with $\mu = 1.17$, $\sigma = 0.03$\footnote{Both
  distributions were generated by running the program 100 times until
  it crashed.  The normal distribution was confirmed using the
  Anderson-Darling and Jarque-Bera tests at the 90\% level.  The
  exponential distribution was confirmed using a $\chi$-squared test
  at the 90\% level.  I also estimated the variance of the $\lambda$
  parameter using a jackknife method; the result was that $\lambda$ is
  itself normally distributed with mean 0.20Hz and standard deviation
  0.01Hz.}.  Cumulative distribution functions for these distributions
are shown in Figure~\ref{fig:eval:simple_toctou:repro_cdfs}.  The
enforcer reduced the median time to reproduce the bug from 4.93
seconds to 1.17, a four-fold reduction in the time taken.

Perhaps more interestingly, it also reduced the ninety-fifth
percentile of the time taken to reproduce the bug from 14 seconds to
1.2 seconds, a nearly twelve-fold reduction.  Without an enforcer,
this bug's reproduction time has a very long tail distribution, so
that some runs will take far longer to reproduce the bug than others.
This can make the bug difficult for a programmer to work with.  The
enforcer almost completely eliminates this behaviour.

\begin{figure}
  \include{eval/crash_times/simple_toctou}
  \caption{Reproduction times for the simple\_toctou bug, with and
    without an enforcer.  Note the log scale.  The stairstep effect is
    caused because the interfering thread runs once a second, and so
    the time to reproduce the bug is almost always near to a multiple
    of one.}
  \label{fig:eval:simple_toctou:repro_cdfs}
\end{figure}

The time taken to build the enforcer is also quite reasonable: $0.13
\pm 0.01$ seconds, again as a mean and standard deviation of ten runs.
This includes all of the time needed to devise the plan, select the
patch strategy, and actually compile the enforcer.

\subsubsection{Fixing the bug}
{\Implementation} can also generate a fix for this bug.  In this case,
the CFG fragments to be protected will be the complete CFGs of both
threads, as the first and last CFG nodes in both threads are involved
in happens-before edges.  This corresponds to modifying the program as
shown in Figure~\ref{fig:eval:simple_toctou:fix}.  This correctly
fixes the bug.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{acquire\_lock();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&t = global\_ptr;\\
        &&release\_lock();\\
        &&*t = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }\hfill %
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &acquire\_lock();\\
        &global\_ptr = NULL;\\
        &release\_lock();\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }\hfill
  \caption{The fix generated by {\implementation} for the simple\_toctou bug..}
  \label{fig:eval:simple_toctou:fix}
\end{figure}

It does, however, have a rather high performance overhead: without a
fix, the crashing thread completes $352.5 \pm 0.2
{\times} 10^6$ iterations of the loop per second; with one, it
completes $95.9 \pm 0.2 {\times} 10^6$ (mean and
standard deviation of ten runs each of ten seconds, discarding any
runs in which the unfixed program crashed, both distributions
confirmed to be normal using Anderson-Darling and Jarque-Bera at the
90\% level).  That gives an overhead of roughly a factor of 3.7.  This
is obviously rather large, but is probably close to {\technique}'s
worst case: the read-side critical section is very small and runs with
very high frequency, and so the patch must acquire and release the
lock with similarly high frequency and these lock operations dominate
the time taken.  Any realistic test would usually have much lower
overhead, assuming lock contention does not become a factor, simply
because the critical sections would run less frequently and the
overhead could be more effectively amortised.  Even in this case, a
factor of four overhead is not completely unreasonable when the
alternative is a program which crashes frequently.

For comparison, I also produced a version of the patch which does
everything except for acquiring the lock.  This version completed $321
\pm 5 {\times} 10^6$ loops per second.  This strongly suggests that
the overhead in this case is mostly caused by the cost of the
uncontended lock operations.

\todo{Worst case ignoring the loss of concurrency, of course.}

\todo{Give details of the machine the test is running on.}\smh{Yes}

\subsubsection{Indexed TOCTOU bug (indexed\_toctou)}
\label{sect:eval:indexed_toctou}

In this variant of a TOCTOU bug, there are multiple instances of the
structure which is being raced on and the bug will only manifest if
the reading and writing threads happen to coincide.  This bug
exercises the side-condition-checking part of SLI\smh{So where does
  this manifest in the fix?? (or the enforcer)??}.  The code involved
in the race is shown in Figure~\ref{fig:eval:indexed_toctou}.  Except
where otherwise noted, \verb|NR_PTRS| is set to 100.

\begin{figure}
  \begin{subfloat}
    \begin{minipage}{70mm}
\begin{verbatim}
while (1) {
  idx = random() % NR_PTRS;
  STOP_ANALYSIS();
  if (global_ptrs[idx] != NULL)
    *(global_ptrs[idx]) = 5;
  STOP_ANALYSIS();
}
\end{verbatim}
    \end{minipage}
    \caption{Read side}
  \end{subfloat}
  \begin{subfloat}
    \begin{minipage}{70mm}
\begin{verbatim}
while (1) {
  idx = random() % NR_PTRS;
  STOP_ANALYSIS();
  global_ptrs[idx] = NULL;
  STOP_ANALYSIS();
  global_ptrs[idx] = &t;
}
\end{verbatim}
    \end{minipage}
    \caption{Write side}
  \end{subfloat}
  \caption{The two sides of the indexed\_toctou bug.}
  \label{fig:eval:indexed_toctou}
\end{figure}

The summary, fix, and enforcer produced here are essentially the same
as for indexed\_toctou except for the addition of a side-condition
requiring the two indices to match up, and so are not discussed in
detail here.

With no enforcer loaded, the bug reproduction time was exponentially
distributed with $\lambda = 0.6 \pm 0.2$ per second.  With an enforcer
loaded, the reproduction time was exponentially distributed with
$\lambda = 6.87 \pm 0.01$ per second, a more than ten-fold improvement
in the reproduction rate.

I also investigated the behaviour of this test with an enforcer loaded
but no side condition checking performed.  In that case, the bug
reproduction time was again exponentially distributed, but this time
$\lambda = 0.054 \pm 0.004$ per second.  This reduced enforcer not
only fails to make the bug reproduce more quickly; it actually makes
it \emph{less} likely to be triggered, per unit time!  This is because
an enforcer without side-condition checking will often slow the
program down in order to impose the happens-before graph even in
situations where doing so is unlikely to trigger the bug, and this
causes the buggy code to run far less frequently than it otherwise
would.

\smh{Ok this is very cool!!  need to make clearer!  (earlier in text
  -- not here)}

\todo{I seem to get completely different results out of this test
  depending on the phase of the moon.  Not sure what to make of that.}

As a further test, I investigated what happens when the length of the
delays is modified, first by reducing it to between 10 and 20
milliseconds and then by increasing it to between 1000 and 2000
milliseconds.  The results are shown in
Table~\ref{table:eval:indexedtoctou:delay}.  The important point to
note here is that disabling side-condition checking and setting a
timeout of one second actually leads to a lower bug reproduction rate
than simply running without an enforcer of any sort, and increasing
the delay further would exacerbate this problem.  Of course, one to
two seconds is a rather long time to wait in order to reproduce a
race, but this might still be appropriate if the bug to be
investigated is in code which is executed very infrequently.

\begin{table}
\begin{tabular}{lll}
Delay range  & With side-condition checking          & Without side-condition checking \\
10-20ms      & $\lambda = 58 \pm 0.7/s$              & $\lambda = 5.3 \pm 1.97/s$ \\
100-200ms    & $t = 100 - 200ms$                     & $\lambda = 1.72 \pm 2.81/s$ \\
1000-2000ms  & $t = 1000 - 2000ms$                   & $\lambda = 0.13 \pm 0.001/s$ \\
\end{tabular}
\caption{Effect of side condition and delay parameter on time taken to
  reproduce the indexed\_toctou bug.}  \todo{Some of those are quite
  small sample sizes; redo.}\smh{Center; layout better}
\label{table:eval:indexedtoctou:delay}
\end{table}

As a final test, I investigated the effect changing the \verb|NR_PTRS|
parameter, and how that interacted with the delay parameter.  The
results are shown in Table~\ref{table:eval:indexedtoctou:nrptrs}.

\todo{The NR\_PTRS = 1000 configuration with no side conditions
  definitely isn't Gaussian, but I don't have enough data to say more
  than that.  Also, for the full enforcer, the NR\_PTRS = 1000 is now
  5--95\% interval, rather than full interval, because we're starting
  to get enough noise that that makes more sense.}

\todo{Not sure why the no-enforcer results aren't monotonic.}

\begin{table}
\begin{tabular}{llll}
NR\_PTRS & Full enforcer             & No side conditions          & No enforcer               \\
10       & $\lambda = 6.4 \pm 0.2/s$ & $\lambda = 1.27 \pm 0.02/s$ & $\lambda = 0.32 \pm 0.05$ \\
100      & $t = 100 - 200ms$         & $\lambda = 0.62 \pm 0.06/s$ & $\lambda = 0.16 \pm 0.02$ \\
1000     & $t = 110 - 201ms$         & $t = 33.1 \pm 8.8$          & $\lambda = 0.24 \pm 0.02$ \\
\end{tabular}
\caption{Time taken to reproduce indexed\_toctou as NR\_PTRs changes.}
\label{table:eval:indexedtoctou:nrptrs}
\end{table}

The automatic fix generator works well with this bug, and produces
roughly the same fix as it did in the simple\_toctou bug: one critical
section which covers the two critical loads in the read thread and one
which covers the critical store in the write thread.  To characterise
the performance overheads of the fix I again counted the number of
times the read and write loops execute per second with and without the
fix applied, running the test for ten seconds and discarding any runs
in which the test program crashed.  Without a fix applied, the test
completed the read loop $9.6 * 10^5 \pm 0.5 * 10^5$ times per second
and the write loop $9.0 \pm 0.2 * 10^5$ times per second; with a fix,
it completed $8.3 * 10^5 \pm 0.3 * 10^5$ read loops and $7.8 * 10^5
\pm 0.2 * 10^5$ write loops (mean and standard deviation of ten runs).
The overhead was therefore roughly 15\% on both the read and write
sides of the test\editorial{Should really put some error bars on
that.}.  This is far smaller than the factor of four reported in the
simple\_toctou case, largely because the test loop in this case
includes a call to \verb|random|, which is rather expensive relative
to simple lock operations and helps to amortise the cost of the
additional synchronisation.\smh{Graphs are better -- visual}

\todo{I don't believe for a minute that those distributions are Gaussian.}

\subsubsection{Biassed indexed TOCTOU bugs (read\_indexed\_toctou, write\_indexed\_toctou)}

These bugs are essentially similar to the indexed\_toctou, except with
additional delays such that either the write operation (for
write\_indexed\_toctou) or the read operation (for
read\_indexed\_toctou) is far more common than the other.  This test
illustrates the importance of rebalancing delays between the various
happens-before edges in the crash enforcement plans.\smh{Phar exp?}

Without an enforcer, these bugs failed to reproduce within the three
minute timeout in any of twenty runs.  With a full enforcer,
write\_indexed\_toctou reproduced consistently in roughly 1.16 seconds
while read\_indexed\_toctou's reproduction time was bimodally
distributed between 0.14 seconds ($p = 0.55$) and 1.16 seconds ($p =
0.45$).  With an enforcer modified to always impose delays on the send
side, read\_indexed\_toctou reproduced after 83 seconds and
write\_indexed\_toctou after 1.15.

With delay always on the receive: bug3 takes 0.147s, bug4 takes more
than a minute and then I got bored.

\todo{Repeat experiments.}

\todo{Integrate these into the main summary table.}

\subsubsection{Multi-variable consistency constraint (multi\_variable)}

\todo{I'm tempted to re-work this so that it only generates a single
  bug, just to make things a bit clearer, and also to avoid overlap
  with the multi\_bugs test.}

\begin{figure}
  \begin{minipage}{70mm}
\begin{verbatim}
while (1) {
   STOP_ANALYSIS();
   v1 = global1;
   v2 = global2;
   assert(v1 == v2);
   STOP_ANALYSIS();
   usleep(10ms);
}
\end{verbatim}  
    \caption{Read side}
  \end{minipage}
  \begin{minipage}{70mm}
\begin{verbatim}
while (1) {
  usleep(100ms);
  STOP_ANALYSIS();
  global1 = 5;
  global2 = 5;
  STOP_ANALYSIS();
  usleep(100ms);
  STOP_ANALYSIS();
  global1 = 7;
  global2 = 7;
  STOP_ANALYSIS();
}
\end{verbatim}
    \caption{Write side}
  \end{minipage}
  \caption{The two sides of the multi\_variable bug.}
  \todo{How were those delays chosen?}
  \label{fig:eval:multi_variable}
\end{figure}

This bug is intended to illustrate that {\technique} is able to
correctly analyse multi-variable atomicity violations.  The code for
the two sides of the bug is shown in
Figure~\ref{fig:eval:multi_variable}.  Note that in this case the race
leads to an assertion failure, whereas previous bugs lead to a bad
pointer dereference.

In this case, {\technique} reports two bugs in the program: one which
corresponds to interleaving the read side of the test with the two
stores which set the global variables to 5 and another which
corresponds to interleaving it with the stores which set the global
variables to 7.  These are both real bugs.  Both are successfully
converted into enforcers and both enforcers cause their matching bugs
to reproduce quickly.

It is also possible to build a single combined enforcer which attempts
to enforce both bugs.  The combined enforcer consistently caused the
first bug to reproduce, as the enforcer for that bug consistently
reproduces its bug on the first attempt, causing the program to crash
before the second enforcer has the opportunity to run.

These bugs can also be converted into fixes.  In the case of the
individual bugs this is unhelpful: one of the bugs is successfully
fixed, but the other bug remains, and the program still crashes very
quickly.  The combined fix, on the other hand, does successfully
prevent both bugs.

This bug could not be handled by tools such as Kivati\needCite{}, as
it involves multiple variables, but can be handled by {\technique}:
the bug is reproduced quickly by the enforcer, and a correct and
low-overhead fix is generated automatically.

\todo{I really need to set those delays to something less crazy so as to
  make this eval less completely bloody pointless.}

\subsubsection{Context-dependent races (context)}

This test is similar to simple\_toctou, except that the behaviour
depends on the function's call context.  It is structured like this:

\begin{verbatim}
f(int **ptr) {
    if (*ptr)
        **ptr = 5;   
}
read_thread() {
    while (1) {
        if (random() % 1000 == 0) {
            STOP_ANALYSIS();
            f(&global_ptr1);
            STOP_ANALYSIS();
        } else {
            STOP_ANALYSIS();
            f(&global_ptr2);
            STOP_ANALYSIS();
        }
    }
}        
write_thread() {
    while (1) {
        STOP_ANALYSIS();
        global_ptr1 = &t;
        STOP_ANALYSIS();
        sleep(1ms);
        STOP_ANALYSIS();
        global_ptr1 = NULL;
        STOP_ANALYSIS();
    }
}
\end{verbatim}
\smh{Maybe indicate there is a different bug?  e.g. a box around
  relevant lines? (can explain in text that in practice its above via
  a macro)}

Here, there are two calls to \verb|f|, only one of which could ever
possibly exhibit the bug, and that call is much less frequent than the
other one.  Reproducing the bug quickly is therefore only possible if
the crash enforcer checks the calling context before inserting any
large additional delays.

Without an enforcer, this test program crashes reasonably
infrequently, reaching the three minute timeout six times out of
twenty runs and taking an average of 80 seconds in the remaining
cases\editorial{Maybe fit a truncated exponential on those so as to
get $\lambda$ parameter out?  Probably not worth it.}.  With a full
enforcer loaded, the time taken to obtain a reproduction is uniformly
distributed between 100 and 200 milliseconds.  With an enforcer
modified to nor perform stack context checking, the bug did not
reproduce within three minutes.

\todo{Run this experiment properly, with a sensible number of repeats
  and the full statistical thing.}

\subsubsection{Write-to-read hazard (write\_to\_read)}

This test demonstrates a bug on the edge of what SLI can handle.
Because of the W isolation property in the bug definition, SLI cannot
handle any bugs in which the write thread reads a memory location
which has been written to by the read thread.  It is, however,
supposed to be able to handle the case where the read thread reads a
location which has been written by the read thread, and in particular
the case where a write in the read thread races with a write in the
write thread and the result is then read by the read thread.  This
test demonstrates that case.

(In other words, there is a write-to-read hazard in the read thread
which is potentially interrupted by the write thread.)

The read side is simple:

\begin{verbatim}
global_ptr = &t;
*global_ptr = 5;
\end{verbatim}

The write side is then this:

\begin{verbatim}
global_ptr = NULL;
\end{verbatim}

Both sides run in a tight loop without any delays.  Despite this, it
still often takes several seconds for the bug to reproduce without an
enforcement patch applied, during which time the two critical sections
may run hundreds of millions of times.  A more realistic test would
run the two sections far less frequently, and so might require
hundreds of years of CPU time to reproduce the bug.  The SLI-generated
enforcer patch, by contrast, can reliably reproduce this bug in a few
hundred milliseconds.

{\Technique} can also generate a fix for this bug, although given the
difficultly of reproducing it that is perhaps less than useful.
Manual inspection nevertheless suggested that the generated fix was
correct and reasonable.

\todo{I think it might be interesting to investigate this a bit more.
  I have a sneaking suspicion that it won't reproduce at all if you
  run on bare metal without any interrupts, and that it pretty much
  only reproes if you get a timer interrupt at precisely the right (or
  wrong) place.  Could maybe get a paragraph or so of useful
  discussion out of that if I had some evidence.  Plus, it's the kind
  of experiment which looks impressive but is actually really easy,
  so, you know, win.}

\subsubsection{Multiple bugs (multi\_bugs)}

This test combines simple\_toctou and write\_to\_read into a single
test, and demonstrates SLI's ability to exercise several bugs using a
single enforcer.  This is useful when testing with large programs;
something like mysqld can easily generate thousands of candidate bugs,
and having to test each one individually would become quite tedious,
but testing dozens at a time is far more reasonable.

The read side of the test is structured like this:

\begin{verbatim}
while (1) {
    r = select_test();
    STOP_ANALYSIS();
    if (r)
        simple_toctou();
    else
        write_to_read();
    STOP_ANALYSIS();
}
\end{verbatim}

The behaviour of \verb|select_test| is configurable at run time.  It
can either choose between the tests at random, always select one, or
always select the other.  The write side of the test is structured
like this:

\begin{verbatim}
for (i = 0; i < 2000000; i++) {
    STOP_ANALYSIS();
    write_to_read_write_side();
    STOP_ANALYSIS();
}
STOP_ANALYSIS();
simple_toctou_write_side();
STOP_ANALYSIS();
\end{verbatim}

The constant \verb|2000000| was chosen such that with no enforcer
applied and with the read side configured to choose the two tests with
equal probability the two bugs reproduce with roughly equal frequency.

In this test, SLI correctly generates two candidate bugs (one for each
component bug).  These can either be instantiated into separate
enforcers (which have the expected behaviour of making one bug
reproduce quickly while having no effect on the other) or combined
into a single enforcer.  If the test is configured to always use one
bug or the other then the combined enforcer consistently reproduces
that, as desired.  Perhaps more surprisingly, if the test is
configured to select a sub-test randomly then the enforcer always
reproduces the write\_to\_read bug, simply because the enforcer
reproduces the bug quickly enough that the write side of the
simple\_toctou write side never runs.

Likewise, both candidate bugs can be instantiated into individual
fixes, and both fixes correctly fix their own bug while leaving the
other bug in place.  Alternatively, a combined fix can be generated,
and this successfully fixes both bugs.

\subsubsection{Multiple read threads (multi\_threads)}

This test is a variant of the indexed\_toctou test in which there are
32 read-side threads and 32 write-side ones, all operating
independently.  This test is intended to show that {\implementation}
correctly handles programs with more than two threads.  Without an
enforcer, the bug reproduces at a rate of roughly $\lambda = 0.15$ per
second.  The behaviour with an enforcer loaded is more complex.  With
high probability (95\%), the bug is reproduced in under a second, but
there is a long tail in which the bug might take several minutes to
occur.  \todo{Need to figure out what the hell is going on there.}

{\Technique} can successfully generate a fix for this bug.

\todo{Currently running all of these tests on a four-processor machine,
  which might have an interesting effects on the results of a
  64-thread test.}

\subsubsection{Complicated happens-before graphs (complex\_hb)}

This test is intended to illustrate {\technique}'s ability to handle
more complicated happens-before graphs which require more than two
context switches.  The test is structured like so:

\begin{verbatim}
while (1) {
   STOP_ANALYSIS();
   x' = x;
   x'' = x;
   x''' = x;
   assert(!((x' == 0) && (x'' == 1) && (x''' == 2) ));
   STOP_ANALYSIS();
}
\end{verbatim}

And the write side is:

\begin{verbatim}
while (1) {
  STOP_ANALYSIS();
  x = 0;
  x = 1;
  x = 2;
  STOP_ANALYSIS();
  delay();
}
\end{verbatim}

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (x0) {x = 0};
    \node[draw, below right = of x0] (xp) {x' = x};
    \node[draw, below left = of xp] (x1) {x = 1};
    \node[draw, below right = of x1] (xpp) {x'' = x};
    \node[draw, below left = of xpp] (x2) {x = 2};
    \node[draw, below right = of x2] (xppp) {x''' = x};
    \draw[->, happensBeforeEdge] (x0) -- (xp);
    \draw[->, happensBeforeEdge] (xp) -- (x1);
    \draw[->, happensBeforeEdge] (x1) -- (xpp);
    \draw[->, happensBeforeEdge] (xpp) -- (x2);
    \draw[->, happensBeforeEdge] (x2) -- (xppp);
    \draw[->] (x0) -- (x1);
    \draw[->] (x1) -- (x2);
    \draw[->] (xp) -- (xpp);
    \draw[->] (xpp) -- (xppp);
  \end{tikzpicture}
  \caption{Happens-before graph necessary to reproduce the complex\_hb
    bug.}
  \label{fig:eval:complex_hb}
\end{figure}

In this case, the bug will only reproduce if the happens-before graph
is as shown in Figure~\ref{fig:eval:complex_hb}.  Note that this graph
requires four happens-before edges.  This means that it would be
difficult for tools such as Chess\needCite{} to handle, as, in its
default configuration, Chess only considers executions with up to
three context switches.  This construction can be easily extended to
require arbitrarily many context switches, and so even if Chess were
configured to use a higher threshold it would still be possible to
defeat it using bugs of this type.

The evaluation includes two variants of this bug; the one illustrated
above, which requires four happens-before edges, and an extended
version which requires 10 edges.  Both of them are analysed reasonably
quickly by {\implementation} (200ms in the 4-edge case, versus 74
seconds in the 10-edge one), and converted into an enforcer which
allows the bug to be reproduced in just a few hundred milliseconds.

\subsection{Summary tables}

\todo{Need to think harder about what variants of the enforcers I want to include.
  At the moment I do a full enforcer and an enforcer without
  side-condition checking for every test, which is kind of silly
  because most of the time that just shows that enforcers without
  side-condition checking don't work very well.  It might be more
  useful to try different timeout strategies instead.}

Tables~\ref{table:eval:analysis_phases}, \ref{table:eval:crash_times}
and \ref{table:eval:perf_dists} summarise the results of these
experiments.  Several important conclusions can be drawn from these
tables:

\begin{itemize}
\item
  The analysis performed by {\technique} is, for the most part, very
  quick on these programs, generally requiring less than a second.
  The only exception is the complex\_hb test with ten happens-before
  edges, which requires a little over a minute to perform its initial
  analysis; even this is not completely unreasonable given the
  complexity of the test.
\item
  Converting the candidate bugs generated by the initial analysis
  fixes completes in all cases in under a second.
\item
  Converting the candidate bugs into enforcers is also very quick,
  mostly completing in under a second.  The only exception is,
  again, complex\_hb with ten happens-before edges, which takes
  roughly a minute.
\item
  The enforcers are all able to reproduce their intended bugs
  quickly, in all cases much more quickly than the original program.
\item
  {\Technique} can automatically generate fixes for all of the bugs,
  and those fixes have acceptably low overhead.
\item
  The DataCollider-like tool is effective for some bugs and
  ineffective for others.  Where it is effective, it is generally less
  effective than the {\technique}-generated enforcers.  The only
  exception is the multi\_variable test, where the {\technique}
  enforcer reproduces the bug roughly twice per second and the
  DataCollider-like tool reproduces it roughly seven times per
  second.  \todo{Need to figure out what's going on there.}
\end{itemize}
        
I also show cumulative distribution functions of the time which the
various test cases take to reproduce their respective bugs in
Figure~\ref{fig:eval:artificial:crash_times_cdfs}.

\begin{sidewaystable}
\input{eval/analysis_phase_times}
\caption{Time taken for the various initial analysis phases}
\todo{Not convinced that changing the number of sig figs like that is entirely wise,
  but it does match up with the derived sd.  Also, not sure what's
  going on with indexed\_toctou and read\_indexed\_toctou, or why
  write\_indexed\_toctou behaves so much differently.}
\label{table:eval:analysis_phases}
\end{sidewaystable}

\begin{sidewaystable}
\input{eval/crash_times}
\caption{Time taken for the test programs to crash under various configurations}
\label{table:eval:crash_times}
\end{sidewaystable}

\begin{sidewaystable}
\input{eval/perf_dists}
\caption{Performance overheads of the generated fixes.}  \todo{Having spent bloody
ages worrying about statistical things, the overhead column is just from
the ratio of means, with no though given to the error distribution.  Need to think
about that some more.}
\label{table:eval:perf_dists}
\end{sidewaystable}

\input{eval/crash_times_cdfs/cdfs}

\section{Semi-artificial bugs}

These are real programs which I've modified to introduce new bugs.  So
far, there's only one in this category, and it's really not terribly
convincing.

\subsection{Bayes}

This is the Bayes test from the STAMP benchmark suite, modified to
remove one if its critical sections.  The critical section looks like
this:

\begin{verbatim}
        acquire_global_lock();
        taskPtr = TMpopTask(TM_ARG  taskListPtr);
        release_global_lock();

learner_task_t*
TMpopTask (TM_ARGDECL  list_t* taskListPtr)
{
    learner_task_t* taskPtr = NULL;

    list_iter_t it;

    TMLIST_ITER_RESET(&it, taskListPtr);

    if (TMLIST_ITER_HASNEXT(&it, taskListPtr)) {
        taskPtr = (learner_task_t*)TMLIST_ITER_NEXT(&it, taskListPtr);
        bool_t status = TMLIST_REMOVE(taskListPtr, (void*)taskPtr);
        assert(status);
    }

    return taskPtr;
}
static bool_t TMLIST_REMOVE(list_t* listPtr, void* dataPtr)
{
    list_node_t* prevPtr;
    list_node_t* nodePtr;

    prevPtr = TMfindPrevious(TM_ARG  listPtr, dataPtr);

    nodePtr = prevPtr->nextPtr;
    if ((nodePtr != NULL) &&
        (listPtr->compare(nodePtr->dataPtr, dataPtr) == 0))
    {
        prevPtr->nextPtr = nodePtr->nextPtr;
        nodePtr->nextPtr = NULL;
        free(nodePtr);
        listPtr->size -= 1;
        assert(listPtr->size >= 0);
        return TRUE;
    }

    return FALSE;
}
\end{verbatim}

I have simplified the code presented slightly by removing parts
related to transactional memory, as these are not relevant for this
test.  I made this program buggy by removing the global lock around
\verb|TMpopTask|.  The resulting program crashed in one of a number of
ways, depending on how the various races played out:

\begin{itemize}
\item The \verb|assert| in \verb|TMpopTask| sometimes fired.
\item The \verb|assert| in \verb|TMLIST_REMOVE| sometimes fired.
\item The \verb|free| in \verb|TMLIST_REMOVE| sometimes caused a
  double-free error in the C library.
\item Other assertions later in the program also occasionally fired
  due to running the same task twice.
\end{itemize}

The first two bugs are of the correct form to be investigated by
{\technique}; the other two are not.  I therefore generated enforcers
for the first two bugs which, as expected, caused those bugs to
reproduce rapidly and reliably (i.e. when one of those enforcers was
applied the target bug consistently reproduced before any of the other
bugs had a chance to).  I also generated fixes for the two bugs.
These both worked in the sense that they were able to prevent the
target bug from reproducing.  Unfortunately, the fix for the first bug
also made the double-free bug reproduce far more frequently, and so
the overall effect was that the program crashed more quickly.  On the
other hand, the fix for the second bug fixed all four bugs, as the
critical sections introduced by the fix were large enough to
completely cover the actual critical sections in the program.

\todo{More discussion of how the fixes work should go here.}

\todo{Somewhat surprisingly, given that it was taken from what's
  nominally a benchmark suite, unmodified bayes shows multiple order
  of magnitude performance differences from run to run in the default
  configuration, which makes it quite hard to talk about the
  performance cost of the fix.  I could just average enough runs that
  the SD drops to something sane, but I don't think that would be
  terribly meaningful.}

\subsection{glibc}

\verb|glibc| is a kernel of glibc bug 2644 \cite{glibc2644}, which
affected versions of glibc up to 2.5 and could lead to a crash if
multiple threads were shut down at the same time.  A simplified
version of the code involved is shown in Figure~\ref{fig:glibc}, where
\verb|forcedunwind| and \verb|done_init| are global variables.  Note
that the bug here depends on the compiler's optimizer, and is not
apparent at the source-code level\footnote{Unfortunately, only the
  32-bit x86 version of gcc optimizes the function like this, and our
  implementation of SLI assumes a 64-bit x86 program, and this
  prevented us from testing with the real bug.}.  SLI operates
entirely at the machine-code level, and so this does not present any
additional complexity.

\begin{figure*}
  \begin{subfloat}
    \begin{minipage}{52mm}
\begin{verbatim}
_Unwind_ForcedUnwind() {
   if (forcedunwind == NULL)
      pthread_cancel_init();
   forcedunwind();
}
pthread_cancel_init() {
   if (done_init) return;
   forcedunwind =
     _forcedunwind_impl;
   done_init = 1;
}
\end{verbatim}
    \end{minipage}
    \caption{Before optimizations}
  \end{subfloat}
  \begin{subfloat}
    \begin{minipage}{52mm}
\begin{verbatim}
  _Unwind_ForcedUnwind() {
1:  l = forcedunwind;
2:  if (l == NULL &&
3:      done_init) {
4:    forcedunwind = l =
5:       _forcedunwind_impl;
6:    done_init = 1;
7:  }
8:  l();
  }
\end{verbatim}
    \end{minipage}
    \caption{After optimizations}
  \end{subfloat}
  \begin{subfloat}
    \begin{minipage}{35mm}
\begin{verbatim}
    while (1) {
10:   pthread_barrier_wait();
11:   _Unwind_ForcedUnwind();
12:   pthread_barrier_wait();
13:   done_init = 0;
14:   forcedunwind = NULL;
    }
\end{verbatim}
    \end{minipage}
    \caption{Test harness}
  \end{subfloat}
  \label{fig:glibc}
  \caption{Source code for the glibc test case.}
\end{figure*}

\section{Bugs from real programs}

\subsection{Thunderbird}

\todo{Rewrite to fit new structure}

\verb|thunderbird| is Mozilla bug number
391259\cite{thunderbird39125}, a simple time-of-check, time-of-use
race in the IMAP client component of Thunderbird, a popular
open-source e-mail client.  We modified Thunderbird to include some
additional debugging messages and used a custom scheduler in order to
make the bug reproduce more readily; the test is otherwise identical
to the behavior which a user might have encountered.  The relevant
parts of the program are as follows:

\begin{verbatim}
void nsImapProtocol::CloseStreams() {
  if (m_transport)
      m_transport = nsnull;
}
PRBool nsImapProtocol::ProcessCurrentURL() {
  if (m_transport)
    m_transport->SetTimeout(
      TIMEOUT_READ_WRITE, PR_UINT32_MAX);
}
\end{verbatim}

\noindent
If \verb|m_transport| is set to \verb|nsnull| by \verb|CloseStreams()|
in between the two accesses in \verb|ProcessCurrentURL| then the
program will crash.  This is essentially the same bug as
\verb|toctou|, but embedded in a much large program.  As such, the
final result is similar: a single suggested fix, with two critical
sections, one containing the two accesses in \verb|ProcessCurrentURL|
and one containing the assignment in \verb|CloseStreams|.  This fixes
the bug.

\subsection{mysql}

Mysql 56324.  Read side:

\begin{verbatim}
void my_thread_end(void)
{
...
  if (PSI_server)
    PSI_server->delete_current_thread();
}
\end{verbatim}

Write side:

\begin{verbatim}
int mysqld_main(int argc, char **argv)
{
....
  if (PSI_server)
  {
    PSI_server->delete_current_thread();
    PSI_server= NULL;
  }
  mysqld_exit(0);
}
\end{verbatim}

And the race is then on \verb|PSI_server|.  This one actually works;
woo.  We find the bug from the whole-program analysis, and it then
gets turned into an enforcer, and the enforcer then makes it
reproduce.  Fix generation also works.

Possibly more interesting: there are multiple very similar bugs
scattered throughout the program, and it found all of the ones I knew
about before I started and a couple which I didn't know about.

Complication: this bug is only present in builds of mysql which don't
have compiler optimisations, because otherwise the compiler caches
\verb|PSI_server| in a register and avoids the crash.

\todo{Need to finish running this on the optimised build.}

\todo{Might be worth pointing out that the fix can lead
  delete\_current\_thread() running an additional time, and then give
  the definition of that function to show that the fix is correct
  anyway?}

\subsubsection{Effect of the induction rule}

Without the induction rule, the tool produced 7935 candidate bugs.
With the induction rule enabled, the tool produced 7783 bugs, or about
a 2\% reduction.  That's not great, but it's not completely terrible
either.

Figure~\ref{fig:eval:mysql:analysis_cdf} shows a CDF of the time taken
to analyse each instruction in an optimised build of mysql, with and
without the induction rule.  As can be seen, the two lines are very
close together, indicating that the induction rule is not usually very
expensive.  It is thus justifiable to use it in spite of the
relatively minor improvement in the final results.

\begin{figure}
\include{eval/mysql-opt}
\caption{CDF of time taken to analyse instructions in an optimised
  build of mysql, with and without the induction rule, with an
  analysis window of twenty instructions.  The analysis window was
  twenty instructions and the per-instruction timeout was 180 seconds.
  Note logarithmic x scale.} \todo{Include some other analysis window
  sizes.}
\label{fig:eval:mysql:analysis_cdf}
\end{figure}
  
\section{Validation of tool implementation}

\subsection{Static analyses}

SLI relies on two forms of whole-program static analysis applied to
the target binary before the main analysis starts:

\begin{itemize}
\item
  The simple points-to analysis.
\item
  An analysis to recover the offset between RSP and RBP, where that is a constant.
\end{itemize}

Both analyses assume that the program to be analyses conforms to the
system ABI.  If that assumption does not hold, or if there is simply a
bug in one of them, then that might invalidate all of the other
results.  I therefore developed some Valgrind-based dynamic analyses
to check that the results of this phase were correct.

\todo{The tool also implements a register liveness analysis, but
  that's only used to build the points-to table, so validating the
  points-to table also implicitly validates the liveness one.  Also,
  I've not described the liveness analysis anywhere, so it'd be hard
  to wedge it in here.}

\subsubsection{Points-to analysis}
\label{sect:eval:validate:pta}
\todo{Shrink this section.}

The static points-to analysis builds an instruction attribute table
for the program which includes, for each instruction:

\begin{itemize}
\item
  Whether the current stack frame might include any pointers to itself
  or to memory outside of the current frame.
\item
  Whether there might be any pointers to the current stack frame in
  from outside of it.
\item
  For each register, a flag saying whether that register might point
  at the current stack frame or to memory.
\end{itemize}

The ``current stack frame'' here is defined to be the region of memory
between \verb|RSP-128| and the value of \verb|RSP| at the time of the
immediately enclosing \verb|call| instruction\footnote{128 bytes is
the size of stack red zone, which effectively forms part of the
current stack frame.}\editorial{talk about effects of tail
calls}\editorial{i.e. current RSP versus RSP at time of last call;
could maybe be clearer about that.}.  The tool to check this analysis
has several parts:

\begin{itemize}
\item
  It must track the extent of the current frame; this is
  straightforward, since the analysis can always see the value
  of \verb|RSP| and all \verb|call| and \verb|ret| instructions.
\item
  It checks, at the start of each instruction, whether any registers
  currently point into the current frame, and, if so, whether that is
  allowed by the instruction attribute table.
\item
  It attempts to track directly whether there exist any pointers to
  the current frame, whether in the frame or outside of it.  This part
  of the analysis assumes that there are no pointers into a frame when
  it is created at the start of a function and then monitors all
  stores to detect when such pointers are created.  This information
  then allows the analysis to directly check the
  might-be-pointer-to-frame flags in the instruction attribute table.
\item
  That assumption holds for most well-behaved programs, but is not
  absolutely guaranteed.  The dynamic analysis therefore also checks
  all load operations to confirm that they only return pointers to the
  current stack frame when the static analysis allows the loaded
  memory to contain pointers to the frame.

  There might, of course, be pointers into the current stack frame
  which are never loaded, but (assuming there are no cross-thread
  stack accesses) they can never be dereferenced, and so don't
  actually matter.
\end{itemize}

This flagged a number of minor problems with the analysis:

\begin{itemize}
\item
  Pointers to the stack frame can sometimes be left behind in dead
  registers, and in particular in call-clobbered registers after
  function calls.  Correct programs which conform to the ABI will
  never make use of the values of these registers, and the static
  analysis makes use of that fact, but it is hard for a dynamic
  analysis to determine when a register is dead.  The solution is
  simple: have the dynamic analysis overwrite all such registers with
  poison values when functions return.  If the program does conform to
  the ABI then this will have no effect, but if it makes use of the
  theoretically-dead values then its behaviour will change.  I
  repeated the analysis with three different poison values: zero, a
  small number which was not a valid pointer, and a large value which
  was not a valid pointer.

  This revealed a single place which did not conform to the ABI in the
  desired way: glibc's internal pthread locking functions are
  guaranteed to never clobber \verb|RSI|, and glibc's syscall stubs
  make use of this in a number of places\editorial{Cite, maybe?  It'll
  be pointing at source rather than a document saying exactly what's
  going on, but at least it's something.}.  This particular static
  analysis is only applied to the program's main binary, and not any
  of the libraries which it is dynamically linked against, and so this
  is not a particular problem.

\item
  \verb|alloca|

\begin{verbatim}
>   9e56c7:       48 29 c4                sub    %rax,%rsp
>   9e56ca:       48 89 e0                mov    %rsp,%rax
>   9e56cd:       48 83 c0 0f             add    $0xf,%rax
>   9e56d1:       48 c1 e8 04             shr    $0x4,%rax
>   9e56d5:       48 c1 e0 04             shl    $0x4,%rax
>   9e56d9:       48 89 45 a8             mov    %rax,-0x58(%rbp)
\end{verbatim}

\todo{Crap, my argument for why this doesn't matter doesn't actually work.  Need to rethink that one.}
\end{itemize}

\subsubsection{RBP offset}

The main analysis removes references to the function frame pointer, if
present, by replacing them with references to the stack pointer.  This
relies on a static analysis which determines, for each instruction in
the program, the offset from the \verb|RBP| register to the stack
pointer (assuming that that's a constant).  This dynamic analysis
checks, at the end of every instruction, that the actual offset
matches the value in the database.  This analysis did not reveal any
important bugs in the algorithm\footnote{Beyond a few implementation
  errors which are fixed in the code used for this evaluation.}.

\subsubsection{CFG generation}

For the bug-detecting mode to hope to detect every bug, the CFG
generation process must be able to generate CFGs which represent all
dynamic fragments of the program of the desired length which either
end in a memory-accessing instruction (for probe CFGs) or start and
end with a store (for store CFGs).  This dynamic analysis attempts to
validate that by capturing a large pool of dynamic traces from the
program and then checking that CFG generator can generate the trace.
Ideally, it would capture every such trace from an execution, but that
has sufficiently high performance overhead that it would be difficult
to exercise a broad cross-section of the program's behaviour while
running such an analysis.  Instead, the analysis applies several
filters to try to obtain a reasonably representative sample:

\begin{itemize}
\item
  Only traces which end in a non-stack memory-accessing instruction
  are considered.
\item
  Amongst those samples, only one in a thousand is used.  This is
  implemented by only sampling if a randomly-generated number is
  congruent to zero modulo a thousand, rather than taking every
  thousandth trace, so as to avoid possible aliasing effects with the
  program's structure.
\item
  I attempt to increase the likelihood of rare traces being sampled
  using a bloom counter table.  This consists of 131072 saturating
  7-bit counters.  When the dynamic analysis is determining whether to
  sample a given trace, it hashes it to select one of these counters,
  then generates a random number, and only takes the trace if the
  random number modulo the counter plus one is zero.  It then
  increments the counter.  This helps to increase the likelihood of
  moderately rare traces being included in the final sample.
\end{itemize}

The end result of this dynamic analysis is a large set of short
fragments of the program's execution.  Each such fragment is
considered in isolation, and appropriate\editorial{?} instructions
from it fed into the CFG generating algorithm.  The CFG can then be
checked to ensure that it includes the desired trace.  This analysis
did not find any problems with the algorithm.

\todo{Should really try to do something to convince myself that the
  sanity checker works.  Collecting more stats on the trace pool
  generated would be good, as would some sensitisation on those
  parameters.}

\subsection{{\STateMachine} generation}

Don't really have a plan for this; might just say that it's a trivial
wrapper for libVEX.

\subsection{Dynamically-collected aliasing model}

This is pretty much what it is; not sure there's a great deal to say
here.  If I had infinite time I could hack up gcc to try to do a
similar analysis at the source level, but I don't, so I can't.  Only
real alternative is to look at the convergence rate.

Things to look at:

\begin{itemize}
\item How fast we add new edges to the aliasing table while running
  the analysis, as a function of time.  You'd hope that this will fall
  very quickly as the analysis runs.
\item mysqld has a reasonably good test harness.  It'd be interesting
  to try running each of the tests in that in some suitable order and
  see how many of them you need in order to get good coverage.
  Probably want to try to normalise that against the actual number of
  distinct instructions run to make it fair.
\item Two things are worth looking at here: the rate at which we add
  edges in toto, and the rate at which we add edges between
  already-known instructions.  The latter is much more important than
  the former, because the first one includes discovering new code for
  the first time, and it's pretty forgivable to not include
  instructions which are never run, whereas the latter is new
  interactions between code which we already know about.
\end{itemize}

\subsection{Symbolic execution engine}

No idea how to validate this one.  It's actually very simple, so I'm
already pretty certain it's mostly correct.

\subsection{{\STateMachine} simplification}

\todo{The problem with this stuff is that there's a lot of machinery
  involved, but the punch line is just that everything works fine,
  which is somewhat unsatisfying.}

\todo{Also, I've not run this in ages, so I should probably do that
  again and see if it turns up anything interesting.}

The {\StateMachine} simplification passes are both complicated and
critical to SLI's correctness; validating that they are correct is
therefore important.  To do so, I collected a selection of pre- and
post-simplification machines from a number of runs, evaluated them in
identical initial configurations, and confirmed that they produced
identical results.  Generating the initial conditions is non-trivial.
At first I simply generated them completely at random, but found that
it required an unreasonably large number of such random configurations
to achieve good coverage of the \StateMachines' behaviour.  This is
because the vast majority of paths through most machines ultimately
report that the machine does not crash, and so a uniform random
sampling of initial configurations will overwhelmingly sample
configurations in which the bug of interest does not reproduce.  This
is unfortunate, as the situations in which the bug does reproduce tend
to be more interesting.

It is worth considering this is slightly more detail.  A reasonably
typical \StateMachine might look like this:

\begin{verbatim}
if (rax == 0) survive();
if (!BadPtr(rax)) survive();
crash();
\end{verbatim}

Here, \verb|rax| is a 64-bit value, and so if its value is randomly
chosen then the {\StateMachine} is highly unlikely to make it past the
first state, which would not constitute a good test.  Of course, it
would be possible to bias the distribution to produce zeroes more
often than other values, and hence avoid this issue.  This works, in
the sense that it makes it easy to generate an effective test suite
for this \StateMachine, but is lacking in two important respects.
First, it is unclear what other special cases might be needed;
generating zero is an obvious thing to test, as is generating one or
minus one, but what if there were a bug in the optimiser when
\verb|rax| is near to the top of its range?  or has only bits in the
top byte set?  or is a large prime number?  These questions would make
it difficult to have faith that such an approach had generated a
reasonably complete set of initial configurations for testing.
Second, considering each register independently is inherently
inefficient, due to the common structure of \StateMachines.  Consider
this machine, for example:

\begin{verbatim}
if (rax % 2 == 0) survive();
if (rbx % 2 == 0) survive();
if (rcx % 2 == 0) survive();
if (rdx % 2 == 0) survive();
crash();
\end{verbatim}

This reflects the common structure of \StateMachines, in which a long
prefix of initial tests capture ways in which the program might avoid
running the code which is suspected of having a bug before the
interesting part of the \StateMachine starts.  One reasonable set of
initial configurations might be:

\begin{itemize}
\item $rax = 0$
\item $rax = 1$, $rbx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 1$, $rdx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 1$, $rdx = 1$
\end{itemize}

A total of five initial configurations; one for each possible final
state.  A randomly-generated configuration, by contrast, will have a
one in sixteen chance of generating each final state, and will require
an average of a little over twenty-five input states in order to cover
every final state, a factor of five worse than the desired
set\editorial{25 from some Python simulations; not sure it's worth
  trying to justify it further.}.  The difference will be larger in
more complex machines, and so coverage will be worse for them, but
those are precisely the ones which are most likely to reveal optimiser
bugs.

I therefore used a slightly more complicated scheme for generating the
initial configurations:

\begin{itemize}
\item[1] First, execute the unoptimised machine in the symbolic
  execution engine, collecting all of the path constraints generated.
  There will be one such constraint for each path (loosely defined)
  which the symbolic execution engine finds through the machine,
  whether that path ultimately crashes, survives, or escapes (due to
  e.g. dereferencing a bad pointer).
\item[2] Pick a constraint which is not satisfied by any
  configuration currently present in the initial configuration set.
  Attempt to generate a configuration which satisfies it and add it to
  the set.  If there are no constraints which are never satisfied,
  pick one which is satisfied in every initial configuration and try
  to generate a configuration which does not satisfy it.
\item[3] Repeat 2 until every constraint is satisfied in at least one
  initial configuration and not satisfied in at least one other
  configuration.
\item[4] Execute the optimised machine in the symbolic execution
  engine, generating another set of path constraints, and then try to
  generate satisfying and unsatisfying initial configurations for them
  in precisely the same way.
\end{itemize}

Generating a configuration which satisfies a particular constraint is
itself non-trivial.  The approach I adopted there was as follows:

\begin{itemize}
\item First, treat the constraint as an expression over boolean
  variables.  For instance, the constraint $x < 5 || x > 73$ would be
  treated as $a || b$, with a note that $a = x < 5$ and $b = x > 73$.
\item Generate a satisfier for this boolean expression using a simple
  brute-force satisfiability checker.  This will produce a
  configuration of the boolean variables, assigning each to one of
  true, false, or doesn't-matter, which causes the original constraint
  to be true.
\item Attempt to generate concrete values for all of the original
  constraint's variables such that all of the boolean variables have
  appropriate values.  Some of these values will be provided by simple
  heuristics (e.g. to make $x > k$ be true, where $k$ has a known
  value, try setting $x$ to $k+1$), but most will be randomly
  generated with a retry if the results do not make the boolean value
  have the desired results.  Even when values have to be randomly
  generated, being able to treat most boolean variables independently
  most of the time reduces the number of attempts necessary by a
  useful amount.
\item If no concrete values have been found which satisfy the boolean
  variable configuration after a certain number of random attempts,
  look for another configuration of boolean variables which satisfies
  the path constraint.
\item If there are no more such configurations give up and report an
  error.
\end{itemize}

The result is a set of initial configurations which will, with
reasonably high probability, exercise a useful selection of
\StateMachine behaviour.  The configurations can contain the following
items:

\begin{itemize}
\item Initial values of some registers\footnote{For SSA {\StateMachines}
  these will be the only values those registers ever have, but for
  non-SSA {\StateMachines} the value might change during
  interpretation.}.
\item Partial contents of memory, in the form of a partial mapping
  from concrete addresses to concrete values.
\item A list of bad addresses.  These are locations for which
  $BadPtr(x)$ must return true.  Any location not in this list is
  assumed to be valid memory, even if it isn't assigned a value.
\item A set of $EntryPoint$ and $ControlFlow$ expressions which
  are to be treated as true.
\end{itemize}

The configurations (hopefully) contain all of the parts of
{\StateMachine} state which might affect the final result, but do not
necessarily contain enough information to actually interpret the
{\StateMachines} to completion.  For example, many {\StateMachines}
will access locations on the stack, and so interpreting them will
require the value of the stack pointer, but it is very rare for their
final result to depend on the precise value of that pointer, and so
the symbolic execution engine will not generate any constraints on
that value and the initial configurations will not contain any value
for it.  If the validator encounters any such unspecified values
during {\StateMachine} interpretation it simply generates new random
values for the relevant variables.  It then runs each initial state
100 times, with different random values each time, and checks that the
{\StateMachines} match for each one.

One final subtlety is what to do with what to do with {\StateMachines}
which escape by, for instance, failing an assertion.  This indicates
that some part of the analysis has determined that the configuration
is not interesting, for some reason, \todo{There's some special
handling here to do with retrying escaping machines, but I don't
remember why I did it that way any more.  Think harder.  Also need to
grab a whole bunch of statistics on what this actually does so as to
convince people that it actually gets decent test coverage.}

I ran the resulting tool on a selection of {\StateMachines} which were
generated while analysing mysql.  It found a number of implementation
bugs, which are now fixed, but did not reveal any fundamental problems
with any of the algorithms involved.
