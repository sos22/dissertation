\section{Things to evaluate}

Things I want to show in the eval:

\begin{itemize}
\item That it works for some simple test cases.  This'll include a
  reasonable amount of discussion about what it actually does.
  This'll include showing that the generated fixes work, and some bits
  about their relative performance.  I'd also like to discuss
  whole-program vs. core dump vs. DRS mode here.  This will also
  include a comparison to my implementation of DataCollider.
\item That it could feasibly scale up to something of realistic size.
  Obvious approach here is just to show that we can run the full
  analysis on mysql, apache, thunderbird, and pbzip2.  It's currently
  looking like this will find at most one or two new bugs, which makes
  it all a bit less convincing.  This'll be mostly just showing that
  the analysis does actually complete in a sensible amount of time.
\item I'm going to pick one of those big programs and do a few more
  experiments on it:

  \begin{itemize}
  \item Show that the monotonicity property holds: run a full analysis
    with window size = 20 instructions, then re-analyse all of the
    resulting bugs at window size = 10, and show that anything which
    generates a candidate bug at 20 generates at least one at 10 as
    well.
  \item Display a CDF of how long it takes to analyse each instruction.
  \item Try turning the various machine simplification passes on and off
    and see what effect that has on the CDF.
  \end{itemize}

\item Ideally, some slightly more realistic artificial bugs, probably
  from ripping sync blocks out of STAMP.  i.e. real-size programs with
  artificial bugs.
\item Something about the effects of compiler optimisations?  Probably
  just a couple of paragraphs comparing optimised mysql to debug
  mysql.  Explain why compiler optimisations make some things easier
  and some things harder.
\item I need to show that the tool is actually correctly implemented
  itself.  The small number of bugs found in real programs means that
  this has more than a little bit of negative-results flavour, so this
  isn't completely trivial.
\end{itemize}

What I have:

\begin{itemize}
\item About a dozen artificial bugs for each mode
\item The glibc bug kernel --- fixed in DRS mode
\item The Thunderbird bug -- fixed in DRS mode
\item The first mysql bug -- fixed in core dump mode.
\item The second mysql bug -- found and fixed in whole-pass mode.
  This one's kind of interesting, because there are two different
  variants of it present in mysql, and the tool finds both, but I only
  knew about one of them when I started.
\end{itemize}

And that's about it.
Where I've said a bug is fixed or found in one mode that indicates that I've tested and confirmed that it works in that mode; it doesn't say anything about whether it would work in one of the other modes.

So I desperately need more real bugs.
Possible ways of getting them:

\begin{itemize}
\item
  enforce\_crash.
  I need to go and apply this to as many programs as possible, which will hopefully give me something more to work with.
\item
  Take an existing program and rip out the synchronisation, then see if we can find the resulting bugs.
  STAMP would be a reasonable place to look here.
\item
  Another bug tracker crawl.
  This would be bloody tedious but might turn up something.
\end{itemize}

Other things I can look at and evaluate:

\begin{itemize}
\item
  Turn the various simplification passes on and off and see what effect that has.
\item
  CDF of how long analysing a given potentially-crashing instruction takes.
\item
  Look at different strategies for combining machines.
\item
  Look at effect of different analysis window sizes: how many bugs do we find, how many false positives, how much time does the analysis take.
\item
  Compare the CFG-based approach to building machines to the old backtracking one.
\item
  Direct eval of the effectiveness of the induction rule.
\item
  Look at how many bugs are killed off by the different clauses of the crash definition.
\item
  Investigate performance hit of the enforcement mode, and how that depends on the mechanism used to generate the enforcers.
\item
  Compare compiled enforcers to interpreted ones.
  Performance, number of bugs detected, cost of building the damn things.
  Relative I- and D-cache footprints.
\item
  How much does the W isolation property actually buy us, in terms of analysis cost?
  How much does it cost us, in terms of lost bugs?
\item
  How large are the CFGs and \StateMachines which we actually generate?
  Also, how large are the intermediate machines generated when we're generating verification conditions?
\item
  Do we actually win anything from using the slightly odd form of SSA?
\item
  We handle multi-rooted CFGs completely differently between read and write modes.
  Does that win/lose anything?
\item
  There's almost certainly something to say about the dynamic analysis phase, probably to do with how quickly it converges.
\item
  Might also be able to say something about how complex the resulting aliasing tables are.
\item
  Look at how long the various phases take.
\item
  There are a bunch of places where we have to decide between using
  the simplifier and the sat checker.  Eval which works better for
  each of them.  Downside of evalling this is that it means
  *explaining* the difference between the simplifier and the sat
  checker, which I'd really rather avoid if I can get away with it.
\item
  When you're doing the cross-product machines you have a choice
  between explicit and implicit happens-before edges.  Explicit is
  basically always better; figure out how much by, and why.
\item
  Crash enforcement: timeout strategies.
\item
  Crash enforcement: how useful is side-condition checking?
\item
  Crash enforcement: how often do we redundantly evaluate side conditions?
\item
  Crash enforcement: Is await-bound-exit actually useful?
  We know it is in silly little artificial test cases, but perhaps not in real programs?
\item
  Currently artificially limit number of live assertions to 5.
  Investigate what effect that has.
\item
  Want something which can evaluate the effectiveness of the
  cross-function alias analysis pass.
\item
  Want something which can evaluate the effectiveness of the phi
  elimination pass.
\item
  Want something which can evaluate the effectiveness of the state
  unification optimisation.
\item
  In order to make the theory behind the crash enforcers work you
  always have to wait for a fixed amount of time when doing unbound
  message operations, even if someone arrives before the time has
  passed.  Most of the time, that extra delay is wasted.  Look at how
  much time is wasted, and how bad it would be to just not do that.
\item
  I have a somewhat sketchy implementation of DataCollider lying
  around somewhere.  It'd be interesting to compare the enforce\_crash
  results to what you can get just with that.  I have tried that
  experiment in the past and the DC results were surprisingly poor on
  the real bugs (although it did find the artificial ones), so that
  might be kind of interesting.
\end{itemize}

\section{Validation of tool implementation}

\subsection{Cross-thread stack accesses}

\subsection{Static analyses}

SLI relies on two forms of whole-program static analysis applied to the target binary before the main analysis starts:

\begin{itemize}
\item
  The simple points-to analysis.
\item
  An analysis to recover the offset between RSP and RBP, where that is a constant.
\end{itemize}

Both analyses assume in at least some places that the program to be analyses conforms to the system ABI.
If that assumption does not hold, or if there is simply a bug in one of them, then that might invalidate all of the other results.
I therefore developed some Valgrind-based dynamic analyses to check that the results of this phase were correct.

\todo{The tool also implements a register liveness analysis, but
  that's only used to build the points-to table, so validating the
  points-to table also implicitly validates the liveness one.}

\subsubsection{Points-to analysis}
\label{sect:eval:validate:pta}
\todo{I kind of end up describing this twice, once in the analysis
  chapter and then again here.  Rationalise that down a bit.}

The static points-to analysis builds an instruction attribute table
for the program which includes, for each instruction:

\begin{itemize}
\item
  Whether the current stack frame might include any pointers to itself.
\item
  Whether there might be any pointers to the current stack frame in memory which is not part of the current stack frame.
\item
  For each register, a flag saying whether that register might point at the current stack frame.
\end{itemize}

The ``current stack frame'' here is defined to be the region of memory between \verb|RSP-128| and the value of \verb|RSP| at the time of the enclosing \verb|call| instruction\editorial{talk about effects of tail calls}.
The tool to check this analysis has several parts:

\begin{itemize}
\item
  It must track the extent of the current frame; this is straightforward, since the analysis can always see the value of \verb|RSP| and all \verb|call| and \verb|ret| instructions.
\item
  It checks, at the start of each instruction, whether any registers currently point into the current frame, and, if so, whether that is allowed by the instruction attribute table.
\item
  It attempts to track directly whether there exist any pointers to the current frame, whether in the frame or outside of it.
  This part of the analysis assumes that there are no pointers into a frame when it is created at the start of a function and then monitors all stores to detect when such pointers are created.
  This information then allows the analysis to directly check the might-be-pointer-to-frame flags in the instruction attribute table.
\item
  That assumption holds for most well-behaved programs, but is not absolutely guaranteed.
  The dynamic analysis therefore also checks all load operations to confirm that there are no pointers which violate the flags in any memory locations accessed by a function.
  There might, of course, be pointers into the current stack frame which are never loaded, but (assuming there are no cross-thread stack accesses) they can never be dereferenced, and so don't actually matter.
\end{itemize}

This flagged a number of minor problems with the analysis:

\begin{itemize}
\item
  Pointers to the stack frame can sometimes be left behind in dead registers, and in particular in call-clobbered registers after function calls.
  Correct programs which conform to the ABI will never make use of the values of these registers, and the static analysis makes use of that fact\editorial{...but doing so doesn't actually buy us anything...}, but it is much less obvious in a dynamic analysis.
  The solution is simple: have the dynamic analysis overwrite all such registers with poison values when functions return.
  If the program does conform to the ABI then this will have no effect, but if it makes use of the theoretically-dead values then its behaviour will change.
  I ran the analysis in three modes, one which used zero as poison, one which used a small value which wasn't a valid pointer, and one which used a large value which wasn't a valid pointer.

  This revealed a single place which did not conform to the ABI in the desired way: glibc's internal pthread locking functions are guaranteed to never clobber \verb|RSI|, and glibc's syscall stubs make use of this in a number of places.
  This particular static analysis is only applied to the program's main binary, and not any of the libraries which it is dynamically linked against, and so this is not a particular problem.

\item
  \verb|alloca|

\begin{verbatim}
>   9e56c7:       48 29 c4                sub    %rax,%rsp
>   9e56ca:       48 89 e0                mov    %rsp,%rax
>   9e56cd:       48 83 c0 0f             add    $0xf,%rax
>   9e56d1:       48 c1 e8 04             shr    $0x4,%rax
>   9e56d5:       48 c1 e0 04             shl    $0x4,%rax
>   9e56d9:       48 89 45 a8             mov    %rax,-0x58(%rbp)
\end{verbatim}

\todo{Crap, my argument for why this doesn't matter doesn't actually work.  Need to rethink that one.}
\end{itemize}

\subsubsection{RBP offset}

The main analysis removes references to the function frame pointer, if
present, by replacing them with references to the stack pointer.  This
relies on a static analysis which determines, for each instruction in
the program, the offset from the \verb|RBP| register to the stack
pointer (assuming that that's a constant).  This dynamic analysis
checks, at the end of every instruction, that the actual offset
matches the value in the database.  This analysis did not reveal any
important bugs in the algorithm\footnote{Beyond a few implementation
  errors which are fixed in the code used for this evaluation.}.

\subsubsection{CFG generation}

For the bug-detecting mode to hope to detect every bug, the CFG
generation process must be able to generate CFGs which represent all
dynamic fragments of the program of the desired length which either
end in a memory-accessing instruction (for probe CFGs) or start and
end with a store (for store CFGs).  This dynamic analysis attempts to
validate that by capturing a large pool of dynamic traces from the
program and then checking that CFG generator can generate the trace.
Ideally, it would capture every such trace from an execution, but that
has sufficiently high performance overhead to make it difficult to
exercise all possible behaviour under it\editorial{blah}.  Instead,
the analysis applies several filters to try to obtain a reasonably
representative sample:

\begin{itemize}
\item
  Only traces which end in a non-stack memory-accessing instruction
  are considered.
\item
  Amongst those samples, only one in a thousand is used.  This is
  implemented by only sampling if a randomly-generated number is
  congruent to zero modulo a thousand, rather than taking every
  thousandth trace, so as to avoid possible aliasing effects with the
  program's structure.
\item
  I attempt to increase the likelihood of rare traces being sampled
  using a bloom counter table.  This consists of 131072 saturating
  7-bit counters.  When the dynamic analysis is determining whether to
  sample a given trace, it hashes it to select one of these counters,
  then generates a random number, and only takes the trace if the
  random number modulo the counter plus one is zero.  It then
  increments the counter.  This helps to increase the likelihood of
  moderately rare traces being included in the final sample.
\end{itemize}

The end result of this dynamic analysis is a large set of short
fragments of the program's execution.  Each such fragment is
considered in isolation, and appropriate instructions from it fed into
the CFG generating algorithm.  The CFG can then be checked to ensure
that the trace is actually possible in the CFG.  This analysis did not
find any problems with the algorithm.

\todo{Should really try to do something to convince myself that the
  sanity checker works.  Collecting more stats on the trace pool
  generated would be good, as would some sensitisation on those
  parameters.}

\subsection{\StateMachine generation}

Don't really have a plan for this; might just say that it's a trivial
wrapper for libVEX.

\subsection{Dynamically-collected aliasing model}

This is pretty much what it is; not sure there's a great deal to say
here.  If I had infinite time I could hack up gcc to try to do a
similar analysis at the source level, but I don't, so I can't.  Only
real alternative is to look at the convergence rate.

Things to look at:

\begin{itemize}
\item How fast we add new edges to the aliasing table while running
  the analysis, as a function of time.  You'd hope that this will fall
  very quickly as the analysis runs.
\item mysqld has a reasonably good test harness.  It'd be interesting
  to try running each of the tests in that in some suitable order and
  see how many of them you need in order to get good coverage.
  Probably want to try to normalise that against the actual number of
  distinct instructions run to make it fair.
\item Two things are worth looking at here: the rate at which we add
  edges in toto, and the rate at which we add edges between
  already-known instructions.  The latter is much more important than
  the former, because the first one includes discovering new code for
  the first time, and it's pretty forgivable to not include
  instructions which are never run, whereas the latter is new
  interactions between code which we already know about.
\end{itemize}

\subsection{\StateMachine simplification}

The \StateMachine simplification passes are both complicated and
critical to SLI's correctness; validating that they are correct is
therefore important.  To do so, I collected a selection of pre- and
post-optimisation machines from a number of runs, evaluated them in
identical initial configurations, and confirmed that they produced
identical results.  Generating the initial conditions is non-trivial.
At first I simply generated them completely at random, but found that
it required an unreasonably large number of such random configurations
to achieve good coverage of the \StateMachines' behaviour.  This is
because the vast majority of paths through most machines ultimately
report that the machine does not crash, and so a uniform random
sampling of initial configurations will overwhelmingly sample
configurations in which the bug of interest does not reproduce.  This
is unfortunate, as the situations in which the bug does reproduce tend
to be more interesting.

It is worth considering this is slightly more detail.  A reasonably
typical \StateMachine might look like this:

\begin{verbatim}
if (rax == 0) survive();
if (!BadPtr(rax)) survive();
crash();
\end{verbatim}

Here, \verb|rax| is a 64-bit value, and so if its value is randomly
chosen then the \StateMachine is highly unlikely to make it past the
first state, which would not constitute a good test.  Of course, it
would be possible to bias the distribution to produce zeroes more
often than other values, and hence avoid this issue.  This works, in
the sense that it makes it easy to generate an effective test suite
for this \StateMachine, but is lacking in two important respects.
First, it is unclear what other special cases might be needed;
generating zero is an obvious thing to test, as is generating one or
minus one, but what if there were a bug in the optimiser when
\verb|rax| is near to the top of its range?  or has only bits in the
top byte set?  or is a large prime number?  These questions would make
it difficult to have faith that such an approach had generated a
reasonably complete set of initial configurations for testing.
Second, considering each register independently is inherently
inefficient, due to the common structure of \StateMachines.  Consider
this machine, for example:

\begin{verbatim}
if (rax % 2 == 0) survive();
if (rbx % 2 == 0) survive();
if (rcx % 2 == 0) survive();
if (rdx % 2 == 0) survive();
crash();
\end{verbatim}

This reflects the common structure of \StateMachines, in which a long
prefix of initial tests capture ways in which the program might avoid
running the code which is suspected of having a bug before the
interesting part of the \StateMachine starts.  One reasonable set of
initial configurations might be:

\begin{itemize}
\item $rax = 0$
\item $rax = 1$, $rbx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 1$, $rdx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 1$, $rdx = 1$
\end{itemize}

A total of five initial configurations; one for each possible final
state.  A randomly-generated configuration, by contrast, will have a
one in sixteen chance of generating each final state, and will require
an average of a little over twenty-five input states in order to cover
every final state, a factor of five worse than the desired
set\editorial{25 from some Python simulations; not sure it's worth
  trying to justify it further.}.  The difference will be larger in
more complex machines, and so coverage will be worse for them, but
those are precisely the ones which are most likely to reveal optimiser
bugs.

I therefore used a slightly more complicated scheme for generating the
initial configurations:

\begin{itemize}
\item[1] First, execute the unoptimised machine in the symbolic
  execution engine, collecting all of the path constraints generated.
  There will be one such constraint for each path (loosely defined)
  which the symbolic execution engine finds through the machine,
  whether that path ultimately crashes, survives, or escapes (due to
  e.g. dereferencing a bad pointer).
\item[2] Pick a constraints which is not satisfied by any
  configuration currently present in the initial configuration set.
  Attempt to generate a configuration which satisfies it and add it to
  the set.  If there are no constraints which are never satisfied,
  pick one which is satisfied in every initial configuration and try
  to generate a configuration which does not satisfy it.
\item[3] Repeat 2 until every constraint is satisfied in at least one
  initial configuration and not satisfied in at least one other
  configuration.
\item[4] Execute the optimised machine in the symbolic execution
  engine, generating another set of path constraints, and then try to
  generate satisfying and unsatisfying initial configurations for them
  in precisely the same way.
\end{itemize}

Generating a configuration which satisfies a particular constraint is
itself non-trivial.  The approach I adopted there was as follows:

\begin{itemize}
\item First, treat the constraint as an expression over boolean
  variables.  For instance, the constraint $x < 5 || x > 73$ would be
  treated as $a || b$, with a note that $a = x < 5$ and $b = x > 73$.
\item Generate a satisfier for this boolean expression using a simple
  brute-force satisfiability checker.  This will produce a
  configuration of the boolean variables, assigning each to one of
  true, false, or doesn't-matter, which causes the original constraint
  to be true.
\item Attempt to generate concrete values for all of the original
  constraint's variables such that all of the boolean variables have
  appropriate values.  Some of these values will be provided by simple
  heuristics (e.g. to make $x > k$ be true, where $k$ has a known
  value, try setting $x$ to $k+1$), but most will be randomly
  generated with a retry if the results do not make the boolean value
  have the desired results.  Even when values have to be randomly
  generated, being able to treat most boolean variables independently
  most of the time reduces the number of attempts necessary by a
  useful amount.
\item If no concrete values have been found which satisfy the boolean
  variable configuration after a certain number of random attempts,
  look for another configuration of boolean variables which satisfies
  the path constraint.
\item If there are no more such configurations give up and report an
  error.
\end{itemize}

The result is a set of initial configurations which will, with
reasonably high probability, exercise a useful selection of
\StateMachine behaviour.  The configurations can contain the following
items:

\begin{itemize}
\item Initial values of some registers\footnote{For SSA \StateMachines
  these will be the only values those registers ever have, but for
  non-SSA \StateMachines the value might change during
  interpretation.}.
\item Partial contents of memory, in the form of a partial mapping
  from concrete addresses to concrete values.
\item A list of bad addresses.  These are locations for which
  $BadPtr(x)$ must return true.  Any location not in this list is
  assumed to be valid memory, even if it isn't assigned a value.
\item A set of $EntryPoint$ and $ControlFlow$ expressions which
  are to be treated as true.
\end{itemize}

The configurations (hopefully) contain all of the parts of
\StateMachine state which might affect the final result, but do not
necessarily contain enough information to actually interpret the
\StateMachines to completion.  For example, many \StateMachines will
access locations on the stack, and so interpreting them will require
the value of the stack pointer, but it is very rare for their final
result to depend on the precise value of that pointer, and so the
symbolic execution engine will not generate any constraints on that
value and the initial configurations will not contain any value for
it.  If the validator encounters any such unspecified values during
\StateMachine interpretation it simply generates new random values for
the relevant variables.  It then runs each initial state 100 times,
with different random values each time, and checks that the
\StateMachines match for each one.

One final subtlety is what to do with what to do with \StateMachines
which escape by, for instance, failing an assertion.  This indicates
that some part of the analysis has determined that the configuration
is not interesting, for some reason, 

\todo{There's some special handling here to do with retrying escaping
  machines, but I don't remember why I did it that way any more.
  Think harder.}

\section{Artificial bugs}

I now present the results of running the tool on a number of
artificial bugs and some discussion of what happens.

Lookup between bug numbers and names:

\begin{itemize}
\item 1 -- Simple TOCTOU
\item 2 -- Indexed TOCTOU
\item 3 -- Indexed TOCTOU with rare writes
\item 4 -- Indexed TOCTOU with rare reads
\item 5 -- Multi-variable consistency constraint
\item 6 -- Context-dependent TOCTOU; multiple calls to \verb|f|, but
  only one has the bug of interest.
\item 7 -- Cross-function TOCTOU; check in one function, use in the
  function it returns to.
\item 8 -- Write to read hazard on the read side races with a wild
  store.
\item 9 -- Multiple possible bugs in one program, requiring combined
  enforcement plans.
\item 10 -- Lots of threads on the read side.
\item 11 -- Something which couldn't be found by Chess or
  DataCollider.  That really means something with at least three
  mandatory preemptions.
\item 12 -- ??? come up with something.
\item 13 -- A false positive -- something where the main analysis
  thinks there might be a bug, but there isn't, and so the enforcer
  can't enforce it.
\item 14 -- Something which nicely illustrates the cross-function
  alias analysis stuff.
\item 15 -- Something which illustrates the importance of the Phi
  elimination pass.
\end{itemize}

\subsection{The bugs}

\subsubsection{Simple time-of-check, time-of-use (TOCTOU) bug (simple\_toctou)}
\label{sect:eval:art:simple_toctou}

\todo{The discussion here is painfully detailed.  I think that's
  reasonable for the first bug discussed, but later ones are going to
  have to be much more high-level.  To be honest, I'm not entirely
  convinced that this bit is all that interesting, but I think I
  should probably copy-and-paste the complete tool output into the
  dissertation at least once.}

This is the simplest possible kind of concurrency bug.  One thread
loops forever executing this pseudocode:

\begin{verbatim}
if (global_ptr != NULL)
    *global_ptr = 5;
\end{verbatim}

Meanwhile, another thread loops executing code like this:

\begin{verbatim}
while (1) {
    global_ptr = &t;
    sleep(1 second);

    STOP_ANALYSIS();
    global_ptr = NULL;
    STOP_ANALYSIS();
}
\end{verbatim}

This is intended to model a very simple structure which is accessed
frequently but updated rarely.  The bug is, of course, that the
updating thread might set \verb|global_ptr| to \verb|NULL| in between
the two reads of it in the reading thread, causing the reading thread
to crash when it dereferences the pointer it loaded.

The disassembly for the read side looks like this:

\begin{verbatim}
  400624:       48 8b 05 15 0a 20 00    mov    0x200a15(%rip),%rax        # 601040 <global_ptr>
  40062b:       48 85 c0                test   %rax,%rax
  40062e:       74 0d                   je     40063d <thr_main+0x19>
  400630:       48 8b 05 09 0a 20 00    mov    0x200a09(%rip),%rax        # 601040 <global_ptr>
  400637:       c7 00 05 00 00 00       movl   $0x5,(%rax)
\end{verbatim}

And that for the write side like this:

\begin{verbatim}
  400815:       48 c7 05 20 08 20 00    movq   $0x0,0x200820(%rip)        # 601040 <global_ptr>
  40081c:       00 00 00 00 
\end{verbatim}

SLI produces a single candidate bug for this program:

\begin{verbatim}
Load Machine:
CFG:
-1-------->cfg6: {0x400624} -> cfg5:branch
.          cfg5: {0x40062b} -> cfg4:branch
.          cfg4: {0x40062e} -> cfg3:default
.          cfg3: {0x400630}
l1: {{0x400624}:LOAD tmp1:1:1:I64 <- *(0x601040:I64):normal@mai1:1 then l2}
l2: {0x40062e}: if ((0x0:I64 == GET:I64(tmp1:1:1))) then l3 else l4
l3: <survive>
l4: {{0x400630}:LOAD tmp1:1:2:I64 <- *(0x601040:I64):normal@mai1:2 then l5}
l5: {0x400637}: if (BadPtr(GET:I64(tmp1:1:2))) then l8 else l3
l8: <crash>
Root: l1
Store Machine:
CFG:
-98------->cfg52: {0x400815}
l7: {{0x400815}:*(0x601040:I64):normal <- 0x0:I64 @ mai98:4 then l8}
l8: <crash>
Root: l7
Memory access identifiers:
        mai1:1 -> {cfg6}
        mai1:2 -> {cfg3}
        mai98:4 -> {cfg52}
Aliasing:
        mai1:1 <-> mai98:4
        mai1:2 <-> mai98:4
Verification condition: (!((mai1:2 <-< mai98:4)) && (mai1:1 <-< mai98:4))
\end{verbatim}

This is hopefully reasonably easy to understand at this point.  The
candidate bug consists of several parts:

\begin{itemize}
\item The \StateMachines for the read and write sides of the race.  In
  this case, the load machine has been assigned thread identifier 1
  and the store machine has been assigned thread identifier 98.  These
  identifiers are completely arbitrary.
\item Control flow graph fragments for the two sides.  The read side
  here contains CFG nodes three through 6, while the store machine has
  CFG node 52; these identifiers are arbitrary.  In this case the CFGs
  are largely uninteresting, but in a more complex example these
  fragments would include details of which nodes were created by loop
  unrolling and some information on the function call structure of the
  program.

  Note that the instruction which actually crashes, \verb|400637| is
  not included in the CFGs.  This is because it is completely
  irrelevant to the behaviour which is actually being investigated: by
  the time that instruction executes, the program has either avoided
  the purported bug or is doomed to crash, and so it cannot tell us
  anything useful about the nature of the bug.
\item A memory access identifier table, showing how memory access
  identifiers map into CFG nodes, or, in other words, how the accesses
  analysed by SLI's main analysis passes correspond to accesses made
  by the program itself.  In this case, that mapping is trivial, and
  each memory access identifier corresponds to a single CFG node, but
  in more complex cases SLI might merge several similar program
  accesses into a single \StateMachine-level accesses, and in that case
  a single memory access identifier might map to multiple CFG nodes.
\item An aliasing table, showing which memory accesses might
  potentially access the same memory, according to the
  dynamically-collected aliasing table.
\item The verification condition.  This is the condition which must
  hold for the bug being investigated to reproduce when the two
  \StateMachines are run in parallel.  In this case, it consists of
  just two happens-before edges, \verb|mai1:1 <-< mai98:4| and
  \verb|!((mai1:2 <-< mai98:4))|, which between them indicate that the
  bug will reproduce precisely when memory access 4 in thread 98
  intervenes between memory accesses 1 and 2 in thread 1.  In other
  words, the program will crash when the store thread sets
  \verb|global_ptr| to \verb|NULL| in between the two loads of
  \verb|global_ptr| in the load thread, precisely as desired.
\end{itemize}

The next step of the pipeline is to take that crash summary and
convert it into a crash enforcement plan (CEP).  The CEP generated
by SLI for this bug is:

\begin{verbatim}
Crash enforcement plan:
        Threads: T1:1 = {t1}; T1:98 = {t2}
        Roots: t1 = {cfg6}; t2 = {cfg52}
        CFG:
                cfg3:t1: successors = {}
                cfg4:t1: successors = {cfg3}
                cfg5:t1: successors = {cfg4}
                cfg6:t1: successors = {cfg5}
                cfg52:t2: successors = {}
        Label to RIP table:
                1:cfg3 -> {0x400630}
                1:cfg4 -> {0x40062e}
                1:cfg5 -> {0x40062b}
                1:cfg6 -> {0x400624}
                1:cfg52 -> {0x400815}
        Happens before map:
                cfg3:t1 -> {43708: cfg52:t2 <-< cfg3:t1, }
                cfg6:t1 -> {43707: cfg6:t1 <-< cfg52:t2, }
                cfg52:t2 -> {43707: cfg6:t1 <-< cfg52:t2, 43708: cfg52:t2 <-< cfg3:t1, }
Patch points = [0x400624, 0x400815]
\end{verbatim}

\todo{Hmm.  A lot of the features of the plan only really make sense
  when you're combining multiple candidate bugs into a single
  enforcement plan, which means they're hard to describe in this
  single-candidate example.}

\todo{Also, I should come up with a less ugly way of displaying the
  damn things.}

This plan is quite simple, reflecting the simple nature of the bug.
It contains a number of components:

\begin{itemize}
\item A mapping from thread identifiers in the bug candidates into
  thread identifiers in the crash enforcement plan.  This is trivial
  in this example, but it can be more complex if either multiple
  candidate bugs are to be combined into a single enforcement plan or
  if a single candidate bug requires multiple plan-level threads to
  handle multiple possible ways of reproducing a bug.
\item CFG fragments covering all of the necessary instructions, and
  mappings from nodes in those fragments back to instruction
  addresses.  Again, this is trivial in this example, but it can be
  more complex if, for instance, the plan is to enforce multiple
  candidate bugs at once and the candidates unroll the same loop in a
  different way.
\item A set of happens-before edges which are to be enforced, along
  with the message identifier for the edge and the data which is to
  included in the message payload.
\item A set of instructions in the original program which must be
  patched so that they branch into the CEP interpreter.
\end{itemize}

This plan can then be loaded into the buggy program in order to make
the bug reproduce more easily.  In this particular case the results
are somewhat unimpressive, as this bug already reproduces quite
easily, but still show a small improvement in reproduction time.

The time to reproduce a bug of this kind follows a roughly exponential
distribution, as each iteration of the write loop has a roughly
independent chance of causing the read loop to crash.  I ran each
configuration (with enforcer, without enforcer) ten times on an
otherwise idle machine and timed how long it took for them to crash.
As might be expected, the version without the enforcer always took a
roughly whole number of seconds to crash, as the write side runs after
a whole number of seconds has passed.  The mean time before the
program crashed was 3.3 seconds, so each iteration of the write side
has a roughly 30\% chance of causing the read side to crash.

With the enforcer loaded, the program always crashed after almost
precisely 1.3 seconds.  This delay consists of two components:

\begin{itemize}
\item One second, which is caused by the call to \verb|sleep| in the
  test harness.
\item .3 seconds, which is caused by the RX message delay.  The write
  thread starts its critical section at one second in and must
  immediately wait for a read thread to arrive.  In this case the
  pseudo-random number generator selects a 0.3 second delay here.
  Note that the thread will then always wait for at least 0.3 seconds,
  even though, in this case, a read thread will arrive almost
  instantly.  This is in case additional real threads arrive later, in
  which case the write thread will attempt to synchronise with all of
  them using the power set construction discussed earlier.
\end{itemize}

\todo{Would this benefit from a time-sequence diagram of some sort?}

The result is that, with the enforcer applied, the bug reproduces
essentially every time the write side runs.

Error bars on those parameters: without an enforcer crash time appears
to be exponentially distributed, so I did a resampling test to get
some error bars.  I ran the test 10,000 times, then drew 10,000 random
selections of 100 samples, with replacement, from that pool of
results, and calculated the lambda parameter for each of those
samples.  The resulting distribution had mean 0.37 and (population)
standard deviation 0.03.  Not sure whether that's normally
distributed, though.

\todo{Time taken to do analysis?}

The fix generated in this case is very simple and contains two
critical sections, one covering \verb|400624| to \verb|400630| and the
other covering the single instruction \verb|400815|.  Notice that, as
in the enforcer, the instruction which crashes, \verb|400637|, is
\emph{not} included in any critical section.  There are entry point
jumps from \verb|400624| and \verb|400815|.  The patch itself is, in
this case, rather large relative to the original program code (200
bytes rather than 36), mostly because the machine code to be protected
is itself quite small and so the function call sequence used to
acquire or release the patch lock completely dominates the total size
of the patch.

Performance: with patch applied, get 9493234 read events per second,
compared to 35197332 per second without (both single repro; need to
run more experiments) i.e. a factor of nearly four overhead, but the
without case crashed after four seconds, so, you know, not that
unreasonable.  Also, this is the micro-est of micro tests, so most of
the time the overhead will be far lower.

\subsubsection{Indexed TOCTOU bug (indexed\_toctou)}

In this variant of a TOCTOU bug, there are multiple instances of the
structure which is being raced on and the bug will only manifest if
the reading and writing threads happen to coincide.  This bug
exercises the side-condition-checking part of SLI.  The reading thread
is:

\begin{verbatim}
idx = random() % NR_PTRS;
STOP_ANALYSIS();
if (global_ptrs[idx] != NULL)
     *(global_ptrs[idx]) = 5;
\end{verbatim}

Here \verb|STOP_ANALYSIS| is a special marker which causes SLI to
truncate the \StateMachine at that point.  The effect is that SLI will
only analyse the final two lines of the above fragment, and will not
attempt to explore the \verb|random()| function.  This makes the test
much easier to understand.

\todo{I should really try without that, just to see what happens.}

The writing thread simply loops constantly repeating this:

\begin{verbatim}
idx = random() % NR_PTRS;
STOP_ANALYSIS();
global_ptrs[idx] = NULL;
STOP_ANALYSIS();
global_ptrs[idx] = &t;
\end{verbatim}

Here the \verb|STOP_ANALYSIS()| markers cause SLI to treat this as two
independent store operations, one of which sets
\verb|global_ptrs[idx]| to a valid value and the other which sets it
to \verb|NULL|.

Without any enforcer loaded: no repro in ten seconds, repro after 7.5
seconds, no repro in ten seconds.

With a full enforcer loaded: repro in 103ms, repro in 485ms, repro in
240ms.

With a partial enforcer loaded, which does delays but no
side-condition checking: no repro in ten seconds, no repro in ten
seconds, no repro in ten seconds.

Performance:

Partial enforcer: 548, 2; 571, 2; 806, 2 (read, write events, per second)
Full enforcer: 434, 4; 320, 1; 1627, 3
No enforcer: 161k, 106k; 161k, 106k; 161k, 106k

Not sure how useful those numbers are, to be honest.

Automatic fix generation:

With fix applied, 151k read events per second, 101k write.  Without
fix, 161k read, 106k write.  So that's ~7\% overhead, which I
personally think is Pretty Damn Good.  The fix itself is roughly what
you'd expect: one critical section to cover the two critical loads on
the read side and another one to cover the critical store on the write
side.  As usual, the actual crashing instruction is not included in
any critical section.

\subsubsection{Biassed indexed TOCTOU bugs (read\_indexed\_toctou, write\_indexed\_toctou)}

These bugs are essentially similar to the indexed\_toctou, except with
additional delays such that either the write operation (for
write\_indexed\_toctou) or the read operation (for
read\_indexed\_toctou) is far more common than the other.  This test
illustrates the importance of rebalancing delays between the various
happens-before edges in the crash enforcement plans.

\subsubsection{Multi-variable consistency constraint (multi\_variable)}

\todo{I want to rewrite this test, so describe it later.}

\subsubsection{Context-dependent races (context)}

This test is similar to simple\_toctou, except that the behaviour
depends on the function's call context.  It is structured like this:

\begin{verbatim}
f(int **ptr) {
    if (*ptr)
        **ptr = 5;   
}
read_thread() {
    while (1) {
        if (random() % 1000 == 0) {
            STOP_ANALYSIS();
            f(&global_ptr1);
            STOP_ANALYSIS();
        } else {
            STOP_ANALYSIS();
            f(&global_ptr2);
            STOP_ANALYSIS();
        }
    }
}        
write_thread() {
    while (1) {
        STOP_ANALYSIS();
        global_ptr1 = &t;
        STOP_ANALYSIS();
        sleep(1ms);
        STOP_ANALYSIS();
        global_ptr1 = NULL;
        STOP_ANALYSIS();
    }
}
\end{verbatim}

Here, there are two calls to \verb|f|, only one of which could ever
possibly exhibit the bug, and that call is much less frequent than the
other one.  Reproducing the bug quickly is therefore only possible if
the crash enforcer checks the calling context before inserting any
large additional delays.

\todo{It'd be worthwhile trying this test again with a STOP\_ANALYSIS
  at the top of f, just to confirm that we correctly generate a
  side-condition on ptr with the same effect as checking context even
  when we don't backtrack far enough to know what the context is.}

\subsubsection{Write-to-read hazard (write\_to\_read)}

This test demonstrates a bug on the edge of what SLI can handle.
Because of the W isolation property in the bug definition, SLI cannot
handle any bugs in which the write thread reads a memory location
which has been written to by the read thread.  It is, however,
supposed to be able to handle the case where the read thread reads a
location which has been written by the read thread, and in particular
the case where a write in the read thread races with a write in the
write thread and the result is then read by the read thread.  This
test demonstrates that case.

(In other words, there is a write-to-read hazard in the read thread
which is potentially interrupted by the write thread.)

The read side is simple:

\begin{verbatim}
global_ptr = &t;
*global_ptr = 5;
\end{verbatim}

The write side is then this:

\begin{verbatim}
global_ptr = NULL;
\end{verbatim}

Both sides run in a tight loop without any delays.  Despite this, it
still often takes several seconds for the bug to reproduce without an
enforcement patch applied, during which time the two critical sections
may run hundreds of millions of times.  A more realistic test would
run the two sections far less frequently, and so might require
hundreds of years of CPU time to reproduce the bug.  The SLI-generated
enforcer patch, by contrast, can reliably reproduce this bug in a few
hundred milliseconds.

\todo{I think it might be interesting to investigate this a bit more.
  I have a sneaking suspicion that it won't reproduce at all if you
  run on bare metal without any interrupts, and that it pretty much
  only reproes if you get a timer interrupt at precisely the right (or
  wrong) place.  Could maybe get a paragraph or so of useful
  discussion out of that if I had some evidence.  Plus, it's the kind
  of experiment which looks impressive but is actually really easy,
  so, you know, win.}

\subsubsection{Multiple bugs (multi\_bugs)}

This test combines simple\_toctou and write\_to\_read into a single
test, and demonstrates SLI's ability to exercise several bugs using a
single enforcer.  This is useful when testing with large programs;
something like mysqld can easily generate thousands of candidate bugs,
and having to test each one individually would become quite tedious,
but testing dozens at a time is far more reasonable.

The read side of the test is structured like this:

\begin{verbatim}
while (1) {
    r = select_test();
    STOP_ANALYSIS();
    if (r)
        simple_toctou();
    else
        write_to_read();
    STOP_ANALYSIS();
}
\end{verbatim}

The behaviour of \verb|select_test| is configurable at run time.  It
can either choose between the tests at random, always select one, or
always select the other.  The write side of the test is structured
like this:

\begin{verbatim}
for (i = 0; i < 2000000; i++) {
    STOP_ANALYSIS();
    write_to_read_write_side();
    STOP_ANALYSIS();
}
STOP_ANALYSIS();
simple_toctou_write_side();
STOP_ANALYSIS();
\end{verbatim}

The constant \verb|2000000| was chosen such that with no enforcer
applied and with the read side configured to choose the two tests with
equal probability the two bugs reproduce with roughly equal frequency.

In this test, SLI correctly generates two candidate bugs (one for each
component bug).  These can either be instantiated into separate
enforcers (which have the expected behaviour of making one bug
reproduce quickly while having no effect on the other) or combined
into a single enforcer.  If the test is configured to always use one
bug or the other then the combined enforcer consistently reproduces
that, as desired.  More surprisingly, if the test is configured to
select a sub-test randomly then the enforcer always reproduces the
write\_to\_read bug, simply because the enforcer reproduces the bug
quickly enough that the write side of the simple\_toctou write side
never runs.

\subsubsection{Multiple read threads (multi\_threads)}

\todo{Write me.}

\begin{table}
\begin{tabular}{llllllll}
Test name       & Time to reproduce & Time to reproduce & Performance without & Performance with & Time to produce & Time to produce & Time to produce \\
                & without enforcer  & with enforcer     & fix                 & fix              & summary         & enforcer        & fix             \\
\hline
simple\_toctou  & 7s                & 1s                & 35Mloops/second     & 9.5Mloops/second & 1.7s            & 110ms           & 100ms           \\
indexed\_toctou & 4.5s              & 640ms             & 1.6Mloops/second    & 0.9Mloops/second & 23s             & 230ms           & 110ms           \\
read\_indexed\_toctou &             & 1.2s              & 15.6Mloops/second   & 7Mloops/second   & 3.2s            & 180ms           & 120ms           \\
write\_indexed\_toctou &            & 1.3s              & 6.7Mloops/second (write) & 4.2Mloops/second (write) & 23.3s & 130ms        & 100ms           \\
multi\_variable \\
context \\
write\_to\_read \\
multi\_bugs \\
multi\_threads
\end{tabular}
\caption{Summary of results for artificial bugs.  All times are
  excluding the time taken for dynamic analysis.}

\todo{I should really redo this so that I have multiple samples and
  can do some sensible stats.}

\todo{There's a lot of inexplicable variation in there...}

\todo{Extend with indexed\_toctou results when side-condition checking
  is turned off.}
\end{table}

\section{Bugs from real programs}

\subsection{glibc}

\verb|glibc| is a kernel of glibc bug 2644 \cite{glibc2644}, which
affected versions of glibc up to 2.5 and could lead to a crash if
multiple threads were shut down at the same time.  A simplified
version of the code involved is shown in Figure~\ref{fig:glibc}, where
\verb|forcedunwind| and \verb|done_init| are global variables.  Note
that the bug here depends on the compiler's optimizer, and is not
apparent at the source-code level\footnote{Unfortunately, only the
  32-bit x86 version of gcc optimizes the function like this, and our
  implementation of SLI assumes a 64-bit x86 program, and this
  prevented us from testing with the real bug.}.  SLI operates
entirely at the machine-code level, and so this does not present any
additional complexity.

\begin{figure*}
  \begin{subfloat}
    \begin{minipage}{52mm}
\begin{verbatim}
_Unwind_ForcedUnwind() {
   if (forcedunwind == NULL)
      pthread_cancel_init();
   forcedunwind();
}
pthread_cancel_init() {
   if (done_init) return;
   forcedunwind =
     _forcedunwind_impl;
   done_init = 1;
}
\end{verbatim}
    \end{minipage}
    \caption{Before optimizations}
  \end{subfloat}
  \begin{subfloat}
    \begin{minipage}{52mm}
\begin{verbatim}
  _Unwind_ForcedUnwind() {
1:  l = forcedunwind;
2:  if (l == NULL &&
3:      done_init) {
4:    forcedunwind = l =
5:       _forcedunwind_impl;
6:    done_init = 1;
7:  }
8:  l();
  }
\end{verbatim}
    \end{minipage}
    \caption{After optimizations}
  \end{subfloat}
  \begin{subfloat}
    \begin{minipage}{35mm}
\begin{verbatim}
    while (1) {
10:   pthread_barrier_wait();
11:   _Unwind_ForcedUnwind();
12:   pthread_barrier_wait();
13:   done_init = 0;
14:   forcedunwind = NULL;
    }
\end{verbatim}
    \end{minipage}
    \caption{Test harness}
  \end{subfloat}
  \label{fig:glibc}
  \caption{Source code for the glibc test case.}
\end{figure*}

\subsection{Thunderbird}

\todo{Rewrite to fit new structure}

\verb|thunderbird| is Mozilla bug number
391259\cite{thunderbird39125}, a simple time-of-check, time-of-use
race in the IMAP client component of Thunderbird, a popular
open-source e-mail client.  We modified Thunderbird to include some
additional debugging messages and used a custom scheduler in order to
make the bug reproduce more readily; the test is otherwise identical
to the behavior which a user might have encountered.  The relevant
parts of the program are as follows:

\begin{verbatim}
void nsImapProtocol::CloseStreams() {
  if (m_transport)
      m_transport = nsnull;
}
PRBool nsImapProtocol::ProcessCurrentURL() {
  if (m_transport)
    m_transport->SetTimeout(
      TIMEOUT_READ_WRITE, PR_UINT32_MAX);
}
\end{verbatim}

\noindent
If \verb|m_transport| is set to \verb|nsnull| by \verb|CloseStreams()|
in between the two accesses in \verb|ProcessCurrentURL| then the
program will crash.  This is essentially the same bug as
\verb|toctou|, but embedded in a much large program.  As such, the
final result is similar: a single suggested fix, with two critical
sections, one containing the two accesses in \verb|ProcessCurrentURL|
and one containing the assignment in \verb|CloseStreams|.  This fixes
the bug.

\subsection{mysql1}

Mysql 14747.  Looking at this some more I think it only works on
call-eliding mode, which I really don't want to have to describe on
top of everything else, which is going to make this really quite
tricky.

\subsection{mysql2}

Mysql 56324.  Read side:

\begin{verbatim}
void my_thread_end(void)
{
  struct st_my_thread_var *tmp;
  tmp= my_pthread_getspecific(struct st_my_thread_var*,THR_KEY_mysys);

#ifdef EXTRA_DEBUG_THREADS
  fprintf(stderr,"my_thread_end(): tmp: 0x%lx  pthread_self: 0x%lx  thread_id: %ld\n",
          (long) tmp, (long) pthread_self(), tmp ? (long) tmp->id : 0L);
#endif

#ifdef HAVE_PSI_INTERFACE
  /*
    Remove the instrumentation for this thread.
    This must be done before trashing st_my_thread_var,
    because the LF_HASH depends on it.
  */
  if (PSI_server)
    PSI_server->delete_current_thread();
#endif
\end{verbatim}

Write side:

\begin{verbatim}
#ifdef __WIN__
int win_main(int argc, char **argv)
#else
int mysqld_main(int argc, char **argv)
#endif
{
  /*
    Perform basic thread library and malloc initialization,
    to be able to read defaults files and parse options.
  */
  my_progname= argv[0];
  if (my_basic_init())
  {
    fprintf(stderr, "my_basic_init() failed.");
....
  }
#endif
  clean_up(1);
#ifdef HAVE_PSI_INTERFACE
  /*
    Disable the instrumentation, to avoid recording events
    during the shutdown.
  */
  if (PSI_server)
  {
    PSI_server->delete_current_thread();
    PSI_server= NULL;
  }
  shutdown_performance_schema();
#endif
  mysqld_exit(0);
}
\end{verbatim}

And the race is then on \verb|PSI_server|.  This one actually works;
woo.

\section{Comparison of DRS-based and non-DRS-based modes}

\includegraphics{eval/mysql-timings.eps}

\section{Actually fixing bugs}
