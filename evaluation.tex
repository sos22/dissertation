\chapter{Evaluation}
\label{chapter:eval}

Previous chapters have described the basic {\technique} technique.  I
will now evaluate its effectiveness, and the performance of my
implementation {\implementation}.  This evaluation will consist of the
following parts:

\begin{itemize}
\item Section~\ref{sect:eval:artificial} explores the behaviour of the
  tool and the technique on a number of artificial bugs in simple test
  programs.  This includes a comparison to a
  DataCollider\needCite{}-like tool which I implemented for the
  purpose of the evaluation.
\item Section~\ref{sect:eval:semiartificial} investigates some
  slightly more realistic bugs.  These include a simplified version of
  a real bug in glibc\needCite{} and some bugs which I deliberately
  introduced into the STAMP benchmark suite\needCite{}.
\item Section~\ref{sect:eval:real} applies the tool to some large
  programs: pbzip2\needCite{}, MySQL\needCite{}, and
  Thunderbird\needCite{}.  I show that the analysis completes in a
  tolerable amount of time even for some very large programs,
  and demonstrate that it can both reproduce and fix a (small)
  number of real bugs.
\item Section~\ref{sect:eval:time_details} then investigates the
  tool's performance on large programs in slightly more detail,
  showing how the time taken breaks down across the various phases
  and the effects of some of {\implementation} optimisations.
\item The final section, \ref{sect:eval:validation}, gives the results
  of some experiments intended to validate that {\implementation} is
  itself correctly implemented, and hence to give confidence that the
  other results are reasonable.
\end{itemize}

\section{DataCollider-like implementation}
\label{sect:eval:datacollider}

\todo{Rewrite}

As a point of comparison, I implemented a tool which explores
alternative thread schedules at random, without first analysing the
program to obtain {\StateMachines} and verification conditions,
inspired by DataCollider\needCite{}.  The idea here is to select a
random subset of memory-accessing instructions in the program and
place breakpoint instructions at them.  When these breakpoints fire
the tool examines the program state to determine which memory
locations they access and sets a processor watch point\needCite{} on
that location.  It then waits a short while to see if the watch point
is triggered by any other threads.  If it is, the two instructions
race and the tool selects which to execute first at random, then
allows both threads to proceed unmolested.  Otherwise, the breakpoint
and watch point are cleared and another breakpoint set elsewhere in
the program.  The result is that the program explores its possible
schedules more quickly than it would if run without these
manipulations, which should lead to bugs being discovered more
quickly.

The effectiveness of this tool is obviously highly dependent on the
fraction of instructions which are covered by breakpoints and the
length of the delays inserted.  Some initial experiments\smh{By you?
  or by them?  what workloads?} suggested that these parameters lead
to the most rapid bug reproduction\editorial{Should have done this in
  a more systematic way.}:

\begin{itemize}
\item
  At any given point, half of the program's memory accessing
  instructions will have an instruction breakpoint, excluding stack
  accesses\editorial{Putting a breakpoint on every instruction lead to
    a deadlock; should probably figure out why.}.
\item
  When a breakpoint is hit, the tool will wait for up to 10ms for a
  matching read or write to arrive.  When a thread does arrive, it
  will select which to resume first at random, with equal probability.
\item
  Once a second, the tool discards its current instruction breakpoint
  set and generates a new one.
\end{itemize}

It is instructive to compare these parameters to those used in the
DataCollider paper.  The 10ms timeout is broadly similar, as
DataCollider uses between one and fifteen millisecond timeouts,
depending on the type of instruction.  Setting breakpoints on half of
instructions, on the other hand, is not.  The actual DataCollider
implementation adjusts the breakpoint density dynamically so as to
achieve a particular breakpoint rate, and their paper does not specify
a numerical breakpoint density, which makes a direct comparison
difficult.  It is, however, possible to estimate the breakpoint
density from the information which they do give.  Their evaluation
shows breakpoint rates of up to 1500 breakpoints per second in a
virtual machine with two processors running at 2.4GHz, so $4.8 \times
10^9$ cycles per second.  If one assumes roughly one non-stack memory
access every hundred cycles that translates to 4.8 million memory
accessing instructions per second, and so they must trap roughly
0.03\% of memory accessing instructions.  Even allowing for the fact
that their implementation preferentially places breakpoints on
instructions which execute infrequently this is still likely to
translate to a breakpoint ratio several orders of magnitude lower than
that used in this evaluation.

Setting such a high breakpoint ratio has two main effects: bugs are
detected more quickly, but the overheads of the tool are much higher.
In particular, the high overheads mean that this tool is not an
entirely practical approach to investigating concurrency bugs.  I
present it merely as a baseline against which to evaluate
{\implementation}.  \todo{Should really eval the overheads, rather
  than just asserting that they're massive.  I mean, they are, but I
  should have some evidence to back that up.}\smh{+maybe look at would
  they would be if you uses 0.03\% like the original?}

More fundamentally, using DataCollider to explore alternative
schedules is itself somewhat unfair, as DataCollider was originally
intended to discover races rather than to permute schedules, and,
while it is trivial to extend it to perform schedule exploration, it
is not entirely surprising that the results are somewhat poor.
Nevertheless, it represents the approach which is conceptually close
to {\technique}'s in the existing literature, and so I consider it to
be an interesting reference point.

\todo{I'd really like to do a comparison with CHESS, as well, but
  that's only implemented for 32-bit Windows programs, and
  {\technique} is only implemented for 64-bit Linux ones, which makes
  a bit of a mess of direct comparisons.  It'd be kind of fun to
  re-implement it for Linux, but I don't really have time to do that
  right now.}

\section{Artificial bugs}
\label{sect:eval:artificial}

I now present the results of running {\implementation} on a number of
artificial bugs, showing that it assists in both reproducing the bugs
and fixing them, and that the analysis phases complete very quickly.

\subsection{Simple time-of-check, time-of-use (TOCTOU) bug (simple\_toctou)}\footnote{This bug was previously discussed in
  Section~\ref{sect:derive:simple_toctou_example}.}
\label{sect:eval:simple_toctou}


\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&*global\_ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }\hfill %
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }\hfill
  \caption{The two sides of the simple\_toctou bug.}
  \label{fig:eval:simple_toctou}
\end{figure}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
    \begin{tabular}{rlll}
              & \multicolumn{3}{l}{crashing\_thread:} \\
      400694: & movq  & global\_ptr, &\!\!\!\%rax\\
      40069b: & testq & \%rax,       &\!\!\!\%rax \\
      40069e: & je    & \multicolumn{2}{l}{4006ad}\\
      4006a0: & movq  & global\_ptr, &\!\!\!\%rax\\
      4006a7: & movl  & \$0x5,       &\!\!\!(\%rax)\\
    \end{tabular}
    }
  }%
  \hspace{-5mm}\subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{rlll}
        & \multicolumn{3}{l}{interfering\_thread:} \\
        400816: & lea  & c(\%rsp), &\!\!\!\%rbp \\
        ...\\
        400884: & movq & \%rbp, &\!\!\!global\_ptr\\
        ...\\
        4008fb: & movq & \$0x0, &\!\!\!global\_ptr\\
      \end{tabular}
      }
    }
  \caption{Disassembly of the program fragments in Figure~\ref{fig:eval:simple_toctou}.}
  \label{fig:eval:simple_toctou:compiled}
\end{figure}

This is the simplest possible kind of concurrency bug: a
single-variable time-of-check, time-of-use race.  The two threads
involved are shown in Figure~\ref{fig:eval:simple_toctou}.  The intent
here is to model a very simple structure which is accessed frequently
but updated rarely.  The bug is, of course, that the interfering
thread might set \texttt{global\_ptr} to \texttt{NULL} in between the
two reads of it in the crashing thread, causing the crashing thread to
crash when it dereferences the pointer it loaded.
\texttt{STOP\_ANALYSIS()} is a marker which prevents {\technique}'s
analysis from exploring past that point when building the CFGs from
which {\StateMachines} are constructed.  It is used here to help keep
the crash summaries generated simple and easily explained;
{\implementation} is able to reproduce and fix the bug even without
those markers\editorial{Could last time I tried it, probably worth
  checking that again with the current version.}.

\subsubsection{Generating candidate bugs}

The first step in the analysis is to build the CFG for the
\backref{crashing thread} (see
Section~\ref{sect:derive:build_static_cfg}), which in this case is
straightforward; the result is shown in
Figure~\ref{fig:eval:simple_toctou:cfg}.  This can then be compiled to
produce the {\StateMachine} shown in
Figure~\ref{fig:eval:simple_toctou:sm}.

\begin{figure}
  \subfigure[][Control flow graph.  $\varnothing$ indicates that the
    thread has left the CFG.]{
    \begin{tikzpicture}
      \node (cfg1) [CfgInstr] {\texttt{400694}: cfg1};
      \node (cfg2) [CfgInstr,below=of cfg1] {\texttt{40069b}: cfg2};
      \node (cfg3) [CfgInstr,below=of cfg2] {\texttt{40069e}: cfg3};
      \node (cfg3b) [right = of cfg3] {$\varnothing$};
      \node (cfg4) [CfgInstr,below=of cfg3] {\texttt{4006a0}: cfg4};
      \node (cfg4b) [below = of cfg4] {$\varnothing$};
      \draw[->] (cfg1) -- (cfg2);
      \draw[->] (cfg2) -- (cfg3);
      \draw[->,ifTrue] (cfg3) -- (cfg3b);
      \draw[->,ifFalse] (cfg3) -- (cfg4);
      \draw[->] (cfg4) -- (cfg4b);
    \end{tikzpicture}
    \label{fig:eval:simple_toctou:cfg}
  }
  \subfigure[][{\STateMachine}]{
    \begin{tikzpicture}
      \node (l1) at (0,2) [stateSideEffect] {\stLoad{1}{\mathrm{global\_ptr}} @ cfg1 };
      \node (l2) [stateIf, below=of l1] {\stIf{\smTmp{1} = 0}};
      \node (l4) [stateSideEffect, below=of l2] {\stLoad{2}{\mathrm{global\_ptr}} @ cfg4 };
      \node (l3) [stateTerminal, right=of l4] {\stSurvive };
      \node (l5) [stateIf, below=of l4] {\stIf{\smBadPtr{\smTmp{2}}}};
      \node (l6) [stateTerminal, below=of l5] {\stCrash};
      \draw[->] (l1) -- (l2);
      \draw[->,ifTrue] (l2) -- (l3);
      \draw[->,ifFalse] (l2) -- (l4);
      \draw[->] (l4) -- (l5);
      \draw[->,ifFalse] (l5) -- (l3);
      \draw[->,ifTrue] (l5) -- (l6);
    \end{tikzpicture}
    \label{fig:eval:simple_toctou:sm}
  }
  \caption{\backref{CFG} and {\STateMachine} for the crashing thread
    in Figure~\ref{fig:eval:simple_toctou:compiled}.}
\end{figure}

The next step is to build the \backref{interfering stores} set (see
Section~\ref{sect:derive:write_side}).  The crashing {\StateMachine}
contains two loads, at \texttt{400694} and \texttt{4006a0}, and so the
analysis will ask\editorial{ick} the \backref{program model} what
stores might interact with them, and the \backref{program model} will
use the results of the \backref{dynamic alias analysis} (see
Section~\ref{sect:program_model:dynamic_alias}) to return the set
\{\texttt{400884}, \texttt{4008fb}\}.  The \texttt{STOP\_ANALYSIS()}
markers prevent these from being clustered together, and there are no
other \backref{communicating instructions}, and so there will be two
\backref{interfering thread} CFGs, shown in
Figures~\ref{fig:eval:simple_toctou:interfering_cfg1}
and~\ref{fig:eval:simple_toctou:interfering_cfg2}.

\begin{figure}
  \begin{tabular}{cc}
    \subfigure[][CFG for interfering store \texttt{400884}]{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\texttt{400884}: cfg5};
        \node (b) [below = of a] {$\varnothing$};
        \draw[->] (a) -- (b);
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_cfg1}
    } &
    \subfigure[][{\STateMachine} for interfering store \texttt{400884}, without static analysis]{
      \begin{tikzpicture}
        \node [stateSideEffect] {\stStore{\smReg{rbp}{2}}{\mathrm{global\_ptr}} @ cfg5};
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_sm1}
    } \\
    \subfigure[][CFG for interfering store \texttt{4008fb}]{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\texttt{4008fb}: cfg6};
        \node (b) [below = of a] {$\varnothing$};
        \draw[->] (a) -- (b);
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_cfg2}
    } &
    \subfigure[][{\STateMachine} for interfering store \texttt{4008fb}]{
      \begin{tikzpicture}
        \node [stateSideEffect] {\stStore{0}{\mathrm{global\_ptr}} @ cfg6};
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_sm2}
    }
  \end{tabular}
  \caption{Interfering CFGs and {\StateMachines}.}
\end{figure}

Consider the interfering store at \texttt{400884} first.  Without the
static analysis phases, this would produce the {\StateMachine} shown
in Figure~\ref{fig:eval:simple_toctou:interfering_sm1}.  There is no
way for the program to crash due to interleaving this {\StateMachine}
with the crashing thread, as $\smReg{rbp}{2}$ is never a bad pointer,
but the {\StateMachines} do not contain enough information to show
that.  The analysis will produce the verification condition shown in
Figure~\ref{fig:eval:simple_toctou:inferred_assumption1}. The fixed
register and static aliasing analyses will both eliminate this
candidate bug, as either is capable of showing that $\smReg{rbp}{2}$
is a pointer into thread 2's stack frame, and hence that it is a valid
pointer.  The \backref{interfering store} at \texttt{4008fb}, on the
other hand, does represent a valid bug, as interleaving it with the
\backref{crashing thread} could lead to a crash, and it produces the
\backref{verification condition} shown in
Figure~\ref{fig:eval:simple_toctou:inferred_assumption2}.

\begin{figure}
  \begin{tabular}{lll}
    \backref{CI atomic}: & $\smLoad{\mathrm{global\_ptr}} = 0$ &\!\!\!$\vee\,\, \neg\smBadPtr{\smLoad{\mathrm{global\_ptr}}}$ \\
    \backref{IC atomic}: & $\smReg{rbp}{2} = 0$                &\!\!\!$\vee\,\, \neg\smBadPtr{\smReg{rbp}{2}}$\\
    \backref{Verification condition}: & \multicolumn{2}{l}{$\happensBefore{\mai{cfg1}{1}}{\mai{cfg5}{2}} \wedge \happensBefore{\mai{cfg5}{2}}{\mai{cfg4}{2}} \wedge \smBadPtr{\smReg{rbp}{2}}  \wedge$}\\
                                      & $\smLoad{\mathrm{global\_ptr}} \not= 0$\\
  \end{tabular}
  \caption{\backref{Inferred assumption} and \backref{verification
      condition} produced using the crashing {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:sm} and the interfering
    {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:interfering_sm1}.}
  \label{fig:eval:simple_toctou:inferred_assumption1}
\end{figure}

\begin{figure}
  \begin{tabular}{lll}
    \backref{CI atomic}: & $\smLoad{\mathrm{global\_ptr}} = 0$ &\!\!\!$\vee\,\, \neg\smBadPtr{\smLoad{\mathrm{global\_ptr}}}$ \\
    \backref{IC atomic}: & \true\\
    \backref{Verification condition}: & \multicolumn{2}{l}{$\happensBefore{\mai{cfg1}{1}}{\mai{cfg6}{2}} \wedge \happensBefore{\mai{cfg6}{2}}{\mai{cfg4}{2}} \wedge \smLoad{\mathrm{global\_ptr}} \not= 0$}\\
  \end{tabular}
  \caption{\backref{Inferred assumption} and \backref{verification
      condition} produced using the crashing {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:sm} and the interfering
    {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:interfering_sm2}.}
  \label{fig:eval:simple_toctou:inferred_assumption2}
\end{figure}

The time taken to perform this analysis is quite modest: $0.52 \pm
0.04$ seconds for the static analysis phase and $0.18 \pm 0.01$
seconds for the {\StateMachine} analysis (mean and standard deviation
of mean for ten runs in both cases).  The dynamic analysis phase
achieved complete coverage essentially as soon as the program started.
This is a reasonable lower bound on the time which {\technique} will
take to process a very simple bug; any realistic program will take far
longer than this to process.

\subsubsection{Reproducing the bug}
This \backref{verification condition} can now be turned into a
\backref{crash enforcer}.  The only happens-before graph will be the
one shown in Figure~\ref{fig:eval:simple_toctou:hb_graph}, and it will
have the side condition that $\smLoad{\mathrm{global\_ptr}} \not= 0$.
This side condition can be evaluated completely at either
$\mai{cfg1}{1}$ or $\mai{cfg6}{2}$, and so the crash enforcement plan
will be as shown in Figure~\ref{fig:eval:simple_toctou:enforce_plan}.
In other words, whenever a program thread reaches \texttt{400694} and
global\_ptr is non-zero, the enforcer will wait for an interfering
thread to reach \texttt{4008fb}, after loading global\_ptr.  If one
does arrive, the enforcer will make the crashing thread wait for the
interfering thread to complete its store before proceeding to the load
at \texttt{4006a0}.  This will be sufficient to reproduce the bug.

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (l1) {\texttt{400694}: $\mai{cfg1}{1}$};
    \node[draw, below right = of l1] (l2) {\texttt{4008fb}: $\mai{cfg6}{2}$ };
    \node[draw, below left = of l2] (l3) {\texttt{4006a0}: $\mai{cfg4}{1}$ };
    \draw[->] (l1) -- (l3);
    \draw[->, happensBeforeEdge] (l1) -- (l2);
    \draw[->, happensBeforeEdge] (l2) -- (l3);
  \end{tikzpicture}
  \caption{Happens-before graph to be enforced for simple\_toctou}
  \label{fig:eval:simple_toctou:hb_graph}
\end{figure}

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (l1) {\texttt{400694}: $\mai{cfg1}{1}$};
    \node[left = 0 of l1] {$\smLoad{\mathrm{global\_ptr}} \not= 0$};
    \node[draw, below right = of l1] (l2) {\texttt{4008fb}: $\mai{cfg6}{2}$ };
    \node[right = 0 of l2] {$\smLoad{\mathrm{global\_ptr}} \not= 0$};
    \node[draw, below left = of l2] (l3) {\texttt{4006a0}: $\mai{cfg4}{1}$ };
    \draw[->] (l1) -- (l3);
    \draw[->, happensBeforeEdge] (l1) -- (l2);
    \draw[->, happensBeforeEdge] (l2) -- (l3);
  \end{tikzpicture}
  \caption{Crash enforcement plan for simple\_toctou}
  \label{fig:eval:simple_toctou:enforce_plan}
\end{figure}

This enforcer is effective at reproducing the bug.  Without the
enforcer, the time taken to reproduce the bug is roughly exponentially
distributed with $\lambda = 0.20$Hz; with the enforcer, it is roughly
normally distributed with $\mu = 1.17$, $\sigma = 0.03$\footnote{Both
  distributions were generated by running the program 100 times until
  it crashed.  The normal distribution was confirmed using the
  Anderson-Darling and Jarque-Bera tests at the 90\% level.  The
  exponential distribution was confirmed using a $\chi$-squared test
  at the 90\% level.  I also estimated the variance of the $\lambda$
  parameter using a jackknife method; the result was that $\lambda$ is
  itself normally distributed with mean 0.20Hz and standard deviation
  0.01Hz.}.  Cumulative distribution functions for these distributions
are shown in Figure~\ref{fig:eval:simple_toctou:repro_cdfs}.  The
enforcer reduced the median time to reproduce the bug from 4.93
seconds to 1.17, a four-fold reduction in the time taken.

Perhaps more interestingly, it also reduced the ninety-fifth
percentile of the time taken to reproduce the bug from 14 seconds to
1.2 seconds, a nearly twelve-fold reduction.  Without an enforcer,
this bug's reproduction time has a very long tail distribution, so
that some runs will take far longer to reproduce the bug than others.
This can make the bug difficult for a programmer to work with.  The
enforcer almost completely eliminates this behaviour.

\begin{figure}
  \include{eval/crash_times/simple_toctou}
  \caption{Reproduction times for the simple\_toctou bug, with and
    without an enforcer.  Note the log scale.  The stairstep effect is
    caused because the interfering thread runs once a second, and so
    the time to reproduce the bug is almost always near to a multiple
    of one.}
  \label{fig:eval:simple_toctou:repro_cdfs}
\end{figure}

The time taken to build the enforcer is also quite reasonable: $0.13
\pm 0.01$ seconds, again as a mean and standard deviation of ten runs.
This includes all of the time needed to devise the plan, select the
patch strategy, and actually compile the enforcer.

\subsubsection{Fixing the bug}
{\Implementation} can also generate a fix for this bug.  In this case,
the CFG fragments to be protected will be the complete CFGs of both
threads, as the first and last CFG nodes in both threads are involved
in happens-before edges.  This corresponds to modifying the program as
shown in Figure~\ref{fig:eval:simple_toctou:fix}.  This correctly
fixes the bug.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{acquire\_lock();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&t = global\_ptr;\\
        &&release\_lock();\\
        &&*t = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }\hfill %
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &acquire\_lock();\\
        &global\_ptr = NULL;\\
        &release\_lock();\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }\hfill
  \caption{The fix generated by {\implementation} for the simple\_toctou bug..}
  \label{fig:eval:simple_toctou:fix}
\end{figure}

It does, however, have a rather high performance overhead: without a
fix, the crashing thread completes $352.5 \pm 0.2
{\times} 10^6$ iterations of the loop per second; with one, it
completes $95.9 \pm 0.2 {\times} 10^6$ (mean and
standard deviation of ten runs each of ten seconds, discarding any
runs in which the unfixed program crashed, both distributions
confirmed to be normal using Anderson-Darling and Jarque-Bera at the
90\% level).  That gives an overhead of roughly a factor of 3.7.  This
is obviously rather large, but is probably close to {\technique}'s
worst case: the read-side critical section is very small and runs with
very high frequency, and so the patch must acquire and release the
lock with similarly high frequency and these lock operations dominate
the time taken.  Any realistic test would usually have much lower
overhead, assuming lock contention does not become a factor, simply
because the critical sections would run less frequently and the
overhead could be more effectively amortised.  Even in this case, a
factor of four overhead is not completely unreasonable when the
alternative is a program which crashes frequently.

For comparison, I also produced a version of the patch which does
everything except for acquiring the lock.  This version completed $321
\pm 5 {\times} 10^6$ loops per second.  This strongly suggests that
the overhead in this case is mostly caused by the cost of the
uncontended lock operations.

\todo{Worst case ignoring the loss of concurrency, of course.}

\todo{Give details of the machine the test is running on.}\smh{Yes}

\subsection{Indexed TOCTOU bug (indexed\_toctou)}
\label{sect:eval:indexed_toctou}

\todo{The results of this test are \emph{very} sensitive to set-up of
  the machine on which they're run.  I'm going to need a lot of data
  here to get sensibly small error bars.}

\todo{Generally need to re-run all of these experiments.}

In this variant of a TOCTOU bug, there are multiple instances of the
structure which is being raced on and the bug will only manifest if
the reading and writing threads happen to coincide.  This bug
exercises the side condition-checking part of {\technique}'s crash
enforcers.  The code involved in the race is shown in
Figure~\ref{fig:eval:indexed_toctou}.  Except where otherwise noted,
\verb|NR_PTRS| is set to 100.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{idx = random() \% NR\_PTRS;}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptrs[idx] != NULL) \{}\\
        &&*(global\_ptrs[idx]) = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \}\\
      \end{tabular}
    }
  }%
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        & idx = random() \% NR\_PTRS;\\
        & STOP\_ANALYSIS();\\
        & global\_ptrs[idx] = NULL;\\
        & STOP\_ANALYSIS();\\
        & global\_ptrs[idx] = \&t;\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The two sides of the indexed\_toctou bug.}
  \label{fig:eval:indexed_toctou}
\end{figure}

As with the simple\_toctou test, this test produces a single candidate
bug, with a similar enforcer and fix.  The only important difference
is that the enforcer includes a side condition $\mathtt{idx}_1 =
\mathtt{idx}_2$, where $\mathtt{idx}_1$ is an expression for
\texttt{idx} in the crashing thread and $\mathtt{idx}_2$ that in the
interfering thread, which is checked on the first happens-before edge.

The enforcer was effective at making this bug reproduce more easily.
With no enforcer loaded, the bug reproduction time was exponentially
distributed with $\lambda = 0.6 \pm 0.2$Hz.  With an enforcer loaded,
the reproduction time was exponentially distributed with $\lambda =
6.87 \pm 0.01$Hz, a more than ten-fold improvement in the reproduction
rate.  The median reproduction time is reduced from 1.2 seconds to 0.1
seconds and the $95^{th}$ percentile reproduction time from five
seconds to 0.45 seconds.  \todo{Redo these numbers}

I also investigated the behaviour of this test with an enforcer loaded
but no side condition checking performed.  In that case, the bug
reproduction time was again exponentially distributed, but this time
$\lambda = 0.054 \pm 0.004$Hz \todo{re-do experiment to check that
  number}.  This reduced enforcer not only fails to make the bug
reproduce more quickly; it actually makes it \emph{less} likely to be
triggered, per unit time!  This is because an enforcer without
side-condition checking will often slow the program down in order to
impose the happens-before graph even in situations where doing so is
unlikely to trigger the bug, and this causes the buggy code to run far
less frequently than it otherwise would.

As a final test, I investigated the effect changing the \verb|NR_PTRS|
parameter, and how that interacted with the delay parameter.  The
results without an enforcer loaded were quite surprising.  The time
taken to reproduce the bug with no enforcer loaded does not appear to
vary smoothly with \texttt{NR\_PTRS}.  For instance, with
$\texttt{NR\_PTRS} = 6037$, the bug failed to reproduce within three
hours on three successive runs, whereas with $\texttt{NR\_PTRS} =
6036$ it reproduced after 182 seconds

\begin{table}
\begin{tabular}{llll}
NR\_PTRS & Full enforcer             & No side conditions          & No enforcer               \\
10       & $\lambda = 6.4 \pm 0.2/s$ & $\lambda = 1.27 \pm 0.02/s$ & $\lambda = 0.32 \pm 0.05$ \\
100      & $t = 100 - 200ms$         & $\lambda = 0.62 \pm 0.06/s$ & $\lambda = 0.16 \pm 0.02$ \\
1000     & $t = 110 - 201ms$         & $t = 33.1 \pm 8.8$          & $\lambda = 0.24 \pm 0.02$ \\
\end{tabular}
\caption{Time taken to reproduce indexed\_toctou as NR\_PTRs changes.}
\label{table:eval:indexedtoctou:nrptrs}
\end{table}

The automatic fix generator works well with this bug, and produces
roughly the same fix as it did in the simple\_toctou bug: one critical
section which covers the two critical loads in the read thread and one
which covers the critical store in the write thread.  To characterise
the performance overheads of the fix I again counted the number of
times the read and write loops execute per second with and without the
fix applied, running the test for ten seconds and discarding any runs
in which the test program crashed.  Without a fix applied, the test
completed the read loop $9.6 * 10^5 \pm 0.5 * 10^5$ times per second
and the write loop $9.0 \pm 0.2 * 10^5$ times per second; with a fix,
it completed $8.3 * 10^5 \pm 0.3 * 10^5$ read loops and $7.8 * 10^5
\pm 0.2 * 10^5$ write loops (mean and standard deviation of ten runs).
The overhead was therefore roughly 15\% on both the read and write
sides of the test\editorial{Should really put some error bars on
that.}.  This is far smaller than the factor of four reported in the
simple\_toctou case, largely because the test loop in this case
includes a call to \verb|random|, which is rather expensive relative
to simple lock operations and helps to amortise the cost of the
additional synchronisation.

\todo{I don't believe for a minute that those distributions are Gaussian.}

\subsection{Biassed indexed TOCTOU bugs (crash\_indexed\_toctou, interfering\_indexed\_toctou)}

\todo{Re-do experiments.}

These bugs are similar to the indexed\_toctou with $\texttt{NR\_PTRS}
= 100$, except with an additional one second delay in either the
interfering or crashing thread's loops, such that either the
interfering thread (for interfering\_indexed\_toctou) or the crashing
thread (for crash\_indexed\_toctou) runs far more often than the
other.  These tests are intended to illustrate the importance of
placing delays at appropriate operations in the crash enforcement
message-passing system.  The results are shown in
Table~\ref{fig:biassed_indexed_toctou:times}.  These results show
that, while the delay placement mechanism is not always guaranteed to
find the placing, it does avoid some very poor possible placements.

\begin{table}
  \begin{tabular}{llllll}
                                 & crash\_indexed\_toctou & interfering\_indexed\_toctou \\
    No enforcer                  & No reproduction        & No reproduction\\
    Normal delay positioning \\
    Delays on sends \\
    Delays on receives \\
    Delays on both \\
  \end{tabular}
  \caption{Time taken to reproduce the crash\_indexed\_toctou and
    interfering\_indexed\_toctou bugs in different configurations.
    All experiments were repeated twenty times.}
  \label{fig:biassed_indexed_toctou:times}
\end{table}

\subsection{Multi-variable consistency constraint (multi\_variable)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{} \\
        & STOP\_ANALYSIS();\\
        & v1 = global1;\\
        & v2 = global2;\\
        & assert(v1 == v2);\\
        & STOP\_ANALYSIS();\\
        & sleep(10 milliseconds);\\
        \multicolumn{2}{l}{\}}\\
        \\
        \\
        \\
        \\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        & STOP\_ANALYSIS();\\
        & global1 = 5;\\
        & STOP\_ANALYSIS();\\
        & global2 = 5;\\
        & STOP\_ANALYSIS();\\
        & sleep(100 milliseconds);\\
        & STOP\_ANALYSIS();\\
        & global1 = 7;\\
        & global2 = 7;\\
        & STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The two sides of the multi\_variable bug. The delays were
    chosen so that the program crashed in a reasonable amount of time
    when run unmodified.}
  \label{fig:eval:multi_variable}
\end{figure}

This bug is intended to explore {\technique}'s effects on
multi-variable atomicity violations.  The two sides of the bug are
shown in Figure~\ref{fig:eval:multi_variable}.  Note that in this case
the race leads to an assertion failure, whereas previous bugs lead to
a bad pointer dereference.  {\Technique} reports a single candidate
bug in this program, corresponding to interleaving the crashing thread
with the two stores which set \texttt{global1} and \texttt{global2} to
7 in the interfering thread.  This enforcer causes the bug to
reproduce quickly (in an average of 300ms \todo{more details of
  experiment}).

It is perhaps worth explaining why {\technique} only reported a single
bug here.  {\Technique} discovered that the crashing thread can crash
when interleaved with the stores which set the globals to 7, but
missed the fact that it can also crash when interleaved with the ones
which set them to 5.  This is because of an interaction between the
second \texttt{STOP\_ANALYSIS()} in the interfering thread and the
\backref{inferred assumption}, discussed in
Section~\ref{sect:derive:inferred_assumption}.  The
\texttt{STOP\_ANALYSIS()} causes the two stores to be converted into
independent {\StateMachines}.  Consider the first store; the other is
symmetrical.  Figure~\ref{fig:eval:multi_variable:other_bug} shows how
the \backref{inferred assumption} is derived for this bug\footnote{The
  actual analysis performs this derivation using {\StateMachines},
  rather than by concatenating the program's actual code, but that
  makes no difference in this case.}.  The \backref{CI atomic}
constraint shows that the initial values of \texttt{global1} and
\texttt{global2} must be equal, and the \backref{IC atomic} constraint
shows that the initial value of \texttt{global2} must be 5.  Combining
these two shows that the initial value of \texttt{global} must also be
5, and so the store operation becomes a no-op and no candidate bug is
reported.

\begin{figure}
  \subfigure[][CI atomic]{
    \begin{tabular}{l}
      \hspace{-5mm}\texttt{
        \begin{tabular}{l}
          v1 = global1;\\
          v2 = global2;\\
          assert(v1 == v2);\\
          v1 = 5;\\
        \end{tabular}
      }\\
      \\
      $\smLoad{\texttt{global1}} = \smLoad{\texttt{global2}}$\\
    \end{tabular}
  }
  \hfill
  \subfigure[][IC atomic]{
    \begin{tabular}{l}
      \hspace{-5mm}\texttt{
        \begin{tabular}{l}
          v1 = 5;\\
          v1 = global1;\\
          v2 = global2;\\
          assert(v1 == v2);\\
        \end{tabular}
      }\\
      \\
      $\smLoad{\texttt{global2}} = 5$\\
    \end{tabular}
  }\\
  \\
  Inferred assumption: $\smLoad{\texttt{global2}} = 5 \wedge \smLoad{\texttt{global1}} = 5$
  \caption{Deriving the \backref{inferred assumption} for the other bug in the multi\_variable bug.}
  \label{fig:eval:multi_variable:other_bug}
\end{figure}

The reported bug can also be converted into a fix.  This fix correctly
fixes the reported bug, but does not, of course, fix the one which is
not reported.  As such, the program might still crash (although the
average time before a crash increases from \todo{...} to \todo{...}
seconds).  If the \texttt{STOP\_ANALYSIS()} marker is removed then
both bugs will be discovered, and a fix which fixes both bugs
correctly causes the program to no longer crash.  \todo{That needs
  rephrasing.}

\subsection{Context-dependent races (context)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{f(int **ptr) \{}\\
        &\multicolumn{2}{l}{if (*ptr) \{}\\
        &&**ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        \multicolumn{3}{l}{\}}\\
        \\
        \multicolumn{3}{l}{while (1) \{}\\
        & \multicolumn{2}{l}{if (random() \% 1000 == 0) \{}\\
        &&STOP\_ANALYSIS();\\
        &&f(\&global\_ptr1);\\
        &&STOP\_ANALYSIS();\\
        &\multicolumn{2}{l}{\} else \{}\\
        &&STOP\_ANALYSIS();\\
        &&f(\&global\_ptr2);\\
        &&STOP\_ANALYSIS();\\
        &\multicolumn{2}{l}{\}}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \\
        \\
        \\
        \\
        \\
        \\
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global\_ptr1 = \&t;\\
        &STOP\_ANALYSIS();\\
        &sleep(1 millisecond);\\
        &STOP\_ANALYSIS();\\
        &global\_ptr1 = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
        \\
        \\
      \end{tabular}
    }
  }
  \caption{Crashing and interfering threads for the context test.}
  \label{fig:eval:context}
\end{figure}

This test demonstrates {\technique}'s ability to take account of some
cross-function properties of the program.  The test program is shown
in Figure~\ref{fig:eval:context}.  The function \texttt{f} is called
from two places in the crashing thread, one of which passes
\texttt{\&global\_ptr1} as the pointer argument while the other passes
\texttt{\&global\_ptr2}.  Meanwhile, the interfering thread loops
modifying \texttt{global\_ptr1}.  This means that the call to
\texttt{f(\&global\_ptr1)} can sometimes suffer a crash caused by a
race while the call to \texttt{f(\&global\_ptr2)} cannot.  The safe
call is far more common than the unsafe one, and so it is important
that the enforcer only insert delays in the correct calling context.

Without an enforcer, this test program crashes reasonably
infrequently, reaching the three minute timeout six times out of
twenty runs and taking an average of 80 seconds in the remaining
cases.  With a full crash enforcer, including context checking, the
time taken to obtain a reproduction is uniformly distributed between
100 and 200 milliseconds.  With an enforcer modified to not perform
stack context checking, the bug did not reproduce within three
minutes.

\todo{Need to re-run those experiments with repeats.}

\subsection{Write-to-read hazard (write\_to\_read)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = \&t;\\
        &*global\_ptr = 5;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The write\_to\_read test case.}
  \label{fig:eval:write_to_read}
\end{figure}

This test investigates a form of write-write-read race, whereas all of
the previous ones have considered only write-read ones.  It is shown
in Figure~\ref{fig:eval:write_to_read}.  The crashing thread loops
setting a global variable to point at some location and then proceeds
to use that global variable, while at the same time the interfering
thread loops setting that global variable to \texttt{NULL}.  In other
words, there is a write-to-read hazard in the crashing thread which
might be interrupted by the interfering thread, leading to a crash.

This test program is surprisingly reliable, given that it runs both
sides of the race in a tight loop with no delays or synchronisation,
and can often run for several minutes without encountering an error on
an otherwise idle system, during which time the buggy code might run
billions of times.  A more realistic test would run the two sections
far less frequently, and so might easily require hundreds of years of
CPU time to reproduce the bug.  I suspect that this is because the
store and load instructions in the crashing thread are close enough
together that the load is always satisfied from the processor's write
buffer, and so only returns \texttt{NULL} when the processor receives
an interrupt in precisely the wrong place.  \todo{It probably wouldn't
  be hard to check this with a hacked up kernel.  Should probably do
  that.}  The {\technique}-generated bug enforcer, by contrast, can
reliably reproduce the bug in a few hundred milliseconds.

{\Technique} can also generate a fix for this bug.  The difficulty of
reproducing the bug makes it difficult to validate experimentally that
the generated fix is correct, but manual inspection suggested that it
is.

\subsection{Multiple bugs (multi\_bugs)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        & \multicolumn{2}{l}{r = select\_test();}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        & \multicolumn{2}{l}{if (r) \{}\\
        && simple\_toctou\_crashing();\\
        & \multicolumn{2}{l}{\} else \{}\\
        && write\_to\_read\_crashing();\\
        & \multicolumn{2}{l}{\}}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        & \multicolumn{2}{l}{for (i = 0; i < 2000000; i++) \{}\\
        && STOP\_ANALYSIS(); \\
        && write\_to\_read\_interfering();\\
        && STOP\_ANALYSIS(); \\
        & \multicolumn{2}{l}{\}}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        & \multicolumn{2}{l}{simple\_toctou\_interfering();}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The multi\_bugs test.  The constant \texttt{2000000} was
    chosen so that the two bugs reproduce with roughly equal
    probability.}
  \label{fig:eval:multi_bugs}
\end{figure}

This test combines simple\_toctou and write\_to\_read into a single
test, demonstrating {\technique}'s ability to exercise several bugs
using a single enforcer.  The test program is shown in
Figure~\ref{fig:eval:multi_bugs}.  The behaviour of
\texttt|select\_test| is configurable at run time, and will either
always select simple\_toctou, always select write\_to\_read, or select
randomly.  The dynamic analysis phases were run with it configured to
select randomly.

{\Implementation} is able to generate a candidate bug for each of the
component bugs in the test, each of which can be instantiated into an
enforcer, and these enforcers succeed in reproducing their respective
bugs.  As expected, neither enforcer is able to reproduce both bugs.
On the other hand, if both bugs' happens-before graphs are loaded into
the enforcer then it is able to reproduce either bug, as desired.
Perhaps more surprisingly, when the test is configured to exercise
both bugs the enforcer consistently reproduces the write\_to\_read
bug.  This is simply because the enforcer reproduces the
write\_to\_read bug before the simple\_toctou interfering critical
section ever runs.

In the same way, both candidate bugs can be instantiated into
individual fixes, both of which correctly fix their respective bug
while leaving the other bug in place.  Alternatively, a combined fix
can be generated, and this successfully fixes both bugs.

\subsection{Multiple crashing and interfering threads (multi\_threads)}

This test investigates {\implementation}'s behaviour in programs with
a very large number of threads.  The racing code is the same as in
indexed\_toctou (Figure~\ref{fig:eval:indexed_toctou}) with
$\texttt{NR\_PTRS} = 100$, except that rather than having a single
thread running the crashing and interfering critical sections, this
test has 32 threads running each.  {\Technique} behaves roughly as
expected here: it is able to generate an enforcer and a fix, with the
enforcer making the bug happen more quickly and the fix preventing it
from happening at all.

This test illustrates one subtlety in the implementation of the
enforcer: it must be (at least reasonably) fair in order for the time
to reproduce the bug to be predictable.  An initial version of the
enforcer internally used an unfair lock implementation, and while it
was able to reproduce the bug quickly (in under a second) most of the
time it suffered from a long tail in 5\% of tests took more than a
minute to reproduce.  Switching to a fair lock implementation solved
this problem, and reduced the mean time to reproduction, but at the
expense of slightly increasing the median time to reproduction.
\todo{Insert reproduction time CDFs here.}  All other experiments in
this evaluation were conducted using the fair lock implementation.

\todo{Currently running all of these tests on a four-processor machine,
  which might have an interesting effects on the results of a
  64-thread test.}

\todo{I kind of feel like I ought to say \emph{which} fair and unfair
  lock implementations I'm using here, since I just strongly implied
  that it matters, but it's a bit of a tedious thing to have to
  explain.}

\subsection{Complicated happens-before graphs ($\textrm{complex\_hb}_{\{5,11,17\}}$)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &x1 = global;\\
        &x2 = global;\\
        &x3 = global;\\
        &assert(!(x1 == 0 \&\& x2 == 1 \&\& x3 == 2));\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global = 0;\\
        &global = 1;\\
        &global = 2;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
        \\
      \end{tabular}
    }
  }
  \caption{Crashing and interfering threads for the
    $\textrm{complex\_hb}_5$ test.  The $\textrm{complex\_hb}_{11}$
    and $\textrm{complex\_hb}_{17}$ tests are generated by extending
    this pattern to require additional happens-before edges.}
  \label{fig:eval:complex_hb}
\end{figure}

\begin{figure}
  \subfigure[][]{
    \begin{tabular}{ll}
      \begin{tikzpicture}
        \node (dummy) {};
        \node (ld1) [CfgInstr, below = of dummy] {\texttt{x1 = global;}};
        \node (ld2) [CfgInstr, below = 2 of ld1] {\texttt{x2 = global;}};
        \node (ld3) [CfgInstr, below = 2 of ld2] {\texttt{x3 = global;}};
        \node (st1) [CfgInstr, right = of dummy] {\texttt{global = 0;}};
        \node (st2) [CfgInstr, below = 2 of st1] {\texttt{global = 1;}};
        \node (st3) [CfgInstr, below = 2 of st2] {\texttt{global = 2;}};
        \draw[->] (ld1) -- (ld2);
        \draw[->] (ld2) -- (ld3);
        \draw[->] (st1) -- (st2);
        \draw[->] (st2) -- (st3);
        \draw[->,happensBeforeEdge] (st1) -- (ld1);
        \draw[->,happensBeforeEdge] (ld1) -- (st2);
        \draw[->,happensBeforeEdge] (st2) -- (ld2);
        \draw[->,happensBeforeEdge] (ld2) -- (st3);
        \draw[->,happensBeforeEdge] (st3) -- (ld3);
      \end{tikzpicture}\\
      Side condition: \true
    \end{tabular}
  }
  \hfill
  \subfigure[][]{
    \begin{tabular}{ll}
      \begin{tikzpicture}
        \node (ld1) [CfgInstr] {\texttt{x1 = global;}};
        \node (dummy) [right = of ld1] {};
        \node (ld2) [CfgInstr, below = 3 of ld1] {\texttt{x2 = global;}};
        \node (ld3) [CfgInstr, below = 2 of ld2] {\texttt{x3 = global;}};
        \node (st1) [CfgInstr, below = of dummy] {\texttt{global = 0;}};
        \node (st2) [CfgInstr, below = 0.5 of st1] {\texttt{global = 1;}};
        \node (st3) [CfgInstr, below = 1.7 of st2] {\texttt{global = 2;}};
        \draw[->] (ld1) -- (ld2);
        \draw[->] (ld2) -- (ld3);
        \draw[->] (st1) -- (st2);
        \draw[->] (st2) -- (st3);
        \draw[->,happensBeforeEdge] (ld1) -- (st1);
        \draw[->,happensBeforeEdge] (st2) -- (ld2);
        \draw[->,happensBeforeEdge] (ld2) -- (st3);
        \draw[->,happensBeforeEdge] (st3) -- (ld3);
      \end{tikzpicture}\\
      Side condition: $\smLoad{\mathrm{global}} = 0$
    \end{tabular}
  }
  \caption{Happens-before graphs generated for the $\textrm{complex\_hb}_5$ test.}
  \label{fig:eval:complex_hb:hb}
\end{figure}

This test, shown in Figure~\ref{fig:eval:complex_hb} is intended to
evaluate {\technique}'s ability to handle more complicated
happens-before graphs which require more than two context switches.
As expected, {\implementation} is able to generate both an enforcer
and a fix for this bug, which can either make the bug reproduce easily
or not at all.

The happens-before graphs and side conditions generated for this test
are shown in Figure~\ref{fig:eval:complex_hb:hb}.  The one on the left
is the expected graph, and will trigger the bug.  The one on the right
is not.  It describes the case in which the first store in the
interfering thread happens after the first load in the crashing
thread, but the initial contents of memory happens to contain the
right value.  It is impossible to enforce the desired happens-before
graph when \texttt{global} is initially 0, as the program structure
means that first store will only run when \texttt{global} is 2, but
the {\technique} analysis is not powerful enough to show that.
Without the side condition, the enforcer would have to try to enforce
both graphs at run-time, and so it would take longer to reproduce the
bug; with it, the enforcer can easily discard the spurious
happens-before graph at run time, partially compensating for the
incompleteness of the main analysis.

\subsection{A simple double-free bug (double\_free)}

\begin{figure}
  \subfigure[][Active threads]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{t = global\_ptr;}\\
        &\multicolumn{2}{l}{if (t != NULL) \{}\\
        &&global\_ptr = NULL;\\
        &&free(t);\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{sleep(1 millisecond);}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Environmental thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr == NULL) \{}\\
        &&global\_ptr = malloc(64);\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
        \\
        \\
        \\
      \end{tabular}
    }
  }
  \caption{Threads involved in the double\_free bug.}
  \label{fig:eval:double_free}
\end{figure}

This test demonstrates {\technique}'s ability to handle some simple
double-free bugs.  The test program is shown in
Figure~\ref{fig:eval:double_free}.  Note that in this test, the
crashing and interfering threads (collectively, the active threads)
both run the same code, shown on the left of the figure, while a third
thread, known as the environmental thread, modifies the environment in
which they are operating.  The two active threads loop reading
\texttt{global\_ptr} and, if it is non-\texttt{NULL}, releasing it and
setting it to \texttt{NULL}.  The environment thread is meanwhile
undoing their work by examining \texttt{global\_ptr} and, when it is
\texttt{NULL}, setting it to a newly-allocated block.  Every time it
does so, the two active threads will race trying to release the block
and reset \texttt{global\_ptr}.  In some interleavings, both active
threads will try to release the same block, leading to a double-free
bug.

Note that the program threads here do not map directly on to the
threads in the candidate bug: the interfering thread is whichever of
the two active threads wins the race and releases the block first; the
crashing thread is the other active thread, which \texttt{free}s a
block which has already been released; and the environmental thread
does not appear in the candidate bug at all, despite being necessary
for the bug to reproduce.

{\Implementation} is able to build an enforcer and a fix for this bug,
and they behave as expected.

\subsection{A program with existing synchronisation (existing\_sync\_visible, existing\_sync\_invisible)}

\begin{figure}
  \subfigure[][Crashing thread with {\technique}-visible synchronisation]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{lock(\&global\_lock);}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&*global\_ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{unlock(\&global\_lock);}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
    \label{fig:eval:existing_sync:visible}
  }
  \subfigure[][Crashing thread with {\technique}-invisible synchronisation]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{lock(\&global\_lock);}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&*global\_ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{unlock(\&global\_lock);}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
    \label{fig:eval:existing_sync:invisible}
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &lock(\&global\_lock);\\
        &global\_ptr = NULL;\\
        &unlock(\&global\_lock);\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{Threads for the existing\_sync\_visible and existing\_sync\_invisible tests.}
  \label{fig:eval:existing_sync}
\end{figure}

These tests explore {\technique}'s interactions with the program's
existing synchronisation.  It is the same as simple\_toctou, except
that the program contains calls to \texttt{pthread\_mutex\_lock} and
\texttt{pthread\_mutex\_unlock} which prevent the bug from ever
reproducing.  As discussed previously\editorial{ref?}, {\technique}
has no global model of the program's synchronisation, and so can only
analyse synchronisation within the analysis window \backref{$\alpha$}.
The analysis will therefore be aware of the synchronisation in the
existing\_sync\_visible crashing thread,
Figure~\ref{fig:eval:existing_sync:visible}, but not that in the
existing\_sync\_invisible crashing thread,
Figure~\ref{fig:eval:existing_sync:invisible}.  As such, it generates
a candidate bug for the existing\_sync\_invisible test but not for the
existing\_sync\_visible one.

\begin{figure}
  \centerline{
    \begin{tikzpicture}
      \node (LD1) {First load};
      \path (node cs:name=LD1,anchor=west) -- ++(0,-2) node (LD2) [right] {Second load};
      \node (dummy) [right = 2 of LD1] {};
      \node (ST) [below = 0.6 of dummy] {Store};
      \draw (node cs:name=LD1,anchor=north west) -- ++(-0.2,0) |- (node cs:name=LD2,anchor=south west);
      \draw (node cs:name=ST,anchor=north east) -- ++(0.2,0) |- (node cs:name=ST,anchor=south east);
      \draw[->, happensBeforeEdge] (LD1) -- (ST);
      \draw[->, happensBeforeEdge] (ST) -- (LD2);
      \draw[->] (LD1) -- ++(0,-1.8);
      \path (-1.5,-2) to node [sloped] {locked region} (-1.5,0);
      \node at (5.3,-1.05) {locked region};
    \end{tikzpicture}
  }
  \caption{Happens-before graph which must be enforced for the
    existing\_sync\_invisible test, along with the program's existing
    locked regions.  This bug cannot be reproduced.}
  \label{fig:eval:existing_sync:hb}
\end{figure}

This candidate bug can be instantiated into a bug enforcer, but, as
might be expected, this enforcer cannot cause any bugs to reproduce.
The happens-before graph for the bug is shown in
Figure~\ref{fig:eval:existing_sync:hb}; this clearly contradicts the
program's existing synchronisation strategy, and so trying to enforce
it will lead to a deadlock.  The unbound message operations in the
crash enforcement plan will therefore all time out, preventing
successful plan completion.

It can similarly be instantiated into a fix.  This fix does not fix
any actual bugs, as there are none, but does not otherwise harm the
program's execution.

\subsection{Summary tables}

\todo{Need to think harder about what variants of the enforcers I want to include.
  At the moment I do a full enforcer and an enforcer without
  side-condition checking for every test, which is kind of silly
  because most of the time that just shows that enforcers without
  side-condition checking don't work very well.  It might be more
  useful to try different timeout strategies instead.}

Tables~\ref{table:eval:analysis_phases}, \ref{table:eval:crash_times}
and \ref{table:eval:perf_dists} summarise the results of these
experiments.  Several important conclusions can be drawn from these
tables:

\begin{itemize}
\item
  The analysis performed by {\technique} is, for the most part, very
  quick on these programs, generally requiring less than a second.
  The only exception is the complex\_hb test with ten happens-before
  edges, which requires a little over a minute to perform its initial
  analysis; even this is not completely unreasonable given the
  complexity of the test.
\item
  Converting the candidate bugs generated by the initial analysis
  fixes completes in all cases in under a second.
\item
  Converting the candidate bugs into enforcers is also very quick,
  mostly completing in under a second.  The only exception is,
  again, complex\_hb with ten happens-before edges, which takes
  roughly a minute.
\item
  The enforcers are all able to reproduce their intended bugs
  quickly, in all cases much more quickly than the original program.
\item
  {\Technique} can automatically generate fixes for all of the bugs,
  and those fixes have acceptably low overhead.
\item
  The DataCollider-like tool is effective for some bugs and
  ineffective for others.  Where it is effective, it is generally less
  effective than the {\technique}-generated enforcers.  The only
  exception is the multi\_variable test, where the {\technique}
  enforcer reproduces the bug roughly twice per second and the
  DataCollider-like tool reproduces it roughly seven times per
  second.  \todo{Need to figure out what's going on there.}
\end{itemize}
        
I also show cumulative distribution functions of the time which the
various test cases take to reproduce their respective bugs in
Figure~\ref{fig:eval:artificial:crash_times_cdfs}.

\begin{sidewaystable}
\input{eval/analysis_phase_times}
\caption{Time taken for the various initial analysis phases}
\todo{Not convinced that changing the number of sig figs like that is entirely wise,
  but it does match up with the derived sd.  Also, not sure what's
  going on with indexed\_toctou and read\_indexed\_toctou, or why
  write\_indexed\_toctou behaves so much differently.}
\label{table:eval:analysis_phases}
\end{sidewaystable}

\begin{sidewaystable}
\input{eval/crash_times}
\caption{Time taken for the test programs to crash under various configurations}
\label{table:eval:crash_times}
\end{sidewaystable}

\begin{sidewaystable}
\input{eval/perf_dists}
\caption{Performance overheads of the generated fixes.}  \todo{Having spent bloody
ages worrying about statistical things, the overhead column is just from
the ratio of means, with no though given to the error distribution.  Need to think
about that some more.}
\label{table:eval:perf_dists}
\end{sidewaystable}

\input{eval/crash_times_cdfs/cdfs}

\section{Semi-artificial bugs}

These are real programs which I've modified to introduce new bugs.  So
far, there's only one in this category, and it's really not terribly
convincing.

\subsection{Bayes}

This is the Bayes test from the STAMP benchmark suite, modified to
remove one if its critical sections.  The critical section looks like
this:

\begin{verbatim}
        acquire_global_lock();
        taskPtr = TMpopTask(TM_ARG  taskListPtr);
        release_global_lock();

learner_task_t*
TMpopTask (TM_ARGDECL  list_t* taskListPtr)
{
    learner_task_t* taskPtr = NULL;

    list_iter_t it;

    TMLIST_ITER_RESET(&it, taskListPtr);

    if (TMLIST_ITER_HASNEXT(&it, taskListPtr)) {
        taskPtr = (learner_task_t*)TMLIST_ITER_NEXT(&it, taskListPtr);
        bool_t status = TMLIST_REMOVE(taskListPtr, (void*)taskPtr);
        assert(status);
    }

    return taskPtr;
}
static bool_t TMLIST_REMOVE(list_t* listPtr, void* dataPtr)
{
    list_node_t* prevPtr;
    list_node_t* nodePtr;

    prevPtr = TMfindPrevious(TM_ARG  listPtr, dataPtr);

    nodePtr = prevPtr->nextPtr;
    if ((nodePtr != NULL) &&
        (listPtr->compare(nodePtr->dataPtr, dataPtr) == 0))
    {
        prevPtr->nextPtr = nodePtr->nextPtr;
        nodePtr->nextPtr = NULL;
        free(nodePtr);
        listPtr->size -= 1;
        assert(listPtr->size >= 0);
        return TRUE;
    }

    return FALSE;
}
\end{verbatim}

I have simplified the code presented slightly by removing parts
related to transactional memory, as these are not relevant for this
test.  I made this program buggy by removing the global lock around
\verb|TMpopTask|.  The resulting program crashed in one of a number of
ways, depending on how the various races played out:

\begin{itemize}
\item The \verb|assert| in \verb|TMpopTask| sometimes fired.
\item The \verb|assert| in \verb|TMLIST_REMOVE| sometimes fired.
\item The \verb|free| in \verb|TMLIST_REMOVE| sometimes caused a
  double-free error in the C library.
\item Other assertions later in the program also occasionally fired
  due to running the same task twice.
\end{itemize}

The first two bugs are of the correct form to be investigated by
{\technique}; the other two are not.  I therefore generated enforcers
for the first two bugs which, as expected, caused those bugs to
reproduce rapidly and reliably (i.e. when one of those enforcers was
applied the target bug consistently reproduced before any of the other
bugs had a chance to).  I also generated fixes for the two bugs.
These both worked in the sense that they were able to prevent the
target bug from reproducing.  Unfortunately, the fix for the first bug
also made the double-free bug reproduce far more frequently, and so
the overall effect was that the program crashed more quickly.  On the
other hand, the fix for the second bug fixed all four bugs, as the
critical sections introduced by the fix were large enough to
completely cover the actual critical sections in the program.

\todo{More discussion of how the fixes work should go here.}

\todo{Somewhat surprisingly, given that it was taken from what's
  nominally a benchmark suite, unmodified bayes shows multiple order
  of magnitude performance differences from run to run in the default
  configuration, which makes it quite hard to talk about the
  performance cost of the fix.  I could just average enough runs that
  the SD drops to something sane, but I don't think that would be
  terribly meaningful.}

\subsection{glibc}

\verb|glibc| is a kernel of glibc bug 2644 \cite{glibc2644}, which
affected versions of glibc up to 2.5 and could lead to a crash if
multiple threads were shut down at the same time.  A simplified
version of the code involved is shown in Figure~\ref{fig:glibc}, where
\verb|forcedunwind| and \verb|done_init| are global variables.  Note
that the bug here depends on the compiler's optimizer, and is not
apparent at the source-code level\footnote{Unfortunately, only the
  32-bit x86 version of gcc optimizes the function like this, and our
  implementation of SLI assumes a 64-bit x86 program, and this
  prevented us from testing with the real bug.}.  SLI operates
entirely at the machine-code level, and so this does not present any
additional complexity.

\begin{figure*}
  \begin{subfloat}
    \begin{minipage}{52mm}
\begin{verbatim}
_Unwind_ForcedUnwind() {
   if (forcedunwind == NULL)
      pthread_cancel_init();
   forcedunwind();
}
pthread_cancel_init() {
   if (done_init) return;
   forcedunwind =
     _forcedunwind_impl;
   done_init = 1;
}
\end{verbatim}
    \end{minipage}
    \caption{Before optimizations}
  \end{subfloat}
  \begin{subfloat}
    \begin{minipage}{52mm}
\begin{verbatim}
  _Unwind_ForcedUnwind() {
1:  l = forcedunwind;
2:  if (l == NULL &&
3:      done_init) {
4:    forcedunwind = l =
5:       _forcedunwind_impl;
6:    done_init = 1;
7:  }
8:  l();
  }
\end{verbatim}
    \end{minipage}
    \caption{After optimizations}
  \end{subfloat}
  \begin{subfloat}
    \begin{minipage}{35mm}
\begin{verbatim}
    while (1) {
10:   pthread_barrier_wait();
11:   _Unwind_ForcedUnwind();
12:   pthread_barrier_wait();
13:   done_init = 0;
14:   forcedunwind = NULL;
    }
\end{verbatim}
    \end{minipage}
    \caption{Test harness}
  \end{subfloat}
  \label{fig:glibc}
  \caption{Source code for the glibc test case.}
\end{figure*}

\section{Bugs from real programs}

\subsection{Thunderbird}

\todo{Rewrite to fit new structure}

\verb|thunderbird| is Mozilla bug number
391259\cite{thunderbird39125}, a simple time-of-check, time-of-use
race in the IMAP client component of Thunderbird, a popular
open-source e-mail client.  We modified Thunderbird to include some
additional debugging messages and used a custom scheduler in order to
make the bug reproduce more readily; the test is otherwise identical
to the behavior which a user might have encountered.  The relevant
parts of the program are as follows:

\begin{verbatim}
void nsImapProtocol::CloseStreams() {
  if (m_transport)
      m_transport = nsnull;
}
PRBool nsImapProtocol::ProcessCurrentURL() {
  if (m_transport)
    m_transport->SetTimeout(
      TIMEOUT_READ_WRITE, PR_UINT32_MAX);
}
\end{verbatim}

\noindent
If \verb|m_transport| is set to \verb|nsnull| by \verb|CloseStreams()|
in between the two accesses in \verb|ProcessCurrentURL| then the
program will crash.  This is essentially the same bug as
\verb|toctou|, but embedded in a much large program.  As such, the
final result is similar: a single suggested fix, with two critical
sections, one containing the two accesses in \verb|ProcessCurrentURL|
and one containing the assignment in \verb|CloseStreams|.  This fixes
the bug.

\subsection{mysql}

Mysql 56324.  Read side:

\begin{verbatim}
void my_thread_end(void)
{
...
  if (PSI_server)
    PSI_server->delete_current_thread();
}
\end{verbatim}

Write side:

\begin{verbatim}
int mysqld_main(int argc, char **argv)
{
....
  if (PSI_server)
  {
    PSI_server->delete_current_thread();
    PSI_server= NULL;
  }
  mysqld_exit(0);
}
\end{verbatim}

And the race is then on \verb|PSI_server|.  This one actually works;
woo.  We find the bug from the whole-program analysis, and it then
gets turned into an enforcer, and the enforcer then makes it
reproduce.  Fix generation also works.

Possibly more interesting: there are multiple very similar bugs
scattered throughout the program, and it found all of the ones I knew
about before I started and a couple which I didn't know about.

Complication: this bug is only present in builds of mysql which don't
have compiler optimisations, because otherwise the compiler caches
\verb|PSI_server| in a register and avoids the crash.

\todo{Need to finish running this on the optimised build.}

\todo{Might be worth pointing out that the fix can lead
  delete\_current\_thread() running an additional time, and then give
  the definition of that function to show that the fix is correct
  anyway?}

\section{Validation of tool implementation}

\subsection{Static analyses}

SLI relies on two forms of whole-program static analysis applied to
the target binary before the main analysis starts:

\begin{itemize}
\item
  The simple points-to analysis.
\item
  An analysis to recover the offset between RSP and RBP, where that is a constant.
\end{itemize}

Both analyses assume that the program to be analyses conforms to the
system ABI.  If that assumption does not hold, or if there is simply a
bug in one of them, then that might invalidate all of the other
results.  I therefore developed some Valgrind-based dynamic analyses
to check that the results of this phase were correct.

\todo{The tool also implements a register liveness analysis, but
  that's only used to build the points-to table, so validating the
  points-to table also implicitly validates the liveness one.  Also,
  I've not described the liveness analysis anywhere, so it'd be hard
  to wedge it in here.}

\subsubsection{Points-to analysis}
\label{sect:eval:validate:pta}
\todo{Shrink this section.}

The static points-to analysis builds an instruction attribute table
for the program which includes, for each instruction:

\begin{itemize}
\item
  Whether the current stack frame might include any pointers to itself
  or to memory outside of the current frame.
\item
  Whether there might be any pointers to the current stack frame in
  from outside of it.
\item
  For each register, a flag saying whether that register might point
  at the current stack frame or to memory.
\end{itemize}

The ``current stack frame'' here is defined to be the region of memory
between \verb|RSP-128| and the value of \verb|RSP| at the time of the
immediately enclosing \verb|call| instruction\footnote{128 bytes is
the size of stack red zone, which effectively forms part of the
current stack frame.}\editorial{talk about effects of tail
calls}\editorial{i.e. current RSP versus RSP at time of last call;
could maybe be clearer about that.}.  The tool to check this analysis
has several parts:

\begin{itemize}
\item
  It must track the extent of the current frame; this is
  straightforward, since the analysis can always see the value
  of \verb|RSP| and all \verb|call| and \verb|ret| instructions.
\item
  It checks, at the start of each instruction, whether any registers
  currently point into the current frame, and, if so, whether that is
  allowed by the instruction attribute table.
\item
  It attempts to track directly whether there exist any pointers to
  the current frame, whether in the frame or outside of it.  This part
  of the analysis assumes that there are no pointers into a frame when
  it is created at the start of a function and then monitors all
  stores to detect when such pointers are created.  This information
  then allows the analysis to directly check the
  might-be-pointer-to-frame flags in the instruction attribute table.
\item
  That assumption holds for most well-behaved programs, but is not
  absolutely guaranteed.  The dynamic analysis therefore also checks
  all load operations to confirm that they only return pointers to the
  current stack frame when the static analysis allows the loaded
  memory to contain pointers to the frame.

  There might, of course, be pointers into the current stack frame
  which are never loaded, but (assuming there are no cross-thread
  stack accesses) they can never be dereferenced, and so don't
  actually matter.
\end{itemize}

This flagged a number of minor problems with the analysis:

\begin{itemize}
\item
  Pointers to the stack frame can sometimes be left behind in dead
  registers, and in particular in call-clobbered registers after
  function calls.  Correct programs which conform to the ABI will
  never make use of the values of these registers, and the static
  analysis makes use of that fact, but it is hard for a dynamic
  analysis to determine when a register is dead.  The solution is
  simple: have the dynamic analysis overwrite all such registers with
  poison values when functions return.  If the program does conform to
  the ABI then this will have no effect, but if it makes use of the
  theoretically-dead values then its behaviour will change.  I
  repeated the analysis with three different poison values: zero, a
  small number which was not a valid pointer, and a large value which
  was not a valid pointer.

  This revealed a single place which did not conform to the ABI in the
  desired way: glibc's internal pthread locking functions are
  guaranteed to never clobber \verb|RSI|, and glibc's syscall stubs
  make use of this in a number of places\editorial{Cite, maybe?  It'll
  be pointing at source rather than a document saying exactly what's
  going on, but at least it's something.}.  This particular static
  analysis is only applied to the program's main binary, and not any
  of the libraries which it is dynamically linked against, and so this
  is not a particular problem.

\item
  \verb|alloca|

\begin{verbatim}
>   9e56c7:       48 29 c4                sub    %rax,%rsp
>   9e56ca:       48 89 e0                mov    %rsp,%rax
>   9e56cd:       48 83 c0 0f             add    $0xf,%rax
>   9e56d1:       48 c1 e8 04             shr    $0x4,%rax
>   9e56d5:       48 c1 e0 04             shl    $0x4,%rax
>   9e56d9:       48 89 45 a8             mov    %rax,-0x58(%rbp)
\end{verbatim}

\todo{Crap, my argument for why this doesn't matter doesn't actually work.  Need to rethink that one.}
\end{itemize}

\subsubsection{RBP offset}

The main analysis removes references to the function frame pointer, if
present, by replacing them with references to the stack pointer.  This
relies on a static analysis which determines, for each instruction in
the program, the offset from the \verb|RBP| register to the stack
pointer (assuming that that's a constant).  This dynamic analysis
checks, at the end of every instruction, that the actual offset
matches the value in the database.  This analysis did not reveal any
important bugs in the algorithm\footnote{Beyond a few implementation
  errors which are fixed in the code used for this evaluation.}.

\subsubsection{CFG generation}

For the bug-detecting mode to hope to detect every bug, the CFG
generation process must be able to generate CFGs which represent all
dynamic fragments of the program of the desired length which either
end in a memory-accessing instruction (for probe CFGs) or start and
end with a store (for store CFGs).  This dynamic analysis attempts to
validate that by capturing a large pool of dynamic traces from the
program and then checking that CFG generator can generate the trace.
Ideally, it would capture every such trace from an execution, but that
has sufficiently high performance overhead that it would be difficult
to exercise a broad cross-section of the program's behaviour while
running such an analysis.  Instead, the analysis applies several
filters to try to obtain a reasonably representative sample:

\begin{itemize}
\item
  Only traces which end in a non-stack memory-accessing instruction
  are considered.
\item
  Amongst those samples, only one in a thousand is used.  This is
  implemented by only sampling if a randomly-generated number is
  congruent to zero modulo a thousand, rather than taking every
  thousandth trace, so as to avoid possible aliasing effects with the
  program's structure.
\item
  I attempt to increase the likelihood of rare traces being sampled
  using a bloom counter table.  This consists of 131072 saturating
  7-bit counters.  When the dynamic analysis is determining whether to
  sample a given trace, it hashes it to select one of these counters,
  then generates a random number, and only takes the trace if the
  random number modulo the counter plus one is zero.  It then
  increments the counter.  This helps to increase the likelihood of
  moderately rare traces being included in the final sample.
\end{itemize}

The end result of this dynamic analysis is a large set of short
fragments of the program's execution.  Each such fragment is
considered in isolation, and appropriate\editorial{?} instructions
from it fed into the CFG generating algorithm.  The CFG can then be
checked to ensure that it includes the desired trace.  This analysis
did not find any problems with the algorithm.

\todo{Should really try to do something to convince myself that the
  sanity checker works.  Collecting more stats on the trace pool
  generated would be good, as would some sensitisation on those
  parameters.}

\subsection{{\STateMachine} generation}

Don't really have a plan for this; might just say that it's a trivial
wrapper for libVEX.

\subsection{Dynamically-collected aliasing model}

This is pretty much what it is; not sure there's a great deal to say
here.  If I had infinite time I could hack up gcc to try to do a
similar analysis at the source level, but I don't, so I can't.  Only
real alternative is to look at the convergence rate.

Things to look at:

\begin{itemize}
\item How fast we add new edges to the aliasing table while running
  the analysis, as a function of time.  You'd hope that this will fall
  very quickly as the analysis runs.
\item mysqld has a reasonably good test harness.  It'd be interesting
  to try running each of the tests in that in some suitable order and
  see how many of them you need in order to get good coverage.
  Probably want to try to normalise that against the actual number of
  distinct instructions run to make it fair.
\item Two things are worth looking at here: the rate at which we add
  edges in toto, and the rate at which we add edges between
  already-known instructions.  The latter is much more important than
  the former, because the first one includes discovering new code for
  the first time, and it's pretty forgivable to not include
  instructions which are never run, whereas the latter is new
  interactions between code which we already know about.
\end{itemize}

\subsection{Symbolic execution engine}

No idea how to validate this one.  It's actually very simple, so I'm
already pretty certain it's mostly correct.

\subsection{{\STateMachine} simplification}

\todo{The problem with this stuff is that there's a lot of machinery
  involved, but the punch line is just that everything works fine,
  which is somewhat unsatisfying.}

\todo{Also, I've not run this in ages, so I should probably do that
  again and see if it turns up anything interesting.}

The {\StateMachine} simplification passes are both complicated and
critical to SLI's correctness; validating that they are correct is
therefore important.  To do so, I collected a selection of pre- and
post-simplification machines from a number of runs, evaluated them in
identical initial configurations, and confirmed that they produced
identical results.  Generating the initial conditions is non-trivial.
At first I simply generated them completely at random, but found that
it required an unreasonably large number of such random configurations
to achieve good coverage of the \StateMachines' behaviour.  This is
because the vast majority of paths through most machines ultimately
report that the machine does not crash, and so a uniform random
sampling of initial configurations will overwhelmingly sample
configurations in which the bug of interest does not reproduce.  This
is unfortunate, as the situations in which the bug does reproduce tend
to be more interesting.

It is worth considering this is slightly more detail.  A reasonably
typical \StateMachine might look like this:

\begin{verbatim}
if (rax == 0) survive();
if (!BadPtr(rax)) survive();
crash();
\end{verbatim}

Here, \verb|rax| is a 64-bit value, and so if its value is randomly
chosen then the {\StateMachine} is highly unlikely to make it past the
first state, which would not constitute a good test.  Of course, it
would be possible to bias the distribution to produce zeroes more
often than other values, and hence avoid this issue.  This works, in
the sense that it makes it easy to generate an effective test suite
for this \StateMachine, but is lacking in two important respects.
First, it is unclear what other special cases might be needed;
generating zero is an obvious thing to test, as is generating one or
minus one, but what if there were a bug in the optimiser when
\verb|rax| is near to the top of its range?  or has only bits in the
top byte set?  or is a large prime number?  These questions would make
it difficult to have faith that such an approach had generated a
reasonably complete set of initial configurations for testing.
Second, considering each register independently is inherently
inefficient, due to the common structure of \StateMachines.  Consider
this machine, for example:

\begin{verbatim}
if (rax % 2 == 0) survive();
if (rbx % 2 == 0) survive();
if (rcx % 2 == 0) survive();
if (rdx % 2 == 0) survive();
crash();
\end{verbatim}

This reflects the common structure of \StateMachines, in which a long
prefix of initial tests capture ways in which the program might avoid
running the code which is suspected of having a bug before the
interesting part of the \StateMachine starts.  One reasonable set of
initial configurations might be:

\begin{itemize}
\item $rax = 0$
\item $rax = 1$, $rbx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 1$, $rdx = 0$
\item $rax = 1$, $rbx = 1$, $rcx = 1$, $rdx = 1$
\end{itemize}

A total of five initial configurations; one for each possible final
state.  A randomly-generated configuration, by contrast, will have a
one in sixteen chance of generating each final state, and will require
an average of a little over twenty-five input states in order to cover
every final state, a factor of five worse than the desired
set\editorial{25 from some Python simulations; not sure it's worth
  trying to justify it further.}.  The difference will be larger in
more complex machines, and so coverage will be worse for them, but
those are precisely the ones which are most likely to reveal optimiser
bugs.

I therefore used a slightly more complicated scheme for generating the
initial configurations:

\begin{itemize}
\item[1] First, execute the unoptimised machine in the symbolic
  execution engine, collecting all of the path constraints generated.
  There will be one such constraint for each path (loosely defined)
  which the symbolic execution engine finds through the machine,
  whether that path ultimately crashes, survives, or escapes (due to
  e.g. dereferencing a bad pointer).
\item[2] Pick a constraint which is not satisfied by any
  configuration currently present in the initial configuration set.
  Attempt to generate a configuration which satisfies it and add it to
  the set.  If there are no constraints which are never satisfied,
  pick one which is satisfied in every initial configuration and try
  to generate a configuration which does not satisfy it.
\item[3] Repeat 2 until every constraint is satisfied in at least one
  initial configuration and not satisfied in at least one other
  configuration.
\item[4] Execute the optimised machine in the symbolic execution
  engine, generating another set of path constraints, and then try to
  generate satisfying and unsatisfying initial configurations for them
  in precisely the same way.
\end{itemize}

Generating a configuration which satisfies a particular constraint is
itself non-trivial.  The approach I adopted there was as follows:

\begin{itemize}
\item First, treat the constraint as an expression over boolean
  variables.  For instance, the constraint $x < 5 || x > 73$ would be
  treated as $a || b$, with a note that $a = x < 5$ and $b = x > 73$.
\item Generate a satisfier for this boolean expression using a simple
  brute-force satisfiability checker.  This will produce a
  configuration of the boolean variables, assigning each to one of
  true, false, or doesn't-matter, which causes the original constraint
  to be true.
\item Attempt to generate concrete values for all of the original
  constraint's variables such that all of the boolean variables have
  appropriate values.  Some of these values will be provided by simple
  heuristics (e.g. to make $x > k$ be true, where $k$ has a known
  value, try setting $x$ to $k+1$), but most will be randomly
  generated with a retry if the results do not make the boolean value
  have the desired results.  Even when values have to be randomly
  generated, being able to treat most boolean variables independently
  most of the time reduces the number of attempts necessary by a
  useful amount.
\item If no concrete values have been found which satisfy the boolean
  variable configuration after a certain number of random attempts,
  look for another configuration of boolean variables which satisfies
  the path constraint.
\item If there are no more such configurations give up and report an
  error.
\end{itemize}

The result is a set of initial configurations which will, with
reasonably high probability, exercise a useful selection of
\StateMachine behaviour.  The configurations can contain the following
items:

\begin{itemize}
\item Initial values of some registers\footnote{For SSA {\StateMachines}
  these will be the only values those registers ever have, but for
  non-SSA {\StateMachines} the value might change during
  interpretation.}.
\item Partial contents of memory, in the form of a partial mapping
  from concrete addresses to concrete values.
\item A list of bad addresses.  These are locations for which
  $BadPtr(x)$ must return true.  Any location not in this list is
  assumed to be valid memory, even if it isn't assigned a value.
\item A set of $EntryPoint$ and $ControlFlow$ expressions which
  are to be treated as true.
\end{itemize}

The configurations (hopefully) contain all of the parts of
{\StateMachine} state which might affect the final result, but do not
necessarily contain enough information to actually interpret the
{\StateMachines} to completion.  For example, many {\StateMachines}
will access locations on the stack, and so interpreting them will
require the value of the stack pointer, but it is very rare for their
final result to depend on the precise value of that pointer, and so
the symbolic execution engine will not generate any constraints on
that value and the initial configurations will not contain any value
for it.  If the validator encounters any such unspecified values
during {\StateMachine} interpretation it simply generates new random
values for the relevant variables.  It then runs each initial state
100 times, with different random values each time, and checks that the
{\StateMachines} match for each one.

One final subtlety is what to do with what to do with {\StateMachines}
which escape by, for instance, failing an assertion.  This indicates
that some part of the analysis has determined that the configuration
is not interesting, for some reason, \todo{There's some special
handling here to do with retrying escaping machines, but I don't
remember why I did it that way any more.  Think harder.  Also need to
grab a whole bunch of statistics on what this actually does so as to
convince people that it actually gets decent test coverage.}

I ran the resulting tool on a selection of {\StateMachines} which were
generated while analysing mysql.  It found a number of implementation
bugs, which are now fixed, but did not reveal any fundamental problems
with any of the algorithms involved.
