%\newcommand{\biggraph}[1]{\centerline{\input{#1}}}
\newcommand{\biggraph}[1]{}
%\draftonly{\renewcommand{\biggraph}[1]{}}

\chapter{Evaluation}
\label{chapter:eval}

Previous chapters have described the basic {\technique} technique.  I
will now evaluate its effectiveness and the performance of my
implementation {\implementation}.  This evaluation will consist of
three parts.  \autoref{sect:eval:does_it_work} investigates whether
{\technique} works at all, looking at the time taken to produce
\glspl{verificationcondition}, \glspl{bugenforcer}, and fixes, the
effects of \glspl{bugenforcer} and fixes on the frequency with which
bugs reproduce, and the effects of the fixes on the programs
performance.  \autoref{sect:eval:how_does_it_work} then explores some
aspects of {\technique}'s performance in more detail, showing the
effects of the various analysis phases and parameters.  Finally,
\autoref{sect:eval:why_does_it_work} explores the tool's scalability
as complexity of the program to be analysed increases, along several
metrics.

Unless otherwise stated, all experiments were run on a four-core Intel
Q6600 2.40GHz with 8GiB of memory running Ubuntu Lucid Lynx.  The
\gls{w-isolation} assumption was enabled, \gls{alpha} was set to 20,
both analysis timeouts were set to five minutes, and the timeout used
by \glspl{bugenforcer} was randomly uniformly distributed between 100
and 200ms.  The system compiler, used to build the \glspl{bugenforcer}
and fixes, as well as {\implementation} itself, was gcc version 4.4.3.

\section{Does it work?}
\label{sect:eval:does_it_work}

This part of the evaluation aims to answer one very simple question:
does {\technique} work?  In other words, is it able to reproduce and
fix concurrency bugs?  I will now attempt to answer this question with
reference to a number of real and artificial bugs.

\subsection{Artificial bugs}
\label{sect:eval:artificial_bugs}

\begin{figure}
  \begin{tabular}{ll}
    \multicolumn{2}{c}{\texttt{\#define NR\_PTRS 100}}\\
    \subfigure[][\bugname{toctou} crashing thread]{
      \texttt{
        \begin{tabular}{llll}
          \multicolumn{4}{l}{while (1) \{}\\
          & \multicolumn{3}{l}{idx = random() \% NR\_PTRS;}\\
          & \multicolumn{3}{l}{analysis\_window \{}\\
          & & \multicolumn{2}{l}{if (global\_ptrs[idx] != NULL) \{}\\
          & & & *(global\_ptrs[idx]) = 5;\\
          & & \multicolumn{2}{l}{\}}\\
          & \multicolumn{3}{l}{\}}\\
          \multicolumn{4}{l}{\}}\\
        \end{tabular}
      }
      \label{fig:eval:artificial_bugs:programs:toctou:crashing}
    }
    &
    \subfigure[][\bugname{toctou} interfering thread]{
      \texttt{
        \begin{tabular}{lll}
          \multicolumn{3}{l}{while (1) \{}\\
          & \multicolumn{2}{l}{idx = random() \% NR\_PTRS;}\\
          & \multicolumn{2}{l}{analysis\_window \{}\\
          & & global\_ptrs[idx] = NULL;\\
          & \multicolumn{2}{l}{\}}\\
          & \multicolumn{2}{l}{global\_ptrs[idx] = \&t;}\\
          \multicolumn{2}{l}{\}}\\
          \\
        \end{tabular}
      }
      \label{fig:eval:artificial_bugs:programs:toctou:interfering}
    } \\
    \subfigure[][\bugname{multi\_variable} crashing thread]{
      \texttt{
        \begin{tabular}{lll}
          \multicolumn{3}{l}{while (1) \{} \\
          & \multicolumn{2}{l}{analysis\_window \{}\\
          & & v1 = global1;\\
          & & v2 = global2;\\
          & & assert(v1 == v2);\\
          & \multicolumn{2}{l}{\}}\\
          & \multicolumn{2}{l}{sleep(500$\mu$s);}\\
          \multicolumn{3}{l}{\}}\\
        \end{tabular}
      }
      \label{fig:eval:artificial_bugs:programs:multi_variable:crashing}
    }
    &
    \subfigure[][\bugname{multi\_variable} interfering thread]{
      \texttt{
        \begin{tabular}{lll}
          \multicolumn{3}{l}{while (1) \{}\\
          & \multicolumn{2}{l}{global1 = 5;}\\
          & \multicolumn{2}{l}{global2 = 5;}\\
          & \multicolumn{2}{l}{sleep(500$\mu$s);}\\
          & \multicolumn{2}{l}{analysis\_window \{}\\
          & & global1 = 7;\\
          & & global2 = 7;\\
          & \multicolumn{2}{l}{\}}\\
          \multicolumn{3}{l}{\}}\\
          \\
        \end{tabular}
      }
      \label{fig:eval:artificial_bugs:programs:multi_variable:interfering}
    } \\
    \subfigure[][\bugname{double\_free} active threads]{
      \texttt{
        \begin{tabular}{llll}
          \multicolumn{4}{l}{sleep(random() \% 100ms);}\\
          \multicolumn{4}{l}{while (1) \{}\\
          & \multicolumn{3}{l}{analysis\_window \{}\\
          & & \multicolumn{2}{l}{t = global\_ptr;}\\
          & & \multicolumn{2}{l}{if (t != NULL) \{}\\
          & & & free(t);\\
          & & \multicolumn{2}{l}{\}}\\
          & & \multicolumn{2}{l}{global\_ptr = NULL;}\\
          & \multicolumn{3}{l}{\}}\\
          & \multicolumn{3}{l}{sleep(1ms);}\\
          \multicolumn{4}{l}{\}}\\
        \end{tabular}
      }
      \label{fig:eval:artificial_bugs:programs:double_free:active}
    }
    &
    \subfigure[][\bugname{double\_free} environmental thread]{
      \texttt{
        \begin{tabular}{lll}
          \\
          \\
          \\
          \multicolumn{3}{l}{while (1) \{}\\
          &\multicolumn{2}{l}{if (global\_ptr == NULL) \{}\\
          &&global\_ptr = malloc(64);\\
          &\multicolumn{2}{l}{\}}\\
          \multicolumn{3}{l}{\}}\\
          \\
          \\
          \\
        \end{tabular}
      }
      \label{fig:eval:artificial_bugs:programs:double_free:environmental}
    }
  \end{tabular}
  \caption{Artificial test programs.  \texttt{analysis\_window} shows
    the extent of the \gls{analysiswindow}, which was specified
    manually for these tests.  The various delays were chosen so that
    the bug reproduced in a reasonable time when the program was run
    with neither an enforcer nor a fix applied.}
  \label{fig:eval:artificial_bugs:programs}
\end{figure}

I first consider {\technique}'s behaviour when applied to bugs in
three artificial test programs, shown in
\autoref{fig:eval:artificial_bugs:programs}.  These programs
illustrate several important features of {\technique}:

\begin{itemize}
  \item The \bugname{toctou} test,
    figures~\ref{fig:eval:artificial_bugs:programs:toctou:crashing}
    and~\ref{fig:eval:artificial_bugs:programs:toctou:interfering},
    investigates {\technique}'s ability to reproduce data-dependent
    bad pointer dereferencing bugs, and in particular the importance
    of the side condition checking mechanism.
  \item The \bugname{multi\_variable} test,
    figures~\ref{fig:eval:artificial_bugs:programs:multi_variable:crashing}
    and~\ref{fig:eval:artificial_bugs:programs:multi_variable:interfering},
    shows a multi-variable atomicity violation.  Some previous
    approaches, such as Kivati\needCite{}, were not able to handle
    this kind of bug, but {\technique} is.  This test also
    demonstrates that {\technique} is able to reproduce
    assertion-violation type errors.
  \item The final test, \bugname{double\_free}, is more complex.  The
    test program consists of two ``active'' threads, shown in
    \autoref{fig:eval:artificial_bugs:programs:double_free:active},
    and one environmental thread, shown in
    \autoref{fig:eval:artificial_bugs:programs:double_free:environmental}.
    The crashing and interfering threads correspond to the two active
    threads, but which active thread corresponds to the crashing
    thread and which the interfering one will depend on how the
    various races behave at run time.  This test illustrates
    {\technique}'s ability to determine that correspondence.
\end{itemize}

\subsubsection{Generating \glsentrytext{verificationcondition}s}

\begin{table}
  \begin{tabular}{|l|l|l|}
    \hline
    Test program              & Time building \gls{programmodel} & Time generating \glspl{verificationcondition} \\
    \hline
    \bugname{toctou}          & $0.541 \pm^{10}_p 0.056$s          & $0.527 \pm^{10}_p 0.008$s \\
    \bugname{multi\_variable} & $0.662 \pm^{10}_p 0.067$s          & $0.370 \pm^{10}_p 0.007$s \\
    \bugname{double\_free}    & $0.434 \pm^{10}_p 0.055$s          & $0.342 \pm^{10}_p 0.009$s \\
    \hline
  \end{tabular}
  \caption{Time taken to generate \glspl{verificationcondition} for
    the artificial bugs.}
  \label{tab:eval:artificial_bugs:analysis_time}
\end{table}

Whether the bug is to be fixed or reproduced, the first step is to
build the \gls{programmodel} and thence the
\glspl{verificationcondition}.  As expected, the artificial test
programs produced a single \gls{verificationcondition} each.  The time
taken to do so is shown in
\autoref{tab:eval:artificial_bugs:analysis_time}.  In this table, and
throughout this evaluation, the $\mu \pm^n_p \sigma$ notation
summarises a set of $n$ samples by giving their mean $\mu$ and sample
standard deviation $\sigma$; I use it only when the data contains
sufficiently few outliers that it provides a (qualitatively)
reasonable characterisation.  The table clearly shows that both steps
of the analysis complete reasonably quickly for these simple test
programs.  More realistic programs, would, of course, take much longer
to process.

\subsubsection{Reproducing the bugs}

Once {\implementation} has derived the \glspl{verificationcondition},
it can use them to build \glspl{bugenforcer}.  This is, again, very
quick; see \autoref{tab:eval:artificial_bugs:build_enforcer_times}.
Once built, the \glspl{bugenforcer} can be used to reproduce the bugs.
\autoref{fig:eval:artificial_bugs:cdf1} shows how effective these
enforcers are, giving cumulative distribution functions (CDFs) of the
reproduction times and some summary statistics.  The $[x,y]_b^a$
notation indicates that $[x,y]$ form a 90\% confidence interval for
the statistic derived by a bootstrap over $a$ samples with $b$
replicates.  $x \pm_{\mu}^a y$ indicates that the mean of the
statistic is $x$ and the estimated standard deviation of the mean $y$,
again with $a$ samples.

\begin{table}
  \begin{tabular}{|l|l|}
    \hline
    Test program & Time building enforcer \\
    \hline
    \bugname{toctou}          & $0.206 \pm^{10}_p 0.008$s \\
    \bugname{multi\_variable} & $0.207 \pm^{10}_p 0.006$s \\
    \bugname{double\_free}    & $0.199 \pm^{10}_p 0.009$s \\
    \hline
  \end{tabular}
  \caption{Time taken to build \glspl{bugenforcer} for the artificial
    test programs.}
  \label{tab:eval:artificial_bugs:build_enforcer_times}
\end{table}

\begin{figure}
  \biggraph{eval/artificial_bugs/cdf1.tex}
  \caption{CDF of time taken to reproduce the bugs in the artificial
    test programs, with and without \glspl{bugenforcer}, and some
    summary statistics.  Note log scale.  All times in seconds.  Means
    calculated ignoring timeouts.  Grey region gives a 90\% confidence
    interval, computed using the Dvoretsky-Kiefer-Wolfowitz-Massart
    (DKWM) inequality \todo{Cite for DKWM?}.}
\end{figure}

This figure shows quite clearly that the enforcers are able to reduce
the time taken to reproduce these bugs.  They are particularly
effective at eliminating large outliers in which the bug takes a very
long time to reproduce.  This is a potentially useful property: with
fewer outliers, the behaviour of a bug will be, qualitatively, more
predictable, which would by likely to make it easier for a programmer
to understand the bug even without a reduction in reproduction time.

\subsubsection{Fixing bugs}

\begin{table}
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    Test program    & Time building fix & \multicolumn{3}{c|}{Time to run main loop, microseconds} \\
                    &                   & Unfixed & Fixed & Increase \\
    \hline
    \bugname{toctou}       & $0.142 \pm^{10}_p 0.004$ & & &\\
    \hspace{1em}Crashing thread         & & $0.62 \pm_{\mu}^{10} 0.03$   & \\
    \hspace{1em}Interfering thread      & & $0.64 \pm_{\mu}^{10} 0.06$   & \\
    \hline
    \bugname{multi\_variable} & $0.135 \pm^{10}_p 0.004$ & & &\\
    \hspace{1em}Crashing thread         & & $568 \pm_{\mu}^{10} 7$       & \\
    \hspace{1em}Interfering thread      & & $568 \pm_{\mu}^{10} 7$       & \\
    \hline
    \bugname{double\_free}    & $0.135 \pm^{10}_p 0.003$ & & &\\
    \hspace{1em}Active threads          & & $1106 \pm_{\mu}^{10} 1$      & \\
    \hspace{1em}Environmental thread    & & $0.089 \pm_{\mu}^{10} 0.001$ & \\
    \hline
  \end{tabular}
  \caption{}
  \label{tab:eval:artificial_bugs:fixes}
\end{table}

{\Technique} is able to generate fixes for all three of these
artificial bugs, and these fixes correctly eliminate the bugs and
prevent the fixed programs from crashing.  These fixes have reasonably
low overhead and can be built very quickly, as shown in
\autoref{tab:eval:artificial_bugs:fixes}.

The performance metric used is the time taken by each of the threads
to complete one iteration of its test loop\footnote{Averaged over ten
  intervals each of ten seconds.  Any intervals which crashed were
  repeated.}.  This is to some extent an arbitrary metric, as most of
the tests contain calls to \texttt{sleep} in their main loop, and so
the metric could be set to almost any value by adjusting the length of
the sleep.  The increase in the cost of the loop, shown in the final
column\footnote{The difference is given as $x \pm y$, where $x$ is the
  difference of means and $y$ a standard deviation in the estimate of
  $x$ computed from the standard deviations of the previous two
  columns.}, is more meaningful, and gives an estimate of the cost of
the patching and locking operations which {\implementation} uses to
implement its critical sections.  As can be seen, it is generally on
the order of a few microseconds, suggesting that {\technique} fixes
will have reasonable overhead provided that the buggy code executes
less than on the order of a million times per second.

\subsection{Bugs in real programs}

The previous section showed that {\technique} can be used to reproduce
and to fix bugs in small artificial test programs.  This section
repeats the experiments using two bugs taken from real programs:
\begin{itemize}
\item The first, \bugname{thunderbird}, is Mozilla bug number
  391259\cite{Mery2007}, a time-of-check, time-of-use race in the IMAP
  client component of Thunderbird, a popular open-source e-mail
  client.  The relevant parts of the program are shown in
  figures~\ref{fig:eval:real_bugs:programs:thunderbird:crashing}
  and~\ref{fig:eval:real_bugs:programs:thunderbird:interfering}.  If
  \verb|m_transport| is set to \verb|NULL| by \verb|CloseStreams()| in
  between the two accesses in \verb|ProcessCurrentURL| then the
  program will crash.
\item The second, \bugname{mysql}, is MySQL bug number
  56324\needCite{}.  A simplified version of the buggy code is shown
  in figures~\ref{fig:eval:real_bugs:programs:mysql:crashing}
  and~\ref{fig:eval:real_bugs:programs:mysql:interfering}.  The
  program will crash if the interfering thread sets
  \texttt{PSI\_server} to \texttt{NULL} in between the two accesses to
  that variable in the crashing thread.
\end{itemize}
{\Technique} is able to reproduce and fix both of these bugs.  For
these tests, unless otherwise noted, I assumed that the crashing
instruction had already been identified, rather than attempting to
discover it automatically using {\technique}.

\begin{figure}
  \begin{tabular}{ll}
    \subfigure[][\bugname{thunderbird} crashing thread]{
      \texttt{
        \begin{tabular}{ll}
          \multicolumn{2}{l}{if (this->m\_transport) \{}\\
          & this->m\_transport->SetTimeout();\\
          \multicolumn{2}{l}{\}}\\
        \end{tabular}
      }
      \label{fig:eval:real_bugs:programs:thunderbird:crashing}
    }
    &
    \subfigure[][\bugname{thunderbird} interfering thread]{
      \texttt{
        \hspace{-5mm}
        \begin{tabular}{l}
          \\
          this->m\_transport = NULL;\\
          \\
        \end{tabular}
      }
      \label{fig:eval:real_bugs:programs:thunderbird:interfering}
    }\\
    \subfigure[][\bugname{mysql} crashing thread]{
      \texttt{
        \begin{tabular}{ll}
          \multicolumn{2}{l}{if (PSI\_server) \{}\\
          & PSI\_server->delete\_current\_thread();\\
          \}\\
        \end{tabular}
      }
      \label{fig:eval:real_bugs:programs:mysql:crashing}
    }
    &
    \subfigure[][\bugname{mysql} interfering thread]{
      \texttt{
        \begin{tabular}{l}
          \\
          PSI\_server = NULL;\\
          \\
        \end{tabular}
      }
      \label{fig:eval:real_bugs:programs:mysql:interfering}
    }
  \end{tabular}
  \caption{Test bugs in real programs.}
  \label{fig:eval:real_bugs:programs}
\end{figure}

\subsubsection{Generating \glspl{verificationcondition}}

As with the artificial bugs, the first step of reproducing or fixing
bugs in real programs is to build the \gls{programmodel}, which then
allows {\technique} to derive the \glspl{verificationcondition}.  The
times taken to do so are shown in
\autoref{tab:eval:real_bugs:analysis_time}.  Notice that the time to
build the \gls{programmodel} is dramatically larger for real programs
than it was for the artificial ones considered earlier, but the time
to generate the \glspl{verificationcondition} is largely unchanged.
This is because {\technique} must examine the entire program in order
to build the \gls{programmodel}, but only needs to examine the
\gls{analysiswindow} in order to build the
\glspl{verificationcondition}.  This is somewhat mitigated by the fact
that the \gls{programmodel} is built for the program rather than for
any particular bug, and so the cost of building it could be amortised
if there were several bugs in the same program.

\begin{table}
  \begin{tabular}{|l|l|l|}
    \hline
    Test bug                  & Time building \gls{programmodel} & Time generating \glspl{verificationcondition} \\
    \hline
    \bugname{thunderbird}     & $1240 \pm^{10}_p 10$s              & $0.39 \pm^{10}_p 0.01$s \\
    \bugname{mysql}           &                                  &\\
    \hline
  \end{tabular}
  \caption{Time taken to generate \glspl{verificationcondition} for
    the bugs taken from real programs.}
  \label{tab:eval:real_bugs:analysis_time}
\end{table}

{\Technique} generated one false positive
\glspl{verificationcondition} for each bug.  Both of false positives
were caused by incompleteness in the {\technique}'s model of the
program's behaviour.  In the case of \bugname{thunderbird}, the
problem was the lack of knowledge of the program's existing
synchronisation structure.  {\Technique} located another place in the
program which set \texttt{m\_transport} to \texttt{NULL}, and
correctly identified that interleaving that store with the
\gls{crashingthread} might lead to a crash, but failed to notice that
an existing lock prevented the interleaving from happening.  The
\bugname{mysql} false positive, by contrast, was caused by an
incomplete model of the program's data structure.  {\Technique}
located another instruction which could set \texttt{PSI\_server} and
which could potentially race with the \gls{crashingthread}, but in
that case the value stored was always a valid pointer, and so the
interleaving could not lead to a crash.

\subsubsection{Generating \glspl{bugenforcer}}

\todo{Fill out tables}
\begin{table}
  \begin{tabular}{|l|l|l|l|}
    \hline
                          & \multicolumn{3}{c}{Time building} \\
    Bug                   & True positive enforcer & False positive enforcer & Combined enforcer \\
    \hline
    \bugname{mysql}       & \\
    \bugname{thunderbird} & $0.640 \pm^{10}_p 0.010$s     & $0.640 \pm^{10}_p 0.010$s     & $0.710 \pm^{10}_p 0.020$s\\
    \hline    
  \end{tabular}
  \caption{Time taken building the various \glspl{bugenforcer}.}
  \label{tab:eval:real_bugs:build_enforcer_times}
\end{table}

\begin{table}
  \begin{tabular}{|l|l|l|l|l|}
    \hline
                              & \multicolumn{4}{c}{Number of reproductions} \\
    Bug                       & Without enforcer & True positive enforcer & False positive enforcer & Combined enforcer \\
    \hline
    \bugname{mysql}           &   &    &   &    \\
    \bugname{thunderbird}     & 0 &    &   &    \\
    \hspace{1em}100--200ms timeout &   & 0  & 0 & 0  \\
    \hspace{1em}5s timeout    &   & 10 & 0 & 10 \\
    \hline
  \end{tabular}
  \caption{Reproduction counts for the different bugs and
    configurations.  \bugname{mysql} counts are out of 100;
    \bugname{thunderbird} ones ten.}
  \label{tab:eval:real_bugs:repro_effectiveness}
\end{table}

All four \glspl{verificationcondition} can be converted to
\glspl{bugenforcer}.
Table~\ref{tab:eval:real_bugs:build_enforcer_times} shows how long it
took to do so and Table~\ref{tab:eval:real_bugs:repro_effectiveness}
shows how effective they were at reproducing the bugs.  As can be
seen, building the enforcers is reasonably quick, and they are
effective at reproducing these bugs.

\Glspl{bugenforcer} require a running program to operate, and their
behaviour depends in part on the program's workload:
\begin{itemize}
\item For the \bugname{mysql} test, I used the
  \texttt{rpl\_change\_master} test out of the MySQL test suite, as
  that test runs quickly without manual intervention and exercises the
  buggy code, and ran it one hundred times in each configuration.
\item For the \bugname{thunderbird} test, no convenient automated test
  suite was available, and so I exercised the buggy code via manual
  interaction with the Thunderbird GUI, by clicking on an IMAP folder
  and then quickly clicking the close button.  I repeated this ten
  times, restarting Thunderbird in between each experiment.  The IMAP
  folder was the only folder in an account on a local Dovecot 1.2.9
  IMAP server which had no other users, and no other IMAP accounts
  were configured in Thunderbird.  The IMAP folder itself was empty.
\end{itemize}

\todo{Ugly paragraph} The \bugname{thunderbird} test illustrates an
important weakness of the {\technique} approach.  {\Technique}'s
\glspl{bugenforcer} work by delaying some part of the program's
execution by a fixed amount, in the hope that some other thread will
start executing the racing code during that interval.  Sizing this
delay requires some knowledge of the program's behaviour: if it is too
small then it will be unlikely that any racing thread will arrive
during the interval, but if it is too large then the enforcers will
potentially have a large probe effect, which will also reduce the
likelihood of reproducing the bug.  In this case, the bug reproduces
if the user moves from clicking on an IMAP folder to clicking the
close button before the timeout expires.  That is difficult when the
timeout is set to a small value, and I was unable to reproduce the bug
using the default 100--200ms timeout.  Increasing the timeout to five
seconds made the bug reproduce easily.  The program was still quite
usable even with this long delay, as the bug is in code which executes
infrequently, and only in a background thread.

\subsubsection{Fixing the bugs}

\begin{table}
  \begin{tabular}{ll}
    Bug                  & Time building fix \\
    \bugname{mysql}      & \\
    \bugname{thunderbird} & \\
  \end{tabular}
  \caption{Time taken to convert \glspl{verificationcondition} to fixes.}
  \label{tab:eval:real_bugs:time_building_fixes}
\end{table}

These \glspl{verificationcondition} can also be converted to fixes;
the time taken to do so is shown in
\autoref{tab:eval:real_bugs:time_building_fixes}.  The fixes generated
are similar for both bugs: one critical section covering the two loads
in the \gls{crashingthread}, and another covering the store in the
\gls{interferingthread}.  It is hard to validate experimentally that
these fixes are correct, as the bugs reproduce quite rarely even
without the fix\footnote{Due to implementation limitations, it is not
  possible to load a {\technique} fix and a {\technique}
  \gls{bugenforcer} into the same program.}, but manual inspection
suggested that they had correctly eliminated the dangerous
interleaving.  It is also hard to experimentally evaluate the
performance impact of these fixes, as, in the case of
\bugname{thunderbird}, there is no convenient metric of performance,
beyond noting that performance was qualitatively unaffected, and, in
the case of \bugname{mysql}, the fix is to code which executes
sufficiently rarely that the system-level overhead was immeasurably
small.

These are not, however, the fixes which a human programmer would make.
In particular, the call to \texttt{delete\_current\_thread}, in
\bugname{mysql}, and \texttt{SetTimeout}, in \bugname{thunderbird},
are not protected in any way.  This means that the
\gls{crashingthread} might continue to execute a method in
\texttt{m\_transport} or \texttt{PSI\_server} after those variables
have been cleared.  In this case, the implementations of those methods
means that this is safe, but doing so does not concord with common
programming practice, and changes in the implementation could render
it unsafe without being visible to {\technique}.  This might cause
{\technique} to generate a fix which is qualitatively incomplete,
despite correctly eliminating all of the identified dangerous
interleavings.

The problem here lies in {\technique}'s definition of a bug: an
atomicity violation which might lead to a crash on a particular
instruction.  Once {\technique} has ensured that that instruction
cannot crash, it considers the bug to be fixed.  If the program then
crashes five instructions later, {\technique} defines that to be a
different bug, requiring a different fix.  This will not necessarily
agree closely with a programmer's or user's idea of what it means to
fix a bug.

\subsubsection{Finding unknown bugs}

{\Technique} can also be used to discover unknown bugs, if the program
has a reasonable automated test suite, by having it consider every
possible crash at every instruction in the program, generating a
\glspl{bugenforcer} for each, and then running the test suite with
each of these enforcers loaded.  As mentioned above, MySQL has such a
test suite, and so I generated a complete set of enforcers for it.
This took three hours and twelve minutes\editorial{Repeats?}, using
ten cores on a twelve core AMD Opteron 6168 with 16GiB of memory, and
produced a total of 10188 \glspl{bugenforcer}.  I then re-ran the
\texttt{rpl\_change\_master} test ten times with each of these
enforcers loaded, again running ten tests in parallel.  These tests
were run such that every enforcer was tested $n$ times before any
enforcer ran $n+1$ times, and the order in which enforcers ran, and
hence which enforcers ran in parallel, was randomised for every
repeat.  As a baseline, I also ran the test 10,000 times without any
\glspl{bugenforcer} attached; \todo{...} of these runs suffered
crashes, giving a baseline failure rate of one in \todo{...}.

As expected, one of the enforcers was able to consistently reproduce
the \bugname{mysql} bug discussed above.  Two other enforcers were
also able to reproduce essentially similar bugs elsewhere in MySQL; I
was unaware of these bugs before running the tool.  These three
enforcers caused the relevant bugs to reproduce on every run of the
test suite.

Several other runs of MySQL suffered from one of these bugs, despite
the \glspl{bugenforcer} in use at the time being designed for other
bugs.  Many of these simply reflect the fact that these are real bugs:
if the buggy program is run enough times then it will eventually crash
due to one of them, even without {\technique}'s assistance; the
\glspl{bugenforcer} simply reduce the number of times which the
program needs to be run.


\begin{table}
\todo{Come up with a better way of summarising this data.}
\begin{tabular}{lllllllll}
ID    & res1         & res2         & res3         & res4         & res5         & res6         & res7         & res8\\
17    &              &              &              &              &              & Spurious \\
183   &              &              &              & Spurious \\
219   &              &              & Spurious \\
243   &              &              &              &             & Spurious \\
263   &              &              &              & Spurious \\
264   &              &              &              & Spurious \\
332   &              &              &              &             & Spurious \\
346   &              &              &              & Spurious \\
347   &              &              & Spurious \\
410   & Spurious \\
1041  &              &              &              &             & Spurious \\
1082  &              &              &              &              &              & Spurious \\
1087  &              &              &              &             & Spurious \\
1148  &              &              &              &              &              &              & Spurious \\
1336  &              &              &              &              &              &              & Spurious \\
1342  &              &              &              &             & Spurious \\
1443  &              &              & Spurious \\
1572  &              &              &              & Spurious \\
1586  &              &              & Spurious \\
1653  &              &              &              &              &              & Spurious \\
1687  &              &              &              &             & Spurious \\
1702  &              &              &              &              &              &              &              & Spurious \\
1782  & Spurious \\
1787  &              &              &              & Spurious \\
1802  &              &              &              &              &              &              &              & Spurious \\
1829  &              &              &              &              &              &              & Spurious \\
1886  &              &              &              &              &              &              &              & Spurious \\
1945  &              &              &              & Spurious \\
1956  &              &              &              &              &              &              & Spurious \\
1976  &              &              &              &              &              & Spurious \\
2011  &              &              &              &             & Spurious \\
2031  &              &              &              &             & Spurious \\
2046  &              &              &              &              &              & Spurious \\
2081  &              &              &              &             & Spurious \\
2093  &              &              &              &              &              &              & Spurious \\
2130  &              &              &              &              &              &              & Spurious \\
2174  &              &              &              & Spurious \\
2195  &              &              &              &              &              &              &              & Spurious \\
2206  &              &              &              &              &              &              &              & Spurious \\
2259  & Spurious     &              & Spurious     & Spurious    & Spurious      & Spurious     & Spurious     & Spurious \\
2305  & Spurious \\
2379  &              &              &              &              &              &              &              & Spurious \\
2344  & Spurious \\
2377  &              &              &              &              &              &              & Spurious \\
2408  &              &              & Spurious \\
2418  &              &              &              &              &              &              &              & Spurious \\
2447  &              &              &              & Spurious \\
2469  &              &              & Spurious \\
2486  & Spurious \\
2538  &              &              &              &              &              & Spurious \\
2632  &              &              &              & Spurious \\
2788  &              &              &              &              &              &              & Spurious \\
2801  &              &              &              &              &              & Spurious \\
2839  &              &              &              &             & Spurious \\
2849  &              &              &              &              &              &              &              & Spurious \\
2858  &              &              &              &             & Spurious \\
3028  & Spurious     &              & Spurious \\
3112  &              &              &              & Spurious \\
3152  &              &              &              & Spurious \\
3370  &              & Spurious \\
3372  &              &              &              & Spurious \\
3482  &              &              & Spurious \\
3518  &              &              & Spurious \\
4321  & Spurious \\
10057 &              & Spurious \\
10095 &              &              &              &             & Spurious \\
10065 & spurious \\
10071 &              & Spurious \\
10105 &              &              &              &             & Spurious \\
10110 &              &              &              &             & Spurious \\
10188 &              &              &              &              &              &              &              & Spurious \\
10128 &              &              &              &              &              & Spurious \\
10318 &              &              &              &             & Spurious \\
10444 &              &              &              &             & Spurious \\
10465 &              &              &              &              &              &              & Spurious \\
10468 &              & Spurious \\
10542 &              &              &              &              &              &              &              & Spurious \\
10649 &              &              &              &              &              &              & Spurious \\
10721 &              & Spurious \\
10745 & Spurious \\
10763 &              &              &              &              &              &              & Spurious \\
10782 &              &              &              &              &              &              & Spurious \\
10934 & Two variants & Two variants & Two variants & Two variants & Two variants & Two variants & Two variants & Two variants \\
10935 & Two variants & Two variants & Two variants & Two variants & Two variants & Two variants & Two variants & Two variants \\
10940 &              &              &              &              &              & Spurious \\
10965 &              &              &              &             & Spurious \\
10972 &              &              &              &              &              & Spurious \\
10973 & Real repro   & Real         & Real         & Real        & Real          & Real         & Real \\
11028 &              &              &              &              &              & Spurious \\
11076 &              &              &              & Spurious \\
11078 & Spurious \\
11161 &              &              &              & Spurious \\
11212 &              &              & Spurious \\
11244 &              &              &              &              &              &              &              & Spurious \\
11245 & Spurious \\
\end{tabular}
\caption{Effectiveness of {\technique} at finding unknown bugs.}
\label{tab:eval:does:finding_unknown}
\end{table}

\section{How does it work?}
\label{sect:eval:how_does_it_work}

The previous section demonstrated that {\technique} works, at a basic
level, for at least some real and artificial bugs.  This section aims
to expand upon this by giving more details of the way in which it
works, by breaking the time and memory usage down into the different
phases of the analysis.  For these experiments, I selected 1000
memory-accessing instructions at random from MySQL and produced
\glspl{bugenforcer} and fixes for each in turn, recording the time
spent in each of the various steps of the analysis.

The analysis of a single potentially crashing instruction can be
roughly divided into four phases:
\begin{itemize}
\item Per-\gls{crashingthread} analysis work.  {\Technique} starts
  analysing a potentially-crashing instruction by constructing the
  crashing {\StateMachine}, and from that it builds the interfering
  \glspl{cfg}.  This work is done once for each potentially-crashing
  instruction.
\item Per-\gls{interferingthread} analysis work.  Each crashing
  {\StateMachine} will generate zero or more
  \glspl{interferingthread}, each of which is analysed independently.
\item Building the \gls{bugenforcer}.  Each \gls{interferingthread} in
  turn generates zero or more \glspl{verificationcondition}.  Each
  \gls{verificationcondition} is processed in isolation to produce a
  single \gls{bugenforcer}.
\item Building the fixes.  The \glspl{verificationcondition} can
  instead be converted into fixes.  Again, each
  \gls{verificationcondition} is processed in isolation to produce a
  single fix.
\end{itemize}
I consider each phase in turn.

\subsection{Per-\gls{crashingthread} analysis}

The initial part of the analysis is performed once for each
potentially-crashing instruction.  For each instruction, {\technique}
derives the crashing \gls{cfg}
(\autoref{sect:derive:build_crashing_cfg}), decompiles it to a
{\StateMachine} (\autoref{sect:derive:compile_cfg}), simplifies it
(\autoref{sect:derive:simplify_sm}), builds the interfering
\glspl{cfg} (\autoref{sect:derive:write_side}), and then derives
C-atomic (\autoref{sect:derive:inferred_assumption} and
\autoref{sect:derive:w_isolation}).

\begin{figure}
  \centerline{
    \biggraph{eval/phase_breakdown/per_crashing.tex}
  }
  \caption{Distribution of time taken by the various
    per-\gls{crashingthread} steps. \label{fig:eval:how:per_crashing_times}}
\end{figure}

\autoref{fig:eval:how:per_crashing_times} shows how much time is spent
in each of these steps.  This figure shows several useful pieces of
information:
\begin{itemize}
\item The main part of the figure shows the distribution of the time
  spent in each step.  Note that this is shown on a log scale, and
  that the density is with respect to log time.
\item The median of the distribution is shown as a horizontal line
  across the PDF.
\item The grey areas give 90\% confidence intervals for the PDF and
  median.  These were computed using a bootstrap with 1000 replicates.
\item The (arithmetic) mean, plus or minus one standard deviation of
  the mean, is shown as a cross with bars.  This mean is calculated
  ignoring all failures.
\item The kernel used in estimating the probability density function
  is shown below the PDF itself.  This is the contribution which a
  single sample makes to the PDF.  It is a rectangle of height
  $2.75Rn^{-0.2}$, where $R$ is the log inter-quartile range and $n$
  the number of samples\footnote{This bandwidth was chosen as it is
    reasonably robust to outliers and minimises the expected mean
    square error for Gaussian data.}.
\item The rightmost PDF in the figure shows the distribution of the
  total time taken to process a potentially-crashing instruction,
  measured from the start of processing to the end.
\item The remaining PDF, labelled ``Defect'', gives the difference
  between the total time taken and the sum of all of the measured
  steps.  As can be seen, it is small relative to the other quantities
  measured, which is necessary for the other measurements to be
  meaningful.
\item The boxes above the PDFs give the number of potentially-crashing
  instructions which failed to complete that step.  These boxes are
  sized such that a constant area represents a constant probability
  across the figure.  In this case, nine instructions reached the five
  minute timeout during {\StateMachine} simplification, one reached
  the timeout while deriving the interfering \glspl{cfg}, and three
  instructions ran out of memory while deriving the interfering
  \glspl{cfg}.  Note that the timeout runs from the start of the
  ``build crashing \gls{cfg}'' step, rather than from the start of the
  current step.
\item The boxes below the PDFs give the number of potentially-crashing
  instructions which skipped that step.  For instance, instructions
  which access a fixed location which is mapped by the program's ELF
  metadata are eliminated before building the crashing \gls{cfg} and
  those for which the \gls{programmodel} cannot find any racing
  instructions are eliminated after deriving the interfering
  \glspl{cfg}.  A step is also skipped if any previous steps suffered
  failures.  As can be seen, the majority of steps are skipped for the
  majority of instructions, which is helpful when speculatively
  analysing a large number of instructions, as in {\technique}'s bug
  finding mode.
\end{itemize}
The most important observation to draw from this figure, aside from
the gross summary of how long each step takes, is that most of the
distributions are dominated by their tails, in the sense that the mean
is often more than an order of magnitude greater than the median.  The
time spent deriving C-atomic, for instance, has a median of a little
over 1ms and a mean of 120ms, while the total time taken has a median
of 30ms and a mean of 1500ms.  This reflects the fact that many of the
algorithms have worst-case running time far worse than their expected
case.  Instructions which happen to hit one of the slow cases take a
very long time to complete, while those which avoid the slow cases
complete very quickly.  This is further illustrated by
\autoref{fig:eval:how:per_crashing:structure_complexity}, which shows
the sizes of some of the more important intermediate structures used
by {\technique}, again demonstrating the fat-tailed nature of the
distributions involved.

\begin{figure}
  \todo{Numbers don't jive with previous figure.}
  \centerline{
    \biggraph{eval/phase_breakdown/structure_complexity.tex}
  }
  \caption{CDFs of the sizes of various structures generated by
    {\technique} during the analysis.  Note log scale.  The vertical
    line gives the mean of the distribution, with the grey area
    showing plus or minus one standard deviation of the mean.  The
    grey area around the CDF gives the 90\% DKWM confidence
    interval. \label{fig:eval:how:per_crashing:structure_complexity}}
\end{figure}

\subsection{Per-\gls{interferingthread} analysis}

Once the interfering \glspl{cfg} for a crashing instruction have been
generated, the next step is to analyse them.  The time taken to do so
is illustrated in \autoref{fig:eval:how:per_interfering}, in the same
style as \autoref{fig:eval:how:per_crashing_times}.  This figure shows
the distributions over the 23559 interfering \glspl{cfg} generated by
the per-\gls{crashingthread} analysis.  In the figure, the
{\StateMachine} building steps include simplifying the
{\StateMachines}.  The second step, ``Rederive crashing
{\StateMachine}'', performs various additional simplifications on the
crashing {\StateMachine} which become possible once {\technique} has
identified the interfering {\StateMachine}, such as converting
\stLoad{}{} side-effects into $\smLoad{}$ side-effects if the accessed
memory cannot be modified by the interfering {\StateMachine}.  This is
sometimes sufficient to show that the bug could not possibly reproduce
with this combination of crashing and interfering {\StateMachines},
allowing the rest of the phase to be skipped.  \todo{Rephrase}

This phase suffered a number of failures:
\begin{itemize}
\item The ``build interfering \glspl{cfg}'' phase timed out 21 times
  and ran out of memory once.
\item The ``rederive crashing {\StateMachine}'' phase time out 21
  times.
\item Running the \gls{ic-atomic} {\StateMachine} timed out 12 times
  and ran out of memory and ran out of memory 29 times.
\item Building the cross-product {\StateMachine} timed out once.
\item Running the cross-product {\StateMachine} timed out 17 times and
  ran out of memory once.
\end{itemize}
There were a total of 103 failures, giving a failure rate of 0.4\%.
\todo{So?}

\begin{figure}
  \centerline{
    \biggraph{eval/phase_breakdown/per_interfering}
  }
  \caption{Time taken by per-\gls{interferingthread} analysis steps.}
\end{figure}

As with \autoref{fig:eval:how:per_crashing_times}, this figure shows
that most of the distributions involved are dominated by their tails,
indicating that most of the analysis time is spent processing a small
minority of unusually difficult \glspl{cfg}.  The two symbolic
execution phases are particularly prone to these outliers, because
they must consider every path through the relevant {\StateMachine} and
the number of such paths rises exponentially with the size of the
{\StateMachine}.  The ``Derive C-atomic'' per-\gls{crashingthread}
step likewise shows a particularly long tail in which it is
particularly expensive.

More surprisingly, symbolically executing the \gls{ic-atomic}
{\StateMachine} is more expensive than executing the cross-product
{\StateMachine}, despite having to consider far fewer instruction
orderings.  This is because it has less information about the
configurations in which the {\StateMachines} might start.  The
\gls{ic-atomic} constraint, along with the C-atomic constraint, gives
the necessary condition for the {\StateMachines} to avoid crashing
when run atomically.  The cross-product symbolic execution only looks
for atomicity violation crashes, and so it can assume that the
\gls{ic-atomic} constraint holds.  This often eliminates a large
number of potential initial states of the {\StateMachines}, which
makes the symbolic execution somewhat cheaper.

\begin{figure}
  \centerline{
    \biggraph{eval/phase_breakdown/per_interfering_no_w_atomic}
  }
  \caption{Time taken by per-\gls{interferingthread} analysis steps,
    with the \gls{ic-atomic} steps disabled.}
  \label{fig:eval:how:per_interfering:no_ic_atomic}
\end{figure}

This is further illustrated in
\autoref{fig:eval:how:per_interfering:no_ic_atomic}, which shows how
long the various steps take when {\technique} assumes \gls{ic-atomic}
is just \true, rather than attempting to derive it.  This eliminates
the steps involved in deriving \gls{ic-atomic}, which take 29ms, but
increases the cost of the later steps so that the total reduction in
time taken is only 13ms.  It also increased the number of
\glspl{verificationcondition} generated, from 3501 to 5998.  All of
the extra conditions are false positives which would have needed to be
investigated by run-time enforcers, easily outweighing the reduced
cost of this phase of the analysis.

\todo{I don't think it's a coincidence that 13ms is very close to the
  cost of building the C-atomic condition.  Not sure if I can say
  anything interesting about it, though.}

\subsection{Costs of building \gls{bugenforcer}}

\begin{figure}
  \centerline{
    \biggraph{eval/phase_breakdown/build_enforcer}
  }
  \caption{Time taken by the steps involved in converting a
    \gls{verificationcondition} into a \gls{bugenforcer}.}
  \label{fig:eval:how:build_enforcer}
\end{figure}

Once the \glspl{verificationcondition} have been derived, they can be
converted to \glspl{bugenforcer}.
\autoref{fig:eval:how:build_enforcer} shows how long it takes to do
so, using the 3501 \glspl{verificationcondition} generated from the
experiments in the previous section.  The figure divides the time
taken into five steps:
\begin{itemize}
\item Extracting the happens-before graph, as discussed in
  \autoref{sect:enforce:slice_hb_graph}.  This step is generally
  reasonably quick, with a median time of 16ms, but occasionally takes
  a very long time, reaching the five minute timeout in 28 of the 3501
  runs.  It also ran out of memory 20 times.  This reflects the nature
  of the algorithm used: BDD reordering has a good expected-case cost
  but a very poor worst-case one.  The \glspl{verificationcondition}
  which avoid the worst-case performance complete very quickly and
  those which do not form the long tail.
\item Determining when the verification condition's input expressions
  become available, as discussed in \autoref{sect:enforce:place_vcs}.
  This is, unsurprisingly, a very quick step, with relatively few
  outliers, as the rules governing when inputs become available are
  simple and easily applied.
\item Deciding how to evaluate the non-happens before component of the
  \gls{verificationcondition} by placing side conditions on the
  happens-before and control flow graphs, which is also discussed in
  \autoref{sect:enforce:place_vcs}\editorial{ugg}.  As with extracting
  the happens-before graph, this step is implemented using BDD
  reordering operations, and, as with that step, it is generally quick
  with a long tail of slow operations\editorial{ugg, again}.  It ran
  out of memory twice.
\item Building the patch strategy, expressed as the $\mathit{Cont}$
  and $\mathit{Patch}$ sets, as discussed in
  \autoref{sect:enforce:gain_control}.  \todo{Explain shape a little?}
\item Compiling the resulting enforcer into an ELF shared object.
  {\Implementation} performs this step by generating a C source file
  and passing it off to the system compiler and linker, with the bulk
  of the time spent in those external programs.  This makes it
  difficult to provide any useful analysis on why this step takes as
  long as it does.
\end{itemize}

As discussed above, this phase suffered a total of fifty failures,
giving a failure rate of 1.4\%.  The per-\gls{crashingthread} and
per-\gls{interferingthread} suffered failure rates of 0.1\% and 0.4\%,
respectively; collectively, these suggest that {\technique}, with
these settings, will be unable to generate \glspl{bugenforcer} for
roughly 2\% of potential bugs of the targeted form.  While obviously
worse than a 0\% failure rate, a 2\% one is still reasonably low, and
is unlikely to be a crippling limitation.


\todo{Memory consumption distributions?}


\subsection{Structure of \glspl{bugenforcer}}

\todo{I'm sure there's something I can say about this, but I don't
  know what.}

\subsection{Costs of building fixes}

\Glspl{verificationcondition} can also be converted into fixes.  This
section investigates how long that takes.  \todo{Write me}

\subsection{Structure of generated fixes}

\todo{I'm sure there's something I can say about this, but I don't
  know what.}

\subsection{Dynamic analysis}

{\Technique} relies on a dynamic analysis to build a model of the
program's normal behaviour.  If this does not achieve adequate
coverage then {\technique} will not be able to find all of the bugs in
a program.  Figures~\ref{fig:eval:dyn_convergence:mysqld},
\ref{fig:eval:dyn_convergence:thunderbird},
and~\ref{fig:eval:dyn_convergence:pbzip2} show how long this takes for
some representative programs.  For these experiments, I modified the
dynamic analysis to record periodic snapshots of the alias table and
then ran each of the test programs under this modified analysis.  I
then compared these snapshot alias tables to the final table,
calculating the percentage of final entries which were present in each
snapshot.  As can be seen from the figures, the aliasing table
converged on the final table within a few minutes.  This suggests that
the final table is likely to accurately reflects the program's
behaviour.

For all tests, the dynamic analysis tool was configured to take one
snapshot every 1,000,000 Valgrind basic blocks, which usually
corresponds to a couple of hundred milliseconds of execution
time. This is arguably a somewhat unfair test: taking a snapshot can
itself take several hundred milliseconds, depending on the size of
aliasing table is at the time, and so this experiment actually spent
most of its time taking snapshots rather than running the test
program.  A more realistic test would probably show faster
convergence.

\begin{figure}
  \subfigure[][rpl\_change\_master]{
    \input{eval/dyn_convergence/rpl_change_master.tex}
  }
  \subfigure[][innodb\_multi\_update]{
    \input{eval/dyn_convergence/innodb_multi_update.tex}
  }
  \subfigure[][binlog\_stm\_drop\_tbl]{
    \input{eval/dyn_convergence/binlog_stm_drop_tbl.tex}
  }
  \subfigure[][timestamp\_basic]{
    \input{eval/dyn_convergence/timestamp_basic.tex}
  }
  \caption{Dynamic aliasing coverage against time for MySQL, using
    some tests out of the test suite.  Dashed vertical lines show where the
    program was restarted.}
  \label{fig:eval:dyn_convergence:mysqld}
\end{figure}

\begin{figure}
  \biggraph{eval/dyn_convergence/thunderbird}
  \caption{Dynamic aliasing coverage against time for Thunderbird
    during normal usage.  Dashed vertical lines show where the program was
    restarted.}
  \label{fig:eval:dyn_convergence:thunderbird}
\end{figure}

\begin{figure}
  \biggraph{eval/dyn_convergence/pbzip2}
  \caption{Dynamic aliasing coverage against time for pbzip2 version
    1.1.6 while compressing three randomly-generated 10MiB files.
    Dashed vertical lines show where the program was restarted.}
  \label{fig:eval:dyn_convergence:pbzip2}
\end{figure}

I also briefly investigated the performance of the analysis tool
itself, using pbzip2 as a test program.  For these experiments, I
compressed ten randomly-generated 100MiB files with and without the
dynamic analysis.  Without the dynamic analysis, compressing one file
took $7.8 \pm_\mu^{10} 0.1$ seconds; with the dynamic analysis, this
increased to $274 \pm_\mu^{10} 2$ seconds, a factor of nearly thirty-five.
This is a rather large overhead, and would be completely infeasible in
a production environment, but is probably tolerable for something
which needs to run for a few tens of minutes in a development one.

For comparison, a null Valgrind skin completed this test in $226.6
\pm_\mu^{10} 0.5$ seconds, an overhead of a factor of twenty-nine.  That
suggests that most of the overhead of the dynamic analysis tool comes
simply from the fact that it is implemented as a Valgrind skin, and
that re-implementing it in a faster analysis framework, such as
PIN\cite{Luk2005}, might provide a useful speed-up.

\subsection{Interaction with the program's existing synchronisation}

\begin{figure}
  \subfigure[][Crashing thread with {\technique}-visible synchronisation]{
    \texttt{
      \begin{tabular}{llll}
        \multicolumn{4}{l}{while(1)\{}\\
        &\multicolumn{3}{l}{analysis\_window\{}\\
        &&\multicolumn{2}{l}{lock();}\\
        &&\multicolumn{2}{l}{if (ptr != 0)}\\
        &&&*ptr = 5;\\
        &&\multicolumn{2}{l}{unlock();}\\
        &\multicolumn{3}{l}{\}}\\
        \multicolumn{4}{l}{\}}\\
      \end{tabular}
    }
    \hspace{3mm}
    \label{fig:eval:existing_sync:visible}
  }
  \hspace{-2mm}
  \hfill
  \subfigure[][Crashing thread with {\technique}-invisible synchronisation]{
    \texttt{
      \begin{tabular}{llll}
        \multicolumn{4}{l}{while(1) \{}\\
        &\multicolumn{3}{l}{lock();}\\
        &\multicolumn{3}{l}{analysis\_window\{}\\
        &&\multicolumn{2}{l}{if (ptr != 0)}\\
        &&&*ptr = 5;\\
        &\multicolumn{3}{l}{\}}\\
        &\multicolumn{3}{l}{unlock();}\\
        \multicolumn{4}{l}{\}}\\
      \end{tabular}
    }
    \hspace{3mm}
    \label{fig:eval:existing_sync:invisible}
  }
  \hspace{-3mm}
  \hfill
  \hspace{-3mm}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while(1)\{}\\
        &\multicolumn{2}{l}{ptr = \&t;}\\
        &\multicolumn{2}{l}{analysis\_window\{}\\
        &&lock();\\
        &&ptr = 0;\\
        &&unlock();\\
        &\multicolumn{2}{l}{\}}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
    \hspace{-3mm}
    \label{fig:eval:existing_sync:interfering}
  }
  \vspace{-12pt}
  \caption{A correctly synchronised program.  \texttt{lock()} and
    \texttt{unlock()} acquire and release a global lock,
    respectively.}
  \label{fig:eval:existing_sync}
\end{figure}

{\Technique} largely ignores the program's existing synchronisation if
it falls outside of the \gls{analysiswindow}, instead relying on the
run-time \glspl{bugenforcer} to check the candidate bugs against the
program's observed behaviour.  This section briefly explores the
effects of any existing synchronisation on {\technique}'s behaviour,
using the test program shown in \autoref{fig:eval:existing_sync}.  In
this correctly synchronised program, the interfering thread modifies a
global variable while the two crashing threads simultaneously make use
of it.  The crashing threads differ only in the placement of the
synchronisation operations: the first, in
\autoref{fig:eval:existing_sync:visible}, places the synchronisation
within the \gls{analysiswindow}, making it visible to the {\technique}
analysis, whereas the second, in
\autoref{fig:eval:existing_sync:invisible}, moves it outside of the
window, so {\technique} will be unaware of it.

As expected, {\technique} produces a \gls{verificationcondition}, and
hence a \gls{bugenforcer}, for the second thread but not for the
first.  When loaded into this program, this enforcer attempts to
enforce a happens-before graph which contradicts the program's
existing synchronisation and therefore deadlocks.  This causes the
enforcer's message operations to time out, and so the enforcer exits
and allows the program to run normally (beyond suffering reduced
performance).

The \gls{verificationcondition} can also be converted to a fix.  This
fix does not fix any actual bugs, as there are none to fix, but does
not otherwise harm the program's execution, beyond a slight loss of
performance.

\section{Why does it work?}
\label{sect:eval:why_does_it_work}

Previous sections have established that {\technique} works at a basic
level and given some details of its operation.  This section aims to
expand on that by providing some explanations for the properties
observed.

Several experiments in this section include comparisons to
{\randsched}, a random schedule exploration tool based on
DataCollider\cite{Erickson2010} which I implemented for the purposes
of this evaluation.  The tool works by setting instruction breakpoints
on a selection of the program's memory-accessing instructions and
then, when a thread reaches one of those instructions, setting memory
watchpoints on the accessed locations and waiting for some other
thread to trigger the watchpoint.  When that happens, it selects one
of the threads at random to go first.  This causes the program to
explore its possible schedulings more quickly than it would if run
normally, without needing {\technique}'s expensive initial analysis
phase.

The effectiveness of this kind of tool depends on two parameters: the
placement of breakpoints, and how long the tool waits for another
thread to arrive when one of the watchpoints is triggered.  For this
evaluation, I configured {\randsched} to place breakpoints on half of
the program's memory-accessing instructions, excluding instructions
which access the stack, selecting a new set of instructions every
100ms, and to wait 100$\mu$s after a watchpoint is triggered.  These
parameters minimised the median reproduction time for the
\bugname{toctou} test with \texttt{NR\_PTRS} set to 100.

Note that the 100$\mu$s timeout is much shorter than the 100-200ms
timeout used by {\technique} \glspl{bugenforcer}, and so {\randsched}
will make smaller modifications to the program's behaviour, but that
setting a breakpoint on every other instruction means that it will
make those modifications far more frequently.

\subsection{Importance of side-condition checking}

The most distinctive feature of {\technique}'s \gls{bugenforcer}
mechanism, compared to previous work such as Kivati\cite{Chew2010} or
MUVI\cite{Lu2007}, is side-condition checking, which enables it to
avoid spending time enforcing a particular concurrent ordering if some
aspect of the program's state means that doing so would be
unproductive.  \autoref{fig:eval:indexed_toctou:nr_ptrs} shows the
reproduction time for the \bugname{toctou} test with a full enforcer
and with one which does not perform any side-condition checking.  This
clearly shows not only that reproduction performance without side
condition checking is far worse than with a full enforcer, but also
that it is worse than not using an enforcer at all.  Without
side-condition checking, the enforcer enforces the happens-before
graph every time the buggy code runs, which causes the program to run
far more slowly, so the buggy code runs far less frequently.  In this
case, the happens-before graph is quite simple, but the side-condition
has a very low probability of succeeding, and increasing the
likelihood of reproducing the happens-before graph is insufficient to
outweigh the reduced number of chances to satisfy the
side-condition\editorial{Blah.  I have some maths which makes this a
  bit more formal, but doesn't add enough clarity to justify the space
  it'd take up.}.

\begin{figure}
  \biggraph{eval/artificial_bugs/special/indexed_toctou_no_scs.tex}
  \caption{Effect of side-condition checking on the time taken to
    reproduce the indexed\_toctou bug.}
  \label{fig:eval:indexed_toctou:no_scs}
\end{figure}

The importance of side-condition checking depends on the probability
of satisfying the condition, and hence on \texttt{NR\_PTRS}.
\autoref{fig:eval:indexed_toctou:nr_ptrs} shows this dependency.
Reproduction rates fall as \texttt{NR\_PTRS} increases, for both
configurations, but rises more rapidly and from a higher base without
an enforcer.  This indicates that {\technique} enforcers become more
effective as the probability of a side condition passing falls.  It is
also worth noting that the behaviour with an enforcer is qualitatively
simpler, with smaller inter-quartile ranges, means generally near to
the medians, and a generally smoother shape (the dips at
\texttt{NR\_PTRS} = 400 and \texttt{NR\_PTRS} = 200, for instance, are
much less pronounced when using an enforcer).  It seems likely that a
programmer tasked with fixing this bug would find the behaviour
illustrated in the second chart easier to understand than that in the
first.

\begin{figure}
  \subfigure[][Without enforcer]{ \input{eval/artificial_bugs/special/indexed_toctou_vary_nr_ptrs_no_enforcer.tex} }
  \subfigure[][With enforcer]{ \input{eval/artificial_bugs/special/indexed_toctou_vary_nr_ptrs_enforcer.tex} }
  \caption{Reproduction times with and without an enforcer loaded, for
    varying values of \texttt{NR\_PTRS}.  Note that the two graphs use
    different scales, and that both use a log scale.  Each abscissae
    sampled 100 times.  Boxes show interquartile range and median with
    90\% confidence interval for quantiles in grey.  Cross and bars
    give arithmetic mean and 90\% confidence interval for mean.
    Confidence intervals computed by a bootstrap with 1000
    replicates.}
  \label{fig:eval:indexed_toctou:nr_ptrs}
\end{figure}

\subsection{Scalability with respect to happens-before graph complexity}

\begin{figure}
  {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{} \\
        & \multicolumn{2}{l}{analysis\_window \{} \\
        & & $\texttt{x}_1$ = global;\\
        & & $\texttt{x}_2$ = global;\\
        & & $\vdots$ \\
        & & $\texttt{x}_N$ = global;\\
        & & \hspace{-2mm}\begin{tabular}{ll}
          assert(!(&\hspace{-4mm}$\texttt{x}_1$ == 1 \&\& \\
          &\hspace{-4mm}$\texttt{x}_2$ == 2 \&\& \\
          &$\vdots$ \\
          &\hspace{-4mm}$\texttt{x}_N$ == N)); \\
        \end{tabular}\\
        & \multicolumn{2}{l}{\}} \\
        \multicolumn{3}{l}{\}} \\
      \end{tabular}
    }
  }
  {\hfill}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \\
        \\
        \multicolumn{3}{l}{while (1) \{} \\
        & \multicolumn{2}{l}{analysis\_window \{} \\
        & & global = 1;\\
        & & global = 2;\\
        & & $\vdots$ \\
        & & global = N;\\
        & \multicolumn{2}{l}{\}} \\
        \multicolumn{3}{l}{\}} \\
        \\
        \\
      \end{tabular}
    }
  }
  {\hfill}
  \caption{The \bugname{complex\_hb}$_{N}$ test.}
  \label{fig:eval:why:complex_hb}
\end{figure}

This section explores {\technique}'s behaviour as the complexity of
the happens-before graph increases.  The test program used is shown in
\autoref{fig:eval:why:complex_hb}.  The two threads each consist of a
series of $N$ memory accesses, arranged such that the program crashes
if it alternates accesses between the two threads.  The happens-before
graph for this bug therefore contains $2N$ edges.

\begin{figure}
  \todo{Need to re-run these experiments, and re-plot graph.}
  \centerline{
    \input{eval/complex_hb/complex_hb_build_summaries}
  }
  \caption{Time taken to analyse the \bugname{complex\_hb}$_N$ test,
    for varying values of $N$.  Error bars on samples give plus or
    minus one sample standard deviation.  All abscissae were sampled
    ten times.  The solid line shows a least-squares exponential
    regression over $N < 20$ extrapolated to the full range of $N$;
    the dashed one shows a quartic regression over the same data.
    Grey regions give (point-wise) 90\% confidence intervals for the
    regression lines, computed using a 1,000 replicate bootstrap.}
  \label{fig:eval:complex_hb:analysis_time}
\end{figure}

\autoref{fig:eval:complex_hb:analysis_time} shows how the time taken
and memory used when generating \glspl{verificationcondition} varies
with $N$.  Both quantities clearly increase more than linearly with
$N$, with the memory on the test system exhausted for $N \geq 40$.
This limits the complexity of bugs which can be analysed with
{\technique}, although 40 edges is sufficient to represent the vast
majority of bugs encountered in real software\needCite{}.

\todo{Not sure this adds all that much.} It is perhaps interesting to
note that, although very rapid, the rate of increase is less than
exponential.  The solid line in the figure shows the least-squares
exponential regression on the data for $N<20$, extrapolated over the
complete range of $N$.  It significantly over-estimates the rate at
which the time taken increases.  The dotted line, by contrast, shows
the extrapolation of a fourth-order polynomial regression over the
same data, which gives a much closer fit in the extrapolated region.
This is because, in this particular test, the {\StateMachine}
simplification step is able to make quite dramatic simplifications to
the structure of the {\StateMachines}, and so there is no need for the
symbolic execution steps to consider all $2^N$ interleavings of the
memory accesses.

\todo{Do the same thing for memory usage.}

\todo{Also put in some numbers on how effective the enforcers are.
  The answer is that they're very effective, and how effective they
  are is basically independent of $N$, but I should probably say that.}

\subsection{Scalability with respect to memory access complexity}

\begin{figure}
  {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
    \begin{tabular}{ll}
      \multicolumn{2}{l}{$\texttt{storeIdx}_1$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{storeIdx}_2$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{\vdots}\\
      \multicolumn{2}{l}{$\texttt{storeIdx}_S$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{loadIdx}_1$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{loadIdx}_2$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{\vdots}\\
      \multicolumn{2}{l}{$\texttt{loadIdx}_L$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{analysis\_window \{}\\
      &$\texttt{slots}[\texttt{storeIdx}_1]$ = $\texttt{storeIdx}_1$;\\
      &$\texttt{slots}[\texttt{storeIdx}_2]$ = $\texttt{storeIdx}_2$;\\
      &\vdots\\
      &$\texttt{slots}[\texttt{storeIdx}_S]$ = $\texttt{storeIdx}_S$;\\
      &\hspace{-2.3mm}\begin{tabular}{lll}
         assert((&\hspace{-4mm}$\texttt{slots}[\texttt{loadIdx}_1]$ &\hspace{-3mm} + \\
         &\hspace{-4mm}$\texttt{slots}[\texttt{loadIdx}_2]$ &\hspace{-3mm} + \\
         & \vdots \\
         &\hspace{-4mm}$\texttt{slots}[\texttt{loadIdx}_L]$ &\hspace{-3mm}) != $L{\times}\texttt{NR\_PTRS} + 1$);\\
       \end{tabular}\\
      \multicolumn{2}{l}{\}}
    \end{tabular}
    }
  }
  {\hfill}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{idx = random() \% NR\_PTRS;}\\
        \multicolumn{2}{l}{analysis\_window \{} \\
        & slots[idx] = idx; \\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  {\hfill}
  \caption{The \bugname{complex\_alias}$_{L,S}$ test.
    \texttt{NR\_PTRS} is the constant 100.  \todo{Kind of ugly.}}
  \label{fig:eval:why:complex_aliasing}
\end{figure}

\begin{figure}
  \biggraph{eval/complex_alias/complex_alias}
  \caption{Memory and time used to analyse the
    \bugname{complex\_alias}$_{L,S}$ test.}
  \label{fig:eval:why:complex_aliasing:result1}
\end{figure}

The previous section showed that {\technique} scales well to
complicated happens-before graphs.  It does not, however, scale nearly
as well to complicated memory accessing patterns.  Consider, for
instance, the program shown in
\autoref{fig:eval:why:complex_aliasing}.  The \gls{crashingthread}
\gls{analysiswindow} is structured as $S$ stores and $L$ loads, with
all of the addresses unpredictable, followed by a final test on the
sum of the loaded values.  The program is structured such that this
test always fails, and so the program never crashes, but in order to
prove that {\technique} must consider every possible way of evaluating
the condition.  This program is close to {\technique}'s worst case,
for this number of memory-accessing instructions, as there is no way
to constrain the aliasing problem: the {\StateMachine}-level analysis
provides no useful information because the address computation happens
outside of the \gls{analysiswindow}; the dynamic analysis provides
none because any instruction can alias with any other; and the
\gls{programmodel} static analysis provides none because the address
computation involves the \texttt{random} function.  Most real bugs
would not suffer from all of these difficulties at the same time.

\autoref{fig:eval:why:complex_aliasing:result1} shows the cost of
analysing this program, in terms of time taken and memory consumed, as
functions of $L$ and $S$.  Both functions clearly increase rapidly
with both $L$ and $S$, with the analysis quickly becoming intractable
for even quite modest numbers of memory accessing instructions.
\todo{Say more once I actually have some data to work with.}

\begin{figure}
  {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
    \begin{tabular}{ll}
      \multicolumn{2}{l}{$\texttt{storeIdx}_1$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{storeIdx}_2$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{\vdots}\\
      \multicolumn{2}{l}{$\texttt{storeIdx}_S$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{loadIdx}_1$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{$\texttt{loadIdx}_2$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{\vdots}\\
      \multicolumn{2}{l}{$\texttt{loadIdx}_L$ = random() \% NR\_PTRS;}\\
      \multicolumn{2}{l}{analysis\_window \{}\\
      &$\texttt{slots}[\texttt{storeIdx}_1]$ = 1;\\
      &$\texttt{slots}[\texttt{storeIdx}_2]$ = 2;\\
      &\vdots\\
      &$\texttt{slots}[\texttt{storeIdx}_S]$ = $S$;\\
      &\hspace{-2.3mm}\begin{tabular}{lll}
         assert(&\hspace{-4mm}$\texttt{slots}[\texttt{loadIdx}_1]$ &\hspace{-3mm} != $S+1$ \&\& \\
         &\hspace{-4mm}$\texttt{slots}[\texttt{loadIdx}_2]$ &\hspace{-3mm} != $S+1$ \&\& \\
         & \vdots \\
         &\hspace{-4mm}$\texttt{slots}[\texttt{loadIdx}_L]$ &\hspace{-3mm} != $S+1$);\\
       \end{tabular}\\
      \multicolumn{2}{l}{\}}
    \end{tabular}
    }
  }
  {\hfill}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{idx = random() \% NR\_PTRS;}\\
        \multicolumn{2}{l}{analysis\_window \{} \\
        & slots[idx] = 0; \\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  {\hfill}
  \caption{The \bugname{complex\_alias\_easy}$_{L,S}$ test.
    \texttt{NR\_PTRS} is the constant 100.  \todo{Kind of ugly.}}
  \label{fig:eval:why:complex_aliasing:easy}
\end{figure}

\begin{figure}
  \biggraph{eval/complex_alias/complex_alias_easy}
  \caption{Memory and time used to analyse the
    \bugname{complex\_alias\_easy}$_{L,S}$ test.}
  \label{fig:eval:why:complex_aliasing:result2}
\end{figure}

\autoref{fig:eval:why:complex_aliasing:easy} shows a variant of this
test program.  The \gls{analysiswindow} contains the same number of
loads and stores, but the final test now consists of a series of
independent tests on the loaded values.  The cost of analysing this
bug is shown in \autoref{fig:eval:why:complex_aliasing:result2}.  Note
the change in scale: this program can be analysed far more quickly,
and with far less memory, than the previous one.  The reason is that
{\technique}'s symbolic execution engine resolves aliasing problems
lazily, as discussed in \autoref{sect:derive:symbolic_execute}.  There
is therefore no need to decide precisely which of the $S$ stores
provided the value loaded by a particular load until that value is
used\editorial{ugg}.  In this case, the use of the value will be a
test of the form $l < S+1$, and all of the stores will be of values
less than or equal to $S$, and so {\technique} will be able to show
that the test evaluates to \true without ever determining the precise
correspondence between loads and stores.  This reduces the cost from
$O((1+S)^L)$ to $O(SL)$, which is sufficient that memory aliasing
stops being the limiting factor in the analysis.

\todo{Laziness also allows a kind of factorisation of the final
  condition if there are subsets of the loaded values which don't
  communicate, which can reduce cost from $S^L$ to
  $n(\frac{S}{n})^{\frac{L}{n}}$, but I don't have a nice small test
  case for that one, and there are enough implementation-related
  caveats that I don't really want to talk about it.}

\subsection{Scalability with respect to \gls{alpha}}

\todo{Figures in this section all need to be redone.  Also want to say
  something about memory usage at high alpha, but I'm not sure what
  that something is going to be.}

\begin{figure}
  \biggraph{eval/alpha/unopt/bpm.tex}
  \caption{Effect of the \gls{alpha} parameter on the time taken to
    perform per-\gls{crashingthread} analysis.  A boundary correction
    was applied to the kernel density estimator near the 300 second
    timeout to avoid unnecessarily distorting the estimated PDF.}
  \label{fig:perf:alpha:bpm:unopt}
\end{figure}

\begin{figure}
  \biggraph{eval/alpha/unopt/crashing_size.tex}
  \caption{Effect of the \gls{alpha} parameter on the size of various
    structures generated by {\technique} during the analysis.}
  \label{fig:eval:time_breakdown:crashing_size}
\end{figure}

\begin{figure}
  \biggraph{eval/alpha/unopt/gsc.tex}
  \caption{Effect of the $\alpha$ parameter on the time taken to
    perform per-\gls{interferingthread} analysis. A boundary
    correction was applied to the kernel density estimator near the
    300 second timeout to avoid unnecessarily distorting the estimated
    PDF.}
  \label{fig:perf:alpha:gsc:unopt}
\end{figure}

Previous sections have investigated scalability with respect to some
measures of the complexity of the \gls{analysiswindow} using
artificial test programs.  This section investigates scalability with
respect to the raw size of the \gls{analysiswindow} using real
programs.  To do so, I selected 1000 instructions at random from MySQL
and analysed each at varying values of \gls{alpha}, recording the time
taken by the various analysis phases.  The results are shown in
figures \ref{fig:perf:alpha:bpm:unopt},
\ref{fig:eval:time_breakdown:crashing_size}, and
\ref{fig:perf:alpha:gsc:unopt}.    

\todo{Analysis?}

The configuration used for this experiment was slightly different from
that used for most of the other experiments:
\begin{itemize}
\item The timeout was increased from sixty seconds to five minutes, as
  that made the data easier to interpret for high values of
  \gls{alpha}.
\item The machine used to run this experiment was a 1.9GHz Opteron
  6168 with 16GiB of memory.
\item {\Implementation} was initially configured to analyse ten
  potentially-crashing instructions in parallel, whereas the previous
  experiments analysed one at a time.  Any instructions which suffered
  an out-of-memory error when run in parallel were re-run in series
  after the experiment completed; an operation was only treated as
  running out of memory if it failed on this final run.  Timed-out
  phases of the analysis were not re-run.  The distributions obtained
  without performing any re-runs are shown on the chart with dashed
  grey lines.
\end{itemize}
The results of this experiment are therefore not directly comparable
with the results of the other experiments.

\subsection{Effect of {\StateMachine} simplification on analysis time}

\todo{I've not given many details of how the simplifiers work, so this
  is going to be a little tricky to interpret, but it's also quite
  important.}

\subsection{Effect of the \gls{w-isolation} assumption}

\section{Leftover stuff}

\subsection{A bug which lacks the W isolation property (w\_isolation)}

\begin{figure}
  \centerline{
    {\hfill}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &s = malloc();\\
        &s->v = 7;\\
        &global\_ptr = s;\\
        &assert(s->v == 7);\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
    {\hfill}
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{p = global\_ptr;}\\
        &\multicolumn{2}{l}{if (p != NULL) \{}\\
        &&p->v = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
    {\hfill}
  }
  \caption{Racing threads for the w\_isolation test.  Garbage
    collection-related code is not shown.}
  \label{fig:w_isolation}
\end{figure}

This test illustrates a bug which lacks the W isolation property (see
\autoref{sect:derive:w_isolation}).  In this test, the crashing thread
allocates a fresh data structure, initialises the field \texttt{v},
publishes it via a global pointer, and then asserts that \texttt{v} is
unchanged.  Meanwhile, the interfering thread loops checking for any
published structures and, if it finds one, changing the value of
\texttt{v}.  This bug can only be reproduced when the interfering
thread is able to access the structure which was stored by the
crashing one, and hence lacks the W isolation property.
{\Implementation} is able to correctly analyse this bug, producing an
enforcer and a fix, when configured to not assume the W isolation
property, but cannot otherwise.

\subsection{Summary tables}

I now give a quantitative characterisation of {\technique}'s
performance on these test programs.  This includes CDFs of the time
taken to reproduce the various bugs, in
\autoref{fig:eval:summary_cdfs}, the time taken to perform the various
analysis steps, in \autoref{table:eval:summary_analysis_times}, and
the performance effects of the fixes, in
\autoref{table:eval:fix_overheads}.  The important points here are:

\begin{itemize}
\item {\Technique}'s enforcers make these bugs reproduce more quickly,
  often much more quickly.
\item The various analysis passes are generally very fast, usually
  taking a few hundred milliseconds.  The most time-consuming program
  to analyse is complex\_hb\_17, which takes 1.711 seconds to analyse.
\item The fixes which {\technique} generates for these bugs have
  tolerable overhead, generally from a few tens of percent to a small
  factor.  More realistic tests would probably show lower overheads,
  as the cost of the fix could be more easily amortised over a larger
  program.
\end{itemize}

In other words, {\technique} is an effective and efficient technique
for both finding and fixing bugs in small programs.  The next few
sections will explore how well it scales up to more complex programs.

The performance table shows a measure of the performance impact of the
various fixes generated by {\technique}.  Performance here is measured
in the number of loop iterations completed per second in the two
threads, excluding any threads for which the test harness deliberately
inserts delays.  Each test program was run 20 times for ten seconds,
with any crashing runs repeated.  I report the number of iterations
per second with and without a fix, and the ratio of those two
quantities.  The table reports each quantity as $[a; b; c]$, where $b$
is an estimate of the quantity of interest (the mean number of
iterations per second for the raw performance numbers, or the ratio of
those two means for the ratio column), and $[a;b]$ forms a 95\%
confidence interval for that quantity.  The confidence intervals for
the raw performance measures were calculated using the central limit
theorem (and simply assuming that 20 samples are sufficient for that
to be valid).  The confidence interval for the ratio was calculated
using Luxburg's approximation\needCite{}.  Ratios greater than one
indicate that the program ran more slowly with the fix applied; those
less than one indicate that it ran more quickly.  In the case of the
multi\_threads test, the performance metric given is summed across all
threads.


\section{Performance exploration}
\label{sect:eval:time_details}

\subsection{Finding bugs}

\subsubsection{Effects of {\StateMachine} simplification}

For comparison, I also performed this experiment with the
{\StateMachine} simplifiers disabled.  The results are shown in
figures~\ref{fig:eval:time_breakdown:crashing_no_simple}
and~\ref{fig:eval:time_breakdown:interfering_no_simple}.  These are
roughly as expected: the entire process is much slower, with the
average time per potentially-crashing instruction rising from 3.4 to
22.5 seconds and the time per interfering \gls{cfg} rising from 0.82
to 4.2 seconds, and the failure rate increases from 0.4\% to 3.2\%.

Also, although not shown on the charts, the number of interfering
\glspl{cfg} increased from 2251 to 4154.  These correspond to cases
where the simplifiers were able to show that a particular memory load
in the crashing {\StateMachine} could not possible influence whether
the {\StateMachine} ultimately predicts a crash, so that the
configuration with the simplifiers enabled did not need to consider
races with that location.  The number of \glspl{verificationcondition}
needing run-time validation, by contrast, fell when simplification was
disabled.  Disabling simplification never causes additional
\glspl{verificationcondition} to be found, as the simplifiers are all
sound.  The \glspl{verificationcondition} which were lost correspond
to cases where the analysis succeeded with the simplifiers enabled and
failed with them disabled.

\begin{figure}
  \biggraph{eval/bubble_charts/bubble5.tex}
  \caption{Time taken by the per-\gls{crashingthread} phases with
    {\StateMachine} simplification disabled.  Note the change in
    scale.}
  \label{fig:eval:time_breakdown:crashing_no_simple}
\end{figure}

\begin{figure}
  \biggraph{eval/bubble_charts/bubble6.tex}
  \caption{Time taken by the per-interfering \gls{cfg} phases with
    {\StateMachine} simplification disabled.  Note the change in
    scale.}
  \label{fig:eval:time_breakdown:interfering_no_simple}
\end{figure}

\subsubsection{Effect of the W isolation assumption}
\label{sect:eval:w_isolation}

\todo{Might need a slightly bigger data set here; the effect is kind
  of marginal with this one.}

I now investigate the effect of the \gls{w-isolation} on the analysis,
in terms of the time taken and the set of
\glspl{verificationcondition} generated.  I took the same 1000
instructions from the optimised build of MySQL and analysed them with
and without the \gls{w-isolation} assumption, with \gls{alpha} set to
20 in both cases.  The results are shown in
\autoref{table:eval:w-isolation}.  As expected, the \gls{w-isolation}
assumption modestly decreases both the number of
\glspl{verificationcondition} which must be checked by run-time
\glspl{bugenforcer} and the time taken to generate them.

\begin{table}
  \begin{tabular}{|l|l|l|}
    \hline
    & \multicolumn{2}{c|}{\gls{w-isolation}} \\
    \hline
                                                                & Enabled & Disabled \\
    \hline
    Potentially-crashing instructions examined                  & 1000    & 1000 \\
    Instructions dismissed without generating a {\StateMachine} & $20 \pm_b 1$\% & $20 \pm_b 1$\% \\
    Timeouts building crashing {\StateMachines}                 & $0.1 \pm_b 0.1$\% & $0.4 \pm_b 0.2$\% \\
    Out of memory building crashing {\StateMachines}            & 0                & 0 \\
    Time to build crashing {\StateMachine} *                    & $0.14 \pm_\mu 0.02$s & $0.16 \pm_\mu 0.03$s \\
    \Glspl{crashingthread} generating no interfering \glspl{cfg}& $48 \pm_b 1$\% & $43 \pm 1$\%\\

    Time to process each interfering \gls{cfg}                  & $0.24 \pm_\mu 0.04$s & $0.37 \pm_\mu 0.04$s \\
    Interfering \glspl{cfg} running out of memory               & $0.09 \pm_b 0.06$\% & $0.11 \pm_b 0.06$\% \\
    Interfering \glspl{cfg} running out of time                 & $0.2 \pm_b 0.1$\% & $0.6 \pm_b 0.1$\% \\

    Interfering \glspl{cfg} generating \glspl{verificationcondition}   & $12.7 \pm_b 0.7$\% & $12.6 \pm_b 0.6$\% \\
    \Glspl{verificationcondition} per potentially-crashing instruction & $0.29 \pm_{10000} 0.04$ & $0.34 \pm_{10000} 0.05$\\
    \hline
  \end{tabular}
  \caption{Effects of the \gls{w-isolation} assumption. *: excluding
    timeouts and instructions dismissed without building a
    {\StateMachine}.}
  \label{table:eval:w-isolation}
\end{table}

The table includes three different types of error indicator:

\begin{itemize}
\item $x \pm_\mu y$ shows the mean and standard deviation of mean of
  some quantity, as usual.
\item $x \pm_b y$\% is used to show the frequency of some event.  $x$
  is an estimate of the frequency itself, calculated in the obvious
  way.  $y$ is an estimate of $x$'s standard deviation, calculated as
  $\sqrt{x(1-x)/n}$, where $n$ is the number of samples.  This is
  the standard deviation which would be observed in $x$ if the
  frequency really were $x$ and if the observations were all
  identically independently distributed.
\item $x \pm_{10000} y$ is used to show the number of
  \glspl{verificationcondition} generated for each
  potentially-crashing instruction.  $x$ is a simple average over all
  of the potentially-crashing instructions.  For this error
  indication, I took the set of 1000 results generated by the
  experiment and resampled them with replacement to produce a further
  10,000 sample sets each of size 1000.  I then used these 10,000
  sample sets to compute 10,000 estimates of the ratio.  I report the
  population standard deviation of those estimates as $y$.  I also
  manually inspected the generated estimates to confirm that there
  were not an excessive number of outliers in either sense.
\end{itemize}

\subsubsection{Effects of the $\alpha$ parameter}
\label{sect:eval:alpha}

The overall shape of this chart is roughly what one would expect: the
time taken to analyse a particular potentially-crashing instruction
rises roughly exponentially with the value of \gls{alpha}, and the
proportion of timeouts rises rapidly once \gls{alpha} exceeds a
certain value, which in this case is about 50.

Note that the distributions shown in all three charts exclude any
\gls{crashingthread} which suffered a timeout before generating the
relevant object (so, for instance, the ``Simplified states'' chart
excludes any \glspl{crashingthread} which did not complete
simplification).  This will have caused some selection bias, as the
larger \glspl{crashingthread} are most likely to suffer a timeout.
This will be particularly important for the high quantiles of the high
\gls{alpha} distributions.

Once the \gls{crashingthread} {\StateMachine} has been generated, the
next step is to derive \glspl{cfg} for the \glspl{interferingthread}.
The response of this process to the $\alpha$ parameter is shown in
\autoref{fig:perf:alpha:gsc:unopt}.  In this chart, the
pre-dismissed box includes anything which was dismissed by the
previous stage or where the crashing {\StateMachine} was sufficient to
show that no bug of the desired class could exist without reference to
the interfering \glspl{cfg}.  The pre-failed box gives the number of
times that no interfering \glspl{cfg} could be derived due to a
failure in a previous phase of the analysis.  As before, increasing
$\alpha$ leads to an increase in the both the time taken to perform
the analysis and the number of timeouts.  This behaviour needs little
explanation.

The lower part of the chart, showing the number of interfering
\glspl{cfg} per \gls{crashingthread}, is more surprising.  Many of
\glspl{crashingthread} do not produce any interfering \gls{cfg} at
all.  This indicates that the potential crash could be dismissed based
on contents of the crashing {\StateMachine} and the dynamic aliasing
model, with no need for any further analysis.  There are several ways
that this could happen:

\begin{itemize}
\item It might be that none of the \gls{alpha} instructions leading up
  to the crashing instruction loaded from thread-shared memory, in
  which case they trivially cannot suffer a concurrency bug of this
  form.
\item Similarly, the simplifiers can sometimes show that a the value
  of a load from shared memory cannot possibly influence whether the
  thread will crash, and if so the \gls{crashingthread} can be
  dismissed without needing to derive the interfering \glspl{cfg}.
\item If the \gls{crashingthread} makes only a single load from memory
  then it is safe to dismiss any interfering \glspl{cfg} which make
  only a single store, as in that case there are no interleavings
  which are not equivalent to running the two threads atomically.
\end{itemize}

All of these special cases are clearly more likely when \gls{alpha} is
small, and indeed we observe that many of the small \gls{alpha} tests
do not generate any interfering \glspl{cfg}.  This does not, however,
explain why the large \gls{alpha} tests also show a large number of
\glspl{crashingthread} which do not generate any interfering
\glspl{cfg}.  The explanation here is selection bias: only simple
\glspl{crashingthread} avoid timing out before generating the
interfering \glspl{cfg}, and these tend to generate fewer interfering
\glspl{cfg}.

The next step of the analysis is to take these pairs of crashing
{\StateMachines} and interfering \glspl{cfg} and convert them into
\glspl{verificationcondition}.  This is shown in
\autoref{fig:perf:alpha:gvc:unopt}.  Note that the denominator for
the various proportions has changed: previously, it was the total
number of potentially-crashing instructions, whereas now it is the
number of pairs generated by the previous analysis stages.

The effect of $\alpha$ on the number of \glspl{verificationcondition}
generated is easiest to understand if it is expressed as the fraction
of \glspl{cfg} which generate a
\gls{verificationcondition}\footnote{Recall that there is at most one
  \gls{verificationcondition} for each interfering \gls{cfg}.}  This
initially increases with $\alpha$ and then begins to fall off again
when $\alpha$ increases further.  Increasing $\alpha$ allows
{\technique} to consider interleaving the program's threads over a
larger window of instructions, and so to consider more complicated
concurrency behaviour, and this leads to a modest increase in the
fraction of interfering \glspl{cfg} which might cause undesirable
behaviour in the \gls{crashingthread}, accounting for the initial
increase.  The subsequent decrease is, again, a selection effect, as
only threads with simple aliasing and concurrency behaviour will reach
this stage of the analysis.  The time taken by the analysis follows
broadly the same pattern, for essentially similar reasons.

\begin{figure}
  %\centerline{\biggraph{eval/alpha/unopt/gvc.tex}}
  \caption{Effect of the $\alpha$ parameter on the number of
    \glspl{verificationcondition} generated and the time taken to do
    so.  All timeouts in earlier stages of the analysis are excluded.
    The time taken includes building the interfering {\StateMachine},
    rederiving crashing {\StateMachine}, deriving \gls{ic-atomic}
    constraints, building the \gls{verificationcondition}, and the
    final satisfiability check.}
  \label{fig:perf:alpha:gvc:unopt}
\end{figure}

\begin{figure}
  \biggraph{eval/alpha/opt/bpm.tex}
  \caption{Effect of the $\alpha$ parameter on the time taken to build
    the \gls{crashingthread} {\StateMachines} for an optimised build
    of MySQL.}
  \label{fig:perf:alpha:bpm:opt}
\end{figure}

\begin{figure}
  \biggraph{eval/alpha/opt/gsc.tex}
  \caption{Effect of the $\alpha$ parameter on the number of
    \gls{interferingthread} \glspl{cfg} per \gls{crashingthread}
        {\StateMachine} for an optimised build of MySQL.  \todo{Urk}}
  \label{fig:perf:alpha:gsc:opt}
\end{figure}

\begin{figure}
  \biggraph{eval/alpha/opt/gvc.tex}
  \caption{Effect of the $\alpha$ parameter on the number of
    \glspl{verificationcondition} generated and the time taken to do
    so for an optimised build of MySQL.}
  \label{fig:perf:alpha:gvc:opt}
\end{figure}

The results with an optimised version of MySQL are broadly similar,
and are shown in figures~\ref{fig:perf:alpha:bpm:opt},
\ref{fig:perf:alpha:gsc:opt}, and \ref{fig:perf:alpha:gvc:opt}.  The
most obvious difference between the optimised and unoptimised graphs
is that the optimised program takes longer to analyse, and suffers a
higher timeout rate.  This is not particular surprising: the optimised
program fits more complicated behaviour into the \gls{analysiswindow}
and so takes longer to process.  \todo{Not sure there's much to say
  about that.}

\subsection{Building bug enforcers}

The next thing to look at is how long it takes to convert
\glspl{verificationcondition} to \glspl{bugenforcer}.  For this
experiment, I chose 5000 \glspl{verificationcondition} at random from
those generated by analysing the whole of MySQL, converted each one to
an enforcer, and timed how long each phase took; the results are shown
in \autoref{fig:eval:time_breakdown:convert_to_enforcer}\footnote{I
  also performed a similar experiment using the 297
  \glspl{verificationcondition} generated by the previous phase.  The
  results were broadly similar, but the smaller sample set meant that
  some low-probability outcomes were completely unrepresented in the
  results.}.

Several of the \glspl{verificationcondition} could not be converted to
\glspl{bugenforcer} due to running out of memory; 34 (0.7\%) while
slicing the \gls{verificationcondition} by the happens-before graph
and 4 (0.1\%) while placing the side conditions.  In addition, one
\gls{verificationcondition} reached the 60 second timeout during
slicing.  Re-running that one condition without a timeout showed that
it would eventually have run out of memory.  The system used to run
the test had 8GiB of memory.  To some extent, these out-of-memory
errors reflect a problem with my implementation {\implementation}
rather than the technique {\technique}: {\implementation} manages most
of the data structures involved using a garbage collector which only
runs in between the phases, which simplified the implementation but
will have significantly increased the system's memory consumption.  It
would therefore be reasonable to suppose that a more refined
implementation could usefully reduce the number of
\glspl{verificationcondition} which fail.  It is unlikely, however,
that it would be possible to eliminate all failures completely, as
both phases make heavy use of BDD reordering, which has an exponential
worst-case memory consumption.  The algorithm is designed such that
most of the time the BDDs are in nearly the correct order to begin
with, and so this worst case can usually be avoided, but the minority
of cases which do hit it will always require a very large amount of
memory.

The only other phase which requires a non-trivial amount of time is
the final one, compiling the enforcer.  This involves converting the
generated enforcer to C and passing it off to the system compiler (in
this case, gcc version 4.4.3) to convert it into an ELF library.  The
time taken by this phase is completely dominated by the time taken by
the external compiler.

\begin{figure}
  \biggraph{eval/bubble_charts/bubble4.tex}
  \caption{Time taken by the different phases involved in converting a
    \gls{verificationcondition} to a \gls{bugenforcer}.}
  \label{fig:eval:time_breakdown:convert_to_enforcer}
\end{figure}

\subsection{Generating fixes}
\label{sect:eval:genfix}

The final component of {\technique} which I investigate here is fix
generation.  The performance behaviour here is far simpler than for
the other phases.  It is summarised in
\autoref{tab:eval:gen_fix_perf}.  The time taken is clearly completely
dominated by the time taken by the system compiler, and is perfectly
reasonable for these patches.

\begin{table}
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    Phase & Mean & $5^{th}$\% & $95^{th}$\% & Population standard deviation \\
    \hline
    Find critical sections & $0.50 \pm_\mu 0.02$ & 0.1 & 1.6 & 1.1 \\
    Identify patch points & $0.805 \pm_\mu 0.008$ & 0.3 & 1.4 & 0.6 \\
    Partial evaluation & $0.75 \pm_\mu 0.06$ & 0.2 & 1.5 & 4.4 \\
    System compiler & $105.1 \pm_\mu 0.2$ & 85 & 128 & 14 \\
    \hline
    Total & $107.2 \pm_\mu 0.2$ & 87 & 131 & 15 \\
    \hline
  \end{tabular}
  \caption{Time taken to convert 5000 \gls{verificationcondition}
    generated from MySQL to fixes.  All times in milliseconds.}
  \label{tab:eval:gen_fix_perf}
\end{table}

\autoref{tab:eval:gen_fix_perf:props} gives some statistics related to
the complexity of the patches generated.  The properties examined are:

\begin{itemize}
\item The number of program instructions in the protected region.
\item The number of lock and unlock operations present in the
  generated machine code fragment.
\item The size the generated machine code fragment, in terms of both
  the number of instructions and the number of bytes.  This is further
  broken down into the number of instructions needed to acquire the
  lock, the number to release it, and the number in other forms of
  patching overhead.
\item The number of instructions in the original program which are
  patched to gain control of the program.
\item The number of instructions in the $\mathit{Cont}$ set (see
  \autoref{sect:enforce:gain_control}).
\item The number of relocations.  This is the number of places in the
  generated machine code which must be modified at run-time when the
  patch is loaded, to account for the patch being loaded at an address
  which is unknown at compile time.
\end{itemize}

The table shows the mean and central limit theorem standard deviation
of the mean, the $5^{th}$ and $95^{th}$ percentiles, and the
population standard deviation ($\sigma$) of the parameter.  As can be
seen, the generated patches usually consist of a few dozen
instructions or a few hundred bytes of machine code.  The size is
therefore perfectly manageable.  Note that most of the distributions
shown in the table are non-Gaussian and so, for instance, the $\sigma$
column should be treated with caution.

\begin{table}
  \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
                           & \multicolumn{4}{c|}{Distribution summary} & \multicolumn{3}{c|}{Effect on time taken} \\
    \hline
    Property of fix        & Mean & $5^{th}$\% & $95^{th}$\% & $\sigma$ & $\beta$, ms & $\gamma$ & R\\
    \hline
    Protected instructions & $17.9 \pm_\mu 0.4$  & 3   & 40  & 30  & 0.2  & 0.04 & 0.28 \\
    Lock operations        & $2.58 \pm_\mu 0.04$ & 1   & 4   & 2.7 & 1.6  & 0.009& 0.13 \\
    Unlock operations      & $2.88 \pm_\mu 0.02$ & 1   & 5   & 1.4 & 0.12 & 0.00001& 0.005 \\
    Patch instructions     & $70 \pm_\mu 1$      & 27  & 127 & 72  & 0.06 & 0.009& 0.13\\
    \hspace{5mm}Acquiring lock&$15.5 \pm_\mu 0.2$& 6   & 24  & 16  &$\ast$&$\ast$& $\ast$\\
    \hspace{5mm}Releasing lock&$17.3 \pm_\mu 0.1$& 6   & 30  & 8   &$\ast$&$\ast$& $\ast$\\
    \hspace{5mm}Overhead   & $19.1 \pm_\mu 0.5$  & 0   & 50  & 32  &$\ast$&$\ast$& $\ast$\\
    Patch bytes            & $326 \pm_\mu 5$     & 126 & 594 & 346 & 0.01 & 0.009& 0.13\\
    Patch points           & $2.26 \pm_\mu 0.01$ & 2   & 3   & 0.88& 0.8  & 0.0008& 0.04\\
    $\mathit{Cont}$        &$0.034 \pm_\mu 0.003$& 0   & 0   & 0.2 &$\ast$&$\ast$& $\ast$\\
    Relocations            & $11.0 \pm_\mu 0.1$  & 6   & 18  & 8.0 & 0.5  & 0.03 & 0.24\\
    \hline
  \end{tabular}
  \caption{Summaries of some gross properties of the generated fixes,
    and their effects on the time taken by all phases of fix
    generation.  See text for details. $\ast$: Not meaningful.}
  \label{tab:eval:gen_fix_perf:props}
\end{table}

The table also shows the dependence of the time taken to generate the
patch on some of these measurements.  For these calculations, I
performed a simple linear regression of the total time taken on the
parameter.  The table shows $\beta$, the gradient of the resulting
line, $\gamma$, the proportion of the standard deviation of time which
can be explained by that linear regression, and R, the Pearson
product-moment correlation coefficient.  As can be seen, the number of
protected instructions and the number of relocations are modestly
positively correlated with the total time taken, while the other
parameters show weaker correlations, or none at all.  This gives an
indication of how the patch process would scale to more complex
critical sections.  These results should be treated with some caution,
though: none of the correlations are very strong, and many of the
error distributions are non-Gaussian, the sample size is only modest,
and there are strong correlations between the different properties of
the fix in addition to between the properties and the time taken.

\section{Dynamic analysis}
\label{sect:eval:dynamic_analysis}

\subsection{Effects on analysis performance and correctness}

\todo{This is another one which is lacking error bars.}

Similarly to the investigation of the effects of the \gls{w-isolation}
assumption in \autoref{sect:eval:w_isolation}, I now investigate the
effects of the dynamic aliasing analysis on the generation of
\glspl{verificationcondition}\editorial{gurk}.  It is difficult to
completely avoid using the information from the dynamic analysis while
generating \glspl{verificationcondition}, as it is needed to generate
the \gls{interferingthread} \glspl{cfg}, but it is possible to disable
the parts of the {\StateMachine} simplifiers and the symbolic
execution engine which use it, and that is what I did in this test.
The results are shown in \autoref{table:eval:effect_of_dyn}.  Note
that the aliasing enabled column of this table was produced from the
same experiment as produced the \gls{w-isolation} enabled column in
\autoref{table:eval:w-isolation}.

The results are shown in \autoref{table:eval:effect_of_dyn}, using the
same error indicators as \autoref{table:eval:w-isolation}.  The main
conclusion to draw from this table is that discarding the information
from the dynamic analysis makes {\technique} perform worse at every
step: the time to derive C-atomic and the interfering \glspl{cfg}
increases, the number of \glspl{cfg} increases, the time to process
each \gls{cfg} increases, and the number of
\glspl{verificationcondition} requiring run-timing checking increases.
These results strongly suggest that the dynamic analysis is absolutely
necessary for {\technique} to be useful at all.

\begin{table}
  \begin{tabular}{|l|l|l|}
    \hline
    & \multicolumn{2}{c|}{Dynamic aliasing information} \\
    \hline
                                                                & Enabled & Disabled \\
    \hline
    Potentially-crashing instructions examined                  & 1000    & 1000 \\
    Instructions dismissed without generating a {\StateMachine} & $20 \pm_b 1$\% & $20 \pm_b 1$\% \\

    Time to build crashing {\StateMachine} *                    & $0.14 \pm_\mu 0.02$s & $0.31 \pm_\mu 0.03$s \\
    Timeouts building crashing {\StateMachines}                 & $0.1 \pm_b 0.1$\% & $0.1 \pm_b 0.1$\% \\
    Out of memory building crashing {\StateMachines}            & 0                & 0 \\

    Time to build C-atomic                                      & $0.004 \pm_\mu 0.001s$ & $0.3 \pm_\mu 0.1s$ \\
    Timeouts building C-atomic                                  & 0                   & $0.4 \pm_b 0.2$\% \\
    Out of memory building C-atomic                             & 0                   & $0.1 \pm_b 0.1$\% \\

    \Glspl{crashingthread} generating no interfering \glspl{cfg}& $48 \pm_b 1$\% & $41 \pm_b 1$\%\\

    Time to build interfering \glspl{cfg}                       & $0.4 \pm_\mu 0.1$s & $1.6 \pm_\mu 0.3$s \\
    Interfering \glspl{cfg} per crashing \gls{cfg}              & $6.9 \pm_\mu 0.4$ & $7.5 \pm_\mu 0.4$ \\

    Time to process each interfering \gls{cfg}                  & $0.24 \pm_\mu 0.04$s & $3.5 \pm_\mu 0.2$ \\
    Interfering \glspl{cfg} running out of memory               & $0.09 \pm_b 0.06$\% & $4.8 \pm_b 0.4$\% \\
    Interfering \glspl{cfg} running out of time                 & $0.2 \pm_b 0.1$\% & $32.4 \pm_b 0.9$\% \\

    Interfering \glspl{cfg} generating \glspl{verificationcondition}   & $12.7 \pm_b 0.7$\% & $73 \pm_b 1$\% \\
    \Glspl{verificationcondition} per checked instruction & $0.29 \pm_{10000} 0.04$ & $1.3 \pm_{10000} 0.1$\\
    \hline
  \end{tabular}
  \caption{Effects of dynamic aliasing information on the bug-finding
    analysis. *: excluding timeouts and instructions dismissed without
    building a {\StateMachine}.}
  \label{table:eval:effect_of_dyn}
\end{table}

