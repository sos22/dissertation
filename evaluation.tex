\chapter{Evaluation}
\label{chapter:eval}

\section{Experiments I still need to run}

\begin{itemize}
\item Compare fair and unfair lock versions of multi\_threads
\item indexed\_toctou with side condition checking off.
\item The biassed toctou tests with delays oddly positioned.
\item The multi\_variable tests
\item The context tests.
\item Prod the broken\_publish test to get malloc to actually
  return a buffer which isn't full of zeroes.
\item DataCollider calibration experiments.  In fact, all the DC
  experiments.
\item Re-do labyrinth experiments and actually track the results.
\item Re-do Bayes with current version of tool.
\item Re-do pbzip2 and collect numbers
\item Re-do thunderbird and time the static analysis phase.
\item Re-do unoptimised mysql.  Time static and main analysis phases.
  Check number of candidates generated.  Run candidates.
\item Re-do optimised mysql.  Time static and main analysis phases.
  Check number of candidates generated.  Run candidates.
\end{itemize}

\section{Eval}

Previous chapters have described the basic {\technique} technique.  I
will now evaluate its effectiveness, and the performance of my
implementation {\implementation}.  This evaluation will consist of the
following parts:

\begin{itemize}
\item Section~\ref{sect:eval:artificial} explores the behaviour of the
  tool and the technique on a number of artificial bugs in simple test
  programs.  This includes a comparison to a
  DataCollider\needCite{}-like tool which I implemented for the
  purpose of the evaluation.
\item Section~\ref{sect:eval:semiartificial} investigates some
  slightly more realistic bugs.  These include a simplified version of
  a real bug in glibc\needCite{} and some bugs which I deliberately
  introduced into the STAMP benchmark suite\needCite{}.
\item Section~\ref{sect:eval:real} applies the tool to some large
  programs: pbzip2\needCite{}, MySQL\needCite{}, and
  Thunderbird\needCite{}.  I show that the analysis completes in a
  tolerable amount of time even for some very large programs,
  and demonstrate that it can both reproduce and fix a (small)
  number of real bugs.
\item Section~\ref{sect:eval:time_details} then investigates the
  tool's performance on large programs in slightly more detail,
  showing how the time taken breaks down across the various phases
  and the effects of some of {\implementation} optimisations.
\end{itemize}

\section{Artificial bugs}
\label{sect:eval:artificial}

I now present the results of running {\implementation} on a number of
artificial bugs, showing that it assists in both reproducing the bugs
and fixing them, and that the analysis phases complete very quickly.

\subsection{Simple time-of-check, time-of-use (TOCTOU) bug (simple\_toctou)}\footnote{This bug was previously discussed in
  Section~\ref{sect:derive:simple_toctou_example}.}
\label{sect:eval:simple_toctou}


\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&*global\_ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }\hfill %
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }\hfill
  \caption{The two sides of the simple\_toctou bug.}
  \label{fig:eval:simple_toctou}
\end{figure}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
    \begin{tabular}{rlll}
              & \multicolumn{3}{l}{crashing\_thread:} \\
      400694: & movq  & global\_ptr, &\!\!\!\%rax\\
      40069b: & testq & \%rax,       &\!\!\!\%rax \\
      40069e: & je    & \multicolumn{2}{l}{4006ad}\\
      4006a0: & movq  & global\_ptr, &\!\!\!\%rax\\
      4006a7: & movl  & \$0x5,       &\!\!\!(\%rax)\\
    \end{tabular}
    }
  }%
  \hspace{-5mm}\subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{rlll}
        & \multicolumn{3}{l}{interfering\_thread:} \\
        400816: & lea  & c(\%rsp), &\!\!\!\%rbp \\
        ...\\
        400884: & movq & \%rbp, &\!\!\!global\_ptr\\
        ...\\
        4008fb: & movq & \$0x0, &\!\!\!global\_ptr\\
      \end{tabular}
      }
    }
  \caption{Disassembly of the program fragments in Figure~\ref{fig:eval:simple_toctou}.}
  \label{fig:eval:simple_toctou:compiled}
\end{figure}

This is the simplest possible kind of concurrency bug: a
single-variable time-of-check, time-of-use race.  The two threads
involved are shown in Figure~\ref{fig:eval:simple_toctou}.  The intent
here is to model a very simple structure which is accessed frequently
but updated rarely.  The bug is, of course, that the interfering
thread might set \texttt{global\_ptr} to \texttt{NULL} in between the
two reads of it in the crashing thread, causing the crashing thread to
crash when it dereferences the pointer it loaded.
\texttt{STOP\_ANALYSIS()} is a marker which prevents {\technique}'s
analysis from exploring past that point when building the CFGs from
which {\StateMachines} are constructed.  It is used here to help keep
the crash summaries generated simple and easily explained;
{\implementation} is able to reproduce and fix the bug even without
those markers\editorial{Could last time I tried it, probably worth
  checking that again with the current version.}.

\subsubsection{Generating candidate bugs}

The first step in the analysis is to build the CFG for the
\backref{crashing thread} (see
Section~\ref{sect:derive:build_static_cfg}), which in this case is
straightforward; the result is shown in
Figure~\ref{fig:eval:simple_toctou:cfg}.  This can then be compiled to
produce the {\StateMachine} shown in
Figure~\ref{fig:eval:simple_toctou:sm}.

\begin{figure}
  \subfigure[][Control flow graph.  $\varnothing$ indicates that the
    thread has left the CFG.]{
    \begin{tikzpicture}
      \node (cfg1) [CfgInstr] {\texttt{400694}: cfg1};
      \node (cfg2) [CfgInstr,below=of cfg1] {\texttt{40069b}: cfg2};
      \node (cfg3) [CfgInstr,below=of cfg2] {\texttt{40069e}: cfg3};
      \node (cfg3b) [right = of cfg3] {$\varnothing$};
      \node (cfg4) [CfgInstr,below=of cfg3] {\texttt{4006a0}: cfg4};
      \node (cfg4b) [below = of cfg4] {$\varnothing$};
      \draw[->] (cfg1) -- (cfg2);
      \draw[->] (cfg2) -- (cfg3);
      \draw[->,ifTrue] (cfg3) -- (cfg3b);
      \draw[->,ifFalse] (cfg3) -- (cfg4);
      \draw[->] (cfg4) -- (cfg4b);
    \end{tikzpicture}
    \label{fig:eval:simple_toctou:cfg}
  }
  \subfigure[][{\STateMachine}]{
    \begin{tikzpicture}
      \node (l1) at (0,2) [stateSideEffect] {\stLoad{1}{\mathrm{global\_ptr}} @ cfg1 };
      \node (l2) [stateIf, below=of l1] {\stIf{\smTmp{1} = 0}};
      \node (l4) [stateSideEffect, below=of l2] {\stLoad{2}{\mathrm{global\_ptr}} @ cfg4 };
      \node (l3) [stateTerminal, right=of l4] {\stSurvive };
      \node (l5) [stateIf, below=of l4] {\stIf{\smBadPtr{\smTmp{2}}}};
      \node (l6) [stateTerminal, below=of l5] {\stCrash};
      \draw[->] (l1) -- (l2);
      \draw[->,ifTrue] (l2) -- (l3);
      \draw[->,ifFalse] (l2) -- (l4);
      \draw[->] (l4) -- (l5);
      \draw[->,ifFalse] (l5) -- (l3);
      \draw[->,ifTrue] (l5) -- (l6);
    \end{tikzpicture}
    \label{fig:eval:simple_toctou:sm}
  }
  \caption{\backref{CFG} and {\STateMachine} for the crashing thread
    in Figure~\ref{fig:eval:simple_toctou:compiled}.}
\end{figure}

The next step is to build the \backref{interfering stores} set (see
Section~\ref{sect:derive:write_side}).  The crashing {\StateMachine}
contains two loads, at \texttt{400694} and \texttt{4006a0}, and so the
analysis will ask\editorial{ick} the \backref{program model} what
stores might interact with them, and the \backref{program model} will
use the results of the \backref{dynamic alias analysis} (see
Section~\ref{sect:program_model:dynamic_alias}) to return the set
\{\texttt{400884}, \texttt{4008fb}\}.  The \texttt{STOP\_ANALYSIS()}
markers prevent these from being clustered together, and there are no
other \backref{communicating instructions}, and so there will be two
\backref{interfering thread} CFGs, shown in
Figures~\ref{fig:eval:simple_toctou:interfering_cfg1}
and~\ref{fig:eval:simple_toctou:interfering_cfg2}.

\begin{figure}
  \begin{tabular}{cc}
    \subfigure[][CFG for interfering store \texttt{400884}]{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\texttt{400884}: cfg5};
        \node (b) [below = of a] {$\varnothing$};
        \draw[->] (a) -- (b);
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_cfg1}
    } &
    \subfigure[][{\STateMachine} for interfering store \texttt{400884}, without static analysis]{
      \begin{tikzpicture}
        \node [stateSideEffect] {\stStore{\smReg{rbp}{2}}{\mathrm{global\_ptr}} @ cfg5};
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_sm1}
    } \\
    \subfigure[][CFG for interfering store \texttt{4008fb}]{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\texttt{4008fb}: cfg6};
        \node (b) [below = of a] {$\varnothing$};
        \draw[->] (a) -- (b);
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_cfg2}
    } &
    \subfigure[][{\STateMachine} for interfering store \texttt{4008fb}]{
      \begin{tikzpicture}
        \node [stateSideEffect] {\stStore{0}{\mathrm{global\_ptr}} @ cfg6};
      \end{tikzpicture}
      \label{fig:eval:simple_toctou:interfering_sm2}
    }
  \end{tabular}
  \caption{Interfering CFGs and {\StateMachines}.}
\end{figure}

Consider the interfering store at \texttt{400884} first.  Without the
static analysis phases, this would produce the {\StateMachine} shown
in Figure~\ref{fig:eval:simple_toctou:interfering_sm1}.  There is no
way for the program to crash due to interleaving this {\StateMachine}
with the crashing thread, as $\smReg{rbp}{2}$ is never a bad pointer,
but the {\StateMachines} do not contain enough information to show
that.  The analysis will produce the verification condition shown in
Figure~\ref{fig:eval:simple_toctou:inferred_assumption1}. The fixed
register and static aliasing analyses will both eliminate this
candidate bug, as either is capable of showing that $\smReg{rbp}{2}$
is a pointer into thread 2's stack frame, and hence that it is a valid
pointer.  The \backref{interfering store} at \texttt{4008fb}, on the
other hand, does represent a valid bug, as interleaving it with the
\backref{crashing thread} could lead to a crash, and it produces the
\backref{verification condition} shown in
Figure~\ref{fig:eval:simple_toctou:inferred_assumption2}.

\begin{figure}
  \begin{tabular}{lll}
    \backref{CI atomic}: & $\smLoad{\mathrm{global\_ptr}} = 0$ &\!\!\!$\vee\,\, \neg\smBadPtr{\smLoad{\mathrm{global\_ptr}}}$ \\
    \backref{IC atomic}: & $\smReg{rbp}{2} = 0$                &\!\!\!$\vee\,\, \neg\smBadPtr{\smReg{rbp}{2}}$\\
    \backref{Verification condition}: & \multicolumn{2}{l}{$\happensBefore{\mai{cfg1}{1}}{\mai{cfg5}{2}} \wedge \happensBefore{\mai{cfg5}{2}}{\mai{cfg4}{2}} \wedge \smBadPtr{\smReg{rbp}{2}}  \wedge$}\\
                                      & $\smLoad{\mathrm{global\_ptr}} \not= 0$\\
  \end{tabular}
  \caption{\backref{Inferred assumption} and \backref{verification
      condition} produced using the crashing {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:sm} and the interfering
    {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:interfering_sm1}.}
  \label{fig:eval:simple_toctou:inferred_assumption1}
\end{figure}

\begin{figure}
  \begin{tabular}{lll}
    \backref{CI atomic}: & $\smLoad{\mathrm{global\_ptr}} = 0$ &\!\!\!$\vee\,\, \neg\smBadPtr{\smLoad{\mathrm{global\_ptr}}}$ \\
    \backref{IC atomic}: & \true\\
    \backref{Verification condition}: & \multicolumn{2}{l}{$\happensBefore{\mai{cfg1}{1}}{\mai{cfg6}{2}} \wedge \happensBefore{\mai{cfg6}{2}}{\mai{cfg4}{2}} \wedge \smLoad{\mathrm{global\_ptr}} \not= 0$}\\
  \end{tabular}
  \caption{\backref{Inferred assumption} and \backref{verification
      condition} produced using the crashing {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:sm} and the interfering
    {\StateMachine} in
    Figure~\ref{fig:eval:simple_toctou:interfering_sm2}.}
  \label{fig:eval:simple_toctou:inferred_assumption2}
\end{figure}

The time taken to perform this analysis is quite modest: $0.52 \pm
0.04$ seconds for the static analysis phase and $0.18 \pm 0.01$
seconds for the {\StateMachine} analysis (mean and standard deviation
of mean for ten runs in both cases).  The dynamic analysis phase
achieved complete coverage essentially as soon as the program started.
This is a reasonable lower bound on the time which {\technique} will
take to process a very simple bug; any realistic program will take far
longer than this to process.

\subsubsection{Reproducing the bug}
This \backref{verification condition} can now be turned into a
\backref{crash enforcer}.  The only happens-before graph will be the
one shown in Figure~\ref{fig:eval:simple_toctou:hb_graph}, and it will
have the side condition that $\smLoad{\mathrm{global\_ptr}} \not= 0$.
This side condition can be evaluated completely at either
$\mai{cfg1}{1}$ or $\mai{cfg6}{2}$, and so the crash enforcement plan
will be as shown in Figure~\ref{fig:eval:simple_toctou:enforce_plan}.
In other words, whenever a program thread reaches \texttt{400694} and
global\_ptr is non-zero, the enforcer will wait for an interfering
thread to reach \texttt{4008fb}, after loading global\_ptr.  If one
does arrive, the enforcer will make the crashing thread wait for the
interfering thread to complete its store before proceeding to the load
at \texttt{4006a0}.  This will be sufficient to reproduce the bug.

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (l1) {\texttt{400694}: $\mai{cfg1}{1}$};
    \node[draw, below right = of l1] (l2) {\texttt{4008fb}: $\mai{cfg6}{2}$ };
    \node[draw, below left = of l2] (l3) {\texttt{4006a0}: $\mai{cfg4}{1}$ };
    \draw[->] (l1) -- (l3);
    \draw[->, happensBeforeEdge] (l1) -- (l2);
    \draw[->, happensBeforeEdge] (l2) -- (l3);
  \end{tikzpicture}
  \caption{Happens-before graph to be enforced for simple\_toctou}
  \label{fig:eval:simple_toctou:hb_graph}
\end{figure}

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (l1) {\texttt{400694}: $\mai{cfg1}{1}$};
    \node[left = 0 of l1] {$\smLoad{\mathrm{global\_ptr}} \not= 0$};
    \node[draw, below right = of l1] (l2) {\texttt{4008fb}: $\mai{cfg6}{2}$ };
    \node[right = 0 of l2] {$\smLoad{\mathrm{global\_ptr}} \not= 0$};
    \node[draw, below left = of l2] (l3) {\texttt{4006a0}: $\mai{cfg4}{1}$ };
    \draw[->] (l1) -- (l3);
    \draw[->, happensBeforeEdge] (l1) -- (l2);
    \draw[->, happensBeforeEdge] (l2) -- (l3);
  \end{tikzpicture}
  \caption{Crash enforcement plan for simple\_toctou}
  \label{fig:eval:simple_toctou:enforce_plan}
\end{figure}

This enforcer is effective at reproducing the bug.  Without the
enforcer, the mean time taken to reproduce the bug is $4.3 \pm 0.4$
seconds, mean and standard deviation of mean for 100 runs; with it,
the mean time was $1.158 \pm 0.003$.  Most of this reduction is caused
by eliminating outliers from the distribution: the median time to
reproduce the bug decreased from 3.0 seconds to 1.2, a 2.5-fold
reduction, while the $95^{th}$ percentile was reduced from 12 seconds
to 1.2, a ten-fold reduction.
Figure~\ref{fig:eval:crash_cdf:simple_toctou} shows the complete CDFs
for both configurations.

Even if the enforcer had failed to reduce the time to reproduction,
this increase in predictability would itself be a useful property.
Consider a programmer attempting to fix this bug.  Their basic
approach is likely to be some variant of this procedure:

\begin{itemize}
\item Form a hypothesis as to the cause of the bug.
\item Produce a fix based on that hypothesis.
\item Test whether the bug has been fixed.
\item If so, stop.  Otherwise, start again from the beginning.
\end{itemize}

Bugs with a long tail of reproduction times make the third step
difficult.  The obvious way of testing whether a bug has been fixed is
to simply run the program and see if it reproduces, but if the time
taken for the bug to reproduce is highly unpredictable then it is
difficult to decide how long to run the program for before concluding
that the bug has been fixed.  {\Technique} cannot hope to completely
such unpredictability, but, qualitatively, a bug where the $95^{th}$
percentile differs from the $5^{th}$ percentile by a tenth of a
percent is likely to be far easier to work with than one where they
differ by a factor of twelve\footnote{The obvious alternative strategy
  of measuring what fraction of program runs reach some cut-off time
  is also ineffective.  For this test program, the optimal cut-off
  time for that strategy is 3.003 seconds, and it would have to be run
  four times to achieve a p-value of 0.95, for a total time of
  slightly over 12 seconds; almost precisely the same time as would be
  necessary for the na\"{i}ve strategy of just running the program
  once to reach the same p value.}.

The time taken by the analysis is quite reasonable here.  The
\gls{verificationcondition} generation step takes $0.77 \pm 0.02$
seconds (mean and standard deviation of ten runs), including the
initial static analysis phase, and converting it to a
\gls{bugenforcer} takes $0.13 \pm 0.01$ seconds.  Real programs would,
of course, take far longer.

\subsubsection{Fixing the bug}
{\Implementation} can also generate a fix for this bug.  In this case,
the CFG fragments to be protected will be the complete CFGs of both
threads, as the first and last CFG nodes in both threads are involved
in happens-before edges.  This corresponds to modifying the program as
shown in Figure~\ref{fig:eval:simple_toctou:fix}.  This correctly
fixes the bug.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{acquire\_lock();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&t = global\_ptr;\\
        &&release\_lock();\\
        &&*t = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }\hfill %
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &acquire\_lock();\\
        &global\_ptr = NULL;\\
        &release\_lock();\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }\hfill
  \caption{The fix generated by {\implementation} for the simple\_toctou bug..}
  \label{fig:eval:simple_toctou:fix}
\end{figure}

It does, however, have a rather high performance overhead: without a
fix, the crashing thread completes $352.5 \pm 0.2 {\times} 10^6$
iterations of the loop per second; with one, it completes $95.9 \pm
0.2 {\times} 10^6$ (mean and standard deviation of ten runs each of
ten seconds, discarding any runs in which the unfixed program
crashed).  Neither distribution suffered from a noticeable long tail
in either direction (checked using the Anderson-Darling and
Jarque-Bera tests at the 90\% level and by manual inspection).  That
gives an overhead of roughly a factor of 3.7.  This is obviously
rather large, but is probably close to {\technique}'s worst case: the
read-side critical section is very small and runs with very high
frequency, and so the patch must acquire and release the lock with
similarly high frequency and these lock operations dominate the time
taken.  Any realistic test would usually have much lower overhead,
assuming lock contention does not become a factor, simply because the
critical sections would run less frequently and the overhead could be
more effectively amortised.  Even in this case, a factor of four
overhead is not completely unreasonable when the alternative is a
program which crashes frequently.

\todo{Worst case ignoring the loss of concurrency, of course.}

For comparison, I also produced a version of the patch which does
everything except for acquiring and releasing the lock.  This version
completed $349.2 \pm 0.3 {\times} 10^6$ loops per second, and so in
this case the slow down caused by the patch was a little less than
1\%.  This strongly suggests that the overhead in this case is mostly
caused by the lock operations themselves, rather than the {\technique}
infrastructure.

\begin{table}
  \begin{tabular}{lll}
                     & Gain control with breakpoints & Gain control with branches \\
    Locking enabled  & $6.08 \pm 0.01$               & $95.9 \pm 0.2$\\
    Locking disabled & $6.38 \pm 0.01$               & $349.2 \pm 0.3$\\
  \end{tabular}
  \caption{Performance of some variants of the fix, in millions of
    loop iterations completed per second.  The original program
    completed $352.5 \pm 0.2$ million iterations per second, when it
    did not crash.  All measurements mean and standard deviation of
    ten runs.}
  \label{table:eval:simple_toctou:other_fixes}
\end{table}

As a further point of comparison, I also produced a version of this
fix which used debug breakpoints to gain control of the program rather
than branches.  This completed $6379000 \pm 7000$ loop iterations per
second, even with the actual locking disabled, giving it an overhead
of roughly a factor of 55.

\todo{Give details of the machine the test is running on.}\smh{Yes}

\subsection{Indexed TOCTOU bug (indexed\_toctou)}\footnote{This bug was used as an example in Section~\ref{sect:reproducing_bugs}.}

\label{sect:eval:indexed_toctou}

In this variant of a TOCTOU bug, there are multiple instances of the
structure which is being raced on and the bug will only manifest if
the reading and writing threads happen to coincide.  This bug
exercises the side condition-checking part of {\technique}'s crash
enforcers.  The code involved in the race is shown in
Figure~\ref{fig:eval:indexed_toctou}.  Except where otherwise noted,
\verb|NR_PTRS| is set to 100.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{idx = random() \% NR\_PTRS;}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptrs[idx] != NULL) \{}\\
        &&*(global\_ptrs[idx]) = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \}\\
      \end{tabular}
    }
  }%
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        & idx = random() \% NR\_PTRS;\\
        & STOP\_ANALYSIS();\\
        & global\_ptrs[idx] = NULL;\\
        & STOP\_ANALYSIS();\\
        & global\_ptrs[idx] = \&t;\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The two sides of the indexed\_toctou bug.}
  \label{fig:eval:indexed_toctou}
\end{figure}

As with the simple\_toctou test, this test produces a single candidate
bug, with a similar enforcer and fix.  The only important difference
is that the enforcer includes a side condition $\mathtt{idx}_1 =
\mathtt{idx}_2$, where $\mathtt{idx}_1$ is an expression for
\texttt{idx} in the crashing thread and $\mathtt{idx}_2$ that in the
interfering thread, which is checked on the first happens-before edge.

The enforcer was effective at making this bug reproduce more easily.
With no enforcer loaded, the mean time to reproduce the bug was $1.2
\pm 0.2$ seconds, mean and standard deviation of mean for 100 runs;
with an enforcer, the mean time to reproduce was $0.24 \pm 0.01$
seconds.  As with the simple\_toctou test, most of this reduction was
due to removing the long tail: the $95^{th}$ percentile reproduction
time was reduced from 5.8 seconds to 0.48 seconds, whereas the median
actually increased slightly, from 0.16 seconds to 0.21 seconds.
\todo{Investigate how that changes when you change the delay
  parameter.}

I also investigated the behaviour of this test with an enforcer loaded
but no side condition checking performed.  In that case, the bug
reproduction time was again exponentially distributed, but this time
$\lambda = 0.054 \pm 0.004$Hz \todo{re-do experiment to check that
  number}.  This reduced enforcer not only fails to make the bug
reproduce more quickly; it actually makes it \emph{less} likely to be
triggered, per unit time!  This is because an enforcer without
side-condition checking will often slow the program down in order to
impose the happens-before graph even in situations where doing so is
unlikely to trigger the bug, and this causes the buggy code to run far
less frequently than it otherwise would.

\begin{figure}
  \subfigure[][Without enforcer]{ \input{eval/indexed_toctou_vary_nr_ptrs_no_enforcer.tex} }
  \subfigure[][With enforcer]{ \input{eval/indexed_toctou_vary_nr_ptrs_enforcer.tex} }
  \caption{Reproduction times with and without an enforcer loaded, for
    varying values of \texttt{NR\_PTRS}.  Note that the two graphs use
    different scales, and that both use a log y scale.}
  \label{fig:eval:indexed_toctou:nr_ptrs}
\end{figure}

As a final test, I investigated the effect changing the
\texttt{NR\_PTRS} parameter.  The results are shown in
Figure~\ref{fig:eval:indexed_toctou:nr_ptrs}.  I ran each
configuration 100 times at each of the sampled abscissae and timed how
long it took to reproduce the bug; these charts show the $25^{th}$,
$50^{th}$, and $75^{th}$ percentiles and the mean.

The most obvious property of these charts is that the behaviour of the
case without the enforcer is quite ``noisy''.  Some of this noise is
the usual experimental error; the distributions being measured have a
very long positive tail, and 100 samples is barely sufficient for the
$75^{th}$ percentile to become meaningful.  Much of it, though,
reflects the actual behaviour of the program.  The dips in
reproduction time around $\texttt{NR\_PTRS} = 200$ and
$\texttt{NR\_PTRS} = 400$, for instance, are both reproducible, and
persisted across multiple repeats of the experiment on the same
machine.  I am unable to explain this precise behaviour, beyond
speculating that it might be due to some aliasing effect between those
values of \texttt{NR\_PTRS} and some periodic structure as processor
cache lines, or possibly even some peculiarity the random number
generator.  The complex behaviour of an apparently simple test is
reflective of the general complexity of concurrency-related bugs, and
is one of the reasons why they are often difficult for programmers to
correctly diagnose\needCite{}.  The chart with the enforcer, by
contrast, avoids this peculiar behaviour.  This would make the bug far
easier for a programmer to fix, even without the significant reduction
in the time take to reproduce it.

The automatic fix generator works well with this bug, and produces
roughly the same fix as it did in the simple\_toctou bug: one critical
section which covers the two critical loads in the read thread and one
which covers the critical store in the write thread.  To characterise
the performance overheads of the fix I again counted the number of
times the read and write loops execute per second with and without the
fix applied, running the test for ten seconds and discarding any runs
in which the test program crashed.  Without a fix applied, the test
completed the crashing thread loop $9.6 \pm 0.5 {\times} 10^5$ times
per second and the interfering thread loop $9.0 \pm 0.2 {\times} 10^5$
times per second; with a fix, it completed $8.3 \pm 0.3 \times$
crashing loops and $7.8 \pm 0.2 \times 10^5$ interfering loops (mean
and standard deviation of ten runs).  The overhead was therefore
roughly 15\% on both the read and write sides of the test.  This is
far smaller than the factor of four reported in the simple\_toctou
case, largely because the test loop in this case includes a call to
\verb|random|, which is rather expensive relative to simple lock
operations and helps to amortise the cost of the additional
synchronisation.  \todo{Re-run those experiments.}

\subsection{Biassed indexed TOCTOU bugs (crash\_indexed\_toctou, interfering\_indexed\_toctou)}

These bugs are similar to the indexed\_toctou with $\texttt{NR\_PTRS}
= 100$, except with an additional one second delay in either the
interfering or crashing thread's loops, such that either the
interfering thread (for interfering\_indexed\_toctou) or the crashing
thread (for crash\_indexed\_toctou) runs far more often than the
other.  These tests are intended to illustrate the importance of
placing delays at appropriate operations in the crash enforcement
message-passing system.  The results are shown in
Table~\ref{fig:biassed_indexed_toctou:times}.  These results show
that, while the delay placement mechanism is not always guaranteed to
find the placing, it does avoid some very poor possible placements.

\begin{table}
  \begin{tabular}{llllll}
                                 & crash\_indexed\_toctou & interfering\_indexed\_toctou \\
    No enforcer                  & No reproduction        & No reproduction\\
    Normal delay positioning \\
    Delays on sends \\
    Delays on receives \\
    Delays on both \\
  \end{tabular}
  \caption{Time taken to reproduce the crash\_indexed\_toctou and
    interfering\_indexed\_toctou bugs in different configurations.
    All experiments were repeated twenty times. \todo{Fill this out.}}
  \label{fig:biassed_indexed_toctou:times}
\end{table}

\subsection{Multi-variable consistency constraint (multi\_variable)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{} \\
        & STOP\_ANALYSIS();\\
        & v1 = global1;\\
        & v2 = global2;\\
        & assert(v1 == v2);\\
        & STOP\_ANALYSIS();\\
        & sleep(10 milliseconds);\\
        \multicolumn{2}{l}{\}}\\
        \\
        \\
        \\
        \\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        & STOP\_ANALYSIS();\\
        & global1 = 5;\\
        & STOP\_ANALYSIS();\\
        & global2 = 5;\\
        & STOP\_ANALYSIS();\\
        & sleep(100 milliseconds);\\
        & STOP\_ANALYSIS();\\
        & global1 = 7;\\
        & global2 = 7;\\
        & STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The two sides of the multi\_variable bug. The delays were
    chosen so that the program crashed in a reasonable amount of time
    when run unmodified.}
  \label{fig:eval:multi_variable}
\end{figure}

This bug is intended to explore {\technique}'s effects on
multi-variable atomicity violations.  The two sides of the bug are
shown in Figure~\ref{fig:eval:multi_variable}.  Note that in this case
the race leads to an assertion failure, whereas previous bugs lead to
a bad pointer dereference.  {\Technique} reports a single candidate
bug in this program, corresponding to interleaving the crashing thread
with the two stores which set \texttt{global1} and \texttt{global2} to
7 in the interfering thread.  This enforcer causes the bug to
reproduce quickly (in an average of 300ms \todo{more details of
  experiment}).

It is perhaps worth explaining why {\technique} only reported a single
bug here.  {\Technique} discovered that the crashing thread can crash
when interleaved with the stores which set the globals to 7, but
missed the fact that it can also crash when interleaved with the ones
which set them to 5.  This is because of an interaction between the
second \texttt{STOP\_ANALYSIS()} in the interfering thread and the
\backref{inferred assumption}, discussed in
Section~\ref{sect:derive:inferred_assumption}.  The
\texttt{STOP\_ANALYSIS()} causes the two stores to be converted into
independent {\StateMachines}.  Consider the first store; the other is
symmetrical.  Figure~\ref{fig:eval:multi_variable:other_bug} shows how
the \backref{inferred assumption} is derived for this bug\footnote{The
  actual analysis performs this derivation using {\StateMachines},
  rather than by concatenating the program's actual code, but that
  makes no difference in this case.}.  The \backref{CI atomic}
constraint shows that the initial values of \texttt{global1} and
\texttt{global2} must be equal, and the \backref{IC atomic} constraint
shows that the initial value of \texttt{global2} must be 5.  Combining
these two shows that the initial value of \texttt{global} must also be
5, and so the store operation becomes a no-op and no candidate bug is
reported.

\begin{figure}
  \subfigure[][CI atomic]{
    \begin{tabular}{l}
      \hspace{-5mm}\texttt{
        \begin{tabular}{l}
          v1 = global1;\\
          v2 = global2;\\
          assert(v1 == v2);\\
          v1 = 5;\\
        \end{tabular}
      }\\
      \\
      $\smLoad{\texttt{global1}} = \smLoad{\texttt{global2}}$\\
    \end{tabular}
  }
  \hfill
  \subfigure[][IC atomic]{
    \begin{tabular}{l}
      \hspace{-5mm}\texttt{
        \begin{tabular}{l}
          v1 = 5;\\
          v1 = global1;\\
          v2 = global2;\\
          assert(v1 == v2);\\
        \end{tabular}
      }\\
      \\
      $\smLoad{\texttt{global2}} = 5$\\
    \end{tabular}
  }\\
  \\
  Inferred assumption: $\smLoad{\texttt{global2}} = 5 \wedge \smLoad{\texttt{global1}} = 5$
  \caption{Deriving the \backref{inferred assumption} for the other bug in the multi\_variable bug.}
  \label{fig:eval:multi_variable:other_bug}
\end{figure}

The reported bug can also be converted into a fix.  This fix correctly
fixes the reported bug, but does not, of course, fix the one which is
not reported.  As such, the program might still crash (although the
average time before a crash increases from \todo{...} to \todo{...}
seconds).  If the \texttt{STOP\_ANALYSIS()} marker is removed then
both bugs will be discovered, and a fix which fixes both bugs
correctly causes the program to no longer crash.  \todo{That needs
  rephrasing.}

\subsection{Context-dependent races (context)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{f(int **ptr) \{}\\
        &\multicolumn{2}{l}{if (*ptr) \{}\\
        &&**ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        \multicolumn{3}{l}{\}}\\
        \\
        \multicolumn{3}{l}{while (1) \{}\\
        & \multicolumn{2}{l}{if (random() \% 1000 == 0) \{}\\
        &&STOP\_ANALYSIS();\\
        &&f(\&global\_ptr1);\\
        &&STOP\_ANALYSIS();\\
        &\multicolumn{2}{l}{\} else \{}\\
        &&STOP\_ANALYSIS();\\
        &&f(\&global\_ptr2);\\
        &&STOP\_ANALYSIS();\\
        &\multicolumn{2}{l}{\}}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \\
        \\
        \\
        \\
        \\
        \\
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global\_ptr1 = \&t;\\
        &STOP\_ANALYSIS();\\
        &sleep(1 millisecond);\\
        &STOP\_ANALYSIS();\\
        &global\_ptr1 = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
        \\
        \\
      \end{tabular}
    }
  }
  \caption{Crashing and interfering threads for the context test.}
  \label{fig:eval:context}
\end{figure}

This test demonstrates {\technique}'s ability to take account of some
cross-function properties of the program.  The test program is shown
in Figure~\ref{fig:eval:context}.  The function \texttt{f} is called
from two places in the crashing thread, one of which passes
\texttt{\&global\_ptr1} as the pointer argument while the other passes
\texttt{\&global\_ptr2}.  Meanwhile, the interfering thread loops
modifying \texttt{global\_ptr1}.  This means that the call to
\texttt{f(\&global\_ptr1)} can sometimes suffer a crash caused by a
race while the call to \texttt{f(\&global\_ptr2)} cannot.  The safe
call is far more common than the unsafe one, and so it is important
that the enforcer only insert delays in the correct calling context.

Without an enforcer, this test program crashes reasonably
infrequently, reaching the three minute timeout six times out of
twenty runs and taking an average of 80 seconds in the remaining
cases.  With a full crash enforcer, including context checking, the
time taken to obtain a reproduction is uniformly distributed between
100 and 200 milliseconds.  With an enforcer modified to not perform
stack context checking, the bug did not reproduce within three
minutes.

\todo{Need to re-run those experiments with repeats.}

\subsection{Write-to-read hazard (write\_to\_read)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = \&t;\\
        &*global\_ptr = 5;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global\_ptr = NULL;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The write\_to\_read test case.}
  \label{fig:eval:write_to_read}
\end{figure}

This test investigates a form of write-write-read race, whereas all of
the previous ones have considered only write-read ones.  It is shown
in Figure~\ref{fig:eval:write_to_read}.  The crashing thread loops
setting a global variable to point at some location and then proceeds
to use that global variable, while at the same time the interfering
thread loops setting that global variable to \texttt{NULL}.  In other
words, there is a write-to-read hazard in the crashing thread which
might be interrupted by the interfering thread, leading to a crash.

This test program is surprisingly reliable, given that it runs both
sides of the race in a tight loop with no delays or synchronisation,
and can often run for several minutes without encountering an error on
an otherwise idle system, during which time the buggy code might run
billions of times.  A more realistic test would run the two sections
far less frequently, and so might easily require hundreds of years of
CPU time to reproduce the bug.  I suspect that this is because the
store and load instructions in the crashing thread are close enough
together that the load is always satisfied from the processor's write
buffer, and so only returns \texttt{NULL} when the processor receives
an interrupt in precisely the wrong place.  \todo{It probably wouldn't
  be hard to check this with a hacked up kernel.  Should probably do
  that.}  The {\technique}-generated bug enforcer, by contrast, can
reliably reproduce the bug in a few hundred milliseconds.

{\Technique} can also generate a fix for this bug.  The difficulty of
reproducing the bug makes it difficult to validate experimentally that
the generated fix is correct, but manual inspection suggested that it
is.

\subsection{Multiple bugs (multi\_bugs)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        & \multicolumn{2}{l}{r = select\_test();}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        & \multicolumn{2}{l}{if (r) \{}\\
        && simple\_toctou\_crashing();\\
        & \multicolumn{2}{l}{\} else \{}\\
        && write\_to\_read\_crashing();\\
        & \multicolumn{2}{l}{\}}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        & \multicolumn{2}{l}{for (i = 0; i < 2000000; i++) \{}\\
        && STOP\_ANALYSIS(); \\
        && write\_to\_read\_interfering();\\
        && STOP\_ANALYSIS(); \\
        & \multicolumn{2}{l}{\}}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        & \multicolumn{2}{l}{simple\_toctou\_interfering();}\\
        & \multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{The multi\_bugs test.  The constant \texttt{2000000} was
    chosen so that the two bugs reproduce with roughly equal
    probability.}
  \label{fig:eval:multi_bugs}
\end{figure}

This test combines simple\_toctou and write\_to\_read into a single
test, demonstrating {\technique}'s ability to exercise several bugs
using a single enforcer.  The test program is shown in
Figure~\ref{fig:eval:multi_bugs}.  The behaviour of
\texttt|select\_test| is configurable at run time, and will either
always select simple\_toctou, always select write\_to\_read, or select
randomly.  The dynamic analysis phases were run with it configured to
select randomly.

{\Implementation} is able to generate a candidate bug for each of the
component bugs in the test, each of which can be instantiated into an
enforcer, and these enforcers succeed in reproducing their respective
bugs.  As expected, neither enforcer is able to reproduce both bugs.
On the other hand, if both bugs' happens-before graphs are loaded into
the enforcer then it is able to reproduce either bug, as desired.
Perhaps more surprisingly, when the test is configured to exercise
both bugs the enforcer consistently reproduces the write\_to\_read
bug.  This is simply because the enforcer reproduces the
write\_to\_read bug before the simple\_toctou interfering critical
section ever runs.

In the same way, both candidate bugs can be instantiated into
individual fixes, both of which correctly fix their respective bug
while leaving the other bug in place.  Alternatively, a combined fix
can be generated, and this successfully fixes both bugs.

\subsection{Multiple crashing and interfering threads (multi\_threads)}

This test investigates {\implementation}'s behaviour in programs with
a very large number of threads.  The racing code is the same as in
indexed\_toctou (Figure~\ref{fig:eval:indexed_toctou}) with
$\texttt{NR\_PTRS} = 100$, except that rather than having a single
thread running the crashing and interfering critical sections, this
test has 32 threads running each.  {\Technique} behaves roughly as
expected here: it is able to generate an enforcer and a fix, with the
enforcer making the bug happen more quickly and the fix preventing it
from happening at all.

This test illustrates one subtlety in the implementation of the
enforcer: it must be (at least reasonably) fair in order for the time
to reproduce the bug to be predictable.  An initial version of the
enforcer internally used an unfair lock implementation, and while it
was able to reproduce the bug quickly (in under a second) most of the
time it suffered from a long tail in 5\% of tests took more than a
minute to reproduce.  Switching to a fair lock implementation solved
this problem, and reduced the mean time to reproduction, but at the
expense of slightly increasing the median time to reproduction.
\todo{Insert reproduction time CDFs here.}  All other experiments in
this evaluation were conducted using the fair lock implementation.

\todo{Currently running all of these tests on a four-processor machine,
  which might have an interesting effects on the results of a
  64-thread test.}

\todo{I kind of feel like I ought to say \emph{which} fair and unfair
  lock implementations I'm using here, since I just strongly implied
  that it matters, but it's a bit of a tedious thing to have to
  explain.}

\subsection{Complicated happens-before graphs ($\textrm{complex\_hb}_{\{5,11,17\}}$)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &x1 = global;\\
        &x2 = global;\\
        &x3 = global;\\
        &assert(!(x1 == 0 \&\& x2 == 1 \&\& x3 == 2));\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &global = 0;\\
        &global = 1;\\
        &global = 2;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
        \\
      \end{tabular}
    }
  }
  \caption{Crashing and interfering threads for the
    $\textrm{complex\_hb}_5$ test.  The $\textrm{complex\_hb}_{11}$
    and $\textrm{complex\_hb}_{17}$ tests are generated by extending
    this pattern to require additional happens-before edges.}
  \label{fig:eval:complex_hb}
\end{figure}

\begin{figure}
  \subfigure[][]{
    \begin{tabular}{ll}
      \begin{tikzpicture}
        \node (dummy) {};
        \node (ld1) [CfgInstr, below = of dummy] {\texttt{x1 = global;}};
        \node (ld2) [CfgInstr, below = 2 of ld1] {\texttt{x2 = global;}};
        \node (ld3) [CfgInstr, below = 2 of ld2] {\texttt{x3 = global;}};
        \node (st1) [CfgInstr, right = of dummy] {\texttt{global = 0;}};
        \node (st2) [CfgInstr, below = 2 of st1] {\texttt{global = 1;}};
        \node (st3) [CfgInstr, below = 2 of st2] {\texttt{global = 2;}};
        \draw[->] (ld1) -- (ld2);
        \draw[->] (ld2) -- (ld3);
        \draw[->] (st1) -- (st2);
        \draw[->] (st2) -- (st3);
        \draw[->,happensBeforeEdge] (st1) -- (ld1);
        \draw[->,happensBeforeEdge] (ld1) -- (st2);
        \draw[->,happensBeforeEdge] (st2) -- (ld2);
        \draw[->,happensBeforeEdge] (ld2) -- (st3);
        \draw[->,happensBeforeEdge] (st3) -- (ld3);
      \end{tikzpicture}\\
      Side condition: \true
    \end{tabular}
  }
  \hfill
  \subfigure[][]{
    \begin{tabular}{ll}
      \begin{tikzpicture}
        \node (ld1) [CfgInstr] {\texttt{x1 = global;}};
        \node (dummy) [right = of ld1] {};
        \node (ld2) [CfgInstr, below = 3 of ld1] {\texttt{x2 = global;}};
        \node (ld3) [CfgInstr, below = 2 of ld2] {\texttt{x3 = global;}};
        \node (st1) [CfgInstr, below = of dummy] {\texttt{global = 0;}};
        \node (st2) [CfgInstr, below = 0.5 of st1] {\texttt{global = 1;}};
        \node (st3) [CfgInstr, below = 1.7 of st2] {\texttt{global = 2;}};
        \draw[->] (ld1) -- (ld2);
        \draw[->] (ld2) -- (ld3);
        \draw[->] (st1) -- (st2);
        \draw[->] (st2) -- (st3);
        \draw[->,happensBeforeEdge] (ld1) -- (st1);
        \draw[->,happensBeforeEdge] (st2) -- (ld2);
        \draw[->,happensBeforeEdge] (ld2) -- (st3);
        \draw[->,happensBeforeEdge] (st3) -- (ld3);
      \end{tikzpicture}\\
      Side condition: $\smLoad{\mathrm{global}} = 0$
    \end{tabular}
  }
  \caption{Happens-before graphs generated for the $\textrm{complex\_hb}_5$ test.}
  \label{fig:eval:complex_hb:hb}
\end{figure}

This test, shown in Figure~\ref{fig:eval:complex_hb} is intended to
evaluate {\technique}'s ability to handle more complicated
happens-before graphs which require more than two context switches.
As expected, {\implementation} is able to generate both an enforcer
and a fix for this bug, which can either make the bug reproduce easily
or not at all.

The happens-before graphs and side conditions generated for this test
are shown in Figure~\ref{fig:eval:complex_hb:hb}.  The one on the left
is the expected graph, and will trigger the bug.  The one on the right
is not.  It describes the case in which the first store in the
interfering thread happens after the first load in the crashing
thread, but the initial contents of memory happens to contain the
right value.  It is impossible to enforce the desired happens-before
graph when \texttt{global} is initially 0, as the program structure
means that first store will only run when \texttt{global} is 2, but
the {\technique} analysis is not powerful enough to show that.
Without the side condition, the enforcer would have to try to enforce
both graphs at run-time, and so it would take longer to reproduce the
bug; with it, the enforcer can easily discard the spurious
happens-before graph at run time, partially compensating for the
incompleteness of the main analysis.

\subsection{A simple double-free bug (double\_free)}

\begin{figure}
  \subfigure[][Active threads]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{t = global\_ptr;}\\
        &\multicolumn{2}{l}{if (t != NULL) \{}\\
        &&global\_ptr = NULL;\\
        &&free(t);\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{sleep(1 millisecond);}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \hfill
  \subfigure[][Environmental thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr == NULL) \{}\\
        &&global\_ptr = malloc(64);\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
        \\
        \\
        \\
      \end{tabular}
    }
  }
  \caption{Threads involved in the double\_free bug.}
  \label{fig:eval:double_free}
\end{figure}

This test demonstrates {\technique}'s ability to handle some simple
double-free bugs.  The test program is shown in
Figure~\ref{fig:eval:double_free}.  Note that in this test, the
crashing and interfering threads (collectively, the active threads)
both run the same code, shown on the left of the figure, while a third
thread, known as the environmental thread, modifies the environment in
which they are operating.  The two active threads loop reading
\texttt{global\_ptr} and, if it is non-\texttt{NULL}, releasing it and
setting it to \texttt{NULL}.  The environment thread is meanwhile
undoing their work by examining \texttt{global\_ptr} and, when it is
\texttt{NULL}, setting it to a newly-allocated block.  Every time it
does so, the two active threads will race trying to release the block
and reset \texttt{global\_ptr}.  In some interleavings, both active
threads will try to release the same block, leading to a double-free
bug.

Note that the program threads here do not map directly on to the
threads in the candidate bug: the interfering thread is whichever of
the two active threads wins the race and releases the block first; the
crashing thread is the other active thread, which \texttt{free}s a
block which has already been released; and the environmental thread
does not appear in the candidate bug at all, despite being necessary
for the bug to reproduce.

{\Implementation} is able to build an enforcer and a fix for this bug,
and they behave as expected.

\subsection{A program with existing synchronisation (existing\_sync\_visible, existing\_sync\_invisible)}

\begin{figure}
  \subfigure[][Crashing thread with {\technique}-visible synchronisation]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{lock(\&global\_lock);}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&*global\_ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{unlock(\&global\_lock);}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
    \label{fig:eval:existing_sync:visible}
  }
  \subfigure[][Crashing thread with {\technique}-invisible synchronisation]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{lock(\&global\_lock);}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{if (global\_ptr != NULL) \{}\\
        &&*global\_ptr = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{unlock(\&global\_lock);}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
    \label{fig:eval:existing_sync:invisible}
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &global\_ptr = \&t;\\
        &sleep(1 second);\\
        &STOP\_ANALYSIS();\\
        &lock(\&global\_lock);\\
        &global\_ptr = NULL;\\
        &unlock(\&global\_lock);\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{Threads for the existing\_sync\_visible and existing\_sync\_invisible tests.}
  \label{fig:eval:existing_sync}
\end{figure}

These tests explore {\technique}'s interactions with the program's
existing synchronisation.  It is the same as simple\_toctou, except
that the program contains calls to \texttt{pthread\_mutex\_lock} and
\texttt{pthread\_mutex\_unlock} which prevent the bug from ever
reproducing.  As discussed previously\editorial{ref?}, {\technique}
has no global model of the program's synchronisation, and so can only
analyse synchronisation within the analysis window \backref{$\alpha$}.
The analysis will therefore be aware of the synchronisation in the
existing\_sync\_visible crashing thread,
Figure~\ref{fig:eval:existing_sync:visible}, but not that in the
existing\_sync\_invisible crashing thread,
Figure~\ref{fig:eval:existing_sync:invisible}.  As such, it generates
a candidate bug for the existing\_sync\_invisible test but not for the
existing\_sync\_visible one.

\begin{figure}
  \centerline{
    \begin{tikzpicture}
      \node (LD1) {First load};
      \path (node cs:name=LD1,anchor=west) -- ++(0,-2) node (LD2) [right] {Second load};
      \node (dummy) [right = 2 of LD1] {};
      \node (ST) [below = 0.6 of dummy] {Store};
      \draw (node cs:name=LD1,anchor=north west) -- ++(-0.2,0) |- (node cs:name=LD2,anchor=south west);
      \draw (node cs:name=ST,anchor=north east) -- ++(0.2,0) |- (node cs:name=ST,anchor=south east);
      \draw[->, happensBeforeEdge] (LD1) -- (ST);
      \draw[->, happensBeforeEdge] (ST) -- (LD2);
      \draw[->] (LD1) -- ++(0,-1.8);
      \path (-1.5,-2) to node [sloped] {locked region} (-1.5,0);
      \node at (5.3,-1.05) {locked region};
    \end{tikzpicture}
  }
  \caption{Happens-before graph which must be enforced for the
    existing\_sync\_invisible test, along with the program's existing
    locked regions.  This bug cannot be reproduced.}
  \label{fig:eval:existing_sync:hb}
\end{figure}

This candidate bug can be instantiated into a bug enforcer, but, as
might be expected, this enforcer cannot cause any bugs to reproduce.
The happens-before graph for the bug is shown in
Figure~\ref{fig:eval:existing_sync:hb}; this clearly contradicts the
program's existing synchronisation strategy, and so trying to enforce
it will lead to a deadlock.  The unbound message operations in the
crash enforcement plan will therefore all time out, preventing
successful plan completion.

It can similarly be instantiated into a fix.  This fix does not fix
any actual bugs, as there are none, but does not otherwise harm the
program's execution.

\subsection{The broken publish pattern (broken\_publish)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{p = global\_ptr;}\\
        &\multicolumn{2}{l}{if (p != NULL) \{}\\
        &&assert(p->v == 7);\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &s = malloc();\\
        &global\_ptr = s;\\
        &s->v = 7;\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
        \\
      \end{tabular}
    }
  }
  \caption{The two sides of the broken\_publish test bug.  Garbage
    collection-related code is not shown.}
  \label{fig:eval:broken_publish}
\end{figure}

This test implements a common bug in the widely-used publish
pattern\needCite{}.  Were it correctly implemented, this pattern would
involves the interfering thread allocating and initialising a data
structure, and then storing a pointer to it in a thread-shared
location so that the crashing thread could access it.  In the test,
though, the interfering thread does not finish initialising the
structure (the store of 7 to \texttt{v}) until after it has published
the structure by storing to \texttt{global\_ptr}.  {\Implementation}
is able to find this bug and to generate an enforcer and a fix, and
they both work in the expected way.

\todo{I was trying to get malloc() to return something which just
  happened to have a 7 in the right place, so that I could say
  something about only being able to handle bugs which are down to
  concurrency and not other forms of undefined behaviour, but it looks
  like this version of libc zeroes out the buffer.  Bah.}

\subsection{A bug which lacks the W isolation property (w\_isolation)}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{while (1) \{}\\
        &STOP\_ANALYSIS();\\
        &s = malloc();\\
        &s->v = 7;\\
        &global\_ptr = s;\\
        &assert(s->v == 7);\\
        &STOP\_ANALYSIS();\\
        \multicolumn{2}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{while (1) \{}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        &\multicolumn{2}{l}{p = global\_ptr;}\\
        &\multicolumn{2}{l}{if (p != NULL) \{}\\
        &&p->v = 5;\\
        &\multicolumn{2}{l}{\}}\\
        &\multicolumn{2}{l}{STOP\_ANALYSIS();}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \caption{Racing threads for the w\_isolation test.  Garbage
    collection-related code is not shown.}
  \label{fig:w_isolation}
\end{figure}

This test illustrates a bug which lacks the W isolation property (see
Section~\ref{sect:derive:w_isolation}).  In this test, the crashing
thread allocates a fresh data structure, initialises the field
\texttt{v}, publishes it via a global pointer, and then asserts that
\texttt{v} is unchanged.  Meanwhile, the interfering thread loops
checking for any published structures and, if it finds one, changing
the value of \texttt{v}.  This bug can only be reproduced when the
interfering thread is able to access the structure which was stored by
the crashing one, and hence lacks the W isolation property.
{\Implementation} is able to correctly analyse this bug when
configured to not assume the W isolation property, but cannot
otherwise.

This bug is quite similar to the broken\_publish bug, which does have
the W isolation property.  The important difference is that in
broken\_publish, data only flows only in one direction between the two
threads, whereas in this bug data flows in both directions and there
are no sub-fragments of the critical section which have a bug and do
not have bi-directional data flow.

\subsection{A bug which is not a race (not\_a\_race)}

\todo{{\Technique} can handle some bugs which aren't actually
  memory-memory races, which isn't something I've seen done with
  ``real'' programs before.  I should include an example of that
  here.}

\subsection{Comparison to DataCollider}
\label{sect:eval:datacollider}

As a point of comparison, I implemented a tool which explores
alternative thread schedules at random, without first analysing the
program to obtain {\StateMachines} and verification conditions,
inspired by DataCollider\needCite{}.  When the program starts, this
tool places breakpoints at a randomly selected subset of the program's
memory accesses.  When one of the breakpoints is hit, it determines
what memory location the instruction is accessing and sets a processor
watch point\needCite{} on that location.  If the watch point is hit by
any other threads, the tool has discovered a race, and it selects one
of the two racing threads to go first at random.  In this way, the
tool encourages the program to explore its available schedules much
more quickly than it otherwise would.

The effectiveness of this tool is obviously highly dependent on the
fraction of instructions which are covered by breakpoints and the
length of the delays inserted.  Some initial experiments\smh{By you?
  or by them?  what workloads?} suggested that these parameters lead
to the most rapid bug reproduction\editorial{Should have done this in
  a more systematic way.}:

\begin{itemize}
\item
  At any given point, half of the program's memory accessing
  instructions will have an instruction breakpoint, excluding stack
  accesses\editorial{Putting a breakpoint on every instruction lead to
    a deadlock; should probably figure out why.}.
\item
  When a breakpoint is hit, the tool will wait for up to 10ms for a
  matching read or write to arrive.  When a thread does arrive, it
  will select which to resume first at random, with equal probability.
\item
  Once a second, the tool discards its current instruction breakpoint
  set and generates a new one.
\end{itemize}

It is instructive to compare these parameters to those used in the
DataCollider paper.  The 10ms timeout is broadly similar, as
DataCollider uses between one and fifteen millisecond timeouts,
depending on the type of instruction.  Setting breakpoints on half of
instructions, on the other hand, is not.  The actual DataCollider
implementation adjusts the breakpoint density dynamically so as to
achieve a particular breakpoint rate, and their paper does not specify
a numerical breakpoint density, which makes a direct comparison
difficult.  It is, however, possible to estimate the breakpoint
density from the information which they do give.  Their evaluation
shows breakpoint rates of up to 1500 breakpoints per second in a
virtual machine with two processors running at 2.4GHz, so $4.8 \times
10^9$ cycles per second.  If one assumes roughly one non-stack memory
access every hundred cycles that translates to 4.8 million memory
accessing instructions per second, and so they must trap roughly
0.03\% of memory accessing instructions.  Even allowing for the fact
that their implementation preferentially places breakpoints on
instructions which execute infrequently this is still likely to
translate to a breakpoint ratio several orders of magnitude lower than
that used in this evaluation.

Setting such a high breakpoint ratio has two main effects: bugs are
detected more quickly, but the overheads of the tool are much higher.
In particular, the high overheads mean that this tool is not an
entirely practical approach to investigating concurrency bugs.  I
present it merely as a baseline against which to evaluate
{\implementation}.  \todo{Should really eval the overheads, rather
  than just asserting that they're massive.  I mean, they are, but I
  should have some evidence to back that up.}\smh{+maybe look at would
  they would be if you uses 0.03\% like the original?}

More fundamentally, using DataCollider to explore alternative
schedules is itself somewhat unfair, as DataCollider was originally
intended to discover races rather than to permute schedules, and,
while it is trivial to extend it to perform schedule exploration, it
is not entirely surprising that the results are somewhat poor.
Nevertheless, it represents the approach which is conceptually close
to {\technique}'s in the existing literature, and so I consider it to
be an interesting reference point.

\todo{I'd really like to do a comparison with CHESS, as well, but
  that's only implemented for 32-bit Windows programs, and
  {\technique} is only implemented for 64-bit Linux ones, which makes
  a bit of a mess of direct comparisons.  It'd be kind of fun to
  re-implement it for Linux, but I don't really have time to do that
  right now.}

\subsection{Summary tables}

I now give some a quantitative characterisation of {\technique}'s
performance on these test programs.  This includes CDFs of the time
taken to reproduce the various bugs, in
\autoref{fig:eval:summary_cdfs}, the time taken to perform the various
analysis steps, in \autoref{table:eval:summary_analysis_times}, and
the performance effects of the fixes, in
\autoref{table:eval:fix_overheads}.  The important points here are:

\begin{itemize}
\item {\Technique}'s enforcers make these bugs reproduce more quickly,
  often much more quickly.
\item The various analysis passes are generally very fast, usually
  taking a few hundred milliseconds.  The most time-consuming program
  to analyse is complex\_hb\_17, which takes 1.711 seconds to analyse.
\item The fixes which {\technique} generates for these bugs have
  tolerable overhead, generally from a few tens of percent to a small
  factor.  More realistic tests would probably shower overheads still.
\end{itemize}

In other words, {\technique} is an effective and efficient technique
for both finding and fixing bugs in small programs.  The next few
sections will explore how well it scales up to more complex programs.

\input{eval/artificial_bugs/crash_time_cdfs}

\begin{sidewaystable}
  \begin{tabular}{lllll}
    Test name                      & Static analysis & Generating verification conditions & Building the enforcers & Building the fixes \\
    simple\_toctou                 & $0.571 \pm 0.060$ &  $0.195 \pm 0.018$ &  $0.126 \pm 0.006$ &  $0.142 \pm 0.005$\\
    indexed\_toctou                & $0.604 \pm 0.089$ &  $0.275 \pm 0.004$ &  $0.146 \pm 0.005$ &  $0.142 \pm 0.004$\\
    crash\_indexed\_toctou         & $0.566 \pm 0.068$ &  $0.289 \pm 0.009$ &  $0.149 \pm 0.007$ &  $0.138 \pm 0.004$\\
    interfering\_indexed\_toctou   & $0.624 \pm 0.055$ &  $0.288 \pm 0.008$ &  $0.149 \pm 0.007$ &  $0.142 \pm 0.010$\\
    context                        & $0.682 \pm 0.058$ &  $0.233 \pm 0.005$ &  $0.132 \pm 0.007$ &  $0.139 \pm 0.005$\\
    cross\_function                & $0.669 \pm 0.036$ &  $0.186 \pm 0.004$ &  $0.132 \pm 0.005$ &  $0.138 \pm 0.006$\\
    double\_free                   & $0.447 \pm 0.029$ &  $0.186 \pm 0.005$ &  $0.130 \pm 0.002$ &  $0.135 \pm 0.003$\\
    multi\_variable                & $0.792 \pm 0.115$ &  $0.228 \pm 0.006$ &  $0.157 \pm 0.004$ &  $0.135 \pm 0.004$\\
    write\_to\_read                & $0.439 \pm 0.035$ &  $0.182 \pm 0.003$ &  $0.125 \pm 0.004$ &  $0.135 \pm 0.003$\\
    broken\_publish                & $0.521 \pm 0.037$ &  $0.181 \pm 0.004$ &  $0.123 \pm 0.005$ &  $0.139 \pm 0.004$\\
    complex\_hb\_5                 & $0.426 \pm 0.011$ &  $0.212 \pm 0.006$ &  $0.149 \pm 0.007$ &  $0.141 \pm 0.008$\\
    complex\_hb\_11                & $0.457 \pm 0.021$ &  $0.343 \pm 0.005$ &  $0.162 \pm 0.004$ &  $0.141 \pm 0.005$\\
    complex\_hb\_17                & $0.451 \pm 0.025$ &  $1.711 \pm 0.012$ &  $0.200 \pm 0.003$ &  $0.140 \pm 0.004$\\
    existing\_sync\_visible        & $0.566 \pm 0.020$ &  $0.156 \pm 0.005$ & \multicolumn{2}{c}{\emph{Nothing generated}} \\
    existing\_sync\_invisible      & $0.572 \pm 0.023$ &  $0.206 \pm 0.006$ &  $0.136 \pm 0.003$ &  $0.140 \pm 0.004$\\
    multi\_bugs                    & $0.625 \pm 0.024$ &  $0.288 \pm 0.005$ & \\
    \hspace{5mm}Bug 1 & & &  $0.161 \pm 0.006$ &  $0.136 \pm 0.003$\\
    \hspace{5mm}Bug 2 & & &  $0.143 \pm 0.006$ &  $0.140 \pm 0.005$\\
    multi\_threads                 & $0.562 \pm 0.025$ &  $0.232 \pm 0.002$ &  $0.160 \pm 0.022$ &  $0.142 \pm 0.009$\\
    w\_isolation                   & $0.509 \pm 0.031$ &  $0.143 \pm 0.004$ &  $0.117 \pm 0.003$ &  $0.137 \pm 0.005$\\
    glibc                          & $0.337 \pm 0.030$ &  $0.193 \pm 0.004$ &  $0.128 \pm 0.004$ &  $0.134 \pm 0.003$\\
  \end{tabular}
  \caption{Time taken in the various analysis phases for the
    artificial bugs.  Times are given as mean and standard deviation
    of ten runs.  The W isolation assumption was enabled for all tests
    except w\_isolation.}
  \label{table:eval:summary_analysis_times}
\end{sidewaystable}

\begin{sidewaystable}
  \begin{tabular}{lllll}
    \multicolumn{2}{l}{Test name} & Performance without fix & Performance with fix & Ratio\\
  \multicolumn{2}{l}{simple\_toctou                }  & $352687762.100 \pm 100802.825$ & $95213282.500 \pm 46110.500$ & 0.27\\
  \multicolumn{2}{l}{indexed\_toctou               } \\
  & \multicolumn{1}{l}{Crashing thread} & $9445195.400 \pm 615047.973$ & $11712742.400 \pm 3691545.455$ & 1.24\\
  & \multicolumn{1}{l}{Interfering thread} & $8444474.800 \pm 588242.365$ & $9036280.700 \pm 2461767.335$ & 1.07\\
  \multicolumn{2}{l}{crash\_indexed\_toctou        }  & $156764723.500 \pm 40045.526$ & $69828135.100 \pm 90795.871$ & 0.45\\
  \multicolumn{2}{l}{interfering\_indexed\_toctou  }  & $71470768.300 \pm 176044.183$ & $45010675.100 \pm 316856.123$ & 0.63\\
  \multicolumn{2}{l}{context                       }  & $147893175.900 \pm 2716478.716$ & $118762469.200 \pm 3049120.604$ & 0.80\\
  \multicolumn{2}{l}{double\_free                  }  & $17777.900 \pm 574.489$ & $18368.100 \pm 677.005$ & 1.03\\
  \multicolumn{2}{l}{write\_to\_read               }  & $490670660.000 \pm 7230567.261$ & $160358763.800 \pm 29886378.464$ & 0.33\\
  \multicolumn{2}{l}{broken\_publish               } \\
  & \multicolumn{1}{l}{Crashing thread} & $3326395.000 \pm 266102.322$ & $2309002.857 \pm 645792.180$ & 0.69\\
  & \multicolumn{1}{l}{Interfering thread} & $3326395.000 \pm 266102.322$ & $2309002.857 \pm 645792.180$ & 0.69\\
  \multicolumn{2}{l}{complex\_hb\_5                } \\
  & \multicolumn{1}{l}{Crashing thread} & $53933335.700 \pm 4254469.921$ & $39369897.400 \pm 2275171.662$ & 0.73\\
  & \multicolumn{1}{l}{Interfering thread} & $52113174.800 \pm 1420470.166$ & $26558298.200 \pm 1612337.510$ & 0.51\\
  \multicolumn{2}{l}{complex\_hb\_11               } \\
  & \multicolumn{1}{l}{Crashing thread} & $42217355.500 \pm 871097.124$ & $9212455.500 \pm 2241103.994$ & 0.22\\
  & \multicolumn{1}{l}{Interfering thread} & $42033609.400 \pm 871081.176$ & $13976596.300 \pm 2186328.234$ & 0.33\\
  \multicolumn{2}{l}{complex\_hb\_17               } \\
  & \multicolumn{1}{l}{Crashing thread} & $85021419.200 \pm 47700627.628$ & $38137153.700 \pm 2763703.844$ & 0.45\\
  & \multicolumn{1}{l}{Interfering thread} & $44493863.000 \pm 14960420.480$ & $28501866.400 \pm 2691108.078$ & 0.64\\
  \multicolumn{2}{l}{existing\_sync\_invisible     }  & $147050783.750 \pm 46761.948$ & $68040702.100 \pm 309415.528$ & 0.46\\
  \multicolumn{2}{l}{multi\_threads                } \\
  & \multicolumn{1}{l}{Crashing thread} & $7470682.900 \pm 120327.951$ & $7240730.600 \pm 88753.993$ & 0.97\\
  & \multicolumn{1}{l}{Interfering thread} & $7126278.700 \pm 118363.226$ & $7127980.600 \pm 75767.339$ & 1.00\\
  \end{tabular}
  \todo{Need to re-run these experiments and then apply sensible rounding.}
  \caption{Performance overheads of automatically-generated fixes,
    measured in loop iterations per second.  All numbers are mean and
    standard deviation of ten runs, with crashing runs repeated.  Test
    programs whose performance is dominated by the test harnes delays
    are not shown.}
  \label{table:eval:fix_overheads}
\end{sidewaystable}

\section{Semi-artificial bugs}
\label{sect:eval:semiartificial}

I now present the results of running the tool on some bugs which are
partly artificial and partly real.  This includes the kernel of a real
bug and two cases produced by deliberately introducing bugs into
programs from the STAMP benchmark suite\needCite{}.

\subsection{A kernel of a real bug (glibc)}
\label{sect:eval:glibc}

\todo{I'm tempted to treat this as an artificial bug instead of a
  semi-artificial one.}

glibc is a kernel of glibc bug 2644 \cite{glibc2644}, which affected
versions of glibc up to 2.5 and could lead to a crash if multiple
threads were shut down at the same time.  A simplified version of the
code involved is shown in Figure~\ref{fig:eval:glibc}, where
\texttt{forcedunwind} and \texttt{done\_init} are global variables.
The program will crash if the load of \texttt{forcedunwind} on line 1
loads a \texttt{NULL} pointer and the load of \texttt{done\_init} on
line 3 loads 1.  This is clearly possible due to interleaving with the
stores on lines 5 and 6.

Note that the bug here depends on the compiler's optimizer, and is not
apparent at the source-code level\footnote{Unfortunately, only the
  32-bit x86 version of gcc optimizes the function like this, and my
  implementation of {\technique} only supports 64-bit programs, which
  prevented me from testing with the real bug.}.  {\Technique}
operates entirely at the machine-code level, and so this does not
present any additional complexity.

\begin{figure}
  \subfigure[][Before optimisation]{
    \texttt{
      \begin{tabular}{lll}
        \multicolumn{3}{l}{\_Unwind\_ForcedUnwind() \{}\\
        & \multicolumn{2}{l}{if (forcedunwind == NULL) \{} \\
        &&pthread\_cancel\_init();\\
        & \multicolumn{2}{l}{\}} \\
        & \multicolumn{2}{l}{forcedunwind();}\\
        \multicolumn{3}{l}{\}}\\
        \multicolumn{3}{l}{pthread\_cancel\_init() \{}\\
        & \multicolumn{2}{l}{if (done\_init) return;}\\
        & \multicolumn{2}{l}{forcedunwind = \_forcedunwind\_impl;}\\
        & \multicolumn{2}{l}{done\_init = 1;}\\
        \multicolumn{3}{l}{\}}\\
      \end{tabular}
    }
  }
  \subfigure[][After optimisation.  Blue indicates the crashing fragment and red the interfering one.]{
    \texttt{
      \begin{tikzpicture}
        \path [use as bounding box] (0,0) rectangle (0,1);
        \draw [fill, color=blue!20] (1.3,0.4) rectangle (7,2.8);
        \path [pattern color=red, pattern=diagonal hatch] (1.3,1) rectangle (7,0.4);
        \draw [fill, color=red!20] (1.3,0.4) rectangle (7,-0.8);
        \draw [fill, color=blue!20] (1.3,-2.1) rectangle (2.5,-1.4);
      \end{tikzpicture}
      \begin{tabular}{lllll}
          & \multicolumn{4}{l}{\_Unwind\_ForcedUnwind() \{}\\
        1 & & \multicolumn{3}{l}{l = forcedunwind;}\\
        2 & & \multicolumn{2}{l}{if} & (l == NULL \&\&\\
        3 & & & &\hspace{3mm}\!done\_init) \{\\
        4 & & & \multicolumn{2}{l}{l = \_forcedunwind\_impl;} \\
        5 & & & \multicolumn{2}{l}{forcedunwind = l;} \\
        6 & & & \multicolumn{2}{l}{done\_init = 1;}\\
        7 & & \multicolumn{3}{l}{\}}\\
        8 & & \multicolumn{3}{l}{l();}\\
          & \multicolumn{4}{l}{\}}\\
        \\
      \end{tabular}
    }
  }
  \caption{Source code for the glibc test case.}
  \label{fig:eval:glibc}
\end{figure}


\todo{Placement of delays is quite subtle here to avoid a deadlock.
  Problem is that I haven't come up with a good way of explaining it,
  and there isn't an easily generalisable moral.  Meh.}

%% \begin{figure}
%%   \begin{tikzpicture}
%%     \node (A) {Thread 1};
%%     \node (CA) [rectangle,draw, below = 2 of A, minimum size = 4cm] {Crashing fragment};
%%     \node (IA) [rectangle,draw, below = of CA, minimum size = 4cm] {Interfering fragment};
%%     \node (dummyA1) at (0,-12.1) {};

%%     \draw[->] (CA) -- (IA);
%%     \draw[->] (IA) -- (dummyA1);
%%     \draw[->] (0,-12.5) .. controls (0,-14.5) and (-2.5, -14.5) .. (-2.5,-12.5) -- (-2.5,-2.3) .. controls (-2.5, -0.3) and (0,-0.3) .. (0,-2.3);


%%     \node (B) [right = 6cm of A] {Thread 2};
%%     \node (CB) [rectangle,draw, below = 2 of B, minimum size = 4cm] {Crashing fragment};
%%     \node (IB) [rectangle,draw, below = of CB, minimum size = 4cm] {Interfering fragment};
%%     \node (dummyB1) at (7.9,-12.1) {};

%%     \draw[->] (CB) -- (IB);
%%     \draw[->] (IB) -- (dummyB1);
%%     \draw[->] (7.9,-12.5) .. controls (7.9,-14.5) and (10.4, -14.5) .. (10.4,-12.5) -- (10.4,-2.3) .. controls (10.4, -0.3) and (7.9,-0.3) .. (7.9,-2.3);

%%     \filldraw (-2,-12) rectangle (10,-12.5);

%%     \draw[->,happensBeforeEdge] (IB) -- (CA);
%%     \draw[->,happensBeforeEdge] (IA) -- (CB);
%%   \end{tikzpicture}
%%   \caption{Happens-before structure of the glibc test.  Note that the
%%     happens-before edges here are those imposed implicitly by the
%%     enforcer in order to run the code fragments in parallel, and not
%%     those of the bug itself.}
%%   \label{fig:eval:glibc:structure}
%% \end{figure}

%% \begin{figure}
%%   \texttt{
%%     \begin{tabular}{ll}
%%       \multicolumn{2}{l}{while (1) \{}\\
%%       &pthread\_barrier();\\
%%       &\_Unwind\_ForcedUnwind();\\
%%       &pthread\_barrier();\\
%%       &done\_init = 0;\\
%%       &forcedunwind = NULL;\\
%%       \multicolumn{2}{l}{\}}
%%     \end{tabular}
%%   }
%%   \caption{Test harness for the glibc test case.}
%%   \label{fig:eval:glibc:harness}
%% \end{figure}

%% This bug demonstrates a subtlety in the placement of delays (see
%% Section~\ref{sect:using:timeout_balancing}): it will not reproduce if
%% every message operation is given the same timeout.  To understand why,
%% consider the happens-before graph shown in
%% Figure~\ref{fig:eval:glibc:structure}.  The crashing program fragment,
%% when run in thread 1, must wait for the interfering fragment in thread
%% 2.  At the same time, the crashing fragment in thread 2 must wait for
%% the interfering fragment in thread 1.  Since the interfering fragment
%% always runs after the crashing one, we have a deadlock.  This is
%% exacerbated by the structure of the test harness, shown in
%% Figure~\ref{fig:eval:glibc:harness}, which imposes a barrier between
%% every iteration of the test and acts to re-synchronise threads;
%% without that, they would eventually drift far enough out of synchrony
%% to avoid the issue, but not until after a significant delay.
%% {\Implementation}'s policy of strongly prefer not to delay at both
%% ends of a message operation.


\subsection{labyrinth}

This test consists of the labyrinth component of the STAMP benchmark
suite\needCite{}, converted to use locks rather than transactional
memory and with one of its critical sections removed.  It is shown in
Figure~\ref{fig:eval:labyrinth}.  This program is structured as a
read-modify-writeback operation: the \texttt{memcpy} on line 8 takes a
local copy of a shared structure, which is then worked on by
\texttt{PdoExpansion} and \texttt{PdoTraceback}, generating results
which can be written back by \texttt{TMgrid\_addPath}.  Before
performing the writeback, \texttt{TMgrid\_addPath} first checks that
no other threads have generated the path which it is adding, crashing
if they have.  Removing the lock operations on lines 7 and 15
introduces a race which can trigger this crash.

\begin{figure}
  \texttt{
    \begin{tabular}{lllll}
        & \multicolumn{4}{l}{TMgrid\_addPath(grid\_t *gridPtr, vector\_t *pointVectorPtr) \{}\\
      1 & & \multicolumn{3}{l}{for (i = 1; i < pointVectorPtr->size() - 1; i++) \{}\\
      2 & & & \multicolumn{2}{l}{long *gridPointPtr = pointVectorPtr->get(i);}\\
      3 & & & \multicolumn{2}{l}{assert(*gridPointPtr == GRID\_POINT\_EMPTY);}\\
      4 & & & \multicolumn{2}{l}{*gridPointPtr = GRID\_POINT\_PTR;}\\
      5 & & \multicolumn{3}{l}{\}}\\
      6 & \multicolumn{4}{l}{\}}\\
      \\
      7  & \multicolumn{4}{l}{acquire\_lock();}\\
      8  & \multicolumn{4}{l}{memcpy(myGridPtr, gridPtr);}\\
      9  & \multicolumn{4}{l}{if (PdoExpansion(myGridPtr)) \{}\\
      10 & & \multicolumn{3}{l}{pointVectorPtr = PdoTraceback(myGridPtr);}\\
      11 & & \multicolumn{3}{l}{if (pointVectorPtr) \{}\\
      12 & & & \multicolumn{2}{l}{TMgrid\_addPath(gridPtr, pointVectorPtr);}\\
      13 & & \multicolumn{3}{l}{\}}\\
      14 & \multicolumn{4}{l}{\}}\\
      15 & \multicolumn{4}{l}{release\_lock();}\\
    \end{tabular}
  }
  \caption{The labyrinth test. \texttt{PdoExpansion} and
    \texttt{PdoTraceback} are thread-local functions.}
  \label{fig:eval:labyrinth}
\end{figure}

{\Implementation} is not able to generate a candidate bug for this
test, and is therefore unable to generate an enforcer or a fix for it.
The reason is simple: \texttt{PdoExpansion} and \texttt{PdoTraceback}
are both large functions, and {\implementation} takes an unreasonable
amount of time and memory to analyse them\editorial{Should probably
  say what I mean by unreasonable there; answer is a couple of days
  and all the memory in the machine, respectively.}.  It is therefore
unable to link the load of \texttt{*gridPointPtr} on line 3 to the
store in the \texttt{memcpy} on line 8, preventing it from discovering
the bug's concurrency behaviour.

\todo{Should probably say more about this e.g. what \backref{$\alpha$}
  you'd need to analyse it and how far I was actually able to push
  it.}

\subsection{bayes}

This test is, like labyrinth, formed by taking one of the STAMP
benchmark programs, in this case bayes, and removing one of the
critical sections.  A simplified version of the original program is
shown in Figure~\ref{fig:eval:bayes}.  It implements a task queue as a
singly-linked list with a C++-like iterator protocol\needCite{}.  When
a worker thread is ready to start a new task, it gets the first
element from the list (line 14), checks whether the list was empty
(line 15), and, if it was not, removes the element from the list (line
16) and proceeds to use it.  The \texttt{list\_remove} operation works
by scanning the list (\texttt{findPrevious}, line 1) to find the
element prior to the one to be removed (which in this case will be a
special dummy head element), using that to check whether the element
to be removed is present (line 3) and, if it is, removing the target
element from the list (lines 4 to 8).

\begin{figure}
  \texttt{
    \begin{tabular}{llll}
      & \multicolumn{3}{l}{bool list\_remove(list, what) \{}\\
      1 & & \multicolumn{2}{l}{prev = findPrevious(list, what);}\\
      2 & & \multicolumn{2}{l}{node = prev->next;}\\
      3 & & \multicolumn{2}{l}{if ((node != NULL) \&\& list->compare(node->data, what) == 0) \{} \\
      4 & & & prev->next = node->next;\\
      5 & & & node->next = NULL;\\
      6 & & & free(node);\\
      7 & & & list->size -= 1;\\
      8 & & & assert(list->size >= 0);\\
      9 & & & return TRUE;\\
      10 & & \multicolumn{2}{l}{\}}\\
      11 & & \multicolumn{2}{l}{return FALSE;}\\
      12 & \multicolumn{3}{l}{\}}\\
      \\
      13 & \multicolumn{3}{l}{acquire\_lock();}\\
      14 & \multicolumn{3}{l}{it = taskList->begin();}\\
      15 & \multicolumn{3}{l}{if (it != taskList->end();) \{}\\
      16 & & \multicolumn{2}{l}{status = list\_remove(taskList, it->get());}\\
      17 & & \multicolumn{2}{l}{assert(status);}\\
      18 & \multicolumn{3}{l}{\}}\\
      19 & \multicolumn{3}{l}{release\_lock();}\\
    \end{tabular}
  }
  \caption{The bayes test program.}
  \label{fig:eval:bayes}
\end{figure}

For this test, I removed the lock operations on lines 13 and 19.
This introduced several possible crashing bugs:

\begin{itemize}
\item The assertion on line 8 would sometimes fire, if the size of the
  list became negative.
\item The assertion on line 17 would sometimes fire, if a list removal
  failed because the relevant item had already been removed.
\item Similarly, the \texttt{free} on line 6 would sometimes release
  already-released memory, leading to a crash in the C library, if
  the same item was removed twice.
\item Other assertions later in the program also occasionally fired
  due to running the same task twice.
\end{itemize}

\todo{I haven't run this test since I added double-free support to the
  tool.  Should fix that.}

The first two bugs are of the correct form to be investigated by
{\technique}; the other two are not.  I therefore generated enforcers
for the first two bugs which, as expected, caused those bugs to
reproduce rapidly and reliably (i.e. when one of those enforcers was
applied the target bug consistently reproduced before any of the other
bugs had a chance to).  I also generated fixes for the two bugs.
These both worked in the sense that they were able to prevent the
target bug from reproducing.  Unfortunately, the fix for the first bug
also made the double-free bug reproduce far more frequently, and so
the overall effect was that the program crashed more quickly.  On the
other hand, the fix for the second bug fixed all four bugs, as the
critical sections introduced by the fix were large enough to
completely cover the actual critical sections in the program.

\todo{More discussion of how the fixes work should go here.}

\todo{Somewhat surprisingly, given that it was taken from what's
  nominally a benchmark suite, unmodified bayes shows multiple order
  of magnitude performance differences from run to run in the default
  configuration, which makes it quite hard to talk about the
  performance cost of the fix.  I could just average enough runs that
  the SD drops to something sane, but I don't think that would be
  terribly meaningful.}

\section{Experiments with real programs}
\label{sect:eval:real}

\subsection{pbzip2}

I ran {\implementation} in its bug-finding mode on pbzip2.  This
included building a program model, generating all {\StateMachines},
converting them to enforcers, and then running pbzip2 under each
enforcer in turn.  This did not find any bugs in pbzip2.

\todo{This is a bit sad.  I'm just going to put in some measurements
  of how long things take to run in here.}

\subsection{Thunderbird}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{llll}
        & \multicolumn{3}{l}{ProcessCurrentURL() \{}\\
        & & \multicolumn{2}{l}{if (m\_transport) \{}\\
        & & & m\_transport->SetTimeout();\\
        & & \}\\
        & \}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{lll}
        & \multicolumn{2}{l}{CloseStreams() \{}\\
        & & m\_transport = NULL;\\
        & \multicolumn{2}{l}{\}}\\
        \\
        \\
      \end{tabular}
    }
  }
  \caption{The thunderbird bug.}
  \label{fig:eval:thunderbird}
\end{figure}

\verb|thunderbird| is Mozilla bug number
391259\cite{thunderbird39125}, a simple time-of-check, time-of-use
race in the IMAP client component of Thunderbird, a popular
open-source e-mail client.  The relevant parts of the program are
shown in Figure~\ref{fig:eval:thunderbird}.  If \verb|m_transport| is
set to \verb|NULL| by \verb|CloseStreams()| in between the two
accesses in \verb|ProcessCurrentURL| then the program will crash.  The
simplest way to trigger this behaviour is for the user to click on an
IMAP folder and to then immediately close Thunderbird.

This is essentially the same bug as simple\_toctou, but embedded in a
much large program.  As such, {\implementation} produces a very
similar candidate bug, bug enforcer, and fix.  The enforcer is,
however, much more difficult to use in this case.  Triggering this bug
requires user interaction, and it is difficult to perform the
necessary operations in {\implementation}'s default 200ms timeout.
Increasing the timeout to five seconds made it trivial to trigger the
desired behaviour: of ten attempts with the enforcer loaded and using
a five second timeout, every one reproduced the bug.  Without the
enforcer, or using the 200ms timeout, ten attempts to reproduce the
bug all failed.  The code involved in the bug runs quite rarely, and
in a background thread, and so the user interface was perfectly usable
even with this very long delay.

This does, however, illustrate one weakness of the {\technique}
approach: it is difficult to apply to programs which do not have a
good automated test suite.  {\Technique} generates a very large number
of false positives in its main analysis phase and relies on the
enforcers to distinguish them from real bugs, but this is only
effective if the enforcers can be run automatically, which is in turn
only possible when it is possible to exercise some reasonable
proportion of the program automatically.  Using {\technique}'s
bug-finding mode on Thunderbird would be completely infeasible for
this reason: it produces several thousand candidate bugs, which is
tolerable if each can be dismissed with a few seconds of further
computation, but not if each requires several minutes of user
interaction.

\subsection{mysql}

This is MySQL bug 56324\needCite{}.  A simplified version of the buggy
code is shown in Figure~\ref{fig:eval:mysqld}.  This is, again, a
variant of the simple\_toctou bug, embedded in a much larger program.
As expected, {\implementation} is able to find the bug, generate an
enforcer, and generate a fix, and they all behave as expected.

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
      \begin{tabular}{ll}
        \multicolumn{2}{l}{if (PSI\_server) \{}\\
        & PSI\_server->delete\_current\_thread();\\
        \}\\
      \end{tabular}
    }
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{l}
        \\
        PSI\_server = NULL;\\
        \\
      \end{tabular}
    }
  }
  \caption{The mysqld bug.}
  \label{fig:eval:mysqld}
\end{figure}

{\Implementation} can also be used in bug-finding mode on mysqld, as
it has an extensive automated test suite.  In this mode it was able to
locate both the bug given here, and was also able to locate several
other, similar, races on \texttt{PSI\_server} which were unknown to
the author before writing the tool.  This suggests that
{\implementation} can sometimes be used to find previously-unknown
bugs, as desired.

\todo{Complication: this bug is only present in builds of mysql which
  don't have compiler optimisations, because otherwise the compiler
  caches \texttt{PSI\_server} in a register and avoids the crash.}

\section{Detailed performance analysis}
\label{sect:eval:time_details}

Basic approach here is going to be to take a random sample of the
instructions in mysql, run the analysis on them, and measure how much
time is spent in each phase.  I think I'll probably have to say
something about the distributions there, since you usually find that
you spend a lot of your time in the outliers.  I'll then show how that
changes when you change $\alpha$.  Also need to show how the number of
candidates varies with $\alpha$.

I'd also like to show how the different variants affect performance.
That'll mean doing compare-and-contrast on variants with:

\begin{itemize}
\item Base thing
\item No dynamic aliasing analysis
\item No static aliasing analysis, fixed regs on
\item No fixed regs analysis, static aliasing on
\item Neither fixed regs nor static aliasing
\item All the clever bits on, W isolation on
\end{itemize}

The performance effect should be a small win, and we should see that
the quality of the results goes up.

\subsection{Dynamic analysis}

I want to say something about how good/bad the dynamic analysis is.
Things to measure:

\begin{itemize}
\item How much it slows programs down.  The answer is ``a lot'', but
  it'd be good to quantify that.
\item How long it takes to converge.  That'll be a graph with time
  running under analysis on the x-axis and number of aliasing pairs on
  the y axis, showing that it tends to a constant fairly quickly.
  I've already done that for thunderbird; should probably do it for
  mysql and pbzip as well.  The artificial tests converge too quickly
  to be interesting.
\item Something about the necessary level of code coverage.  My plan
  there is just to run all of the mysql tests one after another and
  see how much commonality there is between them.  This will be
  normalised for code coverage.  My metric of badness is the number of
  missing edges between A and B where A and B were both observed.
\end{itemize}
