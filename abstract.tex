Major todo items:

\begin{itemize}
\item Need a new introduction.
\item The entire eval section needs a lot more work.
\item Related work is a bit of a mess.
\item Conclusions are missing.
\end{itemize}


\section{Eval plan}

Things I want to show in the eval:

\begin{itemize}
\item That it works for some simple test cases.  This'll include a
  reasonable amount of discussion about what it actually does.
  This'll include showing that the generated fixes work, and some bits
  about their relative performance.  I'd also like to discuss
  whole-program vs. core dump.  This will also include a comparison to
  my implementation of DataCollider.

  Things to show:

  \begin{itemize}
  \item That simple bugs reproduce much more quickly with an enforcer
    than without\smh{Comparison to simpler schemes}.  It'd also be
    nice to show that they reproduce more quickly under SLI than under
    DataCollider.
  \item That the analysis to produce them is nice and cheap.
  \item That the performance overhead of the enforcer when the bug
    doesn't reproduce isn't so massive that it'll make bugs less
    likely to reproduce.
  \item That the side-condition checking stuff is both necessary and
    effective.\smh{+what it is}
  \item That we can generate ``reasonable-looking'' fixes for all of
    the test bugs.
  \item That the fixes generated actually fix the intended bug.

  \item That the fixes generated have vaguely plausible performance
    overhead.\smh{Top bit has to be ``technique can fix x\% of bugs in
      the test set, with an average y\% overhead.}\smh{Second most
      important eval is pushing the limits i.e. why does SLI not work
      for large program or multiple threads or ... . Plus how might
      new research help fix this.}
  \end{itemize}

  It'd also be quite nice if I could show that the summaries are
  useful for a human trying to fix the bug, but that's likely to be a
  rather hard argument to make.

  It might also be worth trying some pseudo-fixes which do everything
  except for acquiring and releasing the lock, just so that I can
  isolate how much of the overhead is the patching machinery and how
  much the lock operation itself.\smh{Sure}

\item Show that it works for some slightly more realistic artificial
  bugs, probably from ripping sync blocks out of STAMP.
  i.e. real-size programs with artificial bugs.  I currently have one
  bug produced by removing synchronisation from one of the STAMP
  benchmarks for which this works reasonably well; should try to
  expand that a bit.\smh{Ack}

  Result of trying to expand it a bit: I now have something where it
  doesn't work.  Not terribly helpful, but what the hell.

\item That it could feasibly scale up to something of realistic size.
  Obvious approach here is just to show that we can run the full
  analysis on mysql, thunderbird, and pbzip2.  It's currently looking
  like this will find at most one or two new bugs, which makes it all
  a bit less convincing.  This'll be mostly just showing that the
  analysis does actually complete in a sensible amount of
  time.  \smh{Yes; this will be very nice.}

\item I'm going to pick one of those big programs (probably mysql,
  since that's where I've done the most testing so far) and do a few
  more experiments on it:

  \begin{itemize}
  \item Display a CDF of how long it takes to analyse each
    instruction.\smh{Why?}
  \item Try turning the various machine simplification passes on and
    off and see what effect that has on the CDF.  Likewise the
    induction rule.\smh{Ok}
  \item Likewise changing the size of the analysis window.
  \item Just directly look at how long the various phases
    take.\smh{Ack}
  \item Alias analysis: try turning off the static analysis bit and
    see how much effect that has.  Likewise try running without the
    dynamic analysis; the results will be very bad, but it'd be
    interesting to put a number on how bad is bad.  It'd also be worth
    looking at how bad it is if you use the dynamic analysis to
    resolve stack accesses.
  \item Try turning off Phi elimination and see how much that costs
    us.
  \item Look at how the number of candidate bugs varies as the
    analysis window changes.
  \item Look at how the time taken to do the analysis varies as the
    analysis window changes.
  \end{itemize}

  Aim here is less proving some hypothesis and more just giving a
  decent feeling for where the main costs are.
\item I really want something to show how much the W isolation
  property costs us, in terms of bugs detected, and how much it buys
  us, in terms of simpler analysis.  The implementation is such that
  turning it on and off is fairly feasible, although I haven't tried
  it in a while.\smh{This may be interesting to eval --- but it
    certainly is something you should explain/justify in English prose
    much \emph{much} earlier.}

\item At some point I'm going to need to do a sensitivity analysis to
  show that the timeouts don't need to be tuned particularly finely to
  get bugs to repro.  Not sure how to do that.  All of the artificial
  bugs work fine for timeouts between 10ms and 1 second, but that
  isn't particularly convincing by itself.
\item I need to show that the dynamic analysis is reasonable\smh{?}.
  Ideally that'd be by using a static analysis to get an upper bound
  and then showing that they're reasonably close, but implementing
  that'd be a pain in the backside.  Alternative is just to look at
  convergence rates.  I should also look at how large the resulting
  alias tables are for different
\item I need to show that the tool is actually correctly implemented
  itself.  The small number of bugs found in real programs means that
  this has more than a little bit of a negative-results flavour, so
  this isn't completely trivial.\smh{Hmm -- not sure how to do this??
    Formal proofoid?}
\end{itemize}

So, here are the graphs and tables I need to be present in the final
eval:

\begin{itemize}
\item Artificial/semi-artificial bugs section:
  \begin{itemize}
  \item Table with a row for each bug and columns for how
    long it takes to find the bug, how long it takes to build the
    enforcer, how long it takes to build the fix, time to repro without
    enforcer, time to repro with enforcer, time to repro with NDC,
    overhead of enforcer, overhead of fix.
  \item For each bug, CDF of how long it takes to reproduce with
    enforcer, without enforcer, and with NDC.
  \item For the indexed\_toctou bug, fan charts showing how
    time-to-repro varies with NR\_PTRS for no enforcer, standard
    enforcer, and no-side-condition enforcer.
  \item For write\_to\_read, I want to investigate overhead with a
    pseudo-fix which skips the actual lock operation.
  \end{itemize}
\item Real programs: Just say how long it takes to do the full
  analysis on each one, the number of candidates discovered, the
  number of bugs reproduced, and the time taken to run the
  reproduction phase.
\item Dynamic analysis: measure overhead of running the analysis on
  some of the artificial bugs.  Quote the size of the types table
  generated for each program (real+artificial), both in megabytes and
  the number of entries.  Show a graph of how the types table grows
  over time for the real programs (artificial ones converge too
  quickly to be interesting).  Show a graph of how the types table
  grows ``horizontally'' for mysqld as you introduce more tests from
  the test harness.
\item I want a fan chart showing how time-to-analyse for mysql
  instructions changes as the analysis window changes.  That'll have
  points for 5, 10, 20, 50, 100, 500, 1000 instruction windows.  I'll
  have to do that for a small sampling of instructions to keep it
  computationally feasible; I'd guess a hundred would be reasonable.
\item Phase analysis.  I want a stacked CDF showing how the time for
  the analysis breaks down into the different phases, analysing one
  instruction at a time, for a full run of mysqld.  I'd also like
  something showing the effects of changing the different
  optimisations, which'll be another CDF with one line for each
  condition:

  \begin{itemize}
  \item All optimisations enabled.
  \item Load elimination disabled, phi enabled.
  \item Phi elimination disabled, load enabled.
  \item Both disabled.
  \item All optimisations enabled but ignoring the results of the
    static analysis.
  \item All optimisations enabled but ignoring the results of the
    dynamic analysis.
  \end{itemize}

  Those'll be taken on a random sampling of 1000 instructions from
  mysqld, probably with a short timeout, just so that they don't take
  forever to collect.  If I have space, I'd also like a couple more
  graphs:

  \begin{itemize}
  \item One with a bit more detail on load elimination.  In
    particular, comparing current load elim to the previous load elim.
  \item Something to explore the interaction of Phi elimination and
    the SSA conversion.  It'd be nice to show how much of phi elim's
    win is just down to my using a slightly stupid phi introduction
    algorithm.
  \end{itemize}
\end{itemize}
