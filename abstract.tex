Major todo items:

\begin{itemize}
\item Need a new introduction.
\item The entire eval section needs a lot more work.
\item Related work is a bit of a mess.
\item Conclusions are missing.
\end{itemize}


\section{Eval plan}

Things I want to show in the eval:

\begin{itemize}
\item I need to show that the tool is actually correctly implemented
  itself.  The small number of bugs found in real programs means that
  this has more than a little bit of a negative-results flavour, so
  this isn't completely trivial.\smh{Hmm -- not sure how to do this??
    Formal proofoid?}
\end{itemize}

So, here are the graphs and tables I need to be present in the final
eval:

\begin{itemize}
\item Artificial/semi-artificial bugs section:
  \begin{itemize}
  \item Table with a row for each bug and columns for how
    long it takes to find the bug, how long it takes to build the
    enforcer, how long it takes to build the fix, time to repro without
    enforcer, time to repro with enforcer, time to repro with NDC,
    overhead of enforcer, overhead of fix.
  \item For each bug, CDF of how long it takes to reproduce with
    enforcer, without enforcer, and with NDC.
  \item For the indexed\_toctou bug, fan charts showing how
    time-to-repro varies with NR\_PTRS for no enforcer, standard
    enforcer, and no-side-condition enforcer.
  \item For write\_to\_read, I want to investigate overhead with a
    pseudo-fix which skips the actual lock operation.
  \end{itemize}
\item Real programs: Just say how long it takes to do the full
  analysis on each one, the number of candidates discovered, the
  number of bugs reproduced, and the time taken to run the
  reproduction phase.
\item Dynamic analysis: measure overhead of running the analysis on
  some of the artificial bugs.  Quote the size of the types table
  generated for each program (real+artificial), both in megabytes and
  the number of entries.  Show a graph of how the types table grows
  over time for the real programs (artificial ones converge too
  quickly to be interesting).  Show a graph of how the types table
  grows ``horizontally'' for mysqld as you introduce more tests from
  the test harness.
\item I want a fan chart showing how time-to-analyse for mysql
  instructions changes as the analysis window changes.  That'll have
  points for 5, 10, 20, 50, 100, 500, 1000 instruction windows.  I'll
  have to do that for a small sampling of instructions to keep it
  computationally feasible; I'd guess a hundred would be reasonable.
\item Phase analysis.  I want a stacked CDF showing how the time for
  the analysis breaks down into the different phases, analysing one
  instruction at a time, for a full run of mysqld.  I'd also like
  something showing the effects of changing the different
  optimisations, which'll be another CDF with one line for each
  condition:

  \begin{itemize}
  \item All optimisations enabled.
  \item Load elimination disabled, phi enabled.
  \item Phi elimination disabled, load enabled.
  \item Both disabled.
  \item All optimisations enabled but ignoring the results of the
    static analysis.
  \item All optimisations enabled but ignoring the results of the
    dynamic analysis.
  \end{itemize}

  Those'll be taken on a random sampling of 1000 instructions from
  mysqld, probably with a short timeout, just so that they don't take
  forever to collect.  If I have space, I'd also like a couple more
  graphs:

  \begin{itemize}
  \item One with a bit more detail on load elimination.  In
    particular, comparing current load elim to the previous load elim.
  \item Something to explore the interaction of Phi elimination and
    the SSA conversion.  It'd be nice to show how much of phi elim's
    win is just down to my using a slightly stupid phi introduction
    algorithm.
  \end{itemize}
\end{itemize}
