\chapter{Reproducing bugs}
\label{sect:reproducing_bugs}

The previous chapter showed how to derive
\glspl{verificationcondition} which characterise the precise
conditions under which a program might suffer a particular type of
bug.  I now show how to use the \glspl{verificationcondition} build
\glspl{bugenforcer} which shepherd programs towards schedules in which
these bugs reproduce easily.  These run-time enforcers complement the
local analysis of the previous chapter, allowing {\technique} to
incorporate global dynamic properties of the program, completely
eliminating the many false positives generated by the {\StateMachine}
analysis.

\begin{sanefig}
  \begin{tabular}{ll}
    \multicolumn{2}{c}{\texttt{int *global\_ptr[];~~~~~~~~~}}\\
    \hspace{-5mm}\subfigure[][Crashing thread source]{
      \texttt{
        \begin{tabular}{lll}
          \multicolumn{3}{l}{void crashing(int idx1) \{}\\
          &\multicolumn{2}{l}{if (global\_ptr[idx1])}\\
          &&*global\_ptr[idx1] = 7;\\
          \}\\
        \end{tabular}
      }
    } & \hspace{-5mm}%
    \subfigure[][Interfering thread source]{
      \texttt{
        \begin{tabular}{ll}
          \multicolumn{2}{l}{void interfering(int idx2) \{}\\
          &global\_ptr[idx2] = NULL;\\
          \}\\
          \\
        \end{tabular}
      }
    }\\
    \hspace{-5mm}\subfigure[][Crashing thread machine code]{
      \texttt{
        \begin{tabular}{rlll}
          & \multicolumn{3}{l}{crashing:} \\
          \rm A: & ADD & global\_ptr+idx1 & \!\!\!$\rightarrow$ reg1 \\
          \rm B: & LOAD & *reg1 & \!\!\!$\rightarrow$ reg2 \\
          \rm C: & CMP & 0, reg2 \\
          \rm D: & JE & \rm G \\
          \rm E: & LOAD & *reg1 & \!\!\!$\rightarrow$ reg3 \\
          \rm F: & STORE & 7 & \!\!\!$\rightarrow$ *reg3 \\
          \rm G:
        \end{tabular}
      }
    } & \hspace{-5mm}%
    \subfigure[][Interfering thread machine code]{
      \texttt{
        \begin{tabular}{rlll}
          & \multicolumn{3}{l}{interfering:} \\
          \\
          \\
          \rm H: & ADD & global\_ptr+idx2 & \!\!\!$\rightarrow$\!\! reg4 \\
          \rm J: & STORE & 0 & \!\!\!$\rightarrow$\!\! *reg4 \\
          \\
          \\
          \\
        \end{tabular}
      }
    }\\
  \end{tabular}
  \caption{Threads which, when run in parallel, might exhibit an
    atomicity violation.}
  \label{fig:enforce:example_threads}
\end{sanefig}

\begin{sanefig}
  {\hfill}
  \begin{tikzpicture}
    \node[CfgInstr] (l1) {$A_0$: \tt ADD global\_ptr + idx1 $\rightarrow$ reg1};
    \node[CfgInstr, below=of l1] (l2) {$B_0$: \tt LOAD *reg1 $\rightarrow$ reg2};
    \node[CfgInstr, below=of l2] (l3) {$C_0$: \tt CMP 0, reg2};
    \node[CfgInstr, below=of l3] (l4) {$D_0$: \tt JMP\_IF\_EQ };
    \node[CfgInstr, below=of l4] (l5) {$E_0$: \tt LOAD *reg1 $\rightarrow$ reg3 };
    \node[CfgInstr, below=of l5] (l6) {$F_0$: \tt STORE 7 $\rightarrow$ *reg3 };
    \node[CfgInstr, below=of l6] (l7) {};
    \node[CfgInstr] at (7.5,-.8) (l8) {$H_0$: \tt ADD global\_ptr + idx2 $\rightarrow$ reg4};
    \node[CfgInstr, below=26.5mm of l8] (l9) {$J_0$: \tt STORE 0 $\rightarrow$ *reg4};
    \draw[->] (l1) -- (l2);
    \draw[->] (l2) -- (l3);
    \draw[->] (l3) -- (l4);
    \draw[->,ifFalse] (l4) -- (l5);
    \draw[->,ifTrue] (l4.west)
      -- ++(-0.7,0)
      .. controls ++(-.3,0) and ++(0,.3) .. ++(-.5,-.5)
      -- ++(0,-3.9)
      .. controls ++(0,-.3) and ++(-.3,0) .. ++(.5,-.5)
      -- (l7.west);
    \draw[->] (l5) -- (l6);
    \draw[->] (l6) -- (l7);
    \draw[->] (l8) -- (l9);
    \draw[->,happensBeforeEdge] (l2) to node [pos=.65, above=3.5mm] {$\texttt{idx1} = \texttt{idx2}$} (l9);
    \draw[->,happensBeforeEdge] (l9) -- (l5);
  \end{tikzpicture}
  {\hfill}
  \caption{The \gls{verificationcondition} generated from the program
    in \autoref{fig:enforce:example_threads}, showing the \gls{dynamic
      cfg} (as solid and dotted lines), the happens-before graph (as
    dashed lines) and the \gls{side condition} ($\texttt{idx1} =
    \texttt{idx2}$).}
  \label{fig:using:example_hb_graph}
\end{sanefig}

Consider, for example, the threads shown in
\autoref{fig:enforce:example_threads}.  There is a risk here that the
crashing thread might crash if \verb|l9| is interleaved between
\verb|l2| and \verb|l5| and \texttt{idx1} and \texttt{idx2} coincide.
The previous analysis phase will detect this bug and produce the
verification condition $\texttt{idx1} = \texttt{idx2} \wedge
\happensBefore{l2}{l9} \wedge \happensBefore{l9}{l5}$, illustrated by
the augmented happens-before graph in
\autoref{fig:using:example_hb_graph}.  In order for the bug to
reproduce, the program must follow the happens-before graph shown as
dashed lines whilst the side condition $\texttt{idx1} = \texttt{idx2}$
holds; imposing the correct happens-before graph when the
side condition is false will not reproduce the bug, and neither will
arranging for the side condition to be true if the schedule does not
match the happens-before graph.

As discussed in \autoref{sect:intro:theory_of_fixing}, {\technique}
only modifies the behaviour of running programs in ways which are
``safe'', defined to be equivalent to inserting delays into its
execution.  As such, it can easily impose the desired happens-before
graph but cannot directly influence the likelihood of the
side condition holding.  It might, therefore, appear that knowing the
side condition is unhelpful.  This is not the case.  Imposing a
happens-before graph necessarily implies slowing down one or more of
the program's threads, which necessarily means that the program will
run more slowly and that the buggy code will run less frequently; if
the happens-before graph reproduces easily but the side condition
passes rarely then this could actually \emph{increase} the time needed
to reproduce the bug.  Careful use of the side condition can reduce
the number of delays which the \glspl{bugenforcer} must insert into
the program, largely avoiding this problem.

\section{Outline of algorithm}

At a high level, the algorithm used has the following phases:
\begin{itemize}
\item
  Extract the happens-before graphs and side conditions from the
  \gls{verificationcondition}.  This is discussed in
  \autoref{sect:enforce:slice_hb_graph}.
\item
  Plan how to evaluate the side condition part of the verification
  condition.  This is discussed in \autoref{sect:enforce:place_vcs}.
\item
  Decide on a strategy for gaining control of the program at
  appropriate points.  The mechanism which {\technique} uses to do so
  is described in \autoref{sect:enforce:gain_control}.
\item
  Combine the results of the previous phases with the \gls{plan
    interpreter}, discussed in \autoref{sect:enforce:interpreting},
  and compile the result into the final \gls{bugenforcer}.
\end{itemize}
The resulting \gls{bugenforcer} can then be loaded into running
program and the two exercised together, hopefully reproducing the bug
under investigation with relatively little manual effort required.

\section{Extracting the happens-before graph}
\label{sect:enforce:slice_hb_graph}

\begin{wrapfigure}{o}{5.2cm}
  \vspace{-12pt}
  \begin{figgure}
  \begin{tikzpicture}
    \node (thread1) {\shortstack{Crashing\\thread}};
    \node (thread2) [right = 0.5 of thread1] {\shortstack{Interfering\\thread}};
    \node (A) [below = 0 of thread2] {$A_1$};
    \node (B) [below = of A] {$B_1$};
    \node (C) [below = of B] {$C_1$};
    \node (D0) [below = 0.7 of thread1] {$x = 7$ $D_0$};
    \node (D1) [below = of D0] {$y = 9$ $D_0$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->,happensBeforeEdge] (A) -- (D0);
    \draw[->,happensBeforeEdge] (D0) -- (B);
    \draw[->,happensBeforeEdge] (B) -- (D1);
    \draw[->,happensBeforeEdge] (D1) -- (C);
  \end{tikzpicture}
  \caption{}
  \label{fig:reproducing:placement_eg_bug}
  \end{figgure}
  \vspace{-12pt}
\end{wrapfigure}
\noindent The first step in building a \gls{bugenforcer} is to extract
the happens-before graphs and \glspl{side condition} from the
\gls{verificationcondition}, so that the \gls{bugenforcer} knows what
instruction ordering it needs to enforce and when it should do so.
Consider, for instance, the graph shown in
\autoref{fig:reproducing:placement_eg_bug}.  This is intended to
indicate that the \gls{crashingthread} has a single instruction,
$D_0$, that the \gls{interferingthread} has three instructions, $A_1$,
$B_1$, and $C_1$, and that the bug reproduces if either $x = 7$ and
$D_0$ happens between $A_1$ and $B_1$ or if $y = 9$ and $D_0$ happens
between $B_1$ and $C_1$.

This bug will produce a \gls{verificationcondition}

\noindent
{\hfill}
\begin{tabular}{lll}
z = & $\happensBefore{A_1}{D_0} \wedge \happensBefore{D_0}{C_1} \wedge (\!\!\!\!$ & $(x = 7 \wedge \happensBefore{D_0}{B_1})\,\vee$ \\
    &                                                                            & $(y = 9 \wedge \happensBefore{B_1}{D_0}))$
\end{tabular}
{\hfill}

\noindent
which must be factorised into a set of happens-before graphs $h_i$,
expressed as a conjunction of $\happensBeforeEdge$ tests, each paired
with a $\happensBeforeEdge$-free side condition $c_i$ such that
$\bigvee_i(h_i \wedge c_i) = z$.  {\Implementation} does so by
constructing a \gls{bdd} representation of the
\gls{verificationcondition} in which all $\happensBeforeEdge$ tests
happen before any non-$\happensBeforeEdge$ one and then examining the
structure of the graph.  Suppose, for example, that the
\gls{verificationcondition} were represented by the \gls{bdd} shown in
\autoref{fig:reproducing:slice_eg:bdd}.  Every path through the initial
$\happensBeforeEdge$-fragment of this diagram corresponds to a
happens-before graph and the remainder of the diagram provides the
side conditions.  In this case, there are four such paths, and so four
happens-before graphs and four side conditions:

\noindent
\centerline{
  \newcommand{\trueRightArrow}{\!\!\raisebox{.6ex}{\tikz{\draw[->,ifTrue] (0,0) -- (0.4,0);}}}
  \newcommand{\falseRightArrow}{\!\!\raisebox{.6ex}{\tikz{\draw[->,ifFalse] (0,0) -- (0.4,0);}}}
  \begin{tabular}{l @{~~~~~~~~~~~~} l @{~~~~~~~~~~~~~~} l @{~~~~~~~~~~~~~} r}
    Path                                                          & $h_i$                                                     & $c_i$   \\
    $\happensBefore{D_0}{B_1}$ \trueRightArrow $\happensBefore{A_1}{D_0}$ \trueRightArrow & $\happensBefore{D_0}{B_1} \wedge \happensBefore{A_1}{D_0}$ & $x = 7$  & \circled{1}\\
    $\happensBefore{D_0}{B_1}$ \trueRightArrow $\happensBefore{A_1}{D_0}$ \falseRightArrow & $\happensBefore{D_0}{B_1} \wedge \happensBefore{D_0}{A_1}$ & $\false$ & \circled{2}\\
    $\happensBefore{D_0}{B_1}$ \falseRightArrow $\happensBefore{D_0}{C_1}$ \trueRightArrow & $\happensBefore{B_1}{D_0} \wedge \happensBefore{D_0}{C_1}$ & $y = 9$  & \circled{3}\\
    $\happensBefore{D_0}{B_1}$ \falseRightArrow $\happensBefore{D_0}{C_1}$ \falseRightArrow & $\happensBefore{B_1}{D_0} \wedge \happensBefore{C_1}{D_0}$ & $\false$ & \circled{4}\\
  \end{tabular}
}

\noindent
In this case, two of the paths, \circled{2} and \circled{4}, produce
side conditions of {\false} and are immediately discarded.  The
\glspl{bugenforcer} will attempt to enforce both of the other two.

\begin{sanefig}
  \centerline{
    \begin{tikzpicture}[node distance = 1.0 and 0]
      \node (n10) [BddNode] {$\happensBefore{D_0}{B_1}$};
      \node (n01) [BddNode, below left = of n10] {$\happensBefore{A_1}{D_0}$};
      \node (n21) [BddNode, below right = of n10] {$\happensBefore{D_0}{C_1}$};
      \node (n02) [BddNode, below = of n01] {$x = 7$};
      \node (n22) [BddNode, below = of n21] {$y = 9$};
      \node (true) [BddLeaf, below = of n02] {\true};
      \node (false) [BddLeaf, below = of n22] {\false};
      \draw[BddTrue] (n10) -- (n01);
      \draw[BddFalse] (n10) -- (n21);
      \draw[BddTrue] (n01) -- (n02);
      \draw[BddFalse] (n01) -- (false);
      \draw[BddTrue] (n02) -- (true);
      \draw[BddFalse] (n02) -- (false);
      \draw[BddTrue] (n21) -- (n22);
      \draw[BddFalse] (n21.east) .. controls +(.3,0) and +(0,.3) .. ++(.5,-.5) -- ++(0,-2.3) ..controls +(0,-.3) and +(.3,0) .. ++(-.5,-.5) -- (false.east);
      \draw[BddTrue] (n22) -- (true);
      \draw[BddFalse] (n22) -- (false);
    \end{tikzpicture}
  }
  \caption{\protect\Gls{bdd} representation of the
    \protect\gls{verificationcondition}.  Solid lines show the path
    taken through the \protect\gls{bdd} when the node conditions are
    true and dotted lines the path when the conditions are false.}
  \label{fig:reproducing:slice_eg:bdd}
\end{sanefig}

This can sometimes be a very expensive algorithm.  In principle, a
single \gls{verificationcondition} with $n$ $\happensBeforeEdge$ tests
could produce up to $2^n$ possible happens-before graphs, each with
its own side condition, and this can become completely unmanageable
for even quite modest values of $n$.  In practice, though, the number
produced is usually far more modest.  Most bugs do not require a
particularly complicated mapping from happens-before graphs to side
conditions, in the sense that they will usually only have a small
number of distinct side conditions, and will only need to test a small
number of happens-before edges to determine which condition to use in
a particular execution.  Provided {\implementation} can identify an
appropriate order in which to test those edges, and hence a suitable
ordering for the \gls{bdd} variables, there is no need to enumerate
all $2^n$ possible paths, and the worst case is avoided.  As I show in
the evaluation, it can do so for slightly over 99\% of
\glspl{verificationcondition} generated from real programs, and so
this algorithm is quite usable in spite of its unfortunate theoretical
worst case.\kern-.9pt\fnote{There is another, less optimistic, reason why this
  usually works: {\implementation} makes extensive use of
  \protect\glspl{bdd} during symbolic execution, and so if the
  \protect\gls{verificationcondition} were one which is hard to
  represent as a \protect\gls{bdd} then one of the symbolic execution
  steps would have failed and it would not have been possible to even
  start building the \protect\gls{bugenforcer}.}

\subsection{Unenforceable graphs}
\label{sect:reproduce:unenforcable}

\begin{sanefig}
  \centerline{
    {\hfill}
  \subfigure[][An unenforceable control-flow and happens-before graph.]{
    \begin{tikzpicture}
      \node (thread0) {Thread 0};
      \node[CfgInstr, below = 1.8 of thread0] (A) {$A_0$};
      \node[CfgInstr, below = of A] (B) {$B_0$};
      \node[right = 2 of thread0] (thread1) {Thread 1};
      \node[CfgInstr, below = 1.8 of thread1] (C) {$C_1$};
      \node[CfgInstr, below = of C] (D) {$D_1$};
      \node[below = 0.5 of D] {};
      \draw[->] (A) -- (B);
      \draw[->] (C) -- (D);
      \draw[->,happensBeforeEdge] (A) to node [above,pos=0.25] {$\alpha$} (D);
      \draw[->,happensBeforeEdge] (C) to node [above,pos=0.25] {$\beta$} (B);
    \end{tikzpicture}
    \label{fig:reproduce:cyclic_synchronous}
  }
    {\hfill}
  \subfigure[][The graph which is actually enforced; the double arrow indicates
    that $C_1$ must happen \emph{immediately} before $B_0$, rather than any time
    before it.]{
    \begin{tikzpicture}
      \node (thread0) {Thread 0};
      \node[CfgInstr, below = of thread0] (A) {$A_0$};
      \node[CfgInstr, below = of A] (B) {$B_0$};
      \node[right = 2 of thread0] (thread1) {Thread 1};
      \node[CfgInstr] at (thread1 |- B) (C) {$C_1$};
      \node[CfgInstr, below = of C] (D) {$D_1$};
      \draw[->] (A) -- (B);
      \draw[->] (C) -- (D);
      \draw[->,double] (C) -- (B);
    \end{tikzpicture}
    \label{fig:reproduce:cyclic_decyclic}
  }
    {\hfill}
  }
  \vspace{-12pt}
  \caption{}
  \label{fig:reproduce:cyclic1}
\end{sanefig}

\noindent
{\Technique} enforces happens-before edges synchronously, in the sense
that to enforce $\happensBefore{A}{B}$ it enforces that $A$ happens
\emph{immediately} before $B$.  This can sometimes restrict the class
of bugs which can be reproduced.  Consider, for instance, the combined
control-flow and happens-before graph in
\autoref{fig:reproduce:cyclic_synchronous}, which shows that both
threads must complete their first instruction before either thread
starts its second.  It is clearly possible for the program to execute
its instructions in this order, but this ordering cannot be enforced
synchronously: in order to enforce edge $\alpha$, $A_0$ must happen
immediately before $D_1$, and so $A_0$ must wait for $D_1$, but in
order to enforce edge $\beta$ $C_1$ must wait for $B_0$.  This creates a
cycle in the graph, and so this program would deadlock if the enforcer
attempted to enforce the entire graph.

{\Technique} resolves these cycles by discarding one of the
happens-before edges, chosen arbitrarily.  In this particular case,
that is sufficient to avoid the problem.  Suppose that {\technique}
selects edge $X$ to discard (the other case is symmetrical).  The
enforcer will now ensure that $C_1$ waits for $B_0$ and then executes
immediately before it, which is also sufficient to ensure that $D_1$
executes after $A_0$, as shown in
\autoref{fig:reproduce:cyclic_decyclic}.  There is, however, no
guarantee that this approach will always work for more complex graphs.

\section{Placing side conditions}
\label{sect:enforce:place_vcs}

Once the \glspl{side condition} have been extracted from the
\gls{verificationcondition}, {\technique} must decide how and when to
evaluate them.  Ideally, this would as soon as the \gls{plan
  interpreter} gains control of the program's execution, so as to
avoid unnecessarily molesting executions which cannot lead to
reproductions of the bug, but this is not always possible.  Most
\glspl{side condition} will involve some cross-thread state, and so
cannot be fully evaluated until the enforcer has determined how
program threads map onto the {\StateMachine}'s crashing and
interfering thread.  Even in the absence of cross-thread constraints,
expressions such as ${\controlEdgeName}()$ cannot be evaluated until
the thread reaches a particular point in its \gls{dynamic cfg}, which
might be some time after the interpreter gains control of it.  The
\gls{bugenforcer} must therefore often defer evaluating part of the
\gls{side condition} until all of the necessary inputs are available.

\begin{sanefig}
  \hspace{6mm}
  \begin{tikzpicture}
    \node (t1) {Thread 1};
    \node[right = 4.5cm of t1] (t2) {Thread 2};
    \node[below = of t1] (A) {$A_0$};
    \node[below = of A] (B) {$B_0$};
    \node[below = of B] (C) {$C_0$};
    \node[below = of C] (D) {$D_0$};
    \node[left = 4cm of D] (D1) {$D_1$};
    \node at (t2 |- A) (X) {$X_0$};
    \node at (t2 |- B) (Y) {$Y_0$};
    \node at (t2 |- D) (Z) {$Z_0$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C) -- (D1);
    \draw[->] (X) -- (Y);
    \draw[->] (Y) -- (Z);
    \draw[->] (D1.south)
      -- ++(0,-.4)
      ..controls +(0,-.3) and +(-.3,0).. ++(.5,-.5)
      -- ++(3.8,0)
      ..controls +(.3,0) and +(0,-.3).. ++(.5,.5)
      -- (D);
    \draw[->,happensBeforeEdge] (B) to node [above] {$\alpha$} (Y);
    \draw[->,happensBeforeEdge] (Z) to node [above] {$\beta$} (D);
  \end{tikzpicture}
  \center{Side condition: $\textsc{rax}_1 = 93 \wedge (\textsc{rbx}_2 =
    72 \vee \controlEdge{1}{C_0}{D_0}) \wedge \textsc{rcx}_1 = \textsc{rcx}_2$}
  \vspace{-4pt}
  \caption{Example control-flow (solid lines) and happens-before
    (dashed lines) graphs.  $D_0$ and $D_1$ are dynamic instructions
    representing the same static instruction (see
    \autoref{sect:derive:build_crashing_cfg}).  They are
    indistinguishable in the original program, but only $D_0$ can
    receive the happens-before edge $\beta$.}
  \label{fig:place_conditions_example}
  \vspace{4pt}
\end{sanefig}

\begin{sanefig}
  \begin{tikzpicture}
    \node (t1) {Thread 1};
    \node[right = 4.5cm of t1] (t2) {Thread 2};
    \node[below = of t1] (A) {$A_0$};
    \node[left = 0 of A] {$\textsc{rax}_1$, $\textsc{rcx}_1$};
    \node[below = of A] (B) {$B_0$};
    \node[left = 0 of B] {$\textsc{rax}_1$, $\textsc{rcx}_1$};
    \node[below = of B] (C) {$C_0$};
    \path (node cs:name=C,anchor=north east) node [below right] {\shortstack[l]{$\textsc{rax}_1$, $\textsc{rcx}_1$,\\$\textsc{rbx}_2$, $\textsc{rcx}_2$}};
    \node[below = of C] (D) {$D_0$};
    \path (node cs:name=D,anchor=north west) node [below left] {\shortstack[r]{$\controlEdge{1}{C_0}{D_0}$,\\$\textsc{rax}_1$, $\textsc{rcx}_1$,\\$\textsc{rbx}_2$, $\textsc{rcx}_2$\phantom{,}}\!\!\!};
    \node[left = 4cm of D] (D1) {$D_1$};
    \path (D1.north) ++(.7,0) node [above] {\shortstack[l]{$\controlEdge{1}{C_0}{D_0}$,\\$\textsc{rax}_1$, $\textsc{rcx}_1$,\\$\textsc{rbx}_2$, $\textsc{rcx}_2$}\!\!\!};
    \node at (t2 |- A) (X) {$X_0$};
    \node[below right = 0 of X.north east] {$\textsc{rbx}_2$, $\textsc{rcx}_2$};
    \node at (t2 |- B) (Y) {$Y_0$};
    \node[below right = 0 of Y.north east] {\shortstack[l]{$\textsc{rax}_1$, $\textsc{rcx}_1$,\\$\textsc{rbx}_2$, $\textsc{rcx}_2$}};
    \node at (t2 |- D) (Z) {$Z_0$};
    \node[below right = 0 of Z.north east] {\shortstack[l]{$\textsc{rax}_1$, $\textsc{rcx}_1$,\\$\textsc{rbx}_2$, $\textsc{rcx}_2$}};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C) -- (D1);
    \draw[->] (X) -- (Y);
    \draw[->] (Y) -- (Z);
    \draw[->] (D1.south)
      -- ++(0,-.4)
      ..controls +(0,-.3) and +(-.3,0).. ++(.5,-.5)
      -- ++(3.8,0)
      ..controls +(.3,0) and +(0,-.3).. ++(.5,.5)
      -- (D);
    \draw[->,happensBeforeEdge] (B) to node [above] {M} node [below] {\shortstack[l]{$\textsc{rax}_1$, $\textsc{rcx}_1$,\\$\textsc{rbx}_2$, $\textsc{rcx}_2$}} (Y);
    \draw[->,happensBeforeEdge] (Z) to node [above] {N} node [below] {\shortstack[c]{$\controlEdge{1}{C_0}{D_0}$,\\$\textsc{rax}_1$, $\textsc{rcx}_1$,\\$\textsc{rbx}_2$, $\textsc{rcx}_2$\phantom{,}}} (D);
  \end{tikzpicture}
  \vspace{-12pt}
  \caption{\autoref{fig:place_conditions_example} extended to show
    input availability.}
  \label{fig:place_conditions_example:availability}
\end{sanefig}

Consider the example in \autoref{fig:place_conditions_example}.  The
side-condition to this bug depends on various values which have to be
collected from the running program: $\textsc{rax}_1$, and
$\textsc{rcx}_1$, registers in the \gls{crashingthread};
$\textsc{rbx}_2$ and $\textsc{rcx}_2$, registers in the interfering
one; and $\controlEdge{1}{C_0}{D_0}$, an expression on the
\gls{crashingthread}'s control flow.
\autoref{fig:place_conditions_example:availability} shows how these
inputs become available as the program progresses through its
\gls{cfg}:
\begin{itemize}
\item $\textsc{rax}_1$ and $\textsc{rcx}_1$ represent the initial
  values of registers \textsc{rax} and \textsc{rcx} in thread 1, and
  are therefore in thread 1 as soon as the enforcer gains control of
  the thread.  Similarly, $\textsc{rbx}_2$ and $\textsc{rcx}_2$ are
  available throughout thread 2.
\item All of the program registers are available when evaluating the
  happens-before edge $\alpha$.  As discussed previously, {\technique}
  \glspl{bugenforcer} enforce happens-before edges synchronously, in
  the sense that both threads must be present simultaneously to
  evaluate the edge, and so anything available in either thread will
  be available during the edge operation.
\item Similarly, all registers are available at instructions $C_0$ and
  $Y_0$, as they can be copied from one thread to the other during the
  happens-before edge operation.  Note that the additional registers
  become available at $Y_0$, the edge's ending instruction, in thread
  2, but not until $C_0$, the instruction after the edge's beginning
  instruction, in thread 1.  This is because the edge $\alpha$ links
  the end of instruction $B_0$ to the start of instruction $Y_0$, and
  so inputs made available by the edge are available at the end of
  $B_0$ and the start of $Y_0$.
\item The control flow expression $\controlEdge{1}{C_0}{D_0}$ becomes
  available once instruction $C_0$ has selected which instruction will
  be executed next, and hence is available in $C_0$'s successors $D_0$
  and $D_1$.  It is also available on happens-before edge $\beta$ because
  $D_0$ is $\beta$'s ending instruction.
\end{itemize}
\begin{sanefig}
  \sloppy
  \subfigure[][Reordered \protect\gls{bdd}]{
    \begin{tikzpicture}
      \node (root) [BddNode,color=blue] {$\textsc{rax}_1\!= 93$};
      \node (alpha) [BddNode, below = of root] {$\controlEdge{1}{C_0}{D_0}$\hspace{-1.45cm}~};
      \node (c0) [BddNode, below = of alpha] {$\textsc{rcx}_1\!\!=\!\textsc{rcx}_2$\hspace{-4mm}~};
      \path (node cs:name=c0,anchor=east) node (c1) [BddNode,right] {~~$\textsc{rcx}_1\!\!=\!\textsc{rcx}_2$};
      \path (barycentric cs:c0=0.5,c1=0.5) ++(0,-0.8) node (b) [BddNode, below] {$\textsc{rbx}_2\!= 72$};
      \path (node cs:name=c1,anchor=south) ++(0,-2) node (false) [BddLeaf] {\false};
      \node (true) at (c0 |- false) [BddLeaf] {\true};
      \draw[BddTrue] (root) -- (alpha);
      \draw[BddFalse] (root.east) ..controls +(2.5,-.5) and +(.4,3).. (false);
      \draw[BddTrue] (alpha) -- (c0);
      \draw[BddFalse] (alpha) -- (c1);
      \draw[BddTrue] (c0) -- (true);
      \draw[BddFalse] (c0) to [bend left=30] (false);
      \draw[BddTrue] (c1) -- (b);
      \draw[BddFalse] (c1) -- (false);
      \draw[BddTrue] (b) -- (true);
      \draw[BddFalse] (b) -- (false);
    \end{tikzpicture}
    \label{fig:place_conditions:example:instr_a:reordered}
  } \subfigure[][Condition evaluable at $A_0$]{
    \hspace{-3.5mm}
    \raisebox{2.55cm}{\Large =}
    \hspace{-5mm}
    \begin{tikzpicture}
      \node (root) [BddNode,color=blue] {$\textsc{rax}_1\!= 93$};
      \node (true) [BddLeaf, below = 5 of root] {\true};
      \node (false) [BddLeaf, right = of true] {\false};
      \draw[BddTrue] (root) -- (true);
      \draw[BddFalse] (root.east) ..controls +(2.5,-.5) and +(.4,3).. (false);
    \end{tikzpicture}
    \hspace{-4mm}\raisebox{2.55cm}{$\bigwedge$}
    \label{fig:place_conditions:example:instr_a:evaluable}
  } \hspace{-3.5mm}\subfigure[][Residual condition]{
    \begin{tikzpicture}
      \node (alpha) [BddNode] {$\controlEdge{1}{C_0}{D_0}$\hspace{-1.45cm}~};
      \node (c0) [BddNode, below = of alpha] {$\textsc{rcx}_1\!\!=\!\textsc{rcx}_2$\hspace{-4mm}~};
      \path (node cs:name=c0,anchor=east) node (c1) [BddNode,right] {~~$\textsc{rcx}_1\!\!=\!\textsc{rcx}_2$};
      \path (barycentric cs:c0=0.5,c1=0.5) ++(0,-0.8) node (b) [BddNode, below] {$\textsc{rbx}_2\!= 72$};
      \path (node cs:name=c1,anchor=south) ++(0,-2) node (false) [BddLeaf] {\false};
      \node (true) at (c0 |- false) [BddLeaf] {\true};
      \draw[BddTrue] (alpha) -- (c0);
      \draw[BddFalse] (alpha) -- (c1);
      \draw[BddTrue] (c0) -- (true);
      \draw[BddFalse] (c0) to [bend left=30] (false);
      \draw[BddTrue] (c1) -- (b);
      \draw[BddFalse] (c1) -- (false);
      \draw[BddTrue] (b) -- (true);
      \draw[BddFalse] (b) -- (false);
    \end{tikzpicture}
    \label{fig:place_conditions:example:instr_a:residual}
  }
  \caption{Factorisation of the side condition in
    \autoref{fig:place_conditions_example} at CFG node $A_0$.
    $\textsc{rax}_1$ and $\textsc{rcx}_1$ are the only available input
    expressions.  Evaluable BDD variables are shown in blue.}
  \label{fig:place_conditions_example:instr_a}
\end{sanefig}

\noindent
Once the availability of input expressions has been determined,
{\technique} can decide how to use them to evaluate the side
condition.  {\Technique} starts by placing a copy of the side
condition at every \gls{cfg} entry instruction, factorising it into
evaluable and unevaluable components based on the available inputs,
and evaluating the evaluable component at that instruction and pushing
the remainder of the side condition to the instruction's successors.
The remainder is then split using the inputs available at that
successor, and the process repeats until either the entire side
condition has been evaluated or {\technique} reaches the end of the
\gls{cfg}.

Consider now what happens when {\technique} attempts to evaluate the
side condition in \autoref{fig:place_conditions_example} at $A_0$.
The only available input is $\textsc{rax}_1$, so $\textsc{rax}_1 = 93$
is evaluable but nothing else is.  {\Technique} will re-express the
side condition as a \gls{bdd}, shown in
\autoref{fig:place_conditions:example:instr_a:reordered}, with the
variables ordered so the evaluable expression is at the root.
Extracting the evaluable component,
\autoref{fig:place_conditions:example:instr_a:evaluable}, is then a
simple matter of replacing every edge from an evaluable expression to
an unevaluable one with an edge to \true; likewise, the unevaluable
component, \autoref{fig:place_conditions:example:instr_a:residual},
can be derived by removing every edge from an evaluable expression to
{\false} and then renormalising.  The \gls{bugenforcer} will evaluate
the evaluable component, $\textsc{rax}_1 = 93$, at $A_0$, and will
defer the unevaluable component to later in the \gls{cfg}\kern-.8pt.\kern-.5pt\fnote{This
  is in essence a predicate abstraction problem, and {\technique}'s
  algorithm for doing so can be regarded as a modest generalisation of
  Cavada's \protect\Gls{bdd} Modulo Theory
  algorithm~\cite{cavada2007}, applied in a slightly unusual context.}

To understand why this is a correct factorisation of the condition,
consider the paths through the \gls{bdd}.  Any which test only
evaluable expressions and end at {\false} will be preserved in the
evaluable component, and every other path is preserved in the
unevaluable one.  A \gls{bdd}'s behaviour can be completely
characterised by the set of paths through it, and so if every path is
preserved then the behaviour is as well.

Applying the same algorithm shows that no sub-condition can be
usefully checked at $B_0$, as its set of available inputs is the same
as $A_0$'s, or at $X_0$, as the available inputs there do not allow an
evaluable component other than \true.  {\Technique} will therefore
attempt to evaluate the unevaluable component from $A_0$ as part of
the happens-before edge $\alpha$, as shown in
\autoref{fig:place_conditions:example:hb_m}.  The evaluable component
is now $\textsc{rcx}_1 = \textsc{rcx}_2$, and so the \gls{bugenforcer}
will only attempt to enforce this happens-before edge between threads
whose RCX registers coincide.  Note that in this case the evaluable
expression $\textsc{rbx}_2 = 72$ remains present in the unevaluable
component.  This is because, while the expression could be evaluated
as part of the $\alpha$ edge, doing so would never allow the enforcer
to abort the enforcement process early, and so would have no actual
benefits over delaying the evaluation further, but would incur some
additional bookkeeping complexity.

The residual condition is again propagated through the graph until it
reaches a point with a larger set of available inputs.  In this case,
that will be either happens-before edge $\beta$ or instruction $D_1$
(these can be processed in either order due to their positions in the
graph).  These have the same set of available inputs and hence produce
the same factorisation, shown in
\autoref{fig:place_conditions:example:hb_n}.  Every input is now
available, so every expression evaluable, and the evaluable component
is the entire \gls{bdd} and the residual is just {\true}.  The
resulting placement of side conditions is shown in
\autoref{fig:place_conditions_example:result}.

\begin{sanefig}
  \subfigure[][Reordered \protect\gls{bdd}]{
    \begin{tikzpicture}
      \node (c) [BddNode,color=blue] {$\textsc{rcx}_1 = \textsc{rcx}_2$\hspace{-4mm}\!};
      \node (b) [BddNode,below = of c,color=blue] {$\textsc{rbx}_2 = 72$};
      \node (true) [BddLeaf, below = 2 of b] {\true};
      \node (false) [BddLeaf, right = of true] {\false};
      \path (node cs:name=false) ++(0,1.3) node (alpha) [BddNode] {$\controlEdge{1}{C}{D_0}$};
      \draw[BddTrue] (c) -- (b);
      \draw[BddFalse] (c) ..controls +(2,-1) and +(.5,3).. (false);
      \draw[BddTrue] (b) -- (true);
      \draw[BddFalse] (b) -- (alpha);
      \draw[BddTrue] (alpha) -- (true);
      \draw[BddFalse] (alpha) -- (false);
    \end{tikzpicture}
  } \subfigure[][Condition evaluable at $\alpha$]{
    \hspace{-2.5mm}
    \raisebox{2.1cm}{\Large =}
    \hspace{-1mm}
    \begin{tikzpicture}
      \node (c) [BddNode, color=blue] {$\textsc{rcx}_1 = \textsc{rcx}_2$\hspace{-4mm}\!};
      \node (true) [BddLeaf, below = 3.6 of root] {\true};
      \node (false) [BddLeaf, right = of true] {\false};
      \draw[BddTrue] (c) -- (true);
      \draw[BddFalse] (c) ..controls +(2,-1) and +(.5,3).. (false);
    \end{tikzpicture}
    \raisebox{2.1cm}{$\bigwedge$} 
  } \subfigure[][Residual condition]{
    \begin{tikzpicture}
      \node (b) [BddNode,color=blue] {$\textsc{rbx}_2 = 72$\hspace{-1cm}~};
      \node (true) [BddLeaf, below = 2 of b] {\true};
      \node (false) [BddLeaf, right = of true] {\false};
      \path (node cs:name=false) ++(0,1.3) node (alpha) [BddNode] {$\controlEdge{1}{C}{D_0}$};
      \draw[BddTrue] (b) -- (true);
      \draw[BddFalse] (b) -- (alpha);
      \draw[BddTrue] (alpha) -- (true);
      \draw[BddFalse] (alpha) -- (false);
    \end{tikzpicture}
    \label{fig:place_conditions:example:hb_m:residual}    
  }
  \caption{Factorisation of the \protect\gls{bdd} in
    \autoref{fig:place_conditions:example:instr_a:residual} at
    happens-before edge $\alpha$.}
  \label{fig:place_conditions:example:hb_m}
\end{sanefig}

\begin{sanefig}
  \subfigure[][Reordered \protect\gls{bdd}]{
    \begin{tikzpicture}
      \node (b) [BddNode,color=blue] {$\textsc{rbx}_2 = 72$};
      \node (true) [BddLeaf, below = 2 of b] {\true};
      \node (false) [BddLeaf, right = of true] {\false};
      \path (node cs:name=false) ++(0,1.3) node (alpha) [BddNode, color=blue] {$\controlEdge{1}{C}{D_0}$};
      \draw[BddTrue] (b) -- (true);
      \draw[BddFalse] (b) -- (alpha);
      \draw[BddTrue] (alpha) -- (true);
      \draw[BddFalse] (alpha) -- (false);
    \end{tikzpicture}
  } \subfigure[][Condition evaluable at $\beta$ and $D_1$]{
    \hspace{-2.5mm}
    \raisebox{15mm}{\Large =}
    \hspace{-1.5mm}
    \begin{tikzpicture}
      \node (b) [BddNode,color=blue] {$\textsc{rbx}_2 = 72$};
      \node (true) [BddLeaf, below = 2 of b] {\true};
      \node (false) [BddLeaf, right = of true] {\false};
      \path (node cs:name=false) ++(0,1.3) node (alpha) [BddNode, color=blue] {$\controlEdge{1}{C}{D_0}$};
      \draw[BddTrue] (b) -- (true);
      \draw[BddFalse] (b) -- (alpha);
      \draw[BddTrue] (alpha) -- (true);
      \draw[BddFalse] (alpha) -- (false);
    \end{tikzpicture}
  } \subfigure[][Residual condition]{
    \raisebox{15mm}{$\bigwedge$~~\true~~~~~~~~~~~~~}
  }
  \caption{Factorisation of the BDD in
    \autoref{fig:place_conditions:example:hb_m:residual} at
    happens-before edge $\beta$ and instruction $D_1$.  Every variable is
    now evaluable, and so the residual condition is just \true.}
  \label{fig:place_conditions:example:hb_n}
\end{sanefig}

\begin{sanefig}
  \begin{tikzpicture}
    \node (t1) {Thread 1};
    \node[right = 4.5cm of t1] (t2) {Thread 2};
    \node[below = of t1] (A) {$A_0$};
    \node[left = 0 of A] {$\textsc{rax}_1 = 93$};
    \node[below = of A] (B) {$B_0$};
    \node[below = of B] (C) {$C_0$};
    \node[below = of C] (D) {$D_0$};
    \node[left = 4cm of D] (D1) {$D_1$};
    \path (D1.north) ++(1,0) node [above] {\shortstack[l]{$\controlEdge{1}{C_0}{D_0} \vee$\\$\textsc{rbx}_2 = 72$}};
    \node at (t2 |- A) (X) {$X_0$};
    \node at (t2 |- B) (Y) {$Y_0$};
    \node at (t2 |- D) (Z) {$Z_0$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C) -- (D1);
    \draw[->] (X) -- (Y);
    \draw[->] (Y) -- (Z);
    \draw[->] (D1.south)
      -- ++(0,-.4)
      ..controls +(0,-.3) and +(-.3,0).. ++(.5,-.5)
      -- ++(3.8,0)
      ..controls +(.3,0) and +(0,-.3).. ++(.5,.5)
      -- (D);
    \draw[->,happensBeforeEdge] (B) to node [above] {$\alpha$} node [below] {$\textsc{rcx}_1 = \textsc{rcx}_2$} (Y);
    \draw[->,happensBeforeEdge] (Z) to node [above] {$\beta$} node [below] {\shortstack{$\controlEdge{1}{C_0}{D_0} \vee$\\$\textsc{rbx}_2 = 72$}} (D);
  \end{tikzpicture}
  \caption{Final placement of side-condition checks for the example
    control flow graph in \autoref{fig:place_conditions_example}.}
  \label{fig:place_conditions_example:result}
\end{sanefig}

\section{Gaining control of a program}
\label{sect:enforce:gain_control}

In order to manipulate a program's execution a \gls{bugenforcer} must
first gain control of it.  {\Implementation} does so by patching the
program's binary and replacing certain critical instructions with
branches to the \gls{plan interpreter}.  This is highly efficient,
when compared to other techniques such as dynamic binary
rewriting~\cite{Luk2005,Nethercote2007} or processor breakpoint
registers~\cite[Chapter 16.2: Debug Registers]{Intel2009}, and avoids
unnecessarily disturbing the program's state, but requires care to be
implemented safely.  In particular, the branch operation might be
larger than the instruction it is replacing\footnote{The normal AMD64
  jump instruction is five bytes long, and some preliminary
  experiments showed that 56\% of instructions in a small selection of
  programs were four bytes or smaller.  The precise ratio will vary
  depending on the program examined; the important point is that these
  small instructions are far too common to ignore.} and so the
replacement risks corrupting the instructions immediately after the
one which is to be replaced.  The patching mechanism must ensure that
the program never executes one of these corrupted instructions.
{\Implementation} does so by introducing further patches which ensure
that it has control whenever the program attempts to branch to one of
these corrupted instructions.

\begin{sanefig}
  \begin{displaymath}
    \textsc{Strategy} = \left\{\begin{array}{rl}
    \mathit{Patch}: & \{\textsc{Instruction}\} \\
    \mathit{Cont}: & \{\textsc{Instruction}\} \\
    \mathit{Pending}: & \{\textsc{Instruction}\}
    \end{array}\right\}
  \end{displaymath}
  \vspace{-12pt}
  \caption{The \textsc{Strategy} type}
  \label{fig:patch_strategy_type}
\end{sanefig}

{\Implementation} represents solutions and partial solutions to the
patch problem using the \textsc{Strategy} type, illustrated in
\autoref{fig:patch_strategy_type}:
\begin{itemize}
\item $\mathit{Patch}$ is the set of instructions in the original
  program which will be replaced with branch instructions.
\item $\mathit{Cont}$ is the set of instructions at which
  {\implementation} will have control of the program.  The enforcer
  will not gain control of the program simply because it runs one of
  these instructions, unless the instruction happens to also be a
  member of $\mathit{Patch}$, but it will also not relinquish control
  at one of these instructions, and will instead continue to emulate
  the program until it leaves the $\mathit{Cont}$ set.
\item $\mathit{Pending}$, contains instructions at which
  {\implementation} must have control of the program's execution but
  which are not controlled by the current strategy.
\end{itemize}
A \textsc{Strategy} is said to be \emph{valid} if it gaining
control at all $\mathit{Pending}$ and $\mathit{Patch}$ instructions,
and refusing to relinquish it at $\mathit{Cont}$ ones, would ensure
that the program never executes a corrupted instruction; it is
\emph{complete} if the $\mathit{Pending}$ set is empty; and it is
\emph{sufficient} if it is complete and $\mathit{Cont}$ includes every
instruction at which the enforcer must gain control of the program.
{\Implementation} must, at a minimum, find a strategy which is both
valid and sufficient, and should, ideally, find one which is
efficient, in the sense of minimising the number of instructions which
must be emulated.  The algorithm used to do so is shown in
\autoref{fig:patch_search_algorithm}.

The \textsc{patch} and \textsc{prefix} helper functions perhaps
require additional explanation.  These transform a
\textsc{Strategy} into a set of new strategies by considering
each $\mathit{Pending}$ instruction $i$ in turn and either replacing
$i$ with a branch (\textsc{patch}) or arranging to gain control before
the program can run $i$ (\textsc{prefix}).  Both functions correctly
gains control at $i$, but they have different costs and constraints:
\begin{itemize}
\item The \textsc{patch} function gains control directly at
  instruction $i$, and so avoids forcing any additional programs to be
  run in the interpreter, but potentially corrupts other instructions
  in the program.  The function must then ensure that it gains control
  before the program executes any of these corrupted instructions, and
  hence must add them to the $\mathit{Pending}$ set.  It is,
  mechanically, only possible to patch $i$ if doing so will neither
  corrupt not be corrupted by any of the existing entries in the
  $\mathit{Patch}$ set.
\item The \textsc{prefix} function, by contrast, does not attempt to
  gain control at $i$, but instead arranges to gain control at the
  predecessors of $i$ and then emulate the program until it reaches
  $i$.  This does not involve modifying the program, and so cannot
  cause any instruction corruption, but requires the function to gain
  control at any instruction which can precede $i$ and hence to add
  $i.\mathit{pred}$ to $\mathit{Pending}$.
\end{itemize}
The algorithm starts with the (valid, complete, insufficient) empty
\textsc{Strategy} (line 2) and then \textsc{extend}s it with one
needed instruction at a time (line 4).  \textsc{extend}ing a
\textsc{Strategy} renders it incomplete, and so the main loop of the
algorithm restores completeness using the \textsc{patch} and
\textsc{prefix} functions (lines 5 to 13).  Once every instruction has
been added the resulting strategy is both valid and sufficient, and is
hence a correct solution to the patching problem.\fnote{Equivalently,
  \textsc{buildStrategy} implements a search over the space of
  \textsc{Strategy}s using the \textsc{prefix} and \textsc{patch}
  operators with a cut point between every instruction at which the
  enforcer must gain control.}
\begin{sanefig}
  \begin{algorithmic}[1]
    \Function{buildPatchStrategy}{$\mathit{gainControl}$}
    \State {$\mathit{soln} \gets \textsc{Strategy}(\mathit{Patch} = \{\}, \mathit{Cont} = \{\}, \mathit{Pending} = \{\})$}
    \For {$i \in \mathit{gainControl}$}
      \State {$q \gets \queue{\textsc{extend}(i, \mathit{soln})}$}
      \While {\true}
        \State {$\mathit{s} \gets \mathit{pop}(q)$}
        \If {$s.\mathit{Pending} = \{\}$}
          \State {$\mathit{soln} \gets s$}
          \State \textbf{break}
        \Else
          \State {$q \gets q \cup \textsc{prefix}(s) \cup \textsc{patch}(s)$}
        \EndIf
      \EndWhile
    \EndFor
    \State \Return $\mathit{soln}$
    \EndFunction
  \end{algorithmic}
  \vspace{8pt}
  \shortstack[l]{
    \begin{math}
      \textsc{extend}(i,s) = \textsc{Strategy}\!\!\left(\!\!\!\!\begin{array}{rcl}
      \mathit{Patch} & = & s.\mathit{Patch},\\
      \mathit{Cont}  & = & s.\mathit{Cont},\\
      \mathit{Pending} & = & s.\mathit{Pending} + i
      \end{array}\!\!\right)
    \end{math}\\
    \begin{math}
      \textsc{prefix}(s) \hspace{5mm} = \left\{\textsc{Strategy}\!\!\left(\!\!\!\!\begin{array}{rcl}
        \mathit{Patch}   & = & s.\mathit{Patch},\\
        \mathit{Cont}    & = & s.\mathit{Cont} + i,\\
        \mathit{Pending} & = & s.\mathit{Pending} \cup i.\mathit{pred}
      \end{array}\!\!\right)\middle| i \in s.\mathit{Pending}\!\!\right\}
    \end{math}\\
    $\textsc{patch}(s) \hspace{6mm} =$\\
    \hspace{.9cm}\begin{math}
      \left\{\textsc{Strategy}\!\!\left(\!\!\!\!\begin{array}{rcl}
      \mathit{Patch}   & = & s.\mathit{Patch} + i,\\
      \mathit{Cont}    & = & s.\mathit{Cont} + i,\\
      \mathit{Pending} & = & s.\mathit{Pending} - i \\
      &   & \cup \textsc{corrupt}(i)
      \end{array}\!\!\right)\middle| \begin{array}{lc}
        i \in s.\mathit{Pending} & \wedge \\
        \textsc{corrupt}(i) \cap s.\mathit{Patch} = \varnothing & \wedge \\
        i \not\in \bigcup\limits_{j \in s.\mathit{Patch}}\textsc{corrupt}(j)
      \end{array}\!\!\right\}
    \end{math}
  }
  \caption{The patch search algorithm \textsc{buildPatchStrategy}.
    $\mathit{gainControl}$ is the set of instructions at which the
    enforcer must gain control of the program.  Not shown:
    {\implementation}'s implementation records all of the strategies
    which it has visited so far so as to avoid re-visiting them.}
  \label{fig:patch_search_algorithm}
\end{sanefig}

This algorithm, as stated, leaves the order in which
\textsc{Strategy}s are removed from the queue ambiguous.  This can
sometimes have a large effect on the number of instructions which must
be executed in the interpreter, and hence on the performance overheads
of the patch.  It is hard to predict precisely how large an impact a
given strategy will have on performance without a detailed model of
the program's behaviour, but it can reasonably be approximated to be
proportional to the number of static instructions affected,
$|\mathit{Cont} \cup \mathit{Patch}|$.  {\Implementation} will
therefore always choose to expand the strategy where that quantity is
smallest, which usually causes it to find the strategy which minimises
the cost of the patch.\fnote{The result is only usually minimal,
  rather than guaranteed to be minimal, because the
  $\mathit{gainControl}$ set is processed incrementally, with no
  backtracking from one instruction to an earlier one, and so it might
  be that the first instruction processed produces a strategy which is
  incompatible with the optimal strategy for the second instruction.
  There is an alternative non-incremental version of the algorithm
  which starts by setting $\mathit{Pending}$ to
  $\mathit{gainControl}$, rather than adding one instruction at a
  time, and that alternative version would be guaranteed to find the
  optimal solution.  On the other hand, the search process itself
  would be far more expensive, as it would attempt to explore from
  every $\mathit{gainControl}$ instruction simultaneously.  This would
  raise the cost from $O(nk)$ to $O(n^k)$, where $k =
  |\mathit{gainControl}|$ and $n/k$ is the cost of the current
  algorithm.  In practice, the algorithm given here rarely needs to
  add more than one or two instructions to the $\mathit{Cont}$ set
  anyway, and so this would not be a good trade-off.}

\section{Enforcing the plan}
\label{sect:enforce:interpreting}

Previous sections described how to find the desired happens-before
graph and side condition and how to place the components of the side
condition on the control-flow graph.  Collectively, these form the
crash enforcement plan.  {\Technique} must now somehow arrange for the
program to follow this plan.

As an example, consider the bug and crash enforcement plan shown in
\autoref{fig:enforcement:example_bug}.  If asked to impose this plan
on the program, {\technique} will start by arranging to gain control
of any program threads which execute instructions {\tt a} or {\tt c}.
When it does gain control, it will check whether \texttt{loc1} is
equal to \texttt{7} and, if it is, immediately return control to the
original program.  What happens next depends on which instruction
{\technique} gained control at: if {\tt a}, it will emulate the
original instruction {\tt a} and then wait for another thread to run
instruction {\tt c}; if {\tt c}, it will simply wait for something to
run {\tt a}.  \texttt{a} is said to send a message to \texttt{c}.
Note three things:
\begin{enumerate}
\item When the enforcer gains control at {\tt a}, it emulates {\tt a}
  before waiting for {\tt c}, whereas when it gains control at {\tt c}
  it waits for {\tt a} without first emulating {\tt c}.  This is
  because the happens-before edge orders {\tt a} before {\tt c} and so
  links the end of {\tt a} to the beginning of {\tt c}.
\item {\tt c} must wait for {\tt a}, even though the only requirement
  imposed by the happens-before edge is for {\tt c} to happen some
  time after {\tt a}; as discussed in
  \autoref{sect:reproduce:unenforcable}, {\technique} enforces
  happens-before edges synchronously.
\item Both wait operations have a timeout.  If no other thread runs
  the appropriate matching instruction within this timeout then the
  enforcer abandons the plan and returns the program to normal
  operation.
\end{enumerate}
This wait operation both enforces the happens-before edge itself and,
just as importantly, identifies the crashing and interfering threads
out of the many which might be executing within the program at the
time.  Whichever program thread executed {\tt a} will become the
crashing thread and whichever executed {\tt c} will become the
interfering one.  The two threads are said to have been bound together
and will execute the rest of the plan together.

Once the wait has completed and the threads have been bound, the
\gls{bugenforcer} continues through the plan, emulating \texttt{c} in
the interfering thread and moving on to the final happens-before edge.
This is handled similarly to the first one: each threads wait for the
other to arrive at its matching happens-before operation, the
operation is discharged, and the two threads continue.  In this case,
the happens-before edge is the final operation in the plan, and so
both threads will exit the \gls{plan interpreter} and return to normal
operation once it has finished (regardless of whether the edge
completes successfully or times out).  The crashing thread will resume
at instruction {\tt b} by loading \texttt{y} from \texttt{loc1}.
\texttt{loc1} was last modified when the \gls{interferingthread}
emulated \texttt{c}, and so \texttt{y} is now set to \texttt{7}.  The
side condition imposed at the start of the plan will have ensured that
\texttt{x} is not \texttt{7}, and so the \gls{crashingthread}'s
assertion will fail and the bug will be correctly
reproduced.\kern-.6pt\fnote{Note that the \protect\gls{bugenforcer} finishes
  before the bug reproduces and that the fatal \texttt{assert} is run
  without direct interference from {\technique}.  This simplifies the
  integration of {\technique} with existing debugging tools which
  might be confused by the interposition of an unknown interpreter.}
The complete procedure is shown in
\autoref{fig:enforcement:example_bug:replacements}.

\begin{sanefig}
  \subfigure[][Crashing thread]{
    \raisebox{7mm}{
    \begin{tabular}{ll}
      {\tt a:} & {\tt x = loc1;}\\
      {\tt b:} & {\tt y = loc1;}\\
      & {\tt assert(x == y);}\\
    \end{tabular}
    }
  }
  {\hfill}
  \subfigure[][Interfering thread]{
    \raisebox{7mm}{
     \begin{tabular}{ll}
      \\
      {\tt c:} & {\tt loc1 = 7;}\\
      \\
    \end{tabular}
    }
  }
  {\hfill}
  \raisebox{0mm}{
    \subfigure[][Crash enforcement plan]{
      \raisebox{-3mm}{
      \begin{tikzpicture}
        \node (a) [CfgInstr] {\tt a};
        \node (b) [CfgInstr, below = of a] {\tt b};
        \path (a) ++(1,-.8) node (c) [CfgInstr] {\tt c};
        \node [left = 0 of a] {$\texttt{loc1} \not= 7$};
        \node [right = 0 of c] {$\texttt{loc1} \not= 7$};
        \draw[->] (a) -- (b);
        \draw[->,happensBeforeEdge] (a) -- (c);
        \draw[->,happensBeforeEdge] (c) -- (b);
      \end{tikzpicture}
      }
    }
  }
  {\hfill}
  \caption{An example bug.}
  \label{fig:enforcement:example_bug}
\end{sanefig}
\begin{sanefig}
  {\hfill}
  \begin{tikzpicture}
    \node (n0m) {Crashing thread};
    \node (n1m) [right = of n0m] {Interfering thread};

    \node (n00) [CfgInstr, below = of n0m] {\shortstack{Gain control at\\instruction {\tt a}}};
    \node (n10) at (n1m |- n00) [CfgInstr] {\shortstack{Gain control at\\instruction {\tt c}}};

    \node (n01) [CfgInstr, below = of n00] {Require $loc1 \not= 7$};
    \node (nm1) [CfgInstr, left = of n01] {\shortstack[r]{Return to\\instruction {\tt a}}};
    \node (n11) at (n1m |- n01) [CfgInstr] {Require $loc1 \not= 7$};
    \node (n21) [CfgInstr, right = of n11] {};

    \node (n02) [CfgInstr, below = of n01] {\tt a: x = loc1};

    \node (n03) [CfgInstr, below = of n02] {Send message 1};
    \node (n13) at (n1m |- n03) {};

    \node (n14) [below = of n13, CfgInstr] {Receive message 1};
    \node (n24) at (n21 |- n14) [CfgInstr] {};
    \node (n2a) at (barycentric cs:n24=0.5,n21=0.5) [CfgInstr,right = -1] {\shortstack[l]{Return to\\instruction {\tt c}}};

    \node (n15) [below = of n14, CfgInstr] {\tt c: loc1 = 7};

    \node (n16) [below = of n15, CfgInstr] {Send message 2};
    \node (n06) at (n0m |- n16) {};

    \node (n07) [below = of n06, CfgInstr] {Receive message 2};

    \node (n08) [below = of n07, CfgInstr] {\shortstack{Return to\\instruction {\tt b}}};
    \node (n18) at (n1m |- n08) [CfgInstr] {\shortstack{Return to instruction\\after {\tt c}}};

    \draw[->] (n00) -- (n01);
    \draw[->] (n01) -- (n02);
    \draw[->] (n02) -- (n03);
    \draw[->] (n03) -- (n07);
    \draw[->] (n07) -- (n08);

    \draw[->] (n10) -- (n11);
    \draw[->] (n11) -- (n14);
    \draw[->] (n14) -- (n15);
    \draw[->] (n15) -- (n16);
    \draw[->] (n16) -- (n18);

    \draw[->,ifFalse] (n01) -- (nm1);
    \draw[->,ifFalse] (n03.west)
      -- ++(-.7,0)
      ..controls +(-.3,0) and +(0,.3).. ++(-.5,-.5)
      -- ++(0,-7.05)
      ..controls +(0,-.3) and +(-0.3,0).. ++(.5,-.5)
      -- (n08.west);
    \draw[->,ifFalse] (n07.west)
      ..controls +(-.3,0) and +(0,0.3).. ++(-.5,-.5)
      -- ++(0,-.86)
      ..controls +(0,-.3) and +(-0.3,0).. ++(.45,-.45)
      -- (n08.west);

    \draw[->,ifFalse] (n11.east)
      -- ++(1.11,0)
      ..controls +(0.3,0) and +(0,.3).. ++(.5,-.5)
      -- (n2a);
    \draw[->,ifFalse] (n14.east)
      -- ++(0.96,0)
      ..controls +(0.3,0) and +(0,-.3).. ++(.5,.5)
      -- (n2a);
    \draw[->,ifFalse] (n16.east)
      -- ++(.7,0)
      ..controls +(.3,0) and +(0,.3).. ++(.5,-.5)
      -- ++(0,-2.28)
      ..controls +(0,-.3) and +(.3,0).. ++(-.5,-.5)
      -- (n18.east);

    \draw[happensBeforeEdge,->] (n03) -- (n14);
    \draw[happensBeforeEdge,->] (n16) -- (n07);
  \end{tikzpicture}
  {\hfill}
  \caption{Instruction replacements for the example bug in
    \autoref{fig:enforcement:example_bug}.  Dashed lines indicate
    message passing operations and dotted ones indicate error paths.}
  \label{fig:enforcement:example_bug:replacements}
\end{sanefig}

This behaviour is correct when enforcing a single plan at a time, but
this is unlikely to be sufficient in a realistic system.  A single
program could easily produce hundreds of
\glspl{verificationcondition}, each of which
\gls{verificationcondition} can produce multiple plans, and it would
be quite tedious to test each one individually.  {\Technique}
\glspl{bugenforcer} therefore track the program's progression through
a set of plans, imposing delays and checking side conditions in a way
which allows all of the plans to make progress.

This set construct also provides a convenient way of handling
ambiguities caused by the \gls{cfg} unrolling algorithm (see
\autoref{sect:derive:build_crashing_cfg}).  The plan is defined in
terms of the \gls{dynamic cfg}, but only the \gls{static cfg} can be
directly observed at run time,\kern-.4pt\fnote{More precisely, the
  enforcer can observe a thread's position in the \gls{static cfg}, as
  that is given by its instruction pointer, but not its position in
  the \gls{dynamic cfg}.} and this can sometimes make it difficult to
determine a thread's position within a plan.  {\Technique} deals with
these ambiguities by forking the plan state and having one fork follow
each possible outcome of the ambiguous choice.  Instruction $C_0$ in
\autoref{fig:place_conditions_example:result} provides an example: in
the \gls{dynamic cfg}, $C_0$ can be followed by either $D_0$ or $D_1$,
and the plan must perform different actions depending on which is
chosen, but the enforcer simply observes that static instruction C is
followed by static instruction D and cannot tell which to use.  It
avoids the issue by pretending that there were two copies of the plan
in its initial plan set and having one go to $D_0$ and the other to
$D_1$\kern-.2pt.\kern-.3pt\fnote{There is a potentially useful analogy here with the power
  set construction used to convert a non-deterministic finite state
  automaton into a deterministic one.  In that model, a deterministic
  FSA emulates a non-deterministic one by tracking the set of states
  which the non-deterministic automaton might possibly occupy,
  allowing it to defer non-deterministic choices until it has enough
  information to resolve them unambiguously.  In the same way,
  {\technique}s set of plans allows it to defer an ambiguous choice
  until one or other of the possible options leads to a failed plan,
  at which point it can arrange to have chosen the other one.}

I now give more details of how the plan enforcement interpreter works,
starting with a more complete description of the single-plan version
(\autoref{sect:enforce:llis}) and then showing how to convert it to
operate with sets of plans (\autoref{sect:enforce:succ} and
\autoref{sect:enforce:hli_messages}), before discussing some ways in
which incompatible plans in the \gls{bugenforcer}'s plan sets can
interfere with each other (\autoref{sect:enforce:plan_interference}).

\subsection{The single-plan interpreter}
\label{sect:enforce:llis}

The single-plan interpreter runs some simple stages in a tight loop:
\begin{itemize}
\item \textbf{Sample} Look at the thread's registers, determine which
  ones might be needed to evaluate later side-conditions, and copy the
  appropriate registers to the plan's local state.

\item \textbf{RX} Receive any messages required by the plan.  Message
  operations are discussed below.

\item \textbf{Emul} Emulate the instruction in the original program.
  {\Implementation}'s instruction emulator is based on the one used in
  the Xen hypervisor~\cite{XenInstructionEmul}.

\item \textbf{TX} Send any messages required by the plan.

\item \textbf{Succ} Determine which node in the \gls{dynamic cfg} the
  plan is going to execute next.
\end{itemize}
The message operation is illustrated in
\autoref{fig:enforce:lli_message}.  It starts by selecting a plan in a
remote thread with which to communicate; the mechanism for doing so
depends on details of the multi-plan interpreter and so is deferred to
\autoref{sect:enforce:hli_messages}.  Each thread then waits for the
other to arrive, at which point they collectively evaluate the
\gls{side condition}, synchronise any state necessary to evaluate
future \glspl{side condition}, and bind the plans together, if they
are not already bound.  They then advance to the next stage in the
interpreter cycle (either \textbf{Emul} for a receive operation or
\textbf{Succ} for a transmit one).

\begin{sanefig}
  {\hfill}
  \begin{tikzpicture}
    \node (n1m) {\textbf{RX}};
    \node (n10) [below = of n1m] {$t \leftarrow$ Select remote plan};
    \node (n11) [below = of n10] {Wait for t};
    \node (n01) [left = of n11] {fail};

    \node (n22) [below right = of n11] {Evaluate side condition};
    \node (n31) [above right = of n22] {Wait for $t'$};
    \node (n41) [right = of n31] {fail};
    \node (n30) [above = of n31] {$t' \leftarrow$ Select remote plan};
    \node (n3m) [above = of n30] {\textbf{TX}};

    \node (n42) at (n22 -| n41) {fail};

    \node (n23) [below = of n22] {Copy state between threads};
    \node (n24) [draw, dashed, below = of n23] {Bind plans together};
    \node (dummy) [below = of n24] {};
    \node (n15) at (dummy -| n11) {Advance to \textbf{Emul}};
    \node (n35) at (dummy -| n31) {Advance to \textbf{Succ}};
    \draw[->] (n1m) -- (n10);
    \draw[->] (n10) -- (n11);
    \draw[->] (n11) -- (n22);

    \draw[->] (n3m) -- (n30);
    \draw[->] (n30) -- (n31);
    \draw[->] (n31) -- (n22);

    \draw[->] (n22) -- (n23);
    \draw[->] (n23) -- (n24);
    \draw[->] (n24) -- (n15);
    \draw[->] (n24) -- (n35);
    \draw[->,ifFalse] (n11) -- (n01);
    \draw[->,ifFalse] (n31) -- (n41);
    \draw[->,ifFalse] (n22) -- (n42);
  \end{tikzpicture}
  {\hfill}
  \caption{The message operation.  The plans are only bound together
    if they have not already been bound.}
  \label{fig:enforce:lli_message}
\end{sanefig}

A message operation can fail in three ways:
\begin{enumerate}
\item
  First, the side condition might evaluate to false.  This simply
  indicates that the data-dependent part of the
  \gls{verificationcondition} cannot be satisfied by this execution.

\item
  A plan's first, unbound, message operation might time out.  In the
  example of \autoref{fig:enforcement:example_bug}, it might be that
  some program thread executes instruction \texttt{a}, and the side
  condition passes, but no thread executes instruction \texttt{c}
  before the timeout expires.  This usually indicates that the program
  is not going to run the two desired fragments of code in parallel.

\item
  Later message operations can also time out.  In the example, it
  might be that \texttt{a} and \texttt{c} happen sufficiently close
  together for the first message operation to succeed, but \texttt{c}
  itself takes so long that the second message times out.  This is
  much less common.  It usually indicates that the program fragments
  being coerced contain some program-level synchronisation of their
  own and that this has deadlocked against the plan-level kind.
\end{enumerate}
Regardless of the cause of the failure, the enforcer will recover in
the same way: the fails and exits, along with its bound plan, if any,
and the program is allowed to return to normal operation.  This
attempt to reproduce the bug has failed and the enforcer must wait
until the program next runs the potentially-buggy fragment of code.

\subsection{The multi-plan interpreter}
\label{sect:enforce:succ}

The single-plan interpreter described in \autoref{sect:enforce:llis}
is sufficient when {\technique} is tasked with reproducing a single
potential bug with a single happens-before graph and an unambiguous
mapping between the dynamic and static \glspl{cfg}, but this is not
the common case: a single program can generate many
\glspl{bugenforcer}, each of which can generate multiple
happens-before graphs, each with potentially many mappings on to the
\gls{static cfg}.  The enforcer must track its progress through all of
these potential plans.

It is useful at this point to draw a distinction between static plans,
which are built before the program's execution starts and reflect
static properties of the program and the bug to be enforced, and
dynamic plans, which are created and destroyed as the program runs and
reflect dynamic properties of a particular execution's interactions
with one of the static plans.  Each enforcer will have a single set of
static plans, but will maintain one set of dynamic plans for each
program thread, and these dynamic plan sets will vary over time.
Freshly-created dynamic plans are added to the dynamic plan set
whenever a thread reaches the start instruction of one of the static
plans.  They then cycle through the stages of the single-plan
interpreter, starting from \textbf{Sample}, until the single-plan
interpreter would exit, at which point the dynamic plan is removed
from the dynamic plan set.  If the dynamic plan set ever becomes empty
then the enforcement interpreter exits and the program is returned to
normal operation.

The simplest phase to implement in this model is \textbf{Succ}, which
is responsible for determining which \glspl{dynamic cfg} node each
dynamic plan might execute next.  The \textbf{Emul} stage will
determine which \gls{static cfg} node the program thread will move to
next, but, as already noted, this can map to several dynamic ones, and
the multi-plan interpreter must somehow resolve this ambiguity.  It
does so by transforming the set of dynamic plans:
\begin{displaymath}
\mathit{newPlans} = \{p[n = n'] | p \in \mathit{plans}, n' \in p.n.\mathit{succ}, n'\!.\mathit{static} = \mathbf{Emul}.\mathit{next} \}
\end{displaymath}
Where:
\begin{itemize}
\item $\mathit{newPlans}$ is the new set of dynamic plans;
\item $\mathit{plans}$ is the current set of dynamic plans;
\item $n' \in p.n.\mathit{succ}$ is true precisely when $n'$ is a
  successor node of $p$'s current \gls{dynamic cfg} node;
\item $n'.\mathit{static}$ is the \gls{static cfg} node corresponding
  to the dynamic node $n'$ and $\mathbf{Emul}.\mathit{next}$ the next
  static node to execute, as determined by the \textbf{Emul} stage;
\item $p[n = n']$ is a new dynamic plan constructed from $p$ by
  setting its current node in the \gls{dynamic cfg} to $n'$.
\end{itemize}
The resulting set will contain one new dynamic plan for every
combination of existing dynamic plan and \gls{dynamic cfg} node,
provided that the new \gls{cfg} node is a successor of the current
\gls{cfg} node and that the new \gls{cfg} node's raw instruction
pointer matches that produced by the \textbf{Emul} phase.  Note that
all of the successors will have the same \gls{static cfg} node; this
is necessary to correctly implement the \textbf{Emul} phase of the
next instruction cycle.

It is perhaps informative to consider what happens to the individual
dynamic plans in the input set.  There are three interesting cases:
\begin{enumerate}
\item In the common case, the mapping from static to \gls{dynamic cfg}
  nodes is unambiguous and so precisely one of the current dynamic
  node's successors will match the next instruction address produced
  by \textbf{Emul}.  In effect, all that happens is that the dynamic
  plan moves from its current node in the \gls{dynamic cfg} to that
  one successor node.

\item Alternatively, the \gls{dynamic cfg} node might not have any
  successors which match the desired static node.  This might be
  because the dynamic plan has reached the end of the static plan, in
  which case this thread's part of the plan has succeeded, or it might
  be because the program has diverged from the static plan, in which
  case the plan has failed.  In either case, the dynamic plan produces
  no successors.  If this causes the set of dynamic plans to become
  empty then the enforcer will also exit and the program thread return
  to normal operation.

\item Finally, the \gls{dynamic cfg} node might have multiple matching
  successors.  The dynamic plan will produce one new plan for each
  such successor, in effect forking its state so as to lazily resolve
  the ambiguity.
\end{enumerate}

\subsection{Message operations in the multi-plan interpreter}
\label{sect:enforce:hli_messages}

As mentioned previously, the first step in an unbound message
operation is to select a dynamic plan with which to communicate.
{\Technique}'s approach to doing so is simple: wait for some short
interval, gathering up all of the dynamic plans which reach an
appropriate unbound message operation during that time, and then fork
the local and remote dynamic plans as many times as necessary to
represent every possible pattern of communication.  The decision as to
which precise plan communicates with which is thus made lazily, in the
same way that the successor \gls{dynamic cfg} node is decided lazily
in the \textbf{Succ} phase.

\subsection{Plan interference}
\label{sect:enforce:plan_interference}

Attempting to enforce multiple crash enforcement plans at the same
time is not always without disadvantage.  In particular, plans can
sometimes interfere with each other in ways which make it more
difficult to reproduce one or both of the bugs.  Consider, for
instance, the program fragments in
\autoref{fig:enforce:plan_interference:eg}.  The first happens-before
graph requires F and G to occur between A and B while the second
requires them to occur between C and D.  {\Technique} will encounter a
deadlock while attempting to reproduce these bugs.  Suppose that the
crashing thread arrives first and that the enforcer gains control at
instruction F.  It will then create two dynamic plans $p_0$ and $p_1$,
at \gls{dynamic cfg} nodes $F_0$ and $F_1$ respectively, to represent
its progress through the two enforcement plans.  Both plans start with
a message receive operation, with $p_0$ receiving $\alpha_0$ and $p_1$
$\alpha_1$, and so the enforcer will put the thread to sleep waiting
for one of those messages to arrive.  Now suppose that, after some
suitably short delay, the interfering thread arrives at instruction A.
The enforcer will now gain control of that thread and create a third
dynamic plan, $p_2$, at $A_0$, and sends message $\alpha_0$,
unblocking $p_0$.  $p_1$ now has a problem: it cannot advance until
the \gls{interferingthread} reaches C, creates a $C_1$ plan, and sends
message $\alpha_1$, but that cannot happen until the
\gls{interferingthread} has finished with instruction A, which is only
possible once message operation $\alpha_0$ is complete.  The
\gls{crashingthread}, meanwhile, has effectively tied messages
$\alpha_0$ and $\alpha_1$ together, and will not complete its receive
of $\alpha_0$ until something sends $\alpha_1$.  The enforcer has
deadlocked.  It will not proceed until $p_1$'s message operation times
out, causing the enforcer to abandon that plan and continue with $p_0$
alone.

\begin{sanefig}
  {\hfill}%
  \subfigure[][\Gls{static cfg}]{
    \begin{tikzpicture}
      \node at (0,0) {\shortstack{Crashing\\thread}};
      \node at (0,-2.5) (F) [CfgInstr] {F};
      \node at (0,-3.5) (G) [CfgInstr] {G};

      \node at (2,0) {\shortstack{Interfering\\thread}};
      \node at (2,-1) (A) [CfgInstr] {A};
      \node at (2,-2) (B) [CfgInstr] {B};
      \node at (2,-4) (C) [CfgInstr] {C};
      \node at (2,-5) (D) [CfgInstr] {D};

      \draw[->] (A) -- (B);
      \draw[->] (B) -- (C);
      \draw[->] (C) -- (D);
      \draw[->] (F) -- (G);
    \end{tikzpicture}
  }
  {\hfill}%
  \subfigure[][First happens-before\\graph to enforce]{
    \begin{tikzpicture}
      \node at (0,0) {\shortstack{Crashing\\thread}};
      \node at (2,0) {\shortstack{Interfering\\thread}};

      \node at (2,-1.5) (A) [CfgInstr] {$A_0$};
      \node at (0,-2.5) (F) [CfgInstr] {$F_0$};
      \node at (0,-3.5) (G) [CfgInstr] {$G_0$};
      \node at (2,-4.5) (B) [CfgInstr] {$B_0$};

      \node at (2,-5) {\phantom{B}};

      \draw[->] (A) -- (B);
      \draw[->] (F) -- (G);
      \draw[->,happensBeforeEdge] (A) to node [above] {$\alpha_0$} (F);
      \draw[->,happensBeforeEdge] (G) to node [below] {$\beta_0$} (B);
    \end{tikzpicture}
  }
  {\hfill}%
  \subfigure[][Second happens-before\\graph to enforce]{
    \begin{tikzpicture}
      \node at (0,0) {\shortstack{Crashing\\thread}};
      \node at (2,0) {\shortstack{Interfering\\thread}};
      \node at (2,-1.5) (A) [CfgInstr] {$C_1$};
      \node at (0,-2.5) (F) [CfgInstr] {$F_1$};
      \node at (0,-3.5) (G) [CfgInstr] {$G_1$};
      \node at (2,-4.5) (B) [CfgInstr] {$D_1$};

      \node at (2,-5) {\phantom{B}};

      \draw[->] (A) -- (B);
      \draw[->] (F) -- (G);
      \draw[->,happensBeforeEdge] (A) to node [above] {$\alpha_1$} (F);
      \draw[->,happensBeforeEdge] (G) to node [below] {$\beta_1$} (B);
    \end{tikzpicture}
  }%
  {\hfill}~%
  \caption{An example of plan interference.  The rest of the program
    contains branches to instructions {\rm A} and {\rm F} but not to
    any of the other instructions shown here.  The program's structure
    means that enforcing the first happens-before graph makes it
    difficult, but not impossible, to enforce the other.}
  \label{fig:enforce:plan_interference:eg}
\end{sanefig}

This is a less serious problem than might it might at first appear.
If the first plan succeeds then the program will crash anyway and the
behaviour of the second plan becomes moot, so assume that the first
plan fails for some reason.  The \gls{interferingthread} will then
continue to advance through the program's \gls{cfg} until it reaches
instruction C, at which point the enforcer will again control and
create another dynamic plan $p_3$ starting at $C_1$.  This plan will
then wait to try to send message $\alpha_1$.  If another thread
reaches instruction F then these two threads will bind together and
attempt the second plan, and, this time, there is no risk of deadlock.
The enforcer will thus, in practice, alternate between the two crash
enforcement plans.  Lacking any other information, that is a perfectly
reasonable strategy for reproducing this particular bug.  Similar
deadlocks in more complicated bugs can lead to enforcers behaving
quite inefficiently, in the sense that they either impose many
pointless delays or make poor use of rare events, but will not usually
render any of the graphs completely unenforceable.

This example showed interference between unbound message operations.
It is also possible for plans to interfere with each other's bound
message operations.  Consider, for instance, the example in
\autoref{fig:enforce:bound_interference}.  This plan requires
instructions A, B, and C to occur between instructions D and E,
enforced using messages $\alpha$ and $\beta$; instruction B can also
receive another message $\gamma$.  Suppose that the path from D to E
is very quick and that
\begin{wrapfigure}{o}{4cm}
  \begin{figgure}
    \centerline{
      \begin{tikzpicture}
        \node (left) {};
        \node [right = of left] (right) {};
        \node [left = of left] (leftleft) {};
        \node at (left) (row1) {};
        \node [below = of row1] (row2) {};
        \node [below = of row2] (row3) {};
        \node [below = of row3] (row4) {};
        \node [below = of row4] (row5) {};
        \node at (right |- row1) [CfgInstr] (D) {D};
        \node at (left |- row2) [CfgInstr] (A) {A};
        \node at (left |- row3) [CfgInstr] (B) {B};
        \node at (left |- row4) [CfgInstr] (C) {C};
        \node at (right |- row5) [CfgInstr] (E) {E};
        \node at (leftleft |- row2) (dummy1) {};
        \node [right = .5 of right] {};
        \draw[->] (D) -- (E);
        \draw[->] (A) -- (B);
        \draw[->] (B) -- (C);
        \draw[->,happensBeforeEdge] (D) to node [above] {$\alpha$} (A);
        \draw[->,happensBeforeEdge] (C) to node [above] {$\beta$} (E);
        \draw[->,happensBeforeEdge] (dummy1) to node [above] {$\gamma$} (B);
      \end{tikzpicture}
    }
    \caption{Another example of plan interference.}
    \label{fig:enforce:bound_interference}
  \end{figgure}
  \vspace{-12pt}
\end{wrapfigure}
receiving the message $\gamma$ requires a very
long delay (or, equivalently, that there are actually many other
message operations between A and C).  The thread on the right will
then reach the $\beta$ operation a long time before the thread on the
left, which could potentially lead to $\beta$ timing out spuriously.
This issue can be avoided by defining the timeout carefully.  Rather
than running from the start of the operation, a bound message timeout
runs from the end of the \emph{previous} message operation in the
other thread and excludes any time spent waiting for other plans.  So,
in the example, if the message timeout were 10ms then the thread on
the left would have 10ms to go from receiving $\alpha$ to being ready
to send $\beta$, excluding time spent processing $\gamma$, regardless
of how long the thread on the right took to advance from sending
$\alpha$ to receiving $\beta$.  Similarly, the thread on the right
would have 10ms to advance from $\alpha$ to $\beta$, excluding time
spent in any other message operations along the way.  This ensures
that bound message timeouts reflect incompatibilities between the
enforcement plan and the structure of the program and not
incompatibilities between multiple enforcement plans.

\section{Discussion}

The enforcer mechanism described here allows {\technique} to shepherd
programs towards schedules which the prior analysis identified as
potentially dangerous, and hence to determine which suspected bugs can
actually reproduce.  This eliminates all of the (many) false positives
produced by the basic analysis described in \autoref{sect:derive}.
Its main weakness is that it also eliminate some true positives: as a
run-time technique, \glspl{bugenforcer} only consider parts of the
program which are exercised during the shepherded execution, and so
cannot distinguish between behaviour which \emph{cannot} happen and
behaviour which merely \emph{did not} happen.  Of course, the same
could be said of many dynamic analysis tools in widespread use today,
and so, while this is a definite limitation of the technique, it is
perhaps not a crippling one.

A more fundamental problem with \glspl{bugenforcer} is what happens
when they \emph{do} work: the program crashes.  While reliable
reproduction of bugs is often useful in a development environment, it
is highly undesirable in a production one.  The next chapter
demonstrates a different, related, technique which instead makes bug
reproduction much less likely, in many cases eliminating the bug
completely.

