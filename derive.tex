\chapter{Finding bugs}
\label{sect:derive}

{\Technique} is, at the most basic level, a system for taking
potential bugs, described by \glspl{verificationcondition}, and
turning them into either \glspl{bugenforcer}, which check which ones
are real, or fixes, which eliminate some specific bugs.  This chapter
describes one approach to finding the \glspl{verificationcondition}.
The basic algorithm proceeds as follows:
\begin{itemize}
\item Identify all of the instructions which the \gls{crashingthread}
  might have executed in the \gls{alpha}-instruction
  \gls{analysiswindow} leading up to the crash.  These are represented
  as an acyclic, unrolled, \gls{cfg}; the details are given in
  \autoref{sect:derive:build_crashing_cfg}, in the case where the
  crashing instruction is identified in advance, and
  \autoref{sect:derive:unknown_bugs}, in the case where that must be
  identified as well.
\item Decompile the \gls{crashingthread}'s \gls{cfg} into a
  {\StateMachine}, converting the program's machine code into a form
  which is more amenable to later analysis.  The {\StateMachine}
  abstraction is described in \autoref{sect:derive:state_machines} and
  the decompilation process in \autoref{sect:derive:compile_cfg}.
\item Simplify the {\StateMachine}.  {\STateMachines}, when initially
  derived, are faithful representations of the program's behaviour,
  and as such often contain a large amount of information which is
  irrelevant to the bug under investigation.  The simplification step
  is responsible for removing this redundant information.  It is
  described in \autoref{sect:derive:simplify_sm}.
\item Examine the \gls{crashingthread}'s {\StateMachine} to discover
  what it might have raced with.  This information is then used to
  build \glspl{cfg} and {\StateMachines} for all of the
  \glspl{interferingthread}; this is described in
  \autoref{sect:derive:write_side}, in the case where the crashing
  instruction is identified in advance, and
  \autoref{sect:derive:unknown_bugs}, in the case where it must be
  identified.
\item Symbolically execute the crashing and interfering
  {\StateMachines} so as to convert them into a
  \gls{verificationcondition}.  This is a predicate over the program's
  state and happens-before graph which is true if there is any
  possibility of the bug under investigation reproducing.  This step
  is described in \autoref{sect:using:check_realness}.
\end{itemize}
At this stage, {\technique} is concerned primarily with the local
structure of the program, meaning those instructions which fit within
the \gls{analysiswindow}.  Non-local properties, such as the structure
of the heap or existing synchronisation, are checked later via the
\gls{bugenforcer} mechanism.

\section{Building the crashing thread's CFG}
\label{sect:derive:build_crashing_cfg}

\begin{sanefig}
\begin{algorithmic}
\Function{buildStaticCrashing}{}
\State $\mathit{depth} \gets 0$
\State $\mathit{pendingAtDepth} \gets \queue{\text{crashing instruction}}$
\State $\mathit{result} \gets \map{}$
\While{$\mathit{depth} < \alpha$}
  \State $\mathit{pendingAtNextDepth} \gets \queue{}$
  \While{$\neg{}\mathit{empty}(\mathit{pendingAtDepth})$}
    \State $\mathit{currentInstr} \gets \mathit{pop}(\mathit{pendingAtDepth})$
    \If {$\mathit{result} \textrm{ has entry for } \mathit{currentInstr}$}
      \State \textbf{continue}
    \EndIf
    \State $\mathit{current} \gets \text{decode instruction at } \mathit{currentInstr}$
    \State $\mapIndex{\mathit{result}}{\mathit{currentInstr}} \gets \mathit{current}$
    \State $\mathit{predecessors} \gets \text{predecessors of } \mathit{currentInstr}$
    \State Add $\mathit{predecessors}$ to $\mathit{pendingAtNextDepth}$
  \EndWhile
  \State $\mathit{pendingAtDepth} \gets \mathit{pendingAtNextDepth}$
  \State $\mathit{depth} \gets \mathit{depth} + 1$
\EndWhile
\EndFunction
\end{algorithmic}
\vspace{-6pt}
\caption{The \textsc{buildStaticCrashing} algorithm, which builds the
  static \protect\gls{cfg} for the \protect\gls{crashingthread} when
  the address of the crashing instruction is already known.}
\label{fig:derive:static_read_cfg_single_function}
\end{sanefig}

\noindent
{\Technique} considers concurrency bugs caused by unfortunate
interleavings of the \gls{alpha} instructions prior to the crash, and
hence the first step is to find the \gls{alpha} instructions which
might have executed in the \gls{crashingthread} leading up to the
crash.  These are represented as a \gls{cfg} showing both the
instructions involved and the relationships between them.  Note that
the instructions in this \gls{cfg} are dynamic rather than static, so
if the same program instruction executes twice on a single path then
it will be represented twice in the \gls{cfg}.  This means that the
generated \gls{cfg} is inherently acyclic, simplifying later analysis.
More importantly, it also makes it easy to identify specific
memory-accessing operations so that it is possible to talk
meaningfully about a particular memory access happening before or
after some other access.

\subsection[Building the \glsentrytext{crashingthread}'s static \glsentrytext{cfg}]{Building the \gls{crashingthread}'s static \gls{cfg}}
\label{sect:derive:build_static_cfg}

\noindent
The first step in building the \gls{crashingthread}'s dynamic
\gls{cfg} is to find its static \gls{cfg} using the algorithm shown in
\autoref{fig:derive:static_read_cfg_single_function}.  This implements
a depth-limited breadth-first search backwards through the program's
control flow, starting at the crashing instruction\footnote{Note that
  this can result in a \gls{cfg} with multiple roots.}.  This
algorithm depends on being able to find the predecessors of an
arbitrary instruction, which is not always straightforwards given only
the program binary.  {\Technique} relies on information collected
during the dynamic analysis phase to do so, and in particular a
mapping from indirect branches to the sets of instructions which can
follow those branches.  That mapping, together with a list of the
program's entry points, allows {\technique} to build a complete static
CFG of the program's instructions, allowing it to find an
instruction's predecessors when necessary.

\subsection[Converting the static \glsentrytext{cfg} to a dynamic one]{Converting the static \gls{cfg} to a dynamic one}
\label{sect:derive:handling_loops}

The static \gls{cfg} must now be converted to a dynamic one.  For
completely acyclic \glspl{cfg} this is trivial, as each static
instruction executes at most once and it is safe to simply identify
the static and dynamic instructions.  Loops are more difficult to
handle, as the instructions in a loop might execute multiple times,
and no simple correspondence exists.  {\Technique} solves this problem
by simply unrolling such loops until they exceed \gls{alpha}
instructions, so that the loop can only execute at most once during
the \gls{analysiswindow}, restoring the problem to the acyclic case.

\begin{sanefig}
\begin{tikzpicture}
  [node distance=0.9 and 0.2, inner sep = 1.5pt]
  \begin{scope}[node distance=0.9 and 0.2]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$}; 
    \node (C) [CfgInstr] [below=of B] {$C_0$}; 
    \node (D) [CfgInstr] [below=of C] {$D_0$}; 
    \node (dummy) [right = of B] {~~~};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C.east) to [bend right=45] (B.east) node (edge1) [right] {};
    \begin{pgfonlayer}{bg}
      \node (box1) [fill=black!10,fit=(A) (B) (C) (D) (edge1) (dummy)] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=3.8cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$}; 
    \node (C) [CfgInstr] [below=of B] {$C_0$}; 
    \node (D) [CfgInstr] [below=of C] {$D_0$};  
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (B) to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B);
    \begin{pgfonlayer}{bg}
      \node (box2) [fill=black!10,fit=(A) (B) (C) (D) (C')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=7.8cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$};
    \node (B') [CfgInstr] [right=of B] {$B_1$};
    \node (C) [CfgInstr] [below=of B] {$C_0$};
    \node (D) [CfgInstr] [below=of C] {$D_0$};
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B');
    \begin{pgfonlayer}{bg}
      \node (box3) [fill=black!10,fit=(A) (B) (C) (D) (C') (B')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=11.8cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$};
    \node (B') [CfgInstr] [right=of B] {$B_1$};
    \node (C) [CfgInstr] [below=of B] {$C_0$};
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \node (C'') [CfgInstr] [right=of C'] {$C_2$};
    \node (D) [CfgInstr] [below=of C] {$D_0$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') -- (C');
    \draw[->] (C'') to [bend right=10] (B');
    \draw[->] (B') to [bend right=10] (C'');
    \begin{pgfonlayer}{bg}
      \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
    \end{pgfonlayer}
  \end{scope}
  \draw[->,thick] (box1) -- (box2) node [above,midway] {duplicate $C_0$};
  \draw[->,thick] (box2) -- (box3) node [above,midway] {duplicate $B_0$};
  \draw[->,thick] (box3) -- (box4) node [above,midway] {duplicate $C_1$};
  \draw[->,thick] (box4) -- +(2.28,0) node [above,midway] {$\cdots$};
\end{tikzpicture}
\caption{A CFG containing a cycle.}
\label{fig:cyclic_cfg}
\end{sanefig}

As an example, consider the \gls{cfg} shown at the left of
\autoref{fig:cyclic_cfg}, which contains a loop between instructions
$B_0$ and $C_0$.  This loop must be removed from the \gls{cfg} while
maintaining all paths which terminate at $D_0$ and which contain
\gls{alpha} or fewer instructions.  The algorithm starts by performing
a depth-first traversal backwards through the graph from $D_0$ until
it finds an edge which closes a cycle.  In this case, that is the edge
from $C_0$ to $B_0$, and {\technique} therefore breaks this edge by
duplicating $C_0$, along with all of its
\begin{wrapfigure}{o}{3.9cm}
\begin{figgure}
\begin{tikzpicture}
  [node distance=1 and 0.3]
  \node (A) at (0,2) [CfgInstr] {$A_0$};
  \node (B) [CfgInstr] [below=of A] {$B_0$};
  \node (B') [CfgInstr] [right=of B] {$B_1$};
  \node (C) [CfgInstr] [below=of B] {$C_0$};
  \node (C') [CfgInstr] [right=of C] {$C_1$};
  \node (C'') [CfgInstr] [above right=of B'] {$C_2$};
  \node (D) [CfgInstr] [below=of C] {$D_0$};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) -- (D);
  \draw[->] (C') -- (B);
  \draw[->] (A) -- (B');
  \draw[->] (B') -- (C');
  \draw[->] (C'') -- (B');
  \begin{pgfonlayer}{bg}
    \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
  \end{pgfonlayer}
\end{tikzpicture}
\caption{Fully unrolled version of the CFG in
  \autoref{fig:cyclic_cfg}, preserving all paths of length six or
  fewer instructions.  Note that an additional root has been
  introduced at $C_2$.}
\label{fig:unrolled_cyclic_cfg}
\end{figgure}
\vspace{-24pt}
\end{wrapfigure}
incoming edges, and redirecting the original edge to come from this
new instruction.  All paths which were possible in the old graph will
also be possible in the new one, if duplicated nodes are treated as
semantically equivalent, and the loop is now one instruction further
away from the crashing instruction $D_0$.  The process repeats, moving
the cycle steadily further and further away from $D_0$ until all paths
of length \gls{alpha} ending at $D_0$ are acyclic.  The cycle can then
be safely removed from the graph.  The complete algorithm is shown in
\autoref{fig:derive:read:unroll_cycle_break}.

Note that the edge which is modified is the back edge, from $C_0$ to
$B_0$, which points ``away from $D_0$'', and not the forwards edge
from $B_0$ to $C_0$.  Trying to break the $B_0$ to $C_0$ edge would
have moved the cycle away from $A_0$ rather than away from $D_0$,
which would not be helpful.

\todo{Not sure how much this adds, really.} This algorithm is
guaranteed to preserve all paths of length \gls{alpha} which end at
the crashing instruction.  The only place in the algorithm which
removes an edge from the graph is line 4 which removes the edge
$(\mathit{start}, \mathit{end})$.  If $\mathit{start}$ is more than
\gls{alpha} instructions from the crashing instruction this edge clearly
cannot participate in any length \gls{alpha} paths and so remove it is
safe.  Assume instead that it is closer then \gls{alpha} instructions
away so that the body of the \textbf{if} executes.  That will create
new duplicate of $\mathit{start}$, $\mathit{newNode}$, a new edge
$(\mathit{newNode}, \mathit{end})$, and new edges $(s,
\mathit{newNode})$ for every $s$ with an edge to $\mathit{start}$.
Any path which used the $(\mathit{start}, \mathit{end})$ edge can then
be re-routed to go via $\mathit{newNode}$ instead of $\mathit{start}$.
Similarly, the algorithm will not introduce any new paths, as all
paths through $\mathit{newNode}$ are equivalent to paths which went
via $\mathit{start}$ in the old graph.

This might appear, on the face of it, to be a rather expensive
algorithm: it must explore every path of length \gls{alpha} ending at
the crashing instruction, and the number of such paths is potentially
exponential in \gls{alpha}.  This is true, but several other
algorithms used by {\implementation} also have exponential worst-case
running time and larger constant factors, and so in practice deriving
the crashing \gls{cfg} accounts only a small percentage of the total
analysis time.

\begin{sanefig}
\begin{algorithmic}[1]
  \Function{staticToDynamicCrashing}{}
  \While {Graph contains a cycle}
     \State $(\mathit{start}, \mathit{end}) \gets \textsc{findCycleEdgeCrashing}(\text{crashing instruction})$
     \State {Remove edge $(\mathit{start}, \mathit{end})$}
     \If {$\mathit{start}$ is no more than $\alpha$ instructions from crashing instruction}
        \State {$newNode \gets$ duplicate $\mathit{start}$}
        \State {Create a new edge $(\mathit{newNode}, \mathit{end})$}
        \For {Edges $(\mathit{start}', \_)$ entering $\mathit{start}$}
           \State {Create a new edge $(\mathit{start}', \mathit{newNode})$}
        \EndFor
     \EndIf
  \EndWhile
  \EndFunction
\end{algorithmic}
\caption{Loop unrolling and cycle breaking algorithm.
  \textsc{findCycleEdgeCrashing} simply performs a depth-first search
  of the graph backwards from the crashing instruction and returns the
  first edge which completes a cycle.}
\label{fig:derive:read:unroll_cycle_break}
\end{sanefig}

\section{Generating \protect\glspl{cfg} for unknown bugs}
\label{sect:derive:unknown_bugs}

The \textsc{buildStaticCrashing} algorithm given in
\autoref{sect:derive:build_crashing_cfg} assumed that the crashing
instruction was somehow pre-identified, but {\technique} can also be
used to discover unknown bugs.  It does so by enumerating all of the
instructions in a program which might crash due to a bug of the class
being investigated and considering each in turn.  This is obviously
only feasible if the majority of instructions can be dismissed
quickly.  Fortunately, they can be: {\implementation} takes, on
average, a few hundred of milliseconds per potentially-crashing
instruction on fairly modest hardware, allowing even large programs
with millions of instructions to be analysed in a few days.  Further,
this approach is embarrassingly parallel, and so would be expected to
scale well as hardware concurrency increases, which is precisely the
scenario in which it would be most useful.

Identifying instructions which might crash depends on the type of bug
which is to be investigated.  {\Implementation} considers three types
of bug:
\begin{itemize}
\item Assertion-failure type crashes.  These are caused by the program
  calling a function such as \verb|__assert_fail| or \verb|abort|
  provided by an operating system library.  Finding such functions is
  generally straightforward given the usual dynamic linker
  information, and the whole-program \gls{cfg}, mentioned earlier, can
  then provide all of their callers.  Each such caller is treated as a
  potential bug.
\item Double free errors.  These are caused by the program calling
  \verb|free| in an incorrect manner.  Again, the dynamic linker
  information allows {\implementation} to quickly find all calls to
  \verb|free| in the program, and these calls are used as the
  potentially-crashing instructions in the program.
\item Bad pointer dereferences.  Any memory-accessing instruction
  could potentially dereference a bad pointer, and so
  {\implementation} simply enumerates all memory accessing
  instructions discovered by the initial static analysis.
\end{itemize}
Other classes of bugs would generate different sets of potentially
crashing instructions.

\subsection{Timeouts}

Many of the algorithms used by {\technique} require far more time in
their worst case than in their expected case, in many cases by many
orders of magnitude.  This is irritating but tolerable when the
analysis is being used to investigate a specific bug, but far more of
a problem when the analysis is applied speculatively to a very large
number of potential bugs.  Suppose, for instance, that the analysis
completes in 500ms in 99.9\% of cases but takes three years in the
remaining 0.1\% of cases.  An analysis which fails 0.1\% of the time
would still be quite useful, and so this is reasonable for analysing
specific bugs.  On the other hand, if the analysis is run 10,000 times
then the probability of one of at least one of the steps taking three
years is very close to one, and the analysis is effectively useless.

{\Technique} works around this problem by applying timeouts to the
various analysis steps, ensuring that it can produce at least some
useful results in a reasonable time even when it occasionally
encounters one of its bad cases.  {\Implementation} uses two
independent timeouts, one for per-\gls{crashingthread} work and the
other for per-\gls{interferingthread} work, both of which are set by
default to five minutes.  I discuss their effects in more detail in
the evaluation.

\section{\STateMachines}
\label{sect:derive:state_machines}

The dynamic \gls{cfg} represents all of the instructions in the
\gls{crashingthread} in the \gls{analysiswindow}, and therefore in
principle contains all of the information needed to analyse the bug.
It is, however, very close to machine code, and is therefore difficult
to work with, and so the next step in the algorithm is to decompile it
into a more amenable form.  For {\technique}, this is a
{\StateMachine}.  {\STateMachines} are, in effect, programs in a
simple language which model the relevant behaviour of the program
under investigation.  Programs in this language consist of a directed
acyclic graph of states, and these are defined in
figures~\ref{fig:state_machine_states}
and~\ref{fig:state_machine_exprs}\footnote{The actual implementation
  used by {\implementation} include several features, such as sub-word
  memory accesses and bounded-range integers, not given in these
  figures, but these are not essential to the {\technique} technique
  and I do not give details here.}.

\begin{sanefig}
{\hfill}
\begin{tabular}{llllp{6.05cm}}
\multicolumn{2}{l}{State}       & \multicolumn{2}{l}{Fields} & Meaning \\
\hline
\multicolumn{2}{l}{\state{If}}  & \state{cond} & Boolean expression        & Conditional branch with two successor states.  Evaluates \state{cond}, branching to one successor if it is true and the other if it is false. \\
\hline
\multicolumn{2}{l}{Terminal states} &          &             & Terminal states.  {\STateMachine} execution finishes when it reaches one of these. \\
\hdashline
 & {\stSurvive}              &              &             & The bug has been avoided. \\
\hdashline
 & {\stCrash}                &              &             & The bug will definitely happen. \\
\hdashline
 & {\stUnreached}            &              &             & A contradiction has been reached. \\
\hline
\multicolumn{2}{l}{Side-effect states}\\
 & \state{Load}                 & \state{addr} & Integer expression & \multirow{3}{6.05cm}{Load from the speicher at address \state{addr} and store the result in {\StateMachine} variable \state{var}.} \\
 &                              & \state{var}  & {\STateMachine} variable \\
\\
\hdashline
 & \state{Store}                & \state{addr} & Integer expression & \multirow{2}{6.05cm}{Store the value of \state{data} to the speicher at address \state{addr}.}\\
 &                              & \state{data} & Integer expression \\
\hdashline
 & \state{Copy}                 & \state{data} & Integer expression & \multirow{2}{6.05cm}{Evaluate an expression and store the result in a {\StateMachine} variable.} \\
 &                              & \state{var}  & {\STateMachine} temporary \\
\hdashline
 & \state{Assert}               & \state{expr} & Boolean expression & Note that a given condition is true at a particular point in the {\StateMachine}'s execution. \\
\hdashline
 & $\Phi$                       &              &                 & Implement an SSA $\Phi$ node~\cite{cytron1991}. \\
\hdashline
 & {\stStartAtomic}          &              &                 & \multirow{3}{6.05cm}{Delimit atomic blocks, used to constrain the set of schedules which must be considered; see \autoref{sect:using:build_cross_product}.} \\
 & {\stEndAtomic}            \\
\\
\\
\end{tabular}
{\hfill}
\caption{Types of {\StateMachine} states.  The expression language is
  described in \autoref{fig:state_machine_exprs}.}
\label{fig:state_machine_states}
\end{sanefig}

The details of the analysis language are important for analysis
performance but are not critical to the overall structure of the
{\technique} technique.  I therefore give only a brief overview of the
language's most important features:
\begin{itemize}
\item {\STateMachine} states do not directly correspond to
  instructions in the original program: one state might represent
  several instructions, and a single instruction might be represented
  by multiple states or none at all.
\item The {\StateMachine} evaluation relation
  $\stackrel{e}{\rightarrow}$ reduces {\StateMachines} to either
  \textsc{Crash}, \textsc{Survive}, or \textsc{Unreached}.
  \textsc{Crash} and \textsc{Survive} indicate that the program
  reached either suffered the bug under investigation or avoided it,
  respectively.  \textsc{Unreached} indicates that the current path
  through the {\StateMachine} is uninteresting for some reason,
  usually because the corresponding path through the original program
  would crash for reasons other than the bug under investigation.
  \textsc{Unreached} can be treated as either crashing or surviving
  depending on the analysis being performed.
\item {\STateMachines} have access to an infinite set of
  {\StateMachine} variables, each of which contains a single integer
  value.  The {\StateMachine} is maintained in single static
  assignment form~\cite{cytron1991} with respect to these variables.
\item In addition to the variables, {\StateMachines} also have access
  to a mutable mapping from integers to integers called the
  \emph{speicher} which is initialised to the contents of program
  memory.  The speicher is accessed only via the \state{Load} and
  \state{Store} side-effects.
\item {\STateMachines} execute in the context of a particular
  \emph{environment} which provides some information on the run-time
  state of the program which is being modelled.  This includes the
  contents of processor registers and program memory when the
  {\StateMachine} started, the path which each thread takes through
  the dynamic \gls{cfg}, and the order in which memory accesses happen
  when multiple threads are being modelled.
\end{itemize}
This model of computation is not Turing complete; in particular,
{\StateMachines} are guaranteed to terminate in a finite number of
steps.  This simplifies analysis, but implies that they cannot capture
the entire program's complete behaviour.  {\Technique} uses them only
to model the \gls{alpha}-instruction \gls{analysiswindow}, and so this
is not, in practice, a problem.

\begin{sanefig}
\begin{tabular}{lp{11.2cm}}
Expression & Meaning \\
\hline
$\smVar{A}$ & The value of {\StateMachine} variable $A$. \\
$\happensBefore{A}{B}$ & True if memory access $A$ happens before memory access $B$. \\
$\entryExpr{\mai{tid}{instr}}$ & True if thread $\mathit{tid}$ starts with instruction $\mathit{instr}$, and false otherwise. \\
$\controlEdge{tid}{A}{B}$ & True if thread $\mathit{tid}$ executed instruction $B$ immediately after instruction $A$. False if it executed some other instruction after $A$ and undefined if it did not execute $A$ at all.\\
$\smBadPtr{expr}$ & True if $\mathit{expr}$ evaluates to a value which is not a valid pointer.\\
$\smLoad{expr}$ & The initial value of program memory at address $\mathit{expr}$.\\
$\smReg{r}{t}$ & The initial value of register \textsc{r} in thread $t$.\\
\end{tabular}
\caption{Expressions in the {\StateMachine} expression language.  The
  usual arithmetic operators, such as $+$, $\times$, $\wedge$, etc.,
  are also supported.}
\label{fig:state_machine_exprs}
\end{sanefig}

\begin{sanefig}
  \begin{tabular}{lc}
    Initial configuration & $\mathrm{X} \stackrel{e}{\rightarrow} \{X, \varnothing, e.\mathit{memory}\}$\\
    \\
    \multirow{2}{*}{\raisebox{-55pt}{\state{If} states}} &
    \AxiomC{$\mathit{a} \downarrow_{c,t} \true$}
    \UnaryInfC{
      \begin{math}
        \left\{
          \begin{tikzpicture}[baseline = (current bounding box.center), node distance = 0.3 and -0.5]
            \node (r) [stateIf] {\stIf{\mathit{a}}};
            \node (X) [below left = of r] {X};
            \node (Y) [below right = of r] {Y};
            \draw[->,ifTrue] (r) -- (X);
            \draw[->,ifFalse] (r) -- (Y);
          \end{tikzpicture}, t, m
        \right\} \stackrel{e}{\rightarrow} \{\mathrm{X}, t, m\}
      \end{math}
    }
    \DisplayProof
    \vspace{4pt}\\ &
    \AxiomC{$\mathit{a} \downarrow_{c,t} \false$}
    \UnaryInfC{
      \begin{math}
        \left\{
          \begin{tikzpicture}[baseline = (current bounding box.center), node distance = 0.3 and -0.5]
            \node (r) [stateIf] {\stIf{\mathit{a}}};
            \node (X) [below left = of r] {X};
            \node (Y) [below right = of r] {Y};
            \draw[->,ifTrue] (r) -- (X);
            \draw[->,ifFalse] (r) -- (Y);
          \end{tikzpicture}, t, m
        \right\}
        \stackrel{e}{\rightarrow} \{\mathrm{Y}, t, m\}
      \end{math}
    }
    \DisplayProof
    \vspace{30pt}\\

    \multirow{2}{*}{\raisebox{-14pt}{Terminal states}} &
    \AxiomC{}
    \UnaryInfC{$\{\tikz[baseline = (s.base)]{\node (s)[stateTerminal] {\stSurvive};}, t, m\} \stackrel{e}{\rightarrow} \stSurvive$}
    \DisplayProof
    \vspace{8pt}\\
    &\AxiomC{}
    \UnaryInfC{$\{\tikz[baseline = (s.base)]{\node (s) [stateTerminal] {\stCrash};}, t, m\} \stackrel{e}{\rightarrow} \stCrash$}
    ~\DisplayProof\vspace{8pt}
    \vspace{30pt}\\

    \multirow{4}{*}{\raisebox{-110pt}{Side-effect states}} &
    \AxiomC{$\mathit{addr} \downarrow_{c,t} \mathit{addr}'$}
    \AxiomC{$t' = t[\smVar{T} = m(\mathit{addr}')]$}
    \BinaryInfC{
      \begin{math}
        \left\{
          \begin{tikzpicture}[baseline = (current bounding box.center), node distance = .3]
            \node (r) [stateSideEffect] {\stLoad{T}{\mathit{addr}}};
            \node (X) [below = of r] {X};
            \draw[->] (r) -- (X);
          \end{tikzpicture}, t, m
        \right\}
        \stackrel{e}{\rightarrow} \{\mathrm{X}, t', m\}
      \end{math}
    }
    \DisplayProof
    \vspace{4pt}\\
    & \AxiomC{$\mathit{addr} \downarrow_{c,t} \mathit{addr}'$}
    \AxiomC{$\mathit{data} \downarrow_{c,t} \mathit{data}'$}
    \AxiomC{$m' = m[\mathit{addr'} = \mathit{data}']$}
    \TrinaryInfC{
      \begin{math}
        \left\{
          \begin{tikzpicture}[baseline = (current bounding box.center), node distance = .3]
            \node (r) [stateSideEffect] {\stStore{\mathit{data}}{\mathit{addr}}};
            \node (X) [below = of r] {X};
            \draw[->] (r) -- (X);
          \end{tikzpicture}, t, m
        \right\}
        \stackrel{e}{\rightarrow} \{\mathrm{X}, t, m'\}
      \end{math}
    }
    \DisplayProof
    \vspace{4pt}\\
    & \AxiomC{$\mathit{data} \downarrow_{c,t} \mathit{data}'$}
    \AxiomC{$t' = t[\smVar{T} = \mathit{data}']$}
    \BinaryInfC{
      \begin{math}
        \left\{
          \begin{tikzpicture}[baseline = (current bounding box.center), node distance = .3]
            \node (r) [stateSideEffect] {\stCopy{T}{\mathit{data}}};
            \node (X) [below = of r] {X};
            \draw[->] (r) -- (X);
          \end{tikzpicture}, t, m
        \right\}
        \stackrel{e}{\rightarrow} \{\mathrm{X}, t', m\}
      \end{math}
    }
    \DisplayProof\\
    & \AxiomC{$\mathit{cond} \downarrow_{c,t} \true$}
    \UnaryInfC{
      \begin{math}
        \left\{
          \begin{tikzpicture}[baseline = (current bounding box.center), node distance = .3]
            \node (r) [stateSideEffect] {\stAssert{\mathit{cond}}};
            \node (X) [below = of r] {X};
            \draw[->] (r) -- (X);
          \end{tikzpicture}, t, m
        \right\}
        \stackrel{e}{\rightarrow} \{\mathrm{X}, t, m\}
      \end{math}
    }
    \DisplayProof
  \end{tabular}
  \caption{Small-step semantics for single-thread {\StateMachines};
    these are generalised to multi-threaded programs in
    \autoref{sect:using:build_cross_product}.  The
    $\stackrel{e}{\rightarrow}$ relationship shows how to evaluate the
    \StateMachine{} configuration $\{\mathrm{X}, t, m\}$, where
    $\mathrm{X}$ is the current {\StateMachine} state, $t$ the
    temporaries, and $m$ the speicher under the environment $e$,
    giving either a new configuration, \textsc{Survive}, or
    \textsc{Crash}.  Configurations which do not match any of these
    rules evaluate to \textsc{Unreached}.  $\mathrm{expr}
    \downarrow_{c,t} \mathrm{value}$ is true when the expression
    $\mathrm{expr}$ evaluates to the value $\mathrm{value}$ under
    environment $c$ and temporaries $t$.}
  \label{fig:derive:sm_semantics}
\end{sanefig}

\section[Decompiling the dynamic \glsentrytext{cfg} to a \StateMachine]{Decompiling the dynamic \gls{cfg} to a \StateMachine}
\label{sect:derive:compile_cfg}

The {\StateMachine} analysis language is powerful enough to make
translating individual instructions in isolation completely
straightforward\footnote{{\Implementation} uses LibVEX~\cite[Chapter
    2]{Seward2012} to decode AMD64 machine code before performing this
  translation.}, but connecting the instructions together is not
always completely trivial.  There are three cases which require
special care:
\begin{itemize}
\item
  Some edges will be erased from the dynamic \gls{cfg}.  For instance,
  in \autoref{fig:unrolled_cyclic_cfg}, the program's original
  \gls{cfg} contained an edge from $C_0$ to $D_0$ but the unrolled
  \gls{cfg} does not include any branches from $C_1$ to a $D$-like
  instruction.  These are converted to branches to the {\stUnreached}
  state, reflecting the fact that these paths are of no interest to
  the rest of the analysis.
\item
  Some additional edges will have been introduced which do not
  correspond to anything in the original program.  In the example,
  instruction $A_0$ had a single successor, $B_0$, in the static
  \gls{cfg} but has multiple successors in the dynamic one.  Each of
  the $B_i$ \gls{cfg} nodes will be represented by a separate
  {\StateMachine} state, but there is no condition on the original
  program's state which can be evaluated at $A_0$ which determines
  which of $B_i$ states the {\StateMachine} must execute next.
  {\Technique} converts these into {\StateMachine}-level control flow
  using ${\controlEdgeName}()$ expressions which test the program's
  path through the dynamic \gls{cfg}.  See, for instance, state $A_0'$
  in \autoref{fig:state_machine_for_cyclic_cfg}.
\item
  The \gls{cfg} can sometimes have multiple roots, each represented by
  a separate {\StateMachine} state, but the {\StateMachine} itself
  must have a single entry state.  {\Technique} handles these using
  $\entryExpr{}$ expressions, which simply test where a thread entered
  the dynamic \gls{cfg}.  The start state of
  \autoref{fig:state_machine_for_cyclic_cfg} is an example.
\end{itemize}

\parshape 1
  0pt 9.5cm
\noindent As a somewhat unrealistic example, suppose that the
\gls{cfg} in \autoref{fig:cyclic_cfg} were generated from the program
shown in \autoref{fig:derive:example_dissassembly1}.  The
\texttt{JMP\_NE} instruction \texttt{C} loads from the memory at
\texttt{rcx+8}, jumping to \texttt{B} if it is non-zero and proceeding
to \texttt{D} otherwise.  This will produce a dynamic \gls{cfg} as in
\autoref{fig:unrolled_cyclic_cfg}, as already discussed, and a
\StateMachine{} as shown in
\autoref{fig:state_machine_for_cyclic_cfg}.

\parshape 1 9.3cm 6.2cm \vspace{-4.35cm}{\parbox{6.2cm}{
    \begin{figgure}
  \begin{centering}
    \texttt{
      \begin{tabular}{lllll}
        \!\!\!\!\!\!A: & MOV  & rdx    &\!\!\!-> & \!\!\!rcx\\
        \!\!\!\!\!\!B: & LOAD & *(rcx) &\!\!\!-> & \!\!\!rcx\\
        \!\!\!\!\!\!C: & \multicolumn{4}{l}{JMP\_NE *(rcx + 8), 0, B}\\
        \!\!\!\!\!\!D: & STORE & \$0 &\!\!\!-> & \!\!\!*(rcx)\\
      \end{tabular}
    }
  \end{centering}
  \captionof{figure}{}
  \label{fig:derive:example_dissassembly1}
  \end{figgure}
}}
\\

\begin{sanefig}
\begin{tikzpicture}[minimum height=1cm, minimum width=6.5cm]
  \node[stateIf] (l1) {\stIf{\entryExpr{A_0}}};
  \node[right = of l1] (dummy1) {};
  \node[stateSideEffect,below = of l1] (l2) {$A_0$: \state{Copy} $\smReg{rdx}{} \rightarrow \smVar{0}$};
  \node[stateIf,below = of l2, inner sep = -5mm] (l3) {$A_0'$: \stIf{{\controlEdgeName}(A_0 \!\rightarrow\! B_0)}};
  \node[stateSideEffect,below = of l3] (l3b) {\state{$\Phi$} $(\smVar{0}, \smVar{4}) \rightarrow \smVar{5}$};
  \node[stateSideEffect,below = of l3b] (l4) {$B_0$: \state{Load} $\ast(\smVar{5}) \rightarrow \smVar{6}$};
  \node[stateSideEffect,below = of l4] (l5) {$C_0$: \stLoad{7}{\smVar{6}+8}};
  \node[stateIf,below = of l5] (l6) {$C_0'$: \stIf{\smVar{7} = 0}};
  \node[stateIf,below = of l6] (l7) {$D_0$: \stIf{\smBadPtr{\smVar{6}}}};

  \node[stateSideEffect] (l11) at (dummy1 |- l2) {$C_2$: \stLoad{1}{\smReg{rcx}{}+8}};
  \node[stateIf] at (l3 -| l11) (l12) {$C_2'$: \stIf{\smVar{1} = 0}};

  \node[stateSideEffect] at (l12 |- l3b) (l8b) {\state{$\Phi$} $(\smVar{0}, \smReg{rcx}{}) \rightarrow \smVar{2}$};
  \node[stateSideEffect] at (l12 |- l4) (l8) {$B_1$: \state{Load} $\ast(\smVar{2}) \rightarrow \smVar{3}$};
  \node[stateSideEffect] at (l8 |- l5) (l9) {$C_1$: \stLoad{4}{\smVar{3}+8}};
  \node[stateIf] at (l9 |- l6) (l10) {$C_1'$: \stIf{\smVar{4} = 0}};

  \node[stateTerminal,below = of l7, minimum width=3.5cm] (lBeta) {\stCrash};
  \node[stateTerminal,minimum width=3.5cm] at (lBeta -| l10)(lAlpha) {\stUnreached};
  \node[stateTerminal,minimum width=3.5cm] at (barycentric cs:lBeta=0.5,lAlpha=0.5)(lGamma) {\stSurvive};

  \draw[->,ifTrue] (l1) -- (l2);
  \draw[->,ifFalse] (l1) -- (l11);
  \draw[->] (l2) -- (l3);
  \draw[->,ifFalse] (l3) -- (l8b);
  \draw[->,ifTrue] (l3) -- (l3b);
  \draw[->] (l3b) -- (l4);
  \draw[->] (l4) -- (l5);
  \draw[->] (l5) -- (l6);
  \draw[->,ifFalse] (l6) -- (lAlpha);
  \draw[->,ifTrue] (l6) -- (l7);
  \draw[->,ifFalse] (l7) -- (lGamma);
  \draw[->,ifTrue] (l7) -- (lBeta);
  \draw[->] (l8b) -- (l8);
  \draw[->] (l8) -- (l9);
  \draw[->] (l9) -- (l10);
  \draw[->,ifTrue] (l10) -- (lAlpha);
  \draw[->,ifFalse] (node cs:name=l10,angle=210) ..controls +(0,-.3) and +(.3,0) .. ++(-.5,-0.5) % Leaving node
      -- ++(-1.9,0) % First horizontal
      .. controls +(-.3,0) and +(0,-.3) .. ++(-.5,.5) % Bend to vertical
      -- ++(0,7) % Vertical
      .. controls +(0,.3) and +(.3,0) .. ++(-.5,.5) % Bend to second horizontal
      -- ++(-.5,0) % Second horizontal
      ..controls +(-.3,0) and +(0,.3) .. (node cs:name=l3b,angle=12);
  \draw[->] (l11) -- (l12);
  \draw[->,ifTrue] (l12.east) .. controls +(.3,0) and +(0,.3) .. ++ (.5,-.5) -- ++(0,-11.08) .. controls +(0,-.3) and +(.3,0) .. ++ (-.5,-.5) -- (lAlpha);
  \draw[->,ifFalse] (l12) -- (l8b);
\end{tikzpicture}
\caption{{\STateMachine} generated from the dynamic \gls{cfg} shown in
  Figure~\ref{fig:cyclic_cfg}.}
\label{fig:state_machine_for_cyclic_cfg}
\end{sanefig}

\subsection{Library functions}
\label{sect:derive:library_functions}

Any non-trivial program will include calls to system library
functions, and {\technique} must incorporate the effects of these
calls into its {\StateMachines}.  It would, in principle, be possible
to treat libraries identically to the main program, allowing the
dynamic \gls{cfg} to cross between the program and its libraries
whenever the program's control flow does.  This approach has two
important disadvantages.  First, it requires the library to be
available during the analysis step, which is not always possible.
More importantly, it does not lead to a particularly effective
analysis.  Many library functions important to the bugs investigated
by {\technique} have simple interfaces but complex implementations;
incorporating these functions into {\technique}'s {\StateMachines}
would overwhelm the \gls{analysiswindow} for little gain.  It would,
for instance, be impractical to detect double-free bugs by
symbolically executing the \texttt{free} function, or at the very
least would require a far more powerful symbolic execution than
{\technique} otherwise needs.

{\Technique} instead relies on manually translating important library
functions into {\StateMachine} programs which can be substituted in
when the program calls one of these functions.  These implementations
are generally quite simplistic and model only the most relevant parts
of the function's behaviour.  For instance, in a real program the
\texttt{write} function takes a buffer of a given size and writes it
to a given file descriptor, updating the file on disk and the file
descriptor offset, and returns either the number of bytes written or
an error code; in a {\technique} {\StateMachine}, it simply returns
the size of the buffer, claiming to have completed successfully
despite discarding most of its arguments.  They are therefore
insufficient to properly investigate bugs which depend on interactions
with library functions (a race which can only be triggered following
an IO error, for instance, will not be detected) , but are sufficient
when library calls incidentally happen near to the buggy code without
meaningfully influencing its behaviour\footnote{Note that even with a
  more complete {\StateMachine} library implementation, {\technique}
  would still be a poor choice for investigating
  environmentally-dependent bugs.  \protect\Glspl{bugenforcer} only
  influence the program's scheduling and so are unlikely to be
  effective for bugs which depend on more complicated interactions
  with the environment.}.

Some library functions are more directly relevant to {\technique} and
deserve more complete implementations.  \texttt{pthread\_mutex\_lock}
and \texttt{pthread\_mutex\_unlock}, for instance, directly influence
the program's concurrency behaviour, and are hence important to many
concurrency bugs.  \autoref{fig:library_mux} shows how they are
implemented in {\technique}.  Note that the lock operation does not
include any logic to wait for the lock to be released, as such
operations are difficult to express in the acyclic {\StateMachine}
structure, but instead simply asserts that it is not currently held.
That is sufficient: the key property of a lock is that it cannot
simultaneously be held by multiple threads, and this assertion ensures
that any paths on which that property is violated will evaluate to
\textsc{Unreached} and hence be discarded by the rest of the analysis.

\begin{sanefig}
  \centerline{
    {\hfill}
  \subfigure[][pthread\_mutex\_lock]{
    \begin{tikzpicture}[minimum height=1cm, minimum width=7cm]
      \node (l1) [stateSideEffect] {\stStartAtomic};
      \node (l2) [below = of l1, stateSideEffect] {\stLoad{0}{\mathit{arg0}}};
      \node (l3) [below = of l2, stateSideEffect] {\stAssert{\smVar{0} = 0}};
      \node (l4) [below = of l3, stateSideEffect] {\stStore{\mathit{tid}}{\mathit{arg0}}};
      \node (l5) [below = of l4, stateSideEffect] {\stEndAtomic};
      \draw[->] (l1) -- (l2);
      \draw[->] (l2) -- (l3);
      \draw[->] (l3) -- (l4);
      \draw[->] (l4) -- (l5);
    \end{tikzpicture}
  }{\hfill}
  \subfigure[][pthread\_mutex\_unlock]{
    \begin{tikzpicture}[minimum height=1cm, minimum width=7cm]
      \node (l1) [stateSideEffect] {\stStartAtomic};
      \node (l2) [below = of l1, stateSideEffect] {\stLoad{0}{\mathit{arg0}}};
      \node (l3) [below = of l2, stateSideEffect] {\stAssert{\smVar{0} = \mathit{tid}}};
      \node (l4) [below = of l3, stateSideEffect] {\stStore{0}{\mathit{arg0}}};
      \node (l5) [below = of l4, stateSideEffect] {\stEndAtomic};
      \draw[->] (l1) -- (l2);
      \draw[->] (l2) -- (l3);
      \draw[->] (l3) -- (l4);
      \draw[->] (l4) -- (l5);
    \end{tikzpicture}
  }
    {\hfill}
  }
  \vspace{-12pt}
  \caption{{\STateMachine} models for the pthread\_mutex\_lock and
    pthread\_mutex\_unlock functions.  $\mathit{arg0}$ is an
    expression for the first argument register.  $\smVar{x}$ is a
    fresh {\StateMachine} temporary.  $\mathit{tid}$ is the constant 1
    for the \gls{crashingthread} and 2 for the interfering one.}
  \label{fig:library_mux}
\end{sanefig}

Similarly, correctly modelling the \texttt{free} function is
particularly important when investigating double-free bugs.
{\Technique} uses two implementations of \texttt{free}; one, the
crashing \texttt{free}, for the call which is being investigated as a
potential crash site, and one, the non-crashing \texttt{free}, for
every other call\footnote{Note that this includes other calls to
  \texttt{free} in the \gls{crashingthread}.}.  Both are shown in
\autoref{fig:library_free}.  Non-crashing \texttt{free}s set the
special $\mathit{last\_free}$ address to the pointer which was
released, and the crashing \texttt{free} then asserts that the pointer
which it releases is not the one which was most recently released.
This scheme is clearly not capable of detecting all possible
double-free bugs, but it is sufficient for the most common kind.

\begin{sanefig}
  \centerline{
  \subfigure[][Crashing \texttt{free}]{
    \begin{tikzpicture}[minimum height=1cm,minimum width=7cm]
      \node (l1) [stateSideEffect] {\stLoad{0}{\mathit{last\_free}} };
      \node (l2) [stateIf, below = of l1] {\stIf{\smVar{0} = \mathit{arg0}} };
      \node (l3) [stateTerminal,minimum width=3.5cm] at (-2cm,-4cm) {\stCrash };
      \node (l4) [stateTerminal,minimum width=3.5cm] at (2cm,-4cm) {\stSurvive };
      \draw[->] (l1) -- (l2);
      \draw[->,ifTrue] (l2) -- (l3);
      \draw[->,ifFalse] (l2) -- (l4);
    \end{tikzpicture}
    \label{fig:library_free:crashing}
  }
  \subfigure[][Non-crashing \texttt{free}]{
    \raisebox{20mm}{
    \begin{tikzpicture}[minimum height=1cm,minimum width=7cm]
      \node [stateSideEffect] {\stStore{\mathit{arg0}}{\mathit{last\_free}} };
    \end{tikzpicture}
    }
    \label{fig:library_free:non_crashing}
  }
  }
  \vspace{-12pt}
  \caption{{\STateMachine} implementations of the \texttt{free}
    function. $\mathit{arg0}$ is an expression for the first argument
    register.  $\mathit{last\_free}$ is any fixed memory location
    which is not used by the program.}
  \label{fig:library_free}
\end{sanefig}

\section{Simplifying the {\StateMachine}}
\label{sect:derive:simplify_sm}

Much as an optimising compiler optimises its intermediate form so as
to make the generated program run faster, {\technique} simplifies its
{\StateMachines} so as to make symbolically executing them less
expensive.  The main techniques used to do so are:
\begin{itemize}
\item \emph{Dead code elimination}, which is used to eliminate
  redundant updates to {\StateMachine} temporaries.
\item \emph{Copy propagation}, based on the algorithm from the
  dcc~\cite{Cifuentes1994} decompiler, is used to shorten long chains
  of assignments.  One minor extension present in {\technique} but not
  dcc is that {\technique} can make use of \state{Assert} side-effects
  during this transformation, so that, for instance, if $x$ is
  asserted to be less than $7$ then the expression $x > 22$ can be
  replaced by \false.  This does not require any important changes to
  the algorithm, beyond a few simple rules describing when such
  rewrites are valid.
\item \emph{{\stPhi} elimination}, which is used to turn SSA {\stPhi}
  side effect states into \state{Copy} ones.  This reifies the
  {\stPhi} states' implicit dependence on {\StateMachine} control flow
  into the explicit structure of a {\StateMachine} expression, making
  it easier for the other simplification steps to manipulate and use
  it.
\item \emph{Alias analysis}, which determines how \state{Store} and
  \state{Load} operations might interact and then uses this
  information to forward values from \state{Store}s to \state{Load}s,
  potentially eliminating both.
\item Various minor peephole simplifications, such as removing empty
  atomic regions or combining long chains of related \state{If}
  operations.
\end{itemize}
The effect of these simplification passes is to take a {\StateMachine}
which represents all of the program's behaviour in the
\gls{analysiswindow} and transform it to one which represents only the
behaviour which is most relevant to the bug under
investigation.

\section{Building the interfering thread's \StateMachines}
\label{sect:derive:write_side}

At this stage, {\technique} has built the crashing thread's
{\StateMachine} for the bug which is to be investigated and it must
now determine whether it might crash due to an atomicity violation.
In principle, this should consider every possible interleaving of the
crashing {\StateMachine} with every \gls{alpha}-instruction long trace
of any other thread in the program, but this would clearly be
completely impractical for all but the most trivial programs.
Fortunately, most such traces can be dismissed without ever needing to
explicitly enumerate them, and this usually makes it possible to
complete the analysis in a reasonable amount of time.

This reduction depends on information in the \gls{programmodel}, which
has not yet been described (see \autoref{sect:program_model}).  For
now, assume that it is possible to map from one memory-accessing
instruction to the set of instructions which might access the same
memory location.  This makes it possible to derive three useful sets
of instructions: $i2c$, the set of stores which might alias with a
load operation in the \gls{crashingthread}; $c2i$, the set of loads
which might alias with a store in the \gls{crashingthread}; and $\beta
= c2i \cup i2c$.  Informally, $i2c$ is the set of instructions which,
when added to the \gls{interferingthread}, might send data to the
\gls{crashingthread}, $c2i$ those which might receive data from it,
and $\beta$ those which communicate in either direction.

These three sets can then be used to restrict the set of interfering
traces which need to be considered.  Most obviously, it is safe to
discard all instructions in the \gls{interferingthread} after the last
member of $i2c$, as these cannot possibly influence the behaviour of
the \gls{crashingthread} and hence cannot influence whether the bug of
interest will reproduce.  It is also possible to discard any
instructions prior to the first member of $\beta$.  Suppose, for
instance, that the \gls{interferingthread} consists of the instruction
sequence $aBc$, where $a$ is some sequence of instructions none of
which belong to $\beta$, $B$ is a member of $\beta$, and $c$ is
another sequence of instructions.  Any bugs present in that sequence
will also be present in the sequence $Bc$.  $a$ cannot directly
influence the \gls{crashingthread}, by definition, and so its only
possible effect is on the behaviour of the \gls{interferingthread}, by
restricting the possible values of program memory and registers when
it starts.  {\Technique} will report a possible bug if \emph{any}
possible initial values could lead to an atomicity violation, and the
set of possible values after running $a$ is clearly a subset of the
set of all possible values, and so discarding $a$ can never lead to
any possible bugs being omitted.  It is therefore always safe to do
so.

\todo{Not sure how much this adds, especially since I don't eval it.}
While safe, it is not always desirable to discard $a$ if the
restricted set of initial values would have been easier to analyse.
For instance, if the bug to be investigated is a bad pointer
dereference, knowing that the value stored into a shared structure had
previously been dereferenced by the interfering thread, and hence must
definitely be a valid pointer, is often useful.  The approach taken by
{\implementation} is to first generate a set of minimal \glspl{cfg}
(i.e. one in which every root is a member of $\beta$) and to then
extend the \glspl{cfg} backwards to include a small amount of
additional context provided that doing so does not increase the number
of paths through the \gls{cfg} or cause the length of any path to
exceed \gls{alpha}.  In other words, a \gls{cfg} rooted at instruction
A will be extended to include its predecessor instruction B provided
that B is A's only predecessor and the longest path starting at A is
at most $\alpha - 1$ instructions long.  Because the number of paths
through the \gls{cfg} does not increase, this leads to only a modest
increase in the eventual cost of symbolically executing the resulting
{\StateMachine}, and this extra context can often provide useful hints
to the analysis.

\subsection[Building the \glsentrytext{interferingthread} \glsentrytext{cfg}s]{Building the \gls{interferingthread} \glspl{cfg}}

The procedure for building the \gls{interferingthread}
{\StateMachines} is similar to that for building a
\gls{crashingthread} one: determine some starting point instructions,
build a static \gls{cfg} of the ``nearby'' instructions, unroll that
static \gls{cfg} to a dynamic one, and decompile the dynamic \gls{cfg}
into a {\StateMachine}.  The nature of the starting point is, however,
quite different (for a crashing {\StateMachine}, the single crashing
instruction; for an interfering one, the $i2c$ and $c2i$ sets), and
this leads to quite different algorithms and quite different results.
In particular, the \gls{interferingthread} algorithms generally work
forwards through the \gls{cfg}, whereas the \gls{crashingthread} ones
worked backwards, and the \gls{interferingthread} algorithms can
produce multiple disjoint {\StateMachines} where the
\gls{crashingthread} ones only ever produced a single {\StateMachine}.
I now describe these differences in more detail.

Building the static \glspl{cfg} is straightforward.  Starting from
each member of $\beta$, {\technique} explores forwards for \gls{alpha}
instructions (crossing function boundaries when necessary), building
the \glspl{cfg} as it goes and merging any \gls{cfg} fragments which
are reachable from multiple $\beta$ instructions.  These are then
trimmed back down to contain only those instructions which can reach a
member of $i2c$ within \gls{alpha} instructions.  Note that this can
sometimes generate multiple disjoint \glspl{cfg}, rather than the
single static \gls{cfg} generated by the \gls{crashingthread}
algorithm.  Each such \gls{cfg} is treated independently, and so a
single \gls{crashingthread} can generate multiple interfering dynamic
\glspl{cfg} and hence multiple interfering {\StateMachines}, each of
which is processed independently by the rest of the analysis.

\begin{sanefig}
\begin{algorithmic}
  \Function{staticToDynamicInterfering}{}
    \State {Compute initial node positions}
    \While {\Gls{cfg} contains a cycle}
       \State $(\mathit{start}, \mathit{end}) \gets \textsc{findCycleEdgeInterfering}()$
       \State $\mathit{newPos} \gets \textsc{combinePositions}(\text{current positions of } \mathit{start} \text{ and } \mathit{end})$
       \State {Remove edge $(\mathit{start}, \mathit{end})$}
       \If {$\min_c(\mathit{newPos}.\mathit{min\_from}_{\mathit{end}}(c)) + \min_i(\mathit{newPos}.\mathit{min\_to}_{\mathit{end}}(i)) < \alpha$}
           \State $\mathit{newNode} \gets \text{duplicate } \mathit{end}$
           \State {Create a new edge $(\mathit{start}, \mathit{newNode})$}
           \For {Edges $(\_, \mathit{end}')$ leaving $\mathit{end}$}
              \State {Create a new edge $(\mathit{newNode}, \mathit{end}')$}
           \EndFor
           \State {Set position of $\mathit{newNode}$ to $\mathit{newPos}$}
       \EndIf
       \State {Recalculate $\mathit{min\_from}$ for $\mathit{end}$ and its successors, if necessary}
    \EndWhile
  \EndFunction
\end{algorithmic}
\caption{Loop unrolling algorithm for interfering thread CFGs.
  \textsc{findCycleEdgeInterfering} and \textsc{combinePositions} are described
  in the text.}
\label{fig:derive:store_cfg_unroll_alg}
\end{sanefig}

The static interfering \glspl{cfg} are then converted to dynamic ones
using the \textsc{staticToDynamicInterfering} algorithm given in
\autoref{fig:derive:store_cfg_unroll_alg}.  This is more complicated
than the \textsc{staticToDynamicCrashing}.  In particular, determining
whether a loop has been sufficiently unrolled is more difficult: where
the \gls{crashingthread} algorithm preserves \gls{alpha}-instruction
paths which end at the (single) crashing instruction, and so need only
track an instruction's distance from that crashing instruction, the
\gls{interferingthread} one must preserve those which go between from
any member of $\beta$ to any member of $i2c$, requiring a more
complicated notion of node position.  In this algorithm, the position
of a node in the graph is given by two maps, $\mathit{min\_from}$ and
$\mathit{min\_to}$:
\begin{itemize}
\item
  $\mathit{min\_from}_l(c)$ is the number of instructions on the
  shortest path from $c$ to $l$, where $c \in \beta$.
\item
  $\mathit{min\_to}_l(i)$ is the number of instructions on the
  shortest path from $l$ to $i$, where $i \in i2c^\sharp$.
  $i2c^\sharp$ consists of all members of $i2c$, plus all of the
  \gls{cfg} nodes created by duplicating one of those instructions.
\end{itemize}
The length of the shortest path from an instruction $c
\in \beta$ to $i \in i2c^\sharp$ via the $l$ is then
$\mathit{min\_from}_l(c) + \mathit{min\_to}_l(i) + 1$, and so it is safe
to discard any instruction $l$ where
\begin{displaymath}
\min_{c \in \beta}\left(\mathit{min\_from}_l(c)\right) + \min_{i \in i2c^\sharp}\left(\mathit{min\_to}_l(i)\right) {\geq} \alpha
\end{displaymath}
The asymmetry, taking the distance from only ``true'' members of
$\beta$ but to any duplicate of a member of $i2c$, is perhaps
surprising.  The key observation is that every path which starts at a
duplicated member of $\beta$ will have a matching path which starts at
the original member, and so the ones which start at the duplicate
instruction are redundant\footnote{The symmetrical statement is also
  true: every path which ends in a member of $i2c^\sharp$ has a
  matching path which ends at a member of $i2c$.  It would therefore
  also be correct to discard paths which end at a duplicate member of
  $i2c$.  It would not, however, be correct to combine the two
  observations and discard all paths which either start with a
  duplicate of $\beta$ or end with a duplicate of $i2c$, as there
  would then be little point in having those duplicates.}.

The remaining differences between \textsc{staticToDynamicInterfering}
and \textsc{staticToDynamicCrashing} are simpler to explain:
\begin{itemize}
\item When \textsc{staticToDynamicCrashing} duplicates a node it
  duplicates all of that nodes incoming edges, whereas
  \textsc{staticToDynamicInterfering} duplicates the nodes outgoing
  edges.  This reflects the fact that interfering \glspl{cfg} are
  built up forwards from known instructions while crashing ones are
  built up backwards.
\item Whereas \textsc{findCycleEdgeCrashing} found a cycle-closing
  edge to break by searching backwards from the crashing instruction,
  \textsc{findCycleEdgeInterfering} finds one by performing a
  breadth-first search forwards from one of the roots of the
  \gls{cfg}.  If the graph reachable from that root is acyclic then it
  moves on to the next one.
\item \textsc{staticToDynamicInterfering} must maintain its node
  positioning information, which was not necessary for
  \textsc{staticToDynamicCrashing}.  This work is done primarily by
  the \textsc{combinePositions} function which is used to compute the
  label for the new node which would be produced by duplicating
  $\mathit{end}$.  This node will have the same outgoing edges as
  $\mathit{end}$, and so the same $min\_to$ label, and a single
  incoming edge from $\mathit{edge}.\mathit{start}$, and hence a
  $\mathit{min\_from}$ label which is just
  $\mathit{edge}.\mathit{start}$'s $\mathit{min\_from}$ with one added
  to every value.
\end{itemize}

\newcommand{\shortrightarrow}{\begin{tikzpicture}[baseline= -1ex*.75]
    \draw[->] (0,0) -- ++(.25,0);
  \end{tikzpicture}
  \hspace{-1pt}
}

\noindent
As an example, consider this cyclic \gls{cfg}:\\
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$} edge [in=30,out=-30,loop] ();
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (dummy) [left = .3 of C] {};
  \node (dummy2) [right = .5 of B] {};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C.west) .. controls +(-.3,0) and +(0,-.3) .. ++(-.5,.5) -- ++(0,2.2) .. controls +(0,.3) and +(-.3,0) .. ++(.5,.5) -- (A.west);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (C) (dummy) (dummy2)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
             & \multicolumn{1}{c}{$\mathit{min\_to}$} & \multicolumn{2}{c}{$\mathit{min\_from}$} & overall min\\
             & $C$ & $A$ & $C$ \\
      $A$    & 2   & 0   & 1 & 2\\
      $B$    & 1   & 1   & 2 & 2\\
      $C$    & 0   & 2   & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent $C$, in green, is a member of $i2c$ (and therefore also
$\beta$); $A$, in blue, is a member of $\beta$ but not $i2c$.  The
overall min column is the minimum $\mathit{min\_to}$ value plus the
minimum $\mathit{min\_from}$ one; it gives the number of edges on the
shortest path involving a given node which starts at a member of
$\beta$ and ends at a member of $i2c^\sharp$.  Suppose that we wish to
render this graph acyclic whilst preserving all paths of length 5 or
less.  \textsc{findCycleEdgeCrashing} will find $A \shortrightarrow B
\shortrightarrow B$, of length three, as the first cyclic path, and so
the algorithm will break the $B \shortrightarrow B$ edge by
duplicating $B$, producing this new graph:\\
\noindent \centerline{ \inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$} edge [in=210,out=150,loop,killEdge] ();
  \node (B1) [NewCfgInstr, right=of B] {$B_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (dummy) [left = .6 of B] {};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->,newCfgEdge] (B) to [bend left=10] (B1);
  \draw[->,newCfgEdge] (B1) to [bend left=10] (B);
  \draw[->,newCfgEdge] (B1) -- (C);
  \draw[->] (C.west) -- ++(-.4,0) .. controls +(-.3,0) and +(0,-.3) .. ++(-.5,.5) -- ++(0,2.23) .. controls +(0,.3) and +(-.3,0) .. ++(.5,.5) -- (A.west);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (dummy)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
            & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$ \\
      $A$   & 2   & 0   & 1 & 2\\
      $B$   & 1   & 1   & 2 & 2\\
      $B_1$ & 1   & 2   & 3 & 3\\
      $C$   & 0   & 2   & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent
New nodes and edges are shown in red, and edges which have been
removed are shown crossed through.  There are now no more length three
paths in the graph, and so \textsc{findCycleEdgeCrashing} will report
the two length four paths $A \shortrightarrow B \shortrightarrow
B_1 \hspace{-.5pt} \shortrightarrow B$ and $A \shortrightarrow B
\shortrightarrow \hspace{1pt} C \shortrightarrow A$.  This will cause
the $B_1 \hspace{-.5pt} \shortrightarrow B$ and $C \shortrightarrow A$
edges to be broken, as shown:\\
\noindent
\centerline{ \inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (A1) [NewCfgInstr,right=of C] {$A_1$};
  \node (dummy) [left = .66 of B] {};
  \draw[->] (A) -- (B);
  \draw[->,newCfgEdge] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->] (B1) -- (C);
  \draw[->] (B1) to [bend left=10] (B);
  \draw[->,newCfgEdge] (C) -- (A1);
  \draw[->,killEdge] (C.west) -- ++(-.4,0) .. controls +(-.3,0) and +(0,-.3) .. ++(-.5,.5) -- ++(0,2.23) .. controls +(0,.3) and +(-.3,0) .. ++(.5,.5) -- (A.west);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (dummy)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
      labels & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2   & 0   & $\infty$ & 2\\
      $A_1$ & 2   & 3   & 1        & 3\\
      $B$   & 1   & 1   & 2        & 2\\
      $B_1$ & 1   & 2   & 3        & 3\\
      $C$   & 0   & 2   & 0        & 0\\
    \end{tabular}
    ~
  };
\end{tikzpicture}
}
}\\
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [NewCfgInstr, right=of B1] {$B_2$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,killEdge] (B1) to [bend left=10] (B);
  \draw[->,newCfgEdge] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->,newCfgEdge] (B2) to [bend left=10] (B1);
  \draw[->,newCfgEdge] (B2) -- (C);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
            & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2   & 0   & $\infty$ & 2\\
      $A_1$ & 2   & 3 & 1 & 3\\
      $B$   & 1   & 1 & 2 & 2\\
      $B_1$ & 1   & 2 & 3 & 3\\
      $B_2$ & 1   & 3 & 4 & 4\\
      $C$   & 0   & 2 & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent
\textsc{findCycleEdgeCrashing} will now discover two length five cyclic
paths, $A \shortrightarrow B \shortrightarrow \hspace{1pt} C \shortrightarrow A_1
\shortrightarrow B$ and $A \shortrightarrow B \shortrightarrow B_1
\hspace{-.5pt} \shortrightarrow B_2 \shortrightarrow B_1$.  The first is eliminated
by duplicating $B$: \\
\noindent
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [NewCfgInstr, below=of A1] {$B_3$};
  \draw[->] (A) -- (B);
  \draw[->,killEdge] (A1) -- (B);
  \draw[->,newCfgEdge] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->,newCfgEdge] (B3) -- (C);
  \draw[->,newCfgEdge] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
            & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2 & 0 & $\infty$ & 2\\
      $A_1$ & 2 & 3 & 1        & 3\\
      $B$   & 1 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 2 & 3        & 3\\
      $B_2$ & 1 & 3 & 4        & 4\\
      $B_3$ & 1 & 4 & 2        & 3\\
      $C$   & 0 & 2 & 0        & 0\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent
The second, by contrast, can be eliminated without needing to
duplicate any further instructions.  Were $B_1$ to be duplicated, the
shortest path from $A$ to $C$ which used the new instruction would be
of length 6, exceeding the desired maximum path length, and so the
cycle-closing edge can simply be deleted:
\\
\noindent
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [CfgInstr, below=of A1] {$B_3$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->,killEdge] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
         & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
         & $C$ & $A$ & $C$\\
      $A$   & 2 & 0 & $\infty$ & 2\\
      $A_1$ & 2 & 3 & 1        & 3\\
      $B$   & 1 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 2 & 3        & 3\\
      $B_2$ & 1 & 3 & 4        & 4\\
      $B_3$ & 1 & 4 & 2 & 3\\
      $C$  & 0 & 2 & 0        & 0\\
      New label & 1 & 4 & 5 & 5\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent
This process iterates, removing one cycle-completing edge at a time,
until the graph becomes completely acyclic:
\\
\noindent
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [CfgInstr, below=of A1] {$B_3$};
  \node (C1) [DupeInterferingCfgInstr, below=of B3] {$C_1$};
  \node (B4) [CfgInstr, right=of B3] {$B_4$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) -- (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (B4);
  \draw[->] (C) -- (A1);
  \draw[->] (B3) -- (C1);
  \draw[->] (B4) -- (C1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (C1) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
            & \multicolumn{2}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $C_1$ & $A$ & $C$ \\
      $A$   & 2 & 5 & 0 & $\infty$ & 2\\
      $A_1$ & $\infty$ & 2 & 3 & 1 & 3\\
      $B$   & 1 & 4 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 4 & 2 & $\infty$ & 3\\
      $B_2$ & 1 & 4 & 3 & $\infty$ & 4\\
      $B_3$ & $\infty$ & 1 & 4 & 2 & 3\\
      $B_4$ & $\infty$ & 1 & 5 & 3 & 4\\
      $C$   & 0 & 3 & 2 & 0 & 0\\
      $C_1$ & $\infty$ & 0 & 5 & 3 & 3\\
    \end{tabular}
  };
\end{tikzpicture}
}
}
\\
\noindent
As desired, the graph has been rendered acyclic while preserving all
paths of length up to five instructions between $\beta$ and $i2c$.

\section{Generating a verification condition}
\label{sect:using:check_realness}

Previous sections have described how to generate pairs of
{\StateMachines} representing fragments of the program which might
interact in interesting ways when run concurrently.  The next step is
to determine, for each pair, whether running the two {\StateMachines}
in parallel might suffer an atomicity violation bug, and if so under
what circumstances.

The core of the approach is to use symbolic execution~\cite{King1976}
to convert the {\StateMachines} into two predicates over the
{\StateMachine} execution environment: the
\gls{verificationcondition}, which is true when interleaving the two
    {\StateMachines} might lead to a crash, and the
    \gls{inferredassumption}, which is true when executing them
    atomically in series will not.  The program potentially has an
    atomicity violation bug it these are ever simultaneously true, and
    so {\technique} generates a run-time \gls{bugenforcer} whenever it
    cannot show that their conjunction is unsatisfiable.  Note that
    discovering a crashing interleaving of the two threads will
    \emph{not} cause an enforcer to be generated if running them
    atomically also crashes; as discussed in
    \autoref{sect:types_of_bugs}, {\technique} is only concerned with
    atomicity violation bugs, and this provides a very useful
    reduction in the number of false positives which must be checked
    at run-time.

\subsection{Symbolically executing {\StateMachines}}
\label{sect:derive:symbolic_execute}

The symbolic execution engine used by {\implementation} is, for the
most part, quite conventional.  I give only a brief overview of the
most important features here:
\begin{itemize}
\item The symbolic execution engine considers only a single
  {\StateMachine} at a time, even when investigating the parallel
  behaviour of two threads.  Rather than investigating thread
  interleavings in the symbolic execution engine, as is done in, for
  instance, ESD~\cite{Zamfir2010}, {\technique} instead encodes them
  into special cross-product {\StateMachines}, described in
  \autoref{sect:using:build_cross_product}.

\item The {\StateMachine} speicher is represented by a sequence of
  update operations in the style of McCarthy's theory of
  arrays\needCite{}, rather than attempting to maintain separate
  models for particular objects or memory locations.  \state{Store}
  operations are implemented by simply adding them to the update list
  and \state{Load} operations have to scan back through the list to
  find a matching \state{Store}.

  This is not a particularly efficient approach.  {\Implementation}
  relies on two facts to mitigate the its performance problems.
  First, where the relationship between \state{Store}s and
  \state{Load}s is simple, the {\StateMachine} simplifiers will
  forward data between them before the symbolic execution starts,
  eliminating both from the {\StateMachine}.  Second,
  {\implementation} maintains a cache of previous aliasing queries,
  and so if, for instance, two paths through the {\StateMachine} both
  need to determine whether \state{Store} A might alias with
  \state{Load} B the symbolic execution engine usually only needs to
  do so once.

\item Aliasing problems are resolved lazily.  This means that if the
  engine must execute a \state{Load} operation and cannot immediately
  determine which \state{Store} operation to use it does not cause an
  immediate fork of its state, but instead causes the \state{Load} to
  return a {\StateMachine} expression which describes the aliasing
  query and selects an appropriate result.  {\Implementation} only
  determines which precise \state{Store} operation should provide the
  loaded data when that data is needed.  This is often faster then
  resolving these queries eagerly\footnote{Note that this is not quite
    the same as the lazy abstraction algorithm used in, for instance,
    BLAST~\cite{Henzinger2002}, which adjusts which aspects of program
    state are represented in the abstract state.  {\Implementation}
    uses a fixed abstraction; the laziness here lies in the \emph{way}
    the abstract state is represented, not in \emph{what} is
    represented.}.

\item The engine does not use any kind of incremental abstraction
  technique such as CEGAR~\cite{Clarke2000}.  This is primarily because
  of the use of a flat memory representation: the speicher is a single
  object so it makes little sense to talk about modelling one part of
  it accurately and another part inaccurately, and without that CEGAR
  provides little benefit.  The use of lazy aliasing resolution
  provides some of the benefit of CEGAR, as it allows some aliasing
  queries which do not affect program behaviour to be skipped.

\item Unlike most symbolic execution engines, the one used by
  {\implementation} does not attempt to detect when it revisits a
  previously-visited configuration.  This is safe because
  {\StateMachines} are acyclic: any path through a {\StateMachine} can
  visit a given state at most once so there is no possibility of a
  single revisiting a configuration and entering an infinite loop.  It
  is also, surprisingly, reasonably performant, because it is
  extremely rare for multiple paths to visit the same configuration,
  and so there is little scope for re-using configurations to reduce
  duplicated work.  This is largely because {\technique} simplifies
  the {\StateMachine} before attempting to symbolically execute it and
  these simplifications tend to remove most easily-exploited forms of
  redundancy.
\end{itemize}
The resulting engine is generally adequate but rarely impressive, and
it is likely that using a more powerful one could usefully improve
{\implementation}'s performance.  Nevertheless, even this relatively
simple system is sufficient to generate some results in a useful
selection of cases.

\subsection{Deriving the inferred assumption}
\label{sect:derive:inferred_assumption}

As previously mentioned, the \gls{inferredassumption} is the condition
under which the program would avoid the bug if the crashing and
interfering {\StateMachines} were run atomically.  It is formed by the
conjunction of two sub-conditions: \gls{ci-atomic}, the condition
under which running the crashing {\StateMachine} and then the
interfering one avoids the bug, and \gls{ic-atomic}, the condition
under which running them in the opposite order does.  These
sub-conditions can be easily generated by concatenating the two
{\StateMachines} in the appropriate order, symbolically executing
them, and taking the disjunction of the path constraints for all paths
which evaluate to \textsc{Survive}.  Note that {\StateMachine}
environments which cause one of the concatenations to evaluate to
\textsc{Unreached} are excluded by the \gls{inferredassumption},
reflecting the intended semantics of \textsc{Unreached} as a path
which is of no interest to the rest of the analysis.

\subsection{Building cross-product {\StateMachines}}
\label{sect:using:build_cross_product}

{\Technique}'s symbolic execution engine is only capable of exploring
the behaviour of one {\StateMachine} at a time, but the
\gls{verificationcondition} is defined in terms of the parallel
composition of two {\StateMachines}.  {\Technique} therefore builds a
new {\StateMachine}, the \emph{cross-product {\StateMachine}} which
emulates this parallel composition.  The chief complication here is
that it is not possible to express an arbitrary cross-product
operation in the {\StateMachine} language\footnote{In particular, the
  number of steps which such a cross-product emulating {\StateMachine}
  could take would be statically bounded, and so the number of states
  of the input {\StateMachines} which it could consider would also be
  statically bounded, and so there is no single {\StateMachine} which
  can compute the cross-product of two other arbitrary
  {\StateMachines}.}, and so it must be split between a first, host,
stage, which runs in the same (Turing-complete) model of computation
as {\technique} itself, and a second, embedded, stage, which runs in
the (less powerful but symbolically executable) model defined by the
{\StateMachine} language.

\begin{sanefig}
  \newlength{\extrapadA}
  \setlength{\extrapadA}{3mm}
  \newlength{\extrapadB}
  \setlength{\extrapadB}{3mm}
  \newcommand{\midcolumn}{~\hspace{\extrapadA}$\Rightarrow$\hspace{\extrapadB}~}
  \newcommand{\lastcolumn}[1]{\production{#1}}
  \newcommand{\minheight}{2cm}
  \newcommand{\minwidth}{2.8cm}
  {\hfill}
  \begin{tabular}{m{3.7cm}m{10.9cm}}
    Terminals: & {\STateMachine} states \\
    Non-terminals: & Pairs \graphNT{$A, B$}, where $A$ is a fragment of the crashing {\StateMachine} and $B$ a fragment of the interfering one. \\
    {\raggedright Initial non-terminal:} & \graphNT{$A_0, B_0$}, where $A_0$ is the entire crashing {\StateMachine} and $B_0$ the entire interfering one, after renaming apart any common {\StateMachine} temporaries. \\
    \raisebox{12pt}{Productions:} &
    \begin{tabular}{lcc @{~\hspace{2.02cm}~} r}
      \tikz[baseline=(current bounding box.center)]{
        \node [style=graphNT, minimum height = \minheight, minimum width=\minwidth] {
          $\begin{tikzpicture}[baseline=(current bounding box.center), minimum height = 0, minimum width = 0]
            \node at (0,0) (r) [stateIf] {\stIf{m}};
            \node at (-5mm,-10mm) (A) {$A_0$};
            \node at (5mm,-10mm) (B) {$A_1$};
            \draw[->,ifTrue] (r) -- (A);
            \draw[->,ifFalse] (r) -- (B);
          \end{tikzpicture}, B$
        };
      } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
        \node at (0,0) (r) [stateIf] {\stIf{m}};
        \node at (-10mm, -10mm) (A) [style=graphNT] { $A_0, B$ };
        \node at (10mm, -10mm) (B) [style=graphNT] { $A_1, B$ };
        \draw[->,ifTrue] (r) -- (A);
        \draw[->,ifFalse] (r) -- (B);
      \end{tikzpicture} & \lastcolumn{1_a} \\

      \tikz[baseline=(current bounding box.center)]{
        \node [style=graphNT, minimum height = \minheight, minimum width=\minwidth] {
          $\hspace{0.2mm}A, \begin{tikzpicture}[baseline=(current bounding box.center), minimum height = 0, minimum width = 0]
            \node at (0,0) (r) [stateIf] {\stIf{m}};
            \node at (-5mm,-10mm) (A) {$B_0$};
            \node at (5mm,-10mm) (B) {$B_1$};
            \draw[->,ifTrue] (r) -- (A);
              \draw[->,ifFalse] (r) -- (B);
          \end{tikzpicture}$
        };
      } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
        \node at (0,0) (r) [stateIf] {\stIf{m}};
        \node at (-10mm, -10mm) (A) [style=graphNT] { $A, B_0$ };
        \node at (10mm, -10mm) (B) [style=graphNT] { $A, B_1$ };
        \draw[->,ifTrue] (r) -- (A);
        \draw[->,ifFalse] (r) -- (B);
      \end{tikzpicture} & \lastcolumn{1_b} \\

      \tikz[baseline=(current bounding box.center)]{
        \node [style=graphNT, minimum height = \minheight, minimum width=\minwidth] {
          \hspace{2.68mm}
          \begin{tikzpicture}[baseline=(current bounding box.center), minimum height=0, minimum width = 0]
            \node at (0,0) (r) [stateSideEffect, minimum height=15pt, minimum width=1em] {$a$};
            \node at (0,-10mm) (A) {$A$};
            \draw[->] (r) -- (A);
          \end{tikzpicture}\hspace{2.78mm},\hspace{2.68mm}\begin{tikzpicture}[baseline=(current bounding box.center), minimum height=0, minimum width = 0]
            \node at (0,0) (r) [stateSideEffect, minimum height=15pt, minimum width=1em] {$b$};
            \node at (0,-10mm) (A) {$B$};
            \draw[->] (r) -- (A);
          \end{tikzpicture}
          \hspace{2.58mm}
        };
      } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
        \node at (0,0) (r) [stateIf] {\stIf{\happensBefore{a}{b}}};;
        \node at (-10mm,-12mm) [stateSideEffect,minimum height=15pt, minimum width=1em] (A) { $a$ };
        \node at (10mm,-12mm) [stateSideEffect,minimum width=1em, minimum height=15pt] (B) { $b$ };
        \node at (-10mm,-28mm) (A2) [style=graphNT, minimum height=1cm, minimum width=0] {
          $A, \begin{tikzpicture}[minimum height=0]
            \node at (0,0) (rr) [stateSideEffect,minimum width=1em, minimum height=15pt] {$b$};
            \node at (0,-10mm) (rb) {$B$};
            \draw[->] (rr) -- (rb);
          \end{tikzpicture}$
        };
        \node at (10mm,-28mm) (B2) [style=graphNT, minimum height=1cm, minimum width = 0] {
          $\begin{tikzpicture}[minimum height=0]
            \node at (0,0) (rr) [stateSideEffect,minimum width=1em, minimum height=15pt] {$a$};
            \node at (0,-10mm) (rb) {$A$};
            \draw[->] (rr) -- (rb);
          \end{tikzpicture}, B$
        };
        \draw[->,ifTrue] (r) -- (A);
        \draw[->] (A) -- (A2);
        \draw[->,ifFalse] (r) -- (B);
        \draw[->] (B) -- (B2);
      \end{tikzpicture} & \lastcolumn{2} \\

      \tikz[baseline=(current bounding box.center)]{
        \node [style=graphNT, minimum height = \minheight, minimum width=\minwidth] {
          \hspace{3mm}%
          \tikz[baseline = (charrr.base), minimum height = 0, minimum width = 0]{\node [stateTerminal] (charrr) {\state{t}};}%
          \hspace{1.8mm},\hspace{3.1mm}$B$\hspace{2.58mm}
        };
      } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
        \node at (0,-4mm) (r) [stateTerminal] {\state{t}};
      \end{tikzpicture} & \lastcolumn{3} \\

      \tikz[baseline=(current bounding box.center)]{
        \node [style=graphNT, minimum height=\minheight, minimum width=\minwidth] {
          \hspace{.8mm}
          \begin{tikzpicture}[baseline=(current bounding box.center), minimum height = 0, minimum width = 0]
            \node at (0,0) (r) [stateSideEffect,minimum height=15pt, minimum width=1em] {$a$};
            \node at (0,-10mm) (A) {$A$};
            \draw[->] (r) -- (A);
          \end{tikzpicture}\hspace{2.2mm},\hspace{2.1mm}\tikz[minimum height = 0, minimum width = 0, baseline = (charrr.base)] {\node [stateTerminal] (charrr) {\state{t}};}
        };
      } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
        \node at (0,2mm) (r) [stateSideEffect,minimum height=15pt, minimum width = 1em]{$a$};
        \node at (0,-11mm) (A) [style=graphNT] {\raisebox{-6pt}{
            $A$,\raisebox{.35em}{\tikz{\node [stateTerminal] {\state{t}};}}
          }
        };
        \draw[->] (r) -- (A);
      \end{tikzpicture} & \lastcolumn{4} \\
    \end{tabular}
  \end{tabular}
  {\hfill}
  \caption{A basic {\StateMachine} cross-product algorithm, expressed
    as a node replacement graph generating grammar.  $m$ matches
    boolean BDDs; $A_0$, $A_1$, and $A$ match fragments of the
    crashing {\StateMachine}; $B_0$, $B_1$, and $B$ match fragments of
    the interfering {\StateMachine}; $a$ and $b$ match individual
    states within the crashing and interfering {\StateMachines},
    respectively; \state{t} matches any terminal state.}
  \label{fig:derive:basic_cross_product}
\end{sanefig}

The algorithm used by {\technique} is rather complicated and so before
discussing it in detail I first present a simplified version,
described in \autoref{fig:derive:basic_cross_product} as a node
replacement graph grammar (see \autoref{sect:intro:graph_grammar}).
The non-terminal type \graphNT{A, B} represents the cross-product of
the {\StateMachine} starting at A with that starting at B and the
productions show how to convert these non-terminals into a single
{\StateMachine} with equivalent behaviour.  For instance, if
{\StateMachine} A begins with an \state{If} state, the output
{\StateMachine} will be formed by an \state{If} state which tests the
same discriminant and advances A to the appropriate successor state
(production \production{1_a}); if both {\StateMachines} are at simple
side-effects, the output {\StateMachine} uses an \state{If} and
happens-before test $\happensBeforeEdge$ to select which side-effect
to do first, does it, and then advances the selected input
{\StateMachine} (production \production{2}).  These rules are
reasonably direct encodings of the usual parallel interleaving
semantics into the {\StateMachine} language.

Note the asymmetry between handling of terminal states in the crashing
{\StateMachine} and those in the interfering one.  The output
{\StateMachine} stops immediately if the crashing {\StateMachine}
terminates, producing the same result (production \production{3}), but
continues if the interfering {\StateMachine} one does (production
\production{4}).  This reflects the different roles of the two
           {\StateMachines}: the crashing one determines whether the
           bug would have reproduced had the program followed the
           modelled execution, whereas the interfering one simply
           provides something for the crashing one to race with.  The
           final result of the crashing {\StateMachine} is therefore
           of critical import while that of the interfering one is
           largely irrelevant.

\begin{sanefig}
  \begin{displaymath}
    \textsc{Configuration} = \graphNT{\begin{tabular}{rrll}
      \multirow{2}{*}{\bigg\{} & \textit{crashingState}: & \textsc{{\STateMachine} state}, & \multirow{2}{*}{\bigg\},}\\
                               & \textit{crashingHasIssued}: & \textsc{Bool}\\
      \multirow{2}{*}{\bigg\{} & \textit{interferingState}: & \textsc{{\STateMachine} state}, & \multirow{2}{*}{\bigg\},} \\
                               & \textit{interferingHasIssued}: & \textsc{Bool}\\
      \multicolumn{2}{r}{\textit{atomic}:} & \multicolumn{2}{l}{\{ $\varnothing$, \textit{crashing}, \textit{interfering} \}}
    \end{tabular}}
  \end{displaymath}
  \caption{\textsc{Configuration} type for the cross-product algorithm.}
  \label{fig:cross_product:configuration}
\end{sanefig}
The actual algorithm used by {\technique} includes several refinements
over this basic one:
\begin{itemize}
\item The input {\StateMachines} may contain atomic blocks, delimited
  by {\stStartAtomic} and {\stEndAtomic} states, and the simple
  grammar does not respect them.  This can be fixed by extending the
  grammar's non-terminal structure with a final field which indicates
  which, if any, of the {\StateMachines} are currently within atomic
  blocks.  This $\mathit{atomic}$ field is maintained as the
  {\StateMachines} perform {\stStartAtomic} and {\stEndAtomic} states
  and is used to restrict the interleavings generated by the grammar.
\item The basic grammar will consider every possible interleaving of
  the two {\StateMachines}, including the trivial ones in which one or
  other completes atomically.  This is correct but inefficient, as the
  atomic orderings have already been considered in the
  \gls{inferredassumption}.  Fixing this is, again, simple, by
  extending the grammar's non-terminal structure with two additional
  fields, $\mathit{crashingHasIssued}$ and
  $\mathit{interferingHasIssued}$, which track whether the crashing
  and interfering machines have issued any speicher-accessing side
  effects.  If either {\StateMachine} terminates without having done
  so then the cross-product {\StateMachine} ends in the {\stUnreached}
  state so that the relevant paths will not be included in the
  \gls{verificationcondition}.

\begin{sanefig}
  \begin{displaymath}
    \mathit{initial} = \graphNT{\begin{tabular}{rrll}
      \multirow{2}{*}{\bigg\{} & \textit{crashingState} = & $\mathit{crashing}_0$, & \multirow{2}{*}{\bigg\},} \\
                               & \textit{crashingHasIssued} = & \false\\
      \multirow{2}{*}{\bigg\{} & \textit{interferingState} = & $\mathit{interfering}_0$, & \multirow{2}{*}{\bigg\},} \\
                               & \textit{interferingHasIssued} = & \false\\
      \multicolumn{2}{r}{\textit{atomic} = } & $\varnothing$ \\
    \end{tabular}}
  \end{displaymath}
  \caption{Initial \textsc{Configuration} for the cross-product
    algorithm.  $\mathit{crashing}_0$ is the first state of the
    crashing {\StateMachine} and $\mathit{interfering}_0$ that of the
    interfering one.}
  \label{fig:cross_product:initial}
\end{sanefig}

  Note that this refinement excludes executions in which the
  {\StateMachines} run in series, rather than those in which they run
  linearizably~\cite{Herlihy1990}.  The latter would perhaps be more
  useful, as it could potentially eliminate more paths, but is far
  more difficult to calculate; since this is purely a performance
  enhancement, the simpler implementation is more appropriate.
\item The basic grammar fails to take account of partial order
  redundancies caused by {\StateMachine}-local side effects such as
  \state{Copy} or \state{Assert}.  The complete grammar ameliorates
  this weakness by only generating happens-before tests for non-local
  side-effects, defined to be those which could possibly influence or
  be influenced by the other {\StateMachine}.  This definition is
  context-dependent: a \state{Load} in the crashing {\StateMachine},
  for instance, will be non-local if it could alias with any of the
  crashing {\StateMachine}'s possible future \state{Store}s, and so
  might switch from being non-local to being local when the crashing
  {\StateMachine} performs a \state{Store}.  A {\stStartAtomic} side
  effect is considered non-local if any of the side effects in the
  atomic block which it starts are non-local.
\item This grammar can sometimes duplicate side effect states, which
  might break static single assignment form.  {\Technique} uses a
  separate post-pass to restore the SSA invariant.
\end{itemize}
The extended non-terminal type, \textsc{Configuration}, is shown in
\autoref{fig:cross_product:configuration}, its initial value in
\autoref{fig:cross_product:initial}, and the extended productions in
\autoref{fig:cross_product:algorithm}.  Many of the productions have
symmetrical versions which simply swap the roles of the crashing and
interfering {\StateMachines}; for the sake of brevity, I show only the
crashing {\StateMachine} production and mark it with a *.

\begin{sidewaysfigure}
\begin{figgure}
  \begin{displaymath}
    \begin{tabular}{ c @{~$\Rightarrow$~} c >{\raggedright}p{5.3cm} c c}
      \tikz [baseline = (current bounding box.center)] {
        \node[style = graphNT] {
          $\left\{\begin{tikzpicture}[baseline = (current bounding box.center),minimum height = 0, minimum width = 0,node distance=0.5cm,font=\small]
          \node at (0,0) (r) [stateIf] {\stIf{m}};
          \node at (-5mm, -10mm) (A) {$A_0$};
          \node at (5mm, -10mm) (B) {$A_1$};
          \draw[->,ifTrue] (r) -- (A);
          \draw[->,ifFalse] (r) -- (B);
          \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\},z$
        };
      } & \begin{tikzpicture}[node distance=0.5cm,font=\small, baseline = (current bounding box.center)]
          \node at (0,0) (r) [stateIf] {\stIf{m} };
          \node at (-20mm, -10mm) (A) [graphNT] { $\{A_0, i_a\}, \{B, i_b\}, z$ };
          \node at (20mm, -10mm) (B) [graphNT] { $\{A_1, i_a\}, \{B, i_b\}, z$ };
          \draw[->,ifTrue] (r) -- (A);
          \draw[->,ifFalse] (r) -- (B);
      \end{tikzpicture} & & \production{1'} & *\\

      \tikz [baseline = (current bounding box.center)] {
        \node[style = graphNT] {
          $\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center) ]
          \node at (0,0) (r) [stateSideEffect, minimum height=15pt, minimum width=1em] {$a$};
          \node at (0,-10mm) (A) {$A$};
          \draw[->] (r) -- (A);
          \end{tikzpicture}, i_a\right\},\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center) ]
          \node at (0,0) (r) [stateSideEffect, minimum height=15pt, minimum width=1em] {$b$};
          \node at (0,-10mm) (A) {$B$};
          \draw[->] (r) -- (A);
          \end{tikzpicture}, i_b\right\}, \varnothing$
        };
      }
      & \begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
          \node at (0,0) (r) [stateIf] {\stIf{\happensBefore{a}{b}}};
          \node at (-25mm, -9mm) [stateSideEffect, minimum height=15pt, minimum width=1em] (A) {$a$};
          \node at (25mm, -9mm) [stateSideEffect, minimum height=15pt, minimum width=1em] (C) {$b$};
          \node at (-25mm, -25mm) (B) [graphNT] {
            $\{A, \true\}, \left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
            \node at (0,0) (Br) [stateSideEffect] {$b$};
            \node at (0,-10mm) (BA) {$B$};
            \draw[->] (Br) -- (BA);
            \end{tikzpicture}, i_b\right\}, \varnothing$
          };
          \node at (25mm, -25mm) (D) [graphNT] {
            $\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
            \node at (0,0) (Dr) [stateSideEffect, minimum height=15pt, minimum width=1em] {$a$};
            \node at (0,-10mm) (DA) {$A$};
            \draw[->] (Dr) -- (DA);
            \end{tikzpicture}, i_a\right\}, \{B, \true\}, \varnothing$
          };
          \draw[->,ifTrue] (r) -- (A);
          \draw[->,ifFalse] (r) -- (C);
          \draw[->] (A) -- (B);
          \draw[->] (C) -- (D);
        \end{tikzpicture} & When $a$ and $b$ are non-local. & \production{2'} & \\

      \tikz [baseline = (current bounding box.center)] {
        \node[style = graphNT] {
          $\left\{\begin{tikzpicture}[font=\small, baseline = (char.base)]
          \node [stateTerminal] (char) {\state{t}};
          \end{tikzpicture}, \true\right\}, \{A, \true\}, z$
        };
      }
      & \begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
          \node [stateTerminal] {\state{t}};
        \end{tikzpicture} &  & \production{3'} \\
        
      \tikz [baseline = (current bounding box.center)] {
        \node[style = graphNT] {
          $\left\{\begin{tikzpicture}[font=\small][baseline = (current bounding box.center)]
          \node at (0,0) (r) [stateSideEffect, minimum height=15pt, minimum width=1em] {$a$};
          \node at (0,-10mm) (A) {$A$};
          \draw[->] (r) -- (A);
          \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\},z$
        };
      }
      & \begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
          \node at (0,0) (r) [stateSideEffect, minimum height=15pt, minimum width=1em] {$a$};
          \node at (0,-10mm) [graphNT] (A) {$\{A, i_a\}, \{B, i_b\}, z$};
          \draw[->] (r) -- (A);
          \end{tikzpicture} & If $a$ is a local side-effect. & \production{4'} & *\\

      \tikz [baseline = (current bounding box.center)] {
        \node[style=graphNT] {
          $\left\{\begin{tikzpicture}[font=\small, baseline = (char.base), minimum height=0]
          \node [stateTerminal] (char) {\state{t}};
          \end{tikzpicture}, i_a\right\}, \{A, i_b\}, z$
        };
      } & \raisebox{1pt}{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
          \node [stateTerminal] {{\stUnreached}};
        \end{tikzpicture}} & If $i_a \wedge i_b = \false$. & \production{5'} & *\\
      
      \tikz [baseline = (current bounding box.center)] {
        \node[style=graphNT] {
          $\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
          \node at (0,0) (r) [stateSideEffect] {{\stStartAtomic}};
          \node at (0,-10mm) (A) {$A$};
          \draw[->] (r) -- (A);
          \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\}, \varnothing$
        };
      }
      & \graphNT{$\{A, i_a\}, \{B, i_b\}, \mathit{crashing}$} & If the atomic block is local. & \production{6'} & *\\

      \tikz [baseline = (current bounding box.center)] {
        \node[style=graphNT] {
          $\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
          \node at (0,0) (r) [stateSideEffect] {{\stEndAtomic}};
          \node at (0,-10mm) (A) {$A$};
          \draw[->] (r) -- (A);
          \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\}, \mathit{crashing}$
        };
      }
      & \graphNT{$\{A, i_a\}, \{B, i_b\}, \varnothing$} & & \production{7'} & *\\

      \tikz [baseline = (current bounding box.center)] {
        \node[style=graphNT] {
          $\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
          \node at (0,0) (r) [stateSideEffect, minimum height=15pt, minimum width=1em] {$a$};
          \node at (0,-10mm) (A) {$A$};
          \draw[->] (r) -- (A);
          \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\},\mathit{crashing}$
        };
      }
      & \begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
          \node at (0,0) (r) [stateSideEffect, minimum height=15pt, minimum width=1em] {$a$};
          \node at (0,-10mm) [graphNT] (A) {$(\{A, \true\}, \{B, i_b\}, \mathit{crashing})$};
          \draw[->] (r) -- (A);
        \end{tikzpicture} & If $a$ is a non-local side-effect. & \production{8'} & *
    \end{tabular}
  \end{displaymath}
  \caption{The cross-product algorithm as a node replacement graph
    grammar.  $A$, $A_0$, and $A_1$ match fragments of the crashing
    {\StateMachine} and $a$ matches a single state from the crashing
    {\StateMachine}.  $B$ and $b$ match fragments of and a single
    state in, respectively, the interfering {\StateMachine}.  $i_a$
    and $i_b$ match either {\true} or {\false}.  $z$ matches any of
    $\varnothing$, $\mathit{crashing}$, or $\mathit{interfering}$.
    $m$ matches a boolean BDD.  \state{T} matches any terminal
    state. *: production also applies with the crashing and
    interfering {\StateMachines} swapped.}
  \label{fig:cross_product:algorithm}
\end{figgure}
\end{sidewaysfigure}
\begin{sanefig}
  \tikzstyle{stateSideEffect}+=[minimum width=4cm, minimum height=1.0cm]
  \tikzstyle{stateIf}+=[minimum width=4cm, minimum height=1.0cm]
  \tikzstyle{stateTerminal}+=[minimum width=4cm, minimum height=1.0cm]
  \begin{subfloat}
    \hspace{-4mm}
    \begin{tikzpicture}
      \node[stateSideEffect] (lA) {$A$: \stLoad{0}{x} };
      \node[stateIf,below = of lA] (lB) {$B$: \stIf{\smVar{0} = 0} };
      \node[stateSideEffect,below = of lB] (lC) {$C$: \stLoad{1}{x} };
      \node[stateIf,below = of lC] (lD) {$D$: \stIf{\smBadPtr{\smVar{1}}} };
      \node[stateTerminal,below = of lD] (lH) {$H$: \stCrash };
      \node[stateTerminal,right = 0.3 of lC] (lG) {$G$: \stSurvive };
      \draw[->] (lA) -- (lB);
      \draw[->,ifTrue] (lB) -- (lG);
      \draw[->,ifFalse] (lB) -- (lC);
      \draw[->] (lC) -- (lD);
      \draw[->,ifTrue] (lD) -- (lH);
      \draw[->,ifFalse] (lD) -- (lG);
    \end{tikzpicture}
    \caption{Crashing thread {\StateMachine} }
  \end{subfloat}
  \hspace{-4mm}
  \begin{subfloat}
    \begin{tikzpicture}
      \node[stateIf] (lE) {$E$: \stIf{y \not= 0}};
      \path (node cs:name=lE) ++(2.3,-1.5) node [stateSideEffect] (lF) {$F$: \stStore{0}{x}};
      \node[stateTerminal,below = 2 of lE] (lI) {$I$: \stSurvive };
      \draw[->,ifTrue] (lE) -- (lI);
      \draw[->,ifFalse] (lE) -- (lF);
      \draw[->] (lF) -- (lI);
    \end{tikzpicture}
    \caption{Interfering thread {\StateMachine} }
  \end{subfloat}
  \caption{A pair of {\StateMachines}.  $x$ is a global memory
    location.  Figure~\ref{fig:cross_product_output} shows their
    cross-product.}
  \label{fig:cross_product_input}
\end{sanefig}

The productions of the extended grammar encode these refinements quite
directly:
\begin{itemize}
\item Production \production{1'} corresponds to productions
  \production{1_a} and \production{1_b} in the simple grammar,
  production \production{2'} to \production{2} and \production{3'} to
  \production{3}.  These need no further explanation.
\item Production \production{4'} allows either of the {\StateMachines}
  to advance past a local side effect without needing to perform a
  happens-before test, implementing a limited form of partial-order
  reduction.
\item Production \production{5'} causes paths in which one
  {\StateMachine} finishes before the other starts to end in the
  {\stUnreached} state, effectively eliminating those paths from
  consideration.
\item Productions \production{6'}, \production{7'}, and
  \production{8'} implement atomic blocks by allowing {\StateMachines}
  to, respectively, enter a local atomic block, leave a block, and
  progress within a block.  Non-local atomic blocks are entered by a
  variant of production \production{2'}, not shown in the figure,
  which races the {\stStartAtomic} side-effect against an appropriate
  non-local side effect in the other {\StateMachine} using a
  $\happensBeforeEdge$ test and, depending on the result of that test,
  either starts the atomic block, in the same way as production
  \production{6'}, or runs the other side effect.
\end{itemize}
\begin{sanefig}
  \newcommand{\boxLabel}[2]{\small \raisebox{-1.5pt}{\production{#1'}} #2}
  \newcommand{\labelBox}[6]{
    \fill [color=blue!20] (#1, #2) rectangle (#1 + #3, #2 + #4);
    \node at (#1, #2 + #4 + .1) [below right] {\boxLabel{#5}{#6}};
  }
  \newcommand{\nodeWidth}{4.3cm}
  \newcommand{\nodeHeight}{1.0cm}
  \begin{tikzpicture}[align=center, node distance = 1 and 0.9]
    \labelBox{-2.4}{-.6}{4.8}{1.65}{1}{\{$A$, {\false}\}, \{$E$, {\false}\}}
    \labelBox{2.8}{-.6}{4.8}{1.65}{5}{(\{$A$, {\false}\}, \{$I$, {\false}\})}

    \fill [color=blue!20] (-2.4, -0.9) -- (7.6, -0.9) -- (7.6, -2.65) -- (2.4, -2.65) -- (2.4, -4.65) -- (-2.4, -4.65);
    \node at (-2.4, -0.8) [below right] {\boxLabel{2}{(\{$A$, {\false}\}, \{$F$, {\false}\})} };

    \labelBox{2.8}{-4.65}{4.8}{1.65}{5}{(\{$A$, {\false}\}, \{$I$, {\true}\})}
    \labelBox{-2.4}{-6.65}{4.8}{1.65}{1}{(\{$B$, {\true}\}, \{$F$, {\false}\})}
    \labelBox{2.8}{-6.65}{4.8}{1.65}{5}{(\{$G$, {\true}\}, \{$F$, {\false}\})}

    \fill [color=blue!20] (-2.4, -6.9) -- (7.6, -6.9) -- (7.6, -8.7) -- (2.4, -8.7) -- (2.4, -10.7) -- (-2.4, -10.7);
    \node at (-2.4, -6.8) [below right] {\boxLabel{2}{(\{$C$, {\true}\}, \{$F$, {\false}\})}};

    \labelBox{8.0}{-8.7}{4.8}{1.8}{4}{(\{$C$, {\true}\}, \{$I$, {\true}\})}
    \labelBox{8.0}{-10.7}{4.8}{1.8}{1}{(\{$D$, {\true}\}, \{$I$, {\true}\})}
    \labelBox{8.0}{-12.7}{4.8}{1.8}{3}{(\{$H$, {\true}\}, \{$I$, {\true}\})}
    \labelBox{2.8}{-12.7}{4.8}{1.8}{3}{(\{$G$, {\true}\}, \{$I$, {\true}\})}
    \labelBox{-2.4}{-12.7}{4.8}{1.8}{1}{(\{$D$, {\true}\}, \{$F$, {\false}\})}

    \labelBox{-2.4}{-14.7}{4.8}{1.8}{1}{(\{$H$, {\true}\}, \{$F$, {\false}\})}
    \labelBox{2.8}{-14.7}{4.8}{1.8}{1}{(\{$G$, {\true}\}, \{$F$, {\false}\})}

    \node at (0,0) [stateIf, minimum width=\nodeWidth, minimum height=\nodeHeight] (A) {\stIf{y \not= 0} };
    \node[stateTerminal, right = of A, minimum width=\nodeWidth, minimum height=\nodeHeight] (B) {{\stUnreached} };

    \node[stateIf, below = of A, minimum width=\nodeWidth, minimum height=\nodeHeight] (C) {\stIf{\happensBefore{A}{F}} };
    \node[stateSideEffect, below = of C, minimum width=\nodeWidth, minimum height=\nodeHeight] (D) {\stLoad{0}{x} };
    \node[stateSideEffect, right = of C, minimum width=\nodeWidth, minimum height=\nodeHeight] (E) {\stStore{0}{x} };
    \node[stateIf, below = of D, minimum width=\nodeWidth, minimum height=\nodeHeight] (F) {\stIf{\smVar{0} = 0} };
    \node[stateTerminal, below = of E, minimum width=\nodeWidth, minimum height=\nodeHeight] (G) {\stUnreached };
    \node[stateTerminal, right = of F, minimum width=\nodeWidth, minimum height=\nodeHeight] (H) {\stUnreached };
    \node[stateIf, below = of F, minimum width=\nodeWidth, minimum height=\nodeHeight] (I) {\stIf{\happensBefore{C}{F}} };
    \node[stateSideEffect, below = of I, minimum width=\nodeWidth, minimum height=\nodeHeight] (J) {\stLoad{1}{x}};
    \node[stateSideEffect, right = of I, minimum width=\nodeWidth, minimum height=\nodeHeight] (K) {\stStore{0}{x}};
    \node[stateIf, below = of J, minimum width=\nodeWidth, minimum height=\nodeHeight] (L) {\!\!\!\stIf{\smBadPtr{\smVar{1}}}\!\!\!};
    \node[stateSideEffect, right = of K, minimum width=\nodeWidth, minimum height=\nodeHeight] (M) {\stLoad{2}{x}};
    \node[stateTerminal, below = of L, minimum width=\nodeWidth, minimum height=\nodeHeight] (N) {\stUnreached };
    \node[stateTerminal, right = of N, minimum width=\nodeWidth, minimum height=\nodeHeight] (O) {\stUnreached };
    \node[stateIf, below = of M, minimum width=\nodeWidth, minimum height=\nodeHeight] (P) {\!\!\!\stIf{\smBadPtr{\smVar{2}}}\!\!\!};
    \node[stateTerminal, minimum width=\nodeWidth, minimum height=\nodeHeight, below = of P] (R) {\stCrash };
    \node[stateTerminal, minimum width=\nodeWidth, minimum height=\nodeHeight] at (R-|K) (Q) {\stSurvive };
    \draw[->,ifTrue] (A) -- (B);
    \draw[->,ifFalse] (A) -- (C);
    \draw[->,ifTrue] (C) -- (D);
    \draw[->,ifFalse] (C) -- (E);
    \draw[->] (D) -- (F);
    \draw[->] (E) -- (G);
    \draw[->,ifTrue] (F) -- (H);
    \draw[->,ifFalse] (F) -- (I);
    \draw[->,ifTrue] (I) -- (J);
    \draw[->,ifFalse] (I) -- (K);
    \draw[->] (J) -- (L);
    \draw[->] (K) -- (M);
    \draw[->,ifTrue] (L) -- (N);
    \draw[->,ifFalse] (L) -- (O);
    \draw[->] (M) -- (P);
    \draw[->,ifFalse] (P) -- (Q);
    \draw[->,ifTrue] (P) -- (R);
  \end{tikzpicture}
  \caption{Cross product of the {\StateMachines} shown in
    \autoref{fig:cross_product_input}.  Blue boxes show the
    non-terminals and productions used to generate each part of the
    graph.  The $\mathit{atomic}$ field of the non-terminal is always
    $\varnothing$ for these input {\StateMachines} and is not shown.}
  \label{fig:cross_product_output}
\end{sanefig}
The cross-product of the {\StateMachines} in
\autoref{fig:cross_product_input} is given in
\autoref{fig:cross_product_output}, showing the
\textsc{Configuration}s and productions used to produce all of the
output states.  This captures every possible interleaving of the two
input {\StateMachines} into a single cross-product {\StateMachine},
allowing {\technique} to build the \gls{verificationcondition} using a
simple single-threaded symbolic execution engine.  Note that while
this example produced a tree-structured {\StateMachine}, in the
general case the result is a directed acyclic graph, with sub-graphs
shared if the same non-terminal is generated multiple times.  This
reduces the worst-case number of states in the cross-product
{\StateMachine} from $O(\binom{L+S}{L})$ to $O(LS)$, where $L$ is the
number of states in the crashing {\StateMachine} and $S$ the number in
the interfering one, which often reduces the cost of analysing it by a
useful amount.

Importantly, the cross-product {\StateMachine} can itself be
simplified using the usual {\StateMachine} simplification passes.
Simplifying the example cross-product {\StateMachine} will produce the
{\StateMachine} shown in Figure~\ref{fig:cross_product_output_opt}.
In this case, the actual symbolic execution step will be trivial, and
will report that the program will reach a {\stCrash} state precisely
when $y = 0 \wedge \happensBefore{A}{F} \wedge LD(x) \not= 0 \wedge
\happensBefore{F}{C}$; in other words, if $y$ is nonzero, the initial
value of $x$ is non-zero, and statement F intercedes between
statements A and C.  Comparison to the input {\StateMachines} will
show that this is precisely the desired result.

The simplifications needed in this example are quite simple and it
would have been possible to include equivalent optimisations in the
symbolic execution engine itself.  This would be more difficult for
more complex simplifications, for two reasons:
\begin{itemize}
\item Simplification passes can easily look ahead in the
  {\StateMachine}, whereas symbolic execution primarily considers a
  single state at a time.  Dead code elimination, for example, is much
  easier to implement as a simplification to the {\StateMachine} than
  as a change to the symbolic execution engine.
\item The results of a simplification pass are inherently shared
  across all paths which reach a particular state, whereas the
  symbolic execution engine needs to perform additional work in order
  to share results.
\end{itemize}
There is also an engineering consideration which argues in favour of
building and simplifying the cross-product {\StateMachine} rather than
integrating equivalent optimisations into the symbolic execution
engine: {\implementation} already needs all of the simplifiers in
order to build the input {\StateMachines}, and so re-using them here
halves the implementation effort.

\begin{sanefig}
  \tikzstyle{stateSideEffect}+=[minimum width=4.3cm, minimum height=1.0cm]
  \tikzstyle{stateIf}+=[minimum width=4.3cm, minimum height=1.0cm]
  \tikzstyle{stateTerminal}+=[minimum width=4.3cm, minimum height=1.0cm]
  \centerline{
  \begin{tikzpicture}
    \node[stateSideEffect] (A) {\stAssert{y = 0 \wedge \happensBefore{A}{F} \wedge \smLoad{x} \not= 0 \wedge \happensBefore{F}{C}} };
    \node[stateTerminal, below = of A] (B) {\stCrash };
    \draw[->] (A) -- (B);
  \end{tikzpicture}
  }
  \caption{Result of simplifying {\StateMachine} shown in
    \autoref{fig:cross_product_output}.}
  \label{fig:cross_product_output_opt}
\end{sanefig}

\subsection{Path explosion}

Path explosion is a common problem in symbolic execution systems.  The
number of paths through a program rises exponentially in the size of
the program, and this can prevent na\"ive symbolic execution systems
from being applied to realistically large programs.  In the case of
\technique, there are two main causes of path explosion:
\begin{itemize}
\item
  \textit{Aliasing}.  If the various simplification passes and the
  \gls{programmodel} cannot determine how memory accessing
  instructions alias then the symbolic execution engine must
  exhaustively consider every possible way in which every \state{Load}
  can be satisfied.  If there are $n$ \state{Load}s and $m$
  \state{Store}s then the number of such patterns to be considered is
  $O((m+1)^n)$, which grows very quickly in the number of accesses.
  The use of lazy alias resolution helps mitigate this to some extent,
  but does not eliminate it completely.  This represents one of the
  major limitations to \technique's scalability.
\item
  \textit{Thread interleaving}.  The cross-product {\StateMachine}
  will have $O(ci)$ states, where $c$ is number of states in the
  crashing {\StateMachine} and $i$ the number in the interfering one.
  The number of paths through the combined {\StateMachine} is then
  $O(2^{ci})$, which again grows rather quickly.
\end{itemize}
The result is that, in the common case where the read-side
{\StateMachine} consists mostly of \state{Load} operations and
write-side one mostly of \state{Store} ones, the symbolic execution
engine might have to consider up to $O(2^{ci}.m^n)$ distinct paths
when evaluating the cross-product {\StateMachine}.  This is
impractical for even moderately complex inputs.  For good performance,
{\technique} relies on the various simplification and analysis
techniques to eliminate most of these paths without needing to fully
symbolically execute them.  Fortunately, as discussed in the
evaluation, they are able to do so in a useful set of cases.

\section{The \glsentrytext{w-isolation} property}
\label{sect:derive:w_isolation}

In practice, {\technique}'s most important limitation is the need to
solve a large number of aliasing problems.  This limitation can be
somewhat ameliorated by assuming that the crashing {\StateMachine}
never stores to any memory locations which are subsequently loaded by
the interfering one; in other words, by restricting the class of bugs
considered from those where two threads are simultaneously working
with a structure to those where one thread is reading from the
structure whilst another updates it.  Such bugs are said to be
I-isolated, or to have the \gls{w-isolation} property, as the
\gls{interferingthread} is effectively isolated from the crashing one.

Restricting the class of bugs considered in this way enables three
main optimisations:
\begin{itemize}
\item
  It directly restricts the aliasing problem, as the analysis no
  longer needs to consider aliasing between stores in the crashing
  {\StateMachine} and loads in the interfering one.
\item
  It reduces the set of interfering \glspl{cfg} which is generated for
  each \gls{crashingthread}, because if the \gls{interferingthread}
  cannot load any location stored to by the \gls{crashingthread} then
  $c2i$ is empty and $\beta = i2c$ (see
  \autoref{sect:derive:write_side}).
\item
  It simplifies the calculation of the \gls{inferredassumption}.
  Normally, the \gls{inferredassumption} is the conjunction of
  \gls{ic-atomic} and \gls{ci-atomic}, but when the \gls{w-isolation}
  property holds the latter can be replaced by C-atomic, the condition
  for the crashing {\StateMachine} to survive when run in isolation.
  This is because the \gls{w-isolation} property implies that
  \gls{ci-atomic} = C-atomic $\wedge$ I-atomic, where I-atomic is the
  condition for the interfering {\StateMachine} to survive in
  isolation, and I-atomic is implied by \gls{ic-atomic}.  C-atomic is
  independent of the interfering {\StateMachine}, and so only needs to
  be calculated once for every crashing {\StateMachine} rather than
  once for every pair of crashing and interfering {\StateMachines},
  which can sometimes provide a modest performance improvement.
\end{itemize}
The analysis is almost always faster with the \gls{w-isolation}
assumption, but cannot handle as broad a class of program behaviour.
I evaluate these effects experimentally in
\autoref{sect:eval:w_isolation}.

\section{The \glsentrytext{programmodel}}
\label{sect:program_model}

In addition to the {\StateMachine}-level aliasing analysis,
{\technique} also makes use of an \gls{programmodel}, built before the
main analysis starts, which describes how the program accesses memory
during normal operation.  This is used both for alias analysis during
{\StateMachine} simplification (\autoref{sect:derive:simplify_sm}) and
symbolic execution (\autoref{sect:using:check_realness}), and also to
find the $\beta$ and $i2c$ sets when building the interfering
\gls{cfg} (\autoref{sect:derive:write_side}).  The memory access model
is itself composed of separate models of stack and non-stack memory.
I describe each in turn.

\subsection{The stack model}
The stack model is built using a fairly conventional function-local
static pointer escape analysis\needCite{} based on the observation
that stack frames are ``created'' when a function starts.  This
implies that there should not be any pointers to function-local
variables unless the function being analysed creates one, and so the
static analysis attempts to track which registers and memory locations
might contain pointers to the local stack frame.  This is usually
sufficient for the {\StateMachine} simplifiers to be able to determine
which, if any, stack frames a given memory access might refer to;
simple arithmetic considerations can then usually determine the
specific local variable.  As part of this analysis, {\technique}
discovers any registers which have constant value at a particular
instruction, or those which are equal to another register plus a
constant, and this information is also used during {\StateMachine}
simplification.

\subsection{The non-stack model}
It is much more difficult to characterise the structure of non-stack
memory, such as the heap, using static analysis.  Not only is the heap
structure itself more complicated, in terms of the number of objects
which point at other objects, but the information is harder to locate,
as it is no longer localised to any particular function or program
module.  These problems make it difficult to accurately model the heap
statically even when the analysis tool has full access to the
program's source code\needCite{}; attempting to do so given only a
binary is completely infeasible.

{\Technique} instead relies on a dynamic analysis to model accesses to
non-stack locations.  This analysis works by dividing memory into
fixed-size chunks\footnote{For {\implementation}, these chunks are
  eight bytes, matching the platform word size. This represents a
  reasonable trade-off between precision, which argues for smaller
  chunks, and analysis performance, which argues for larger ones.} and
then tracking which instructions access each chunk.  If two
instructions are ever observed to access the same chunk without an
intervening call to a function such as \texttt{free} then they are
marked as potentially aliasing; otherwise, they are marked as
non-aliasing.  In practice, this analysis will, if provided with a
sufficient cross-section of the program's potential behaviour,
approximate a source-level static one which considers two accesses to
potentially alias if and only if they access the same field of the
same compound structure type.  Most programs are structured so that
most structure fields are accessed from a relatively small number of
program instructions and so this is generally reasonably precise,
despite being completely flow- and context-insensitive.

The chief weakness of this approach (aside from the fact that, as a
dynamic analysis, it inherently commits the inductive fallacy) is that
it must be able to tell when the types of structure stored at a
particular memory location changes.  For most programs, that means
that it must be able to identify dynamic memory management functions,
and in particular functions such as \texttt{free} which release
memory.  Failing to correctly identify these functions will
effectively merge all of the structures managed by a particular
allocator, potentially leading to a very imprecise analysis.
{\Technique} relies on the user to manually identify these functions.
This is not an unreasonable burden: most programs rely on allocation
functions in libraries, which only have to be identified once for all
users of the library, and the remainder generally use only a very
small number of custom allocators.  MySQL, for instance, had only two
functions which needed to be annotated, and Thunderbird and pbzip2 had
none.  It might be possible to identify such allocators using a
variant of the techniques described by Cozzie et al.~\cite{Cozzie2008},
but I have not investigated that at this time.

\subsection{Escape analysis}
{\Implementation} includes one minor refinement to the basic dynamic
analysis described above.  It is fairly common for programs to
allocate new heap structures using a function such as \texttt{malloc}
and to then initialise this structure using a series of stores.  These
stores will never race, so it would be helpful to avoid spending
excessive time considering what would happen if they did.
{\Technique} avoids doing so by marking blocks of memory returned from
\texttt{malloc} as thread-local, and they remain so until a pointer to
them is stored in non-stack memory.  Entries in the aliasing table
include a flag indicating whether the access is thread-private or
potentially racing, and this is used by later phases of the analysis
to constrain the aliasing problem.

This policy might seem to be overly conservative, in that a block of
memory is marked as shared whenever a pointer to it is stored into any
non-stack memory, even when that non-stack memory is itself marked as
thread-private.  This is necessary because the analysis does not
attempt to track the heap reachability graph, and in particular cannot
map from one block to the set of blocks reachable from that block.  It
is therefore not safe to ``upgrade'' a block from thread-private to
thread-shared if there is any possibility of that block containing a
thread-private pointer; upgrading blocks early and pessimistically
means that it is never necessary to do so.

\section{Implementation infelicities, and their removal}
{\Implementation} makes several important simplifying assumptions,
some of which can impact the correctness of the analysis.  I now
briefly discuss these assumptions and their effects, and possible ways
of removing them.

\subsection{Incomplete library models}
As has already been discussed, {\implementation} relies on
hand-written {\StateMachine} implementations of system library
functions.  If no such implementation is available then
{\implementation} uses a stub implementation which simply sets the
return address register to a new free variable.  While this approach
does have some advantages, most notably in being able to incorporate
useful higher-level information on library interfaces without needing
to analyse complex library implementations and in being able to
perform analysis without access to a library implementation, it
carries obvious risks of incorrectly characterising bugs.  This is
particularly irritating because the {\technique} technique itself is
just as applicable to library code as it is to code in the main
program.  This weakness could be eliminated by arranging to load the
library implementation, when available, into the {\implementation}
analysis tool along with the program itself, and then extending the
static \gls{cfg} algorithms \textsc{buildStaticCrashing} and
\textsc{buildStaticInterfering} to trace across the program/library
boundary in the same way that they already trace across function
boundaries within the program.

\subsection{Simplified memory model}
\label{sect:derive:simpl_mem_model}

In addition to the library limitation, {\implementation} simplifies
the processor's memory model in three ways.  First, the aliasing
resolution algorithm does not support unaligned memory accesses, and
will produce incorrect results if the program makes use of them in the
\gls{analysiswindow}.  Such accesses are unusual in most programs, as
they have very poor performance on most modern
processors\footnote{Indeed, many processors do not support unaligned
  accesses at all and require them to be emulated in the operating
  system, leading to performance which is better described as abysmal
  than as poor.}, and removing this limitation would complicate alias
analysis even for programs which never make unaligned accesses.  Alias
analysis is already one of the main factors limiting
{\implementation}'s scalability and so this would be a poor trade-off.

Second, {\implementation} assumes that every address in the program's
address space is either completely inaccessible or can be freely
accessed for both read and write, and that this status does not change
during the lifetime of the program.  {\Implementation} will therefore
not be able to model bugs which depend on read-only memory or those
where the \gls{analysiswindow} includes calls to functions such as
\texttt{mremap}.  Lifting the first restriction would be relatively
straightforward by extending the $\smBadPtr{}$ expression to include
an indication of the type of access contemplated, and, while this
would lead to a modest increase in analysis complexity, I would not
expect the performance cost to be extravagant.  Lifting the second
restriction would be more challenging.  At present, $\smBadPtr{}$ is a
function only of the address tested and the {\StateMachine}
environment.  Crucially, a $\smBadPtr{}$ in one thread cannot be
affected by any behaviour in the other, and so the {\StateMachine}
simplifiers can make useful inferences about them before deriving the
interfering {\StateMachine} (and these inferences can often be used to
reduce the number of interfering {\StateMachines} which must be
derived).  Having to consider potential races with \texttt{mremap} and
\texttt{munmap} at every $\smBadPtr{}$ would make this far more
difficult, and would probably lead to a noticeable reduction in
analysis performance, even when analysing bugs which do not depend on
such races.

Third, {\implementation} assumes that the program does not make use of
any run-time generated code or run-time code modifications.  This
would be a difficult limitation to remove.  The \glspl{cfg} and
{\StateMachines} are generated from the program binary, and this will
not accurately reflect the program's actual instructions if those
instructions can change while the program is running.  Solving this
problem is impossible in the general case\footnote{Consider a program
  P in a model of computation C which does not permit run-time code
  generation.  Construct a new P' in a model of computation C' which
  is identical to C except for allowing run-time code generation such
  that P' runs P and then uses some run-time generated code.  P' then
  uses run-time generated code if and only if P' terminates.  An
  algorithm to check for run-time code generation in C' would
  therefore solve the halting problem in C, and, since there are
  Turing-powerful models of computation which do not support run-time
  code generation, that is impossible.}, but it might be possible to
handle some simple cases by having the dynamic analysis identify and
log such code.  The later analyses could then generate the \glspl{cfg}
and {\StateMachines} from these logs.  Such an approach would have few
costs in programs which do not generate code at run-time and so could
sensibly be incorporated into {\implementation}, although the rarity
of run-time code generation suggests that this would be a dubious use
of engineering time.  Even with this extension {\implementation} would
not be able to support true self-modifying code, as the
{\StateMachines} cannot be modified while they are being symbolically
executing; removing \emph{that} limitation is probably impractical
while retaining the {\StateMachine} abstraction.  In any case, true
self-modifying code is sufficiently rare in real programs that not
supporting it is highly unlikely to be a problem in practice.

\subsection{Simplified concurrency model}
The final simplification made by {\implementation} is in its
concurrency model.  The only form of concurrency supported by
{\implementation} is simple multi-thread concurrency within a single
program.  It does not, for instance, consider races with signal
handlers, or inter-process races mediated via shared memory regions or
\texttt{ptrace}-like APIs.  While these mechanisms are much less
common than thread-style concurrency, they do still occur in some real
programs, and so it would potentially be useful to have some support
for them.  Signal handlers, and other up-call style concurrency, could
be added relatively easily by treating each signal handling function
as a special kind of \gls{interferingthread}, although the dynamic
analysis might need some modest extensions to determine when it is
useful to do so.  Supporting inter-process concurrency would require
more fundamental changes to {\implementation}, as it would need to be
able to model multiple address spaces at the same time, but would
require few fundamental changes to the {\technique} itself.

\section{Discussion}

This chapter has shown how to derive {\StateMachines} representing
potential program bugs and how to convert these into
\glspl{verificationcondition} specifying the circumstances under which
those bugs might reproduce.  I also discussed some of the limitations
of the techniques presented here and some possible ways of alleviating
these weaknesses.  The most important limitation of this approach,
though, was not discussed: the majority of bugs detected are false
positives which can never occur in real runs of the program.  These
come about because the {\StateMachine} analysis presented here
considers only instructions within the, quite small,
\gls{analysiswindow}, and makes highly conservative assumptions about
the program's behaviour outside of that window.  The next chapter will
show how to convert these \glspl{verificationcondition} into
\glspl{bugenforcer} which make the potential bugs reproduce far more
easily, allowing the true bugs to be confirmed and the false ones
discarded with minimal manual intervention.
