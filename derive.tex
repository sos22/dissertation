\chapter{Finding bugs}
\label{sect:derive}

{\Technique} is, at the most basic level, a system for taking
potential bugs, described by \glspl{verificationcondition}, and
turning them into either \glspl{bugenforcer}, which check which ones
are real, or {\genfixes}, which eliminate some specific bugs.  This
chapter describes one approach to finding the
\glspl{verificationcondition}.  The simplest form of the algorithm
assumes that the crashing instruction has already been identified and
then investigates concurrency errors which might lead to that sort of
crash, and I describe that form first.  It can also be generalised to
finding completely unknown bugs, albeit at the expense of higher
computational cost.

The basic algorithm proceeds as follows:
\begin{itemize}
\item Identify all of the instructions which the \gls{crashingthread}
  might have executed in the \gls{alpha}-instruction
  \gls{analysiswindow} leading up to the crash.  These are represented
  as an acyclic, unrolled, \gls{cfg}; the details are given in
  \autoref{sect:derive:build_crashing_cfg}.
\item Decompile the \gls{crashingthread}'s \gls{cfg} into a
  {\StateMachine}, converting the program's machine code into a form
  which is more amenable to later analysis.  The {\StateMachine}
  abstraction is described in \autoref{sect:derive:state_machines} and
  the decompilation process in \autoref{sect:derive:compile_cfg}.
\item Simplify the {\StateMachine}.  {\STateMachines}, when initially
  derived, are faithful representations of the program's behaviour,
  and as such often contain a large amount of information which is
  irrelevant to the bug under investigation.  The simplification step
  is responsible for removing this redundant information.  It is
  described in \autoref{sect:derive:simplify_sm}.
\item Examine the \gls{crashingthread}'s {\StateMachine} to discover
  what it might have raced with.  This information is then used to
  build \glspl{cfg} and {\StateMachines} for all of the
  \glspl{interferingthread}; this is described in
  \autoref{sect:derive:write_side}.
\item Symbolically execute the crashing and interfering
  {\StateMachines} so as to convert them into a
  \gls{verificationcondition}.  This is a predicate over the program's
  state and happens-before graph which is true if there is any
  possibility of the bug under investigation reproducing.  This step
  is described in \autoref{sect:using:check_realness}.
\end{itemize}
At this stage, {\technique} is concerned primarily with the local
structure of the program, meaning those instructions which fit within
the \gls{analysiswindow}.  It largely ignores more global properties,
including the structure of the heap or most of the program's existing
synchronisation, which are dealt with by the \gls{programmodel} and
\glspl{bugenforcer}.

\section{Building the crashing thread's CFG}
\label{sect:derive:build_crashing_cfg}

{\Technique} considers concurrency bugs caused by unfortunate
interleavings of the \gls{alpha} instructions prior to the crash, and
hence the first step is to find the \gls{alpha} instructions which
might have executed in the \gls{crashingthread} leading up to the
crash.  These are represented as a \gls{cfg} showing both the
instructions involved and the relationships between them.

Note that the instructions in this \gls{cfg} are dynamic rather than
static, so if the same program instruction executes twice on a single
path then it will be represented twice in the \gls{cfg}.  This means
that the generated \gls{cfg} is inherently acyclic, simplifying later
analysis.  More importantly, it also makes it easy to identify
specific memory-accessing operations, so that it is possible to talk
about a particular memory access happening before or after some other
access.

The approach taken by {\technique} is simple: start by deriving a
fragment of the program's static \gls{cfg} which contains all of the
necessary static instructions and then apply a loop unrolling
algorithm which converts that static \gls{cfg} into a dynamic one by
duplicating instructions as necessary.  The next subsections describe
this process in more detail.

\subsection[Building the \glsentrytext{crashingthread}'s static \glsentrytext{cfg}]{Building the \gls{crashingthread}'s static \gls{cfg}}
\label{sect:derive:build_static_cfg}

\begin{sanefig}
\begin{algorithmic}[1]
\State $\mathit{depth} \gets 0$
\State $\mathit{pendingAtDepth} \gets \queue{\mathit{targetInstrAddress}}$
\State $\mathit{result} \gets \map{}$
\While{$\mathit{depth} < \alpha$}
  \State $\mathit{pendingAtNextDepth} \gets \queue{}$
  \While{$\neg{}\mathit{empty}(\mathit{pendingAtDepth})$}
    \State $\mathit{currentInstr} \gets \mathit{pop}(\mathit{pendingAtDepth})$
    \If {$\mathit{result} \textrm{ has entry for } \mathit{currentInstr}$}
      \State \textbf{continue}
    \EndIf
    \State $\mathit{current} \gets \text{decode instruction at } \mathit{currentInstr}$
    \State $\mapIndex{\mathit{result}}{\mathit{currentInstr}} \gets \mathit{current}$
    \State $\mathit{predecessors} \gets \text{predecessors of } \mathit{currentInstr}$
    \State Add $\mathit{predecessors}$ to $\mathit{pendingAtNextDepth}$
  \EndWhile
  \State $\mathit{pendingAtDepth} \gets \mathit{pendingAtNextDepth}$
  \State $\mathit{depth} \gets \mathit{depth} + 1$
\EndWhile
\end{algorithmic}
\vspace{-6pt}
\caption{Building a \gls{crashingthread} static \gls{cfg}.}
\label{fig:derive:static_read_cfg_single_function}
\end{sanefig}

\noindent
The first step in building the \gls{crashingthread}'s dynamic
\gls{cfg} is to find its static \gls{cfg}.  The algorithm for doing so
is shown in Figure~\ref{fig:derive:static_read_cfg_single_function}.
This implements a depth-limited breadth-first search starting at the
crashing instruction and exploring backwards through the program's
control flow.  Note that this can result in a \gls{cfg} with multiple
roots.

This algorithm depends on being able to find the predecessors of an
arbitrary instruction, which is not always straightforwards given only
the program binary.  {\Technique} relies on information collected
during the dynamic analysis phase to do so, and in particular a
mapping from computed branches to the sets of instructions which can
follow those branches.  That mapping, together with a list of the
program's entry points, allows {\technique} to build a complete static
CFG of the program's instructions, allowing it to find an
instruction's predecessors when necessary.

\subsection[Converting the static \glsentrytext{cfg} to a dynamic one]{Converting the static \gls{cfg} to a dynamic one}
\label{sect:derive:handling_loops}

The nodes of the \gls{cfg} generated in
\autoref{sect:derive:build_static_cfg} represent static instructions
in the program, but the \glspl{cfg} used to build {\StateMachines}
must represent the program's dynamic instructions.  {\Technique} must
therefore convert the static \gls{cfg} into a dynamic one.  For
completely acyclic \glspl{cfg} this is easy, as each static
instruction executes at most once and it is safe to simply identify
the static and dynamic instructions.  Loops are more difficult to
handle, as the instructions in a loop might execute multiple times,
and no simple correspondence exists.  {\Technique} solves this problem
by simply unrolling such loops until they exceed \gls{alpha}
instructions\editorial{Cite for loop unrolling, maybe?}, so that the
loop can only execute at most once during the \gls{analysiswindow},
restoring the problem to the acyclic case\editorial{Already said
  that.}.

\begin{sanefig}
\begin{tikzpicture}
  [node distance=0.9 and 0.2, inner sep = 1.5pt]
  \begin{scope}[node distance=0.9 and 0.2]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$}; 
    \node (C) [CfgInstr] [below=of B] {$C_0$}; 
    \node (D) [CfgInstr] [below=of C] {$D_0$}; 
    \node (dummy) [right = of B] {~~~};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C.east) to [bend right=45] (B.east) node (edge1) [right] {};
    \begin{pgfonlayer}{bg}
      \node (box1) [fill=black!10,fit=(A) (B) (C) (D) (edge1) (dummy)] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=3.8cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$}; 
    \node (C) [CfgInstr] [below=of B] {$C_0$}; 
    \node (D) [CfgInstr] [below=of C] {$D_0$};  
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (B) to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B);
    \begin{pgfonlayer}{bg}
      \node (box2) [fill=black!10,fit=(A) (B) (C) (D) (C')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=7.8cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$};
    \node (B') [CfgInstr] [right=of B] {$B_1$};
    \node (C) [CfgInstr] [below=of B] {$C_0$};
    \node (D) [CfgInstr] [below=of C] {$D_0$};
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B');
    \begin{pgfonlayer}{bg}
      \node (box3) [fill=black!10,fit=(A) (B) (C) (D) (C') (B')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=11.8cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$};
    \node (B') [CfgInstr] [right=of B] {$B_1$};
    \node (C) [CfgInstr] [below=of B] {$C_0$};
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \node (C'') [CfgInstr] [right=of C'] {$C_2$};
    \node (D) [CfgInstr] [below=of C] {$D_0$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') -- (C');
    \draw[->] (C'') to [bend right=10] (B');
    \draw[->] (B') to [bend right=10] (C'');
    \begin{pgfonlayer}{bg}
      \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
    \end{pgfonlayer}
  \end{scope}
  \draw[->,thick] (box1) -- (box2) node [above,midway] {duplicate $C_0$};
  \draw[->,thick] (box2) -- (box3) node [above,midway] {duplicate $B_0$};
  \draw[->,thick] (box3) -- (box4) node [above,midway] {duplicate $C_1$};
  \draw[->,thick] (box4) -- +(2.28,0) node [above,midway] {$\cdots$};
\end{tikzpicture}
\caption{A CFG containing a cycle.}
\label{fig:cyclic_cfg}
\end{sanefig}

As an example, consider the \gls{cfg} shown at the left of
Figure~\ref{fig:cyclic_cfg}, which contains a loop between
instructions $B_0$ and $C_0$.  This loop must be removed from the
\gls{cfg} while maintaining all paths which terminate at $D_0$ and
which contain \gls{alpha} or fewer instructions.  The algorithm starts
by performing a depth-first traversal backwards through the graph from
$D_0$ until it finds an edge which closes a cycle.  In this case, that
is the edge from $C_0$ to $B_0$, and {\technique} therefore breaks
this edge by duplicating $C_0$, along with all of its incoming edges,
and redirecting the original edge to come from this new instruction.
All paths which were possible in the old graph will also be possible
in the new one, if duplicated nodes are treated as semantically
equivalent,
\begin{wrapfigure}{O}{3.9cm}
\vspace{-8pt}
\begin{figgure}
\begin{tikzpicture}
  [node distance=1 and 0.3]
  \node (A) at (0,2) [CfgInstr] {$A_0$};
  \node (B) [CfgInstr] [below=of A] {$B_0$};
  \node (B') [CfgInstr] [right=of B] {$B_1$};
  \node (C) [CfgInstr] [below=of B] {$C_0$};
  \node (C') [CfgInstr] [right=of C] {$C_1$};
  \node (C'') [CfgInstr] [above right=of B'] {$C_2$};
  \node (D) [CfgInstr] [below=of C] {$D_0$};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) -- (D);
  \draw[->] (C') -- (B);
  \draw[->] (A) -- (B');
  \draw[->] (B') -- (C');
  \draw[->] (C'') -- (B');
  \begin{pgfonlayer}{bg}
    \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
  \end{pgfonlayer}
\end{tikzpicture}
\caption{Fully unrolled version of the CFG in
  Figure~\ref{fig:cyclic_cfg}, preserving all paths of length six or
  fewer instructions.  Note that an additional root has been
  introduced at $C_2$. \todo{placement}}
\label{fig:unrolled_cyclic_cfg}
\end{figgure}
\vspace{-20pt}
\end{wrapfigure}
and the loop is now one instruction further away from the target
instruction $D_0$.  The process repeats, moving the cycle steadily
further and further away from $D_0$ until all paths of length
\gls{alpha} ending at $D_0$ are acyclic.  The cycle can then be safely
removed from the graph.  The complete algorithm is shown in
Figure~\ref{fig:derive:read:unroll_cycle_break}.

Note that the edge which is modified is the back edge, from $C_0$ to
$B_0$, which points ``away from $D_0$'', and not the forwards edge
from $B_0$ to $C_0$.  Trying to break the $B_0$ to $C_0$ edge would
have moved the cycle away from $A_0$ rather than away from $D_0$,
which would not be helpful.

This algorithm is guaranteed to preserve all paths of length $\alpha$
which end at the target instruction.  This is easy to show.  There are
only two places in the algorithm which remove existing edges, so
consider each in turn.  The first is the erasure on line 4.  This can
only ever affect edges whose shortest path to the target is at least
$\alpha$ instructions long, and so cannot eliminate any of the paths
which must be preserved.  The other is the replacement step at line
10, which replaces an edge from $edge.source$ to $edge.destination$
with one from $newNode$ to $edge.destination$.  This is safe provided
that every path to $newNode$ has a matching path to $edge.source$,
which is ensured by duplicating all of $edge.source$'s incoming edges
to $newNode$.  At the same time, no additional paths will be
introduced, because every path to $newNode$ has a matching path to
$edge.source$.  As such, the algorithm correctly preserves all paths
of length \gls{alpha}, as desired, and does not introduce any more.

This might appear, on the face of it, to be a rather expensive
algorithm: it must explore every path of length \gls{alpha} ending at
the target instruction, and the number of such paths is potentially
exponential in \gls{alpha}.  This is true, but several other
algorithms used by {\implementation} also have exponential worst-case
running time and larger constant factors, and so in practice deriving
the crashing \gls{cfg} accounts only a small percentage of the total
analysis time.

\begin{sanefig}
\begin{algorithmic}[1]
  \While {graph is not cycle-free}
     \State $edge \gets \textsc{findEdgeToBeBroken}(targetInstr)$
     \If {$edge$ is at least $\alpha$ instructions from target instruction}
        \State {Erase $edge$ from graph}
     \Else
        \State {$newNode \gets$ duplicate of $edge.source$}
        \For {$i$ incoming edge of $edge.source$}
           \State {Create a new edge from $i.source$ to $newNode$}
        \EndFor
        \State {Replace $edge$ with an edge from $newNode$ to $edge.destination$}
     \EndIf
  \EndWhile
\end{algorithmic}
\caption{Loop unrolling and cycle breaking algorithm.
  \textsc{findEdgeToBeBroken} simply performs a depth-first search of
  the graph backwards from $targetInstr$ and returns the first edge
  which completes a cycle.}
\label{fig:derive:read:unroll_cycle_break}
\end{sanefig}

\section{\STateMachines}
\label{sect:derive:state_machines}

\todo{I think I should rewrite this giving more emphasis to
  {\StateMachines} as a model of computation i.e. explicitly saying
  what environment they're executing in.}

The dynamic \gls{cfg} represents all of the instructions in the
\gls{crashingthread} in the \gls{analysiswindow}, and therefore in
principle contains all of the information needed to analyse the bug.
It is, however, very close to machine code, and is therefore difficult
to work with.  The next step in the algorithm is to decompile it into
a more amenable form.  In the case of {\technique}, this is a
{\StateMachine}: a directed acyclic graph of states, described in
\autoref{fig:state_machine_states}.

\begin{sanefig}
\begin{tabular}{llllp{6.05cm}}
\multicolumn{2}{l}{State}       & \multicolumn{2}{l}{Fields} & Meaning \\
\hline
\multicolumn{2}{l}{\state{If}}  & \state{cond} & Boolean BDD\editorial{Forward ref}        & Conditional branch with two successor states.  Evaluates \state{cond}, branching to one successor if it is true and the other if it is false. \\
\hline
\multicolumn{2}{l}{Terminal states} &          &             & Terminal states.  {\STateMachine} execution finishes when it reaches one of these. \\
\hdashline
 & {\stSurvive}              &              &             & The bug has been avoided. \\
\hdashline
 & {\stCrash}                &              &             & The bug will definitely happen. \\
\hdashline
 & {\stUnreached}            &              &             & A contradiction has been reached; this path through the {\StateMachine} should be ignored. \\
\hline
\multicolumn{2}{l}{Side-effect states}\\
 & \state{Load}                 & \state{addr} & Expression BDD & \multirow{3}{6.05cm}{Load from program memory\editorial{undef} at address \state{addr} and store the result in {\StateMachine} temporary\editorial{undef} \state{tmp}.} \\
 &                              & \state{tmp}  & {\STateMachine} temporary\editorial{Size of update?} \\
\\
\hdashline
 & \state{Store}                & \state{addr} & Expression BDD & \multirow{3}{6.05cm}{Store the value of \state{data} to program memory at address \state{addr}.}\\
 &                              & \state{data} & Expression BDD \\
\\
\hdashline
 & \state{Copy}                 & \state{data} & Expression BDD & \multirow{2}{6.05cm}{Evaluate an expression and store the result in a {\StateMachine} temporary.} \\
 &                              & \state{tmp}  & {\STateMachine} temporary \\
\hdashline
 & \state{ImportRegister}       & \state{tid}  & Thread ID       & \multirow{2}{6.05cm}{Copy the value which register \state{reg} in program thread \state{tid} had when the {\StateMachine} started into {\StateMachine} temporary \state{tmp}.} \\
 &                              & \state{reg}  & Register ID \\
 &                              & \state{tmp}  & {\STateMachine} temporary \\
\\
\hdashline
 & \state{Assert}               & \state{cond} & Boolean BDD            & Note that a given condition is true at a particular point in the {\StateMachine}'s execution. \\
\hdashline
 & $\Phi$                       &              &                 & Implement an SSA $\Phi$ node\cite{cytron1991}. \\
\hdashline
 & {\stStartAtomic}          &              &                 & \multirow{3}{6.05cm}{Delimit atomic blocks, used to constrain the set of schedules which must be considered; see \autoref{sect:using:build_cross_product}.} \\
 & {\stEndAtomic}            \\
\\
\\
\end{tabular}
\caption{Types of {\StateMachine} states.}
\label{fig:state_machine_states}
\end{sanefig}

{\STateMachine} states do not directly correspond to instructions in
the original program: one state might represent several instructions,
or a single instruction might be represented by multiple states.  For
instance, an instruction in a function $f$ might correspond to one
state when $f$ is called from $g$ and another when $f$ is called from
$h$, and instructions which are not relevant to the behaviour being
investigated might not have any corresponding states at all.

\todo{Distinction between static and dynamic structure of
  {\StateMachines}?} In addition to the graph of states,
     {\StateMachines} may also have some temporary variables.  These
     are simple slots into which the values of expressions and the
     results of \state{Load} operations can be stored.  Temporaries
     can only store simple values, with no internal structure, and
     there is no concept analogous to a pointer to a temporary.  Note
     that {\StateMachine} temporaries do not necessarily correspond to
     any particular bit of program state, and that most program state
     will not be represented by any temporary.

\subsection{{\STateMachine} expression language}
\label{sect:sm_expr_language}

\begin{sanefig}
\begin{tabular}{lp{11.2cm}}
Expression & Meaning \\
\hline
$\smTmp{A}$ & The value of {\StateMachine} temporary $A$. \\
$\happensBefore{A}{B}$ & True if event $A$ happens before event $B$, false if $B$ happens before $A$\editorial{If one never happens?}. \\
$\entryExpr{\mai{tid}{instr}}$ & True if thread $\mathit{tid}$ starts\editorial{undef} with instruction $\mathit{instr}$, and false otherwise. \\
$\controlEdge{tid}{A}{B}$ & True if thread $\mathit{tid}$ executed instruction $B$ immediately after instruction $A$. False if it executed some other instruction after $A$ and undefined if it did not execute $A$ at all.\\
$\smBadPtr{expr}$ & True if $\mathit{expr}$ evaluates to a value which is not a valid pointer.\\
$\smLoad{expr}$ & The initial value of the memory at address $\mathit{expr}$.\\
\end{tabular}
\caption{Expressions in the {\StateMachine} expression language.  The
  usual arithmetic operators, such as addition, multiplication, bit
  shift, etc., are also supported, but logical operators such as
  $\wedge$ and $\vee$ are not.}
\label{fig:state_machine_exprs}
\end{sanefig}

\noindent
Any non-trivial {\StateMachine} will include some expressions over the
original program's state.  These are expressed using an expression
language which is described in \autoref{fig:state_machine_exprs}.
This language is, for the most part, quite conventional, and includes
simple mechanisms for querying the program's behaviour and state and
for obtaining the values of {\StateMachine} temporaries, and for
evaluating simple arithmetic operators.  It does not, however, include
any logical connectives such as $\wedge$ or $\vee$.  The operators are
instead encoded into binary decision diagrams, or
BDDs\cite{Brace1990}, which are themselves expressed in terms of the
expression language.  This is analogous to the way in which
Satisfiability Modulo Theory (SMT) solvers represent the expression
whose satisfiability is to be checked as an expression in a boolean
\begin{wrapfigure}{r}{5cm}
  \vspace{-12pt}
  \begin{figgure}
  \centerline{
  \begin{tikzpicture}
    \node (x) [BddNode] {$\smTmp{A} = 72$};
    \node (y) [BddNode, below = of x] {$\smLoad{\smTmp{B}} = 9$};
    \node (z) [BddNode, below = of y] {$\smTmp{B} > 912$};
    \node (true) at (-1, -5) [BddLeaf] {$715$};
    \node (false) at (1, -5) [BddLeaf] {$\smTmp{C}$};
    \draw [BddTrue] (x) -- (y);
    \draw [BddFalse] (x.east) to [bend left=40] (false);
    \draw [BddTrue] (y) to [bend right=45] (true);
    \draw [BddFalse] (y) -- (z);
    \draw [BddTrue] (z) -- (true);
    \draw [BddFalse] (z) -- (false);
  \end{tikzpicture}
  }
  \caption{An example expression BDD.  This can evaluate to either
    $715$ or $\smTmp{C}$, depending on the values of $\smTmp{A}$,
    $\smTmp{B}$ and the initial contents of program memory.}
  \label{fig:derive:example_expr_bdd}
  \end{figgure}
  \vspace{-12pt}
\end{wrapfigure}
algebra whose input variables are themselves expressions in some other
theory.

\STateMachine{} states use two types of BDD: expression BDDs and
boolean BDDs.  The difference is that\editorial{ick} boolean BDDs are
constrained to evaluate to a simple boolean, whereas expression BDDs
evaluate to an expression in the expression language.  An example
expression BDD is shown in Figure~\ref{fig:derive:example_expr_bdd}.
This BDD evaluates to $\smTmp{C}$ if either $\smTmp{A} \not= 72$ or
both $\smLoad{\smTmp{B}} \not= 9$ and $\smTmp{B} \leq 912$, or to
$715$ otherwise.

Note that $\smLoad{}$ expressions always evaluate to the value which
the selected memory location had when the {\StateMachine}
started\editorial{ugg}, and not to the current value, which may be
different if the {\StateMachine} has used any \state{Store}
side-effects.  The only way to access the current contents of memory
is via a \state{Load} side-effect state.  This means that $\smLoad{}$
is a pure function and can be re-ordered freely across side-effecting
operations, simplifying analysis. \todo{orphan}

\subsection{Example {\StateMachines}}
\label{sect:derive:simple_toctou_example}

\begin{sanefig}
  \begin{tabular}{ll}
    \subfigure[][Crashing thread machine code.  The program crashed at \texttt{4006a7}]{
      \hspace{-6mm}
      \raisebox{3.1cm}{
      \texttt{
        \begin{tabular}{rlll}
          & \multicolumn{3}{l}{crashing\_thread:} \\
          \\
          400694: & mov & global\_ptr, &\!\!\!\%rax\\
          40069b: & test & \%rax, &\!\!\!\%rax \\
          40069e: & je   & \multicolumn{2}{l}{4006ad}\\
          4006a0: & mov  & global\_ptr, &\!\!\!\%rax\\
          4006a7: & movl & \$0x5, &\!\!\!(\%rax)\\
          \\
          \\
        \end{tabular}
      }
      \hspace{-6mm}
      \label{fig:derive:single_threaded_machine_inp:crashing}
    }
    }
    &
    \subfigure[][Crashing {\StateMachine}]{
      \begin{tikzpicture}[minimum height=1cm,minimum width=6.4cm, node distance = 0.5]
        \node (l1) at (0,2) [stateSideEffect] {L1: \stLoad{1}{\mathrm{global\_ptr}} };
        \node (l2) [stateIf, below=of l1] {\!\!\!\raisebox{-3pt}{L2: \stIf{\smTmp{1} = 0}}\!\!\!};
        \node (l4) [stateSideEffect, below=of l2] {L4: \stLoad{2}{\mathrm{global\_ptr}} };
        \node (l5) [stateIf, below=of l4] {\!\!\!\raisebox{-3pt}{L5: \stIf{\smBadPtr{\smTmp{2}}}}\!\!\!};
        \node (l6) at (-1.9, -4) [stateTerminal,minimum width=3.5cm] {L6: \stCrash};
        \node (l3) at (1.9, -4) [stateTerminal,minimum width=3.5cm] {L3: \stSurvive };
        \node [above = -1.1 of l3] {};
        \draw[->] (l1) -- (l2);
        \draw[->,ifTrue] (l2.east) to [bend left=45] (l3);
        \draw[->,ifFalse] (l2) -- (l4);
        \draw[->] (l4) -- (l5);
        \draw[->,ifFalse] (l5) -- (l3);
        \draw[->,ifTrue] (l5) -- (l6);
      \end{tikzpicture}\hspace{-15mm}
      \label{fig:derive:single_threaded_machine}
    }
    \\
    \subfigure[][Interfering machine code]{
      \hspace{-6mm}
      \raisebox{.8cm}{
      \texttt{
        \begin{tabular}{rlll}
          & \multicolumn{3}{l}{interfering\_thread:} \\
          4008fb: & movq & \$0x0, &global\_ptr\\
        \end{tabular}
      }
      }
      \hspace{-6mm}
      \label{fig:derive:single_threaded_machine_inp:interfering}
    } &
    \subfigure[][Interfering {\StateMachine}]{
      \begin{tikzpicture}[minimum height=1cm, minimum width=6.4cm]
        \node (l7) [stateSideEffect] {L7: \stStore{0}{\mathrm{global\_ptr}}};
        \node [right = -7 of l7] {};
        \node [left = -7 of l7] {};
        \node [above = -1.3 of l7] {};
      \end{tikzpicture}
      \label{fig:derive:single_threaded_machine_write}
    }
    \\
  \end{tabular}
  \vspace{-12pt}
  \caption{Two threads with a bug of the right form to be investigated
    by {\technique}, showing their disassembly and {\StateMachines}.
    This example is discussed further in
    \autoref{sect:eval:simple_toctou}.}
  \label{fig:derive:single_threaded_machine_both}
\end{sanefig}

\noindent
\autoref{fig:derive:single_threaded_machine_both} shows a pair of
threads which, when run concurrently, might exhibit a simple
time-of-check, time-of-use bug.  The crashing thread loads from
\texttt{global\_ptr} twice, validating the first load and using the
second one, while the interfering thread might in parallel set it to
0.  The {\StateMachines} for these fragments of program are shown on
the right.  Note that \verb|4006a7|, the instruction which crashes, is
not itself represented in the {\StateMachine}: by the time that
instruction executes, the program is either doomed to crash or has
definitely avoided the bug, and so that instruction is irrelevant to
determining when and whether the bug can actually happen.

\section[Decompiling the dynamic \glsentrytext{cfg} to a \StateMachine]{Decompiling the dynamic \gls{cfg} to a \StateMachine}
\label{sect:derive:compile_cfg}

\begin{wrapfigure}{r}{6.15cm}
  \begin{figgure}
  \begin{centering}
    \texttt{
      \begin{tabular}{lllll}
        \!\!\!\!\!\!A: & MOV  & rdx    &\!\!\!-> & \!\!\!rcx\\
        \!\!\!\!\!\!B: & LOAD & *(rcx) &\!\!\!-> & \!\!\!rcx\\
        \!\!\!\!\!\!C: & \multicolumn{4}{l}{JMP\_NE *(rcx + 8), 0, B}\\
        \!\!\!\!\!\!D: & STORE & \$0 &\!\!\!-> & \!\!\!*(rcx)\\
      \end{tabular}
    }
  \end{centering}
  \caption{\todo{Fix placement}}
  \label{fig:derive:example_dissassembly1}
  \end{figgure}
\end{wrapfigure}
I have already described how to build the \gls{crashingthread}'s
dynamic \gls{cfg}.  The next step is to convert that dynamic \gls{cfg}
into a {\StateMachine}.  The {\StateMachine} analysis language is
powerful enough to make translating individual instructions in
isolation completely straightforward\footnote{{\Implementation} uses
  LibVEX \todo{cite} to decode AMD64 machine code before performing
  this translation.}.  Connecting them together is, however, slightly
more difficult.  There are three cases which require special care:
\begin{itemize}
\item
  Some edges will be erased from the dynamic \gls{cfg}.  For instance,
  in Figure~\ref{fig:unrolled_cyclic_cfg}, the program's original
  \gls{cfg} contained an edge from $C_0$ to $D_0$ but the unrolled
  \gls{cfg} does not include any branches from $C_1$ to a $D$-like
  instruction.  These are converted to branches to the special
  {\stUnreached} state, reflecting the fact that these paths are of no
  interest to the rest of the analysis.
\item
  Some additional edges will have been introduced which do not
  correspond to anything in the original program.  In the example,
  instruction $A_0$ had a single successor, $B_0$, in the static
  \gls{cfg} but has multiple successors in the dynamic one.  Each of
  the $B_i$ \gls{cfg} nodes will be represented by a separate
  {\StateMachine} state, but there is no condition on the original
  program's state which can be evaluated at $A_0$ which determines
  which of $B_i$ states the {\StateMachine} must execute next.
  {\Technique} converts these into {\StateMachine}-level control flow
  using ${\controlEdgeName}()$ expressions which test the program's
  path through the dynamic \gls{cfg}.  See, for instance, state $A_0'$
  in \autoref{fig:state_machine_for_cyclic_cfg}.
\item
  The \gls{cfg} can sometimes have multiple roots, each represented by
  a separate {\StateMachine} state, but the {\StateMachine} itself
  must have a single entry state.  {\Technique} handles these using
  $\entryExpr{}$ expressions, which simply test where a thread entered
  The start state of \autoref{fig:state_machine_for_cyclic_cfg} is an
  example.
\end{itemize}
As a somewhat unrealistic example, suppose that the \gls{cfg} in
\autoref{fig:cyclic_cfg} were generated from the program fragment
shown in \autoref{fig:derive:example_dissassembly1}.  The
\verb|JMP_NE| instruction is supposed to indicate that \verb|C| loads
from the memory at \verb|rcx+8|, jumping to \verb|B| if it is non-zero
and proceeding to \verb|D| otherwise.  This will produce a dynamic
\gls{cfg} as in \autoref{fig:unrolled_cyclic_cfg}, as already
discussed, and a \StateMachine{} as shown in
\autoref{fig:state_machine_for_cyclic_cfg}.  The generated
\StateMachine{} is then converted to static single
assignment\cite{cytron1991}\editorial{first ref} form and passed to
the rest of the analysis.

\begin{sanefig}
\begin{tikzpicture}[minimum height=1cm, minimum width=6.5cm]
  \node[stateIf,initial above] (l1) {\stIf{\entryExpr{\mai{tid}{A_0}}}};
  \node[right = of l1] (dummy1) {};
  \node[stateSideEffect,below = of l1] (l2) {$A_0$: \state{Copy} $\smReg{rdx}{} \rightarrow \smReg{rcx}{}$};
  \node[stateIf,below = of l2, inner sep = -5mm] (l3) {\!\!\!$A_0'$: \stIf{\controlEdge{tid}{A_0}{B_0}}\!\!\!};
  \node[stateSideEffect,below = of l3] (l4) {$B_0$: \state{Load} $\ast(\smReg{rcx}{}) \rightarrow \smReg{rcx}{}$};
  \node[stateSideEffect,below = of l4] (l5) {$C_0$: \stLoad{}{\smReg{rcx}{}+8}};
  \node[stateIf,below = of l5] (l6) {\stIf{\smTmp{} = 0}};
  \node[stateIf,below = of l6] (l7) {$D_0$: \stIf{\smBadPtr{\smReg{rcx}{}}}};

  \node[stateSideEffect] (l11) at (dummy1 |- l2) {$C_2$: \stLoad{}{\smReg{rcx}{}+8}};
  \node[stateIf] at (l3 -| l11) (l12) {\stIf{\smTmp{} = 0}};

  \node[stateSideEffect] at (l12 |- l4) (l8) {$B_1$: \state{Load} $\ast(\smReg{rcx}{}) \rightarrow \smReg{rcx}{}$};
  \node[stateSideEffect] at (l8 |- l5) (l9) {$C_1$: \stLoad{}{\smReg{rcx}{}+8}};
  \node[stateIf] at (l9 |- l6) (l10) {\stIf{\smTmp{} = 0}};

  \node[stateTerminal,below = of l7, minimum width=3.5cm] (lBeta) {\stCrash};
  \node[stateTerminal,minimum width=3.5cm] at (lBeta -| l10)(lAlpha) {\stUnreached};
  \node[stateTerminal,minimum width=3.5cm] at (barycentric cs:lBeta=0.5,lAlpha=0.5)(lGamma) {\stSurvive};

  \draw[->,ifTrue] (l1) -- (l2);
  \draw[->,ifFalse] (l1) -- (l11);
  \draw[->] (l2) -- (l3);
  \draw[->,ifFalse] (l3) -- (l8);
  \draw[->,ifTrue] (l3) -- (l4);
  \draw[->] (l4) -- (l5);
  \draw[->] (l5) -- (l6);
  \draw[->,ifFalse] (l6) -- (lAlpha);
  \draw[->,ifTrue] (l6) -- (l7);
  \draw[->,ifFalse] (l7) -- (lGamma);
  \draw[->,ifTrue] (l7) -- (lBeta);
  \draw[->] (l8) -- (l9);
  \draw[->] (l9) -- (l10);
  \draw[->,ifTrue] (l10) -- (lAlpha);
  \draw[->,ifFalse] (node cs:name=l10,angle=210) ..controls +(0,-.3) and +(.3,0) .. ++(-.5,-0.5) % Leaving node
      -- ++(-1.9,0) % First horizontal
      .. controls +(-.3,0) and +(0,-.3) .. ++(-.5,.5) % Bend to vertical
      -- ++(0,5) % Vertical
      .. controls +(0,.3) and +(.3,0) .. ++(-.5,.5) % Bend to second horizontal
      -- ++(-.5,0) % Second horizontal
      ..controls +(-.3,0) and +(0,.3) .. (node cs:name=l4,angle=12);
  \draw[->] (l11) -- (l12);
  \draw[->,ifTrue] (l12.east) .. controls +(.3,0) and +(0,.3) .. ++ (.5,-.5) -- ++(0,-9) .. controls +(0,-.3) and +(.3,0) .. ++ (-.5,-.5) -- (lAlpha);
  \draw[->,ifFalse] (l12) -- (l8);
\end{tikzpicture}
\caption{{\STateMachine} generated from the dynamic \gls{cfg} shown in
  Figure~\ref{fig:cyclic_cfg}.  tid is an identifier used to
  distinguish the crashing and interfering threads.  \todo{This shows
    modifications to registers, whereas actually the {\StateMachine}
    system only allows modifications to temporaries, and use of
    registers for {\stImportRegister}.  Should probably fix that.
    Also maybe generate in SSA form?}}
\label{fig:state_machine_for_cyclic_cfg}
\end{sanefig}

\subsection{Handling library functions}
\label{sect:derive:library_functions}

{\Technique} deals with calls to functions in the operating system
standard library by re-implementing approximations of them as
fragments of {\StateMachine} which are substituted into the
{\StateMachines} as it is being built.  In effect, the library
function is treated as a special sort of instruction, and compiled in
exactly the same way as any other instruction.  This is generally
straightforwards and I do not give full details here.

\todo{Odd linkage?} Library function handling is particularly
important when investigating double-free bugs, as the actual crash
will occur in the library function rather than in the program under
investigation.  {\Technique} uses two implementations of
\texttt{free}; one, the crashing \texttt{free}, for the call which is
being investigated as a potential crash site, and one, the
non-crashing \texttt{free}, for every other call\footnote{Note that
  this includes other calls to \texttt{free} in the
  \gls{crashingthread}.}.  Both are shown in
\autoref{fig:library_free}.  The effects of these implementations are
hopefully reasonably clear\editorial{ick}: non-crashing \texttt{free}s
set the $\mathit{last\_free}$ address to the pointer which was
released, and the crashing \texttt{free} then asserts that the pointer
which it releases is not the one which was most recently released.
This scheme is clearly not capable of detecting all possible
double-free bugs, but it is sufficient for the most common kind.

\begin{sanefig}
  \centerline{
  \subfigure[][Crashing \texttt{free}]{
    \begin{tikzpicture}[minimum height=1cm,minimum width=7cm]
      \node (l1) [stateSideEffect] {\stLoad{1}{\mathit{last\_free}} };
      \node (l2) [stateIf, below = of l1] {\stIf{\smTmp{1} = \mathit{arg0}} };
      \node (l3) [stateTerminal,minimum width=3.5cm] at (-2cm,-4cm) {\stCrash };
      \node (l4) [stateTerminal,minimum width=3.5cm] at (2cm,-4cm) {\stSurvive };
      \draw[->] (l1) -- (l2);
      \draw[->,ifTrue] (l2) -- (l3);
      \draw[->,ifFalse] (l2) -- (l4);
    \end{tikzpicture}
    \label{fig:library_free:crashing}
  }
  \subfigure[][Non-crashing \texttt{free}]{
    \raisebox{20mm}{
    \begin{tikzpicture}[minimum height=1cm,minimum width=7cm]
      \node [stateSideEffect] {\stStore{\mathit{arg0}}{\mathit{last\_free}} };
    \end{tikzpicture}
    }
    \label{fig:library_free:non_crashing}
  }
  }
  \vspace{-12pt}
  \caption{{\STateMachine} implementations of the \texttt{free}
    function. $arg0$ is an expression for the platform ABI's first
    argument register; for Linux on AMD64, this is RDI.
    $\mathit{last\_free}$ can be any fixed memory location which is
    not used by the program; {\implementation} uses an address in
    kernel space.}
  \label{fig:library_free}
\end{sanefig}

\begin{sanefig}
  \centerline{
    {\hfill}
  \subfigure[][pthread\_mutex\_lock]{
    \begin{tikzpicture}[minimum height=1cm, minimum width=7cm]
      \node (l1) [stateSideEffect] {\stStartAtomic};
      \node (l2) [below = of l1, stateSideEffect] {\stLoad{1}{\mathit{arg0}}};
      \node (l3) [below = of l2, stateSideEffect] {\stAssert{\smTmp{1} = 0}};
      \node (l4) [below = of l3, stateSideEffect] {\stStore{\mathit{tid}}{\mathit{arg0}}};
      \node (l5) [below = of l4, stateSideEffect] {\stEndAtomic};
      \draw[->] (l1) -- (l2);
      \draw[->] (l2) -- (l3);
      \draw[->] (l3) -- (l4);
      \draw[->] (l4) -- (l5);
    \end{tikzpicture}
  }{\hfill}
  \subfigure[][pthread\_mutex\_unlock]{
    \begin{tikzpicture}[minimum height=1cm, minimum width=7cm]
      \node (l1) [stateSideEffect] {\stStartAtomic};
      \node (l2) [below = of l1, stateSideEffect] {\stLoad{1}{\mathit{arg0}}};
      \node (l3) [below = of l2, stateSideEffect] {\stAssert{\smTmp{1} = \mathit{tid}}};
      \node (l4) [below = of l3, stateSideEffect] {\stStore{0}{\mathit{arg0}}};
      \node (l5) [below = of l4, stateSideEffect] {\stEndAtomic};
      \draw[->] (l1) -- (l2);
      \draw[->] (l2) -- (l3);
      \draw[->] (l3) -- (l4);
      \draw[->] (l4) -- (l5);
    \end{tikzpicture}
  }
    {\hfill}
  }
  \vspace{-12pt}
  \caption{{\STateMachine} models for the pthread\_mutex\_lock and
    pthread\_mutex\_unlock functions.  $\mathit{arg0}$ is an
    expression for the first argument register.  $\smTmp{1}$ is a
    fresh {\StateMachine} temporary.  $\mathit{tid}$ is a constant
    identifying the current thread; either 1 for the
    \gls{crashingthread} or 2 for the \gls{interferingthread}.}
  \label{fig:library_mux}
\end{sanefig}

The implementation of library-provided synchronisation functions is
also important for many bugs.  \autoref{fig:library_mux} shows how
{\technique} implements pthread\_mutex\_lock and
pthread\_mutex\_unlock.  Note that the lock operation does not include
any logic to wait for the lock to be released, but instead simply
asserts that it is not currently held.  That is sufficient: the
analysis will not consider any paths on which an assertion fails, and
so these library models will ensure that it does not consider any
paths which are prohibited by the mutex operations.

\section{Simplifying the {\StateMachine}}
\label{sect:derive:simplify_sm}

Much as an optimising compiler optimises its intermediate form so as
to make the generated program run faster, {\technique} simplifies its
{\StateMachines} so as to make symbolically executing them less
expensive.  The most important techniques used to do so are:
\begin{itemize}
\item \emph{Dead code elimination}, which is used to eliminate
  redundant updates to {\StateMachine} temporaries.
\item \emph{Copy propagation}, based on the algorithm from the
  dcc\cite{Cifuentes1994} decompiler, is used to shorten long chains
  of assignments by combining their expression BDDs.  One minor
  extension present in {\technique} but not dcc is that {\technique}
  can make use of \state{Assert} side-effects during this
  transformation, so that, for instance, if $x$ is asserted to be less
  than $7$ then the expression $x > 22$ can be replaced by \false.
  This does not require any important changes to the algorithm, beyond
  a few simple rules describing when such rewrites are valid.
\item \emph{{\stPhi} elimination}, which is used to turn SSA {\stPhi}
  side effect states into \state{Copy} ones.  This reifies the {\stPhi}
  states' implicit dependence on {\StateMachine} control flow into the
  explicit structure of an expression BDD, making it easier for the
  other simplification steps to manipulate and use it.
\item \emph{Alias analysis}, which determines how \state{Store} and
  \state{Load} operations might interact and then uses this
  information to forward values from \state{Store}s to \state{Load}s,
  potentially eliminating both.
\end{itemize}
{\Technique} also employs various minor peephole simplifications, such
as removing empty \state{Atomic} regions or combining long chains of
related \state{If} operations.

The effect of these simplification passes is to take a {\StateMachine}
which represents all of the program's behaviour in the
\gls{analysiswindow} and transform it to one which represents only the
behaviour which is most relevant to the bug under
investigation\editorial{short para}.

\section{Building the interfering thread's \StateMachines}
\label{sect:derive:write_side}

At this stage, {\technique} has built the crashing thread's
{\StateMachine} for the bug which is to be investigated, and it must
now determine whether it might crash due to an atomicity violation.
In principle, this should consider every possible interleaving of the
crashing {\StateMachine} with every \gls{alpha}-instruction long trace
of any other thread in the program, but this would clearly be
completely impractical for all but the most trivial programs.
Fortunately, most such traces can be dismissed without ever needing to
explicitly enumerate them, and this usually makes it possible to
complete the analysis in a reasonable amount of time.

This reduction depends on information in the \gls{programmodel}, which
has not yet been described (see \autoref{sect:program_model}).  For
now, assume that the \gls{programmodel} exposes a function from
memory-accessing instructions to a set of instructions which might
access the same memory location.  This makes it possible to derive
three useful sets of instructions: $i2c$, the set of stores which
might alias with a load operation in the \gls{crashingthread}, $c2i$,
the set of loads which might alias with a store in the
\gls{crashingthread}, and $\beta = c2i \cup i2c$.  Informally, $i2c$
is the set of instructions which, when added to the
\gls{interferingthread}, might send data to the \gls{crashingthread},
$c2i$ those which might receive data from it, and $\beta$ those which
communicate in either direction.

These three sets can then be used to restrict the set of interfering
traces which need to be considered.  Most obviously, it is safe to
discard all instructions in the \gls{interferingthread} after the last
member of $i2c$, as these cannot possibly influence the behaviour of
the \gls{crashingthread} and hence cannot influence whether the bug of
interest will reproduce.  Beyond that, it is also possible to discard
any instructions prior to the first member of $\beta$.  Suppose, for
instance, that the \gls{interferingthread} consists of the instruction
sequence $aBc$, where $a$ is some sequence of instructions none of
which belong to $\beta$, $B$ is a member of $\beta$, and $c$ is
another sequence of instructions.  Any bugs present in that sequence
will also be present in the sequence $Bc$.  $a$ cannot directly
influence the \gls{crashingthread}, by definition, and so its only
possible effect is on the behaviour of the \gls{interferingthread}, by
restricting the possible values of program memory and registers when
it starts.  {\Technique} will report a possible bug if \emph{any}
possible initial values could lead to an atomicity violation, and the
set of possible values after running $a$ is clearly a subset of the
set of all possible values, and so discarding $a$ can never lead to
any possible bugs being omitted.  It is therefore always safe to do
so.

On the other hand\editorial{ugg}, it is not always desirable to do so
if the restricted set of initial values would have been easier to
analyse.  For instance, if the bug to be investigated is a bad pointer
dereference, knowing that the value stored into a shared structure had
previously been dereferenced by the interfering thread, and hence must
definitely be a valid pointer, is often useful.  The approach taken by
{\implementation} is to first generate a set of minimal \glspl{cfg}
(i.e. one in which every root is a member of $\beta$) and to then
extend them backwards to include a small amount of additional context,
provided that doing so does not increase the number of paths through
the \gls{cfg} or cause the length of any path to exceed \gls{alpha}.
In other words\editorial{ugg}, a \gls{cfg} rooted at instruction A
will be extended to include its predecessor instruction B provided
that B is A's only predecessor and that the longest path starting at A
is at most $\alpha - 1$ instructions long.  \todo{So what?}

\subsection[Building the \glsentrytext{interferingthread} \glsentrytext{cfg}s]{Building the \gls{interferingthread} \glspl{cfg}}

The procedure for building interfering {\StateMachines} is similar to
that for building crashing ones: find $i2c$ and $\beta$ using the
\gls{programmodel}, derive a cyclic static \gls{cfg}, unroll it to
form an acyclic dynamic one, potentially extend it with a small amount
of extra context, and then decompile the resulting \glspl{cfg} to a
{\StateMachine}\editorial{Golly that's a long sentence.}.

Building a static interfering \gls{cfg} is simple, given $i2c$ and
$\beta$.  Starting from each member of $\beta$, {\technique} explores
forwards for \gls{alpha} instructions (crossing function boundaries
when necessary), building \glspl{cfg} as it goes, merging any
\gls{cfg} fragments which are reachable from multiple $\beta$
instructions.  It then trims these \glspl{cfg} back down to contain
only instructions which can reach a member of $i2c$ within \gls{alpha}
instructions\editorial{two instructions in one sentence}.  Note that
this can sometimes generate multiple disjoint \gls{cfg} fragments.
These are treated independently, generating multiple
\glspl{interferingthread} which will then be considered in turn by the
rest of the analysis.

The resulting static interfering \glspl{cfg} must then converted to
dynamic ones.  The procedure used is essentially similar to that used
to convert the static crashing \gls{cfg}: unroll all loops until all
paths which involve a loop are at least \gls{alpha} instructions long,
at which point the loop closing edges can be discarded so that each
static instruction executes at most once and can be identified with a
dynamic one.  Checking whether a loop has been sufficiently unrolled
is, however, slightly more complicated in the interfering thread.  In
the crashing thread, the only paths to preserve are those which reach
the potentially-crashing instruction within \gls{alpha} instructions,
and so anything further than \gls{alpha} from that instruction can be
discarded, whereas in the interfering \gls{cfg} we must preserve the
\gls{alpha}-long paths from any member of $\beta$ to any member of
$i2c$.  This requires a slightly more complicated notion of a node's
position in the graph\editorial{ugg}.  {\Technique} solves this
problem\editorial{what problem?} by labelling each node in the graph
with information about where it might occur in $\beta$ to $i2c$ path.
The label on an instruction $l$ consists of two maps,
$\mathit{min\_from}$ and $\mathit{min\_to}$:
\vspace{-1pt}
\begin{enumerate}
\item
  $\mathit{min\_from}_l(c)$ is the number of instructions on the
  shortest path from $c$ to $l$, where $c \in \beta$.
\vspace{-12pt}
\item
  $\mathit{min\_to}_l(i)$ is the number of instructions on the
  shortest path from $l$ to $i$, where $i \in i2c^\sharp$.
  $i2c^\sharp$ consists of all members of $i2c$, plus all of the
  \gls{cfg} nodes created by duplicating one of those instructions.
\end{enumerate}
The length of the shortest path from an instruction $c
\in \beta$ to $i \in i2c^\sharp$ via the $l$ is then
$\mathit{min\_from}_l(c) + \mathit{min\_to}_l(i) + 1$, and so it is safe
to discard any instruction $l$ where
\begin{displaymath}
\min_{c \in \beta}\left(\mathit{min\_from}_l(c)\right) + \min_{i \in i2c^\sharp}\left(\mathit{min\_to}_l(i)\right) {\geq} \alpha
\end{displaymath}

The asymmetry, taking the distance from only ``true'' members of
$\beta$ but to any duplicate of a member of $i2c$, is perhaps
surprising.  The key observation is that every path which starts at a
duplicated member of $\beta$ will have a matching path which starts at
the original member, and so the ones which start at the duplicate
instruction are redundant\footnote{The symmetrical statement is also
  true: every path which ends in a member of $i2c^\sharp$ has a
  matching path which ends at a member of $i2c$.  It would therefore
  also be correct to discard paths which end at a duplicate member of
  $i2c$.  It would not, however, be correct to combine the two
  observations and discard all paths which either start with a
  duplicate of $\beta$ or end with a duplicate of $i2c$, as there
  would then be little point in having those duplicates.}.

\begin{sanefig}
\begin{algorithmic}[1]
  \State {Compute initial labelling of graph}
  \For {$t \in \beta$}
    \While {graph rooted at $t$ is contains a cycle}
       \State $\mathit{edge} \gets \textsc{findEdgeToBeBroken}(t, \{\})$
       \State $\mathit{newLabel} \gets \textsc{combineLabels}(\text{current labels of } \mathit{edge}.\mathit{start} \text{ and } \mathit{edge}.\mathit{end})$
       \If {$\min_c(\mathit{min\_from}_{\mathit{newLabel}}(c)) + \min_i(\mathit{min\_to}_{\mathit{newLabel}}(i)) > \alpha$}
           \State {remove $\mathit{edge}$}
       \Else
           \State $\mathit{newNode} \gets \text{duplicate } \mathit{edge}.\mathit{end}$
           \For {Edges $e$ leaving $\mathit{edge}.\mathit{end}$}
              \State {Create a new edge from $\mathit{newNode}$ to $e.\mathit{end}$}
           \EndFor
           \State {Set label of $\mathit{newNode}$ to $\mathit{newLabel}$}
           \State {Replace $\mathit{edge}$ with an edge from $\mathit{edge}.\mathit{start}$ to $\mathit{newNode}$}
           \State {Recalculate $\mathit{min\_from}$ for $\mathit{edge}.\mathit{end}$ and its successors, if necessary}
       \EndIf
    \EndWhile
  \EndFor
\end{algorithmic}
\vspace{-6pt}
\caption{Loop unrolling algorithm for interfering thread CFGs.
  \textsc{findEdgeToBeBroken} and \textsc{combineLabels} are described
  in the text.}
\label{fig:derive:store_cfg_unroll_alg}
\end{sanefig}

The complete algorithm is shown in
Figure~\ref{fig:derive:store_cfg_unroll_alg}.  Note that in this
algorithm, duplicating a node duplicates its \emph{outgoing} edges,
whereas when building a crashing thread \gls{cfg} the \emph{incoming}
edges are duplicated.  This reflects the fact that interfering thread
\glspl{cfg} are built up forwards from members of $\beta$ while
crashing thread \glspl{cfg} are built up backwards from the target
instruction.

The algorithm relies on two utility functions:
\begin{itemize}
\item \textsc{findEdgeToBeBroken} finds the closing edge of some cycle
  in the graph by performing a breadth-first search starting at $t$
  and reporting the first cycle-closing edge it finds.
\item \textsc{combineLabels} computes the label for the new node which
  would be produced by duplicating $\mathit{edge}.\mathit{end}$.  This
  node will have the same outgoing edges as
  $\mathit{edge}.\mathit{end}$, and so the same $min\_to$ label, and a
  single incoming edge from $\mathit{edge}.\mathit{start}$, and hence
  a $\mathit{min\_from}$ label which is just
  $\mathit{edge}.\mathit{start}$'s $\mathit{min\_from}$ with one added
  to every value.
\end{itemize}
The resulting \gls{cfg} can then be compiled to a {\StateMachine} in
the same way as a \gls{crashingthread}'s \gls{cfg}.

\newcommand{\shortrightarrow}{\begin{tikzpicture}[baseline= -1ex*.75]
    \draw[->] (0,0) -- ++(.25,0);
  \end{tikzpicture}
  \hspace{-1pt}
}

As an example, consider this cyclic \gls{cfg}:\\
\noindent \centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$} edge [in=30,out=-30,loop] ();
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (dummy) [left = .3 of C] {};
  \node (dummy2) [right = .5 of B] {};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C.west) .. controls +(-.3,0) and +(0,-.3) .. ++(-.5,.5) -- ++(0,2.2) .. controls +(0,.3) and +(-.3,0) .. ++(.5,.5) -- (A.west);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (C) (dummy) (dummy2)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
             & \multicolumn{1}{c}{$\mathit{min\_to}$} & \multicolumn{2}{c}{$\mathit{min\_from}$} & overall min\\
             & $C$ & $A$ & $C$ \\
      $A$    & 2   & 0   & 1 & 2\\
      $B$    & 1   & 1   & 2 & 2\\
      $C$    & 0   & 2   & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent $C$, in green, is a member of $i2c$ (and therefore also
$\beta$); $A$, in blue, is a member of $\beta$ only.  The overall min
column is the minimum $\mathit{min\_to}$ value plus the minimum
$\mathit{min\_from}$ one; it gives the number of edges on the shortest
path involving a given node which starts at a member of $\beta$ and
ends at a member of $i2c^\sharp$.  Suppose that we wish to render this
graph acyclic whilst preserving all paths of length 5 or less, and
that the algorithm selects $A$ as the first root on line 3.
\textsc{findEdgeToBeBroken} will find $A \shortrightarrow B
\shortrightarrow B$, of length three, as the first cyclic path, and so
the algorithm will break the $B \shortrightarrow B$ edge by
duplicating $B$, producing this new graph:\\
\noindent \centerline{ \inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$} edge [in=210,out=150,loop,killEdge] ();
  \node (B1) [NewCfgInstr, right=of B] {$B_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (dummy) [left = .6 of B] {};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,swungEdge] (B1) to [bend left=10] (B);
  \draw[->] (B1) -- (C);
  \draw[->] (C.west) -- ++(-.4,0) .. controls +(-.3,0) and +(0,-.3) .. ++(-.5,.5) -- ++(0,2.23) .. controls +(0,.3) and +(-.3,0) .. ++(.5,.5) -- (A.west);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (dummy)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
            & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$ \\
      $A$   & 2   & 0   & 1 & 2\\
      $B$   & 1   & 1   & 2 & 2\\
      $B_1$ & 1   & 2   & 3 & 3\\
      $C$   & 0   & 2   & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent
New nodes are shown in red, as is the edge which is modified, and
edges which have been removed are shown crossed through.  There are
now no more length three paths in the graph, and so
\textsc{findEdgeToBeBroken} will report the two length four paths $A
\shortrightarrow B \shortrightarrow B_1 \hspace{-.5pt} \shortrightarrow B$ and $A
\shortrightarrow B \shortrightarrow \hspace{1pt} C \shortrightarrow A$.  This will
cause the $B_1 \hspace{-.5pt} \shortrightarrow B$ and $C \shortrightarrow A$ edges to
be broken, as shown:\\
\noindent
\centerline{ \inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (A1) [NewCfgInstr,right=of C] {$A_1$};
  \node (dummy) [left = .66 of B] {};
  \draw[->] (A) -- (B);
  \draw[->,swungEdge] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->] (B1) -- (C);
  \draw[->] (B1) to [bend left=10] (B);
  \draw[->] (C) -- (A1);
  \draw[->,killEdge] (C.west) -- ++(-.4,0) .. controls +(-.3,0) and +(0,-.3) .. ++(-.5,.5) -- ++(0,2.23) .. controls +(0,.3) and +(-.3,0) .. ++(.5,.5) -- (A.west);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (dummy)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
      labels & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2   & 0   & $\infty$ & 2\\
      $A_1$ & 2   & 3   & 1        & 3\\
      $B$   & 1   & 1   & 2        & 2\\
      $B_1$ & 1   & 2   & 3        & 3\\
      $C$   & 0   & 2   & 0        & 0\\
    \end{tabular}
    ~
  };
\end{tikzpicture}
}
}\\
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [NewCfgInstr, right=of B1] {$B_2$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,killEdge] (B1) to [bend left=10] (B);
  \draw[->,swungEdge] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
            & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2   & 0   & $\infty$ & 2\\
      $A_1$ & 2   & 3 & 1 & 3\\
      $B$   & 1   & 1 & 2 & 2\\
      $B_1$ & 1   & 2 & 3 & 3\\
      $B_2$ & 1   & 3 & 4 & 4\\
      $C$   & 0   & 2 & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent
\textsc{findEdgeToBeBroken} will now discover two length five cyclic
paths, $A \shortrightarrow B \shortrightarrow \hspace{1pt} C \shortrightarrow A_1
\shortrightarrow B$ and $A \shortrightarrow B \shortrightarrow B_1
\hspace{-.5pt} \shortrightarrow B_2 \shortrightarrow B_1$.  The first is eliminated
by duplicating $B$: \\
\noindent
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [NewCfgInstr, below=of A1] {$B_3$};
  \draw[->] (A) -- (B);
  \draw[->,killEdge] (A1) -- (B);
  \draw[->,swungEdge] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
      labels & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2 & 0 & $\infty$ & 2\\
      $A_1$ & 2 & 3 & 1        & 3\\
      $B$   & 1 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 2 & 3        & 3\\
      $B_2$ & 1 & 3 & 4        & 4\\
      $B_3$ & 1 & 4 & 2        & 3\\
      $C$   & 0 & 2 & 0        & 0\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent
The second, by contrast, can be eliminated without needing to
duplicate any further instructions.  Were $B_1$ to be duplicated, the
shortest path from $A$ to $C$ which used the new instruction would be
of length 6, exceeding the desired maximum path length, and so the
cycle-closing edge can simply be deleted:
\\
\noindent
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [CfgInstr, below=of A1] {$B_3$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->,killEdge] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
         & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
         & $C$ & $A$ & $C$\\
      $A$   & 2 & 0 & $\infty$ & 2\\
      $A_1$ & 2 & 3 & 1        & 3\\
      $B$   & 1 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 2 & 3        & 3\\
      $B_2$ & 1 & 3 & 4        & 4\\
      $B_3$ & 1 & 4 & 2 & 3\\
      $C$  & 0 & 2 & 0        & 0\\
      New label & 1 & 4 & 5 & 5\\
    \end{tabular}
  };
\end{tikzpicture}
}
}\\
\noindent
This process iterates, removing one cycle-completing edge at a time,
until the graph becomes completely acyclic:
\\
\noindent
\centerline{
\inlinebox{
\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [CfgInstr, below=of A1] {$B_3$};
  \node (C1) [DupeInterferingCfgInstr, below=of B3] {$C_1$};
  \node (B4) [CfgInstr, right=of B3] {$B_4$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) -- (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (B4);
  \draw[->] (C) -- (A1);
  \draw[->] (B3) -- (C1);
  \draw[->] (B4) -- (C1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (C1) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
            & \multicolumn{2}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $C_1$ & $A$ & $C$ \\
      $A$   & 2 & 5 & 0 & $\infty$ & 2\\
      $A_1$ & $\infty$ & 2 & 3 & 1 & 3\\
      $B$   & 1 & 4 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 4 & 2 & $\infty$ & 3\\
      $B_2$ & 1 & 4 & 3 & $\infty$ & 4\\
      $B_3$ & $\infty$ & 1 & 4 & 2 & 3\\
      $B_4$ & $\infty$ & 1 & 5 & 3 & 4\\
      $C$   & 0 & 3 & 2 & 0 & 0\\
      $C_1$ & $\infty$ & 0 & 5 & 3 & 3\\
    \end{tabular}
  };
\end{tikzpicture}
}
}
\\
\noindent
As desired, the graph has been rendered acyclic while preserving all
paths of length up to five instructions.

\section{Generating {\StateMachines} for unknown bugs}
\label{sect:derive:unknown_bugs}

The discussion so far focused on the case where the crashing
instruction was somehow identified before {\technique} starts.
{\Technique} can also be used to discover unknown bugs, by simply
enumerating all of the instructions in a program which might crash due
to a bug of the class being investigated and considering each in turn.
This is obviously only feasible if the majority of instructions can be
dismissed quickly.  Fortunately, they can be: {\implementation} takes,
on average, a few hundred of milliseconds per instruction on fairly
modest hardware, allowing even large programs with millions of
instructions to be analysed in a few days.  Further, this approach is
embarrassingly parallel, and so would be expected to scale well as
hardware concurrency increases; precisely the scenario in which it
would be most useful.

Identifying instructions which might crash depends on the type of bug
which is to be investigated.  {\Implementation} considers three types
of bug:

\begin{itemize}
\item Assertion-failure type crashes.  These are caused by the program
  calling a function such as \verb|__assert_fail| or \verb|abort|
  provided by an operating system library.  Finding such functions is
  generally straightforward given the usual dynamic linker
  information, and the whole-program CFG, discussed earlier, can then
  provide all of their callers.  Each such caller is treated as a
  potential bug.
\item Double free errors.  These are caused by the program calling
  \verb|free| in an incorrect manner.  Again, the dynamic linker
  information allows {\implementation} to quickly find all calls to
  \verb|free| in the program, and these calls are used as the
  potentially-crashing instructions in the program.
\item Bad pointer dereferences.  Any memory-accessing instruction
  could potentially dereference a bad pointer, and so
  {\implementation} simply enumerates all memory accessing
  instructions discovered by the initial static analysis.
\end{itemize}

\subsection{Timeouts}

Many of the algorithms used by {\technique} require far more time in
their worst case than in their expected case, in many cases by many
orders of magnitude.  This is irritating but tolerable when the
analysis is being used to investigate a specific bug, but far more of
a problem when the analysis is applied speculatively to a very large
number of potential bugs.  Suppose, for instance, that the analysis
completes in 500ms in 99.9\% of cases but takes three years in the
remaining 0.1\% of cases.  An analysis which fails 0.1\% of the time
would still be quite useful, and so this is reasonable for analysing
specific bugs.  On the other hand, if the analysis is run 10,000 times
then the probability of one of at least one of the steps taking three
years is very close to one, and the analysis is effectively useless.

{\Technique} works around this problem by applying timeouts to the
various analysis steps, ensuring that it can produce at least some
useful results in a reasonable time even when it occasionally
encounters one of its bad cases.  {\Implementation} uses two
independent timeouts: one for the per-\gls{crashingthread} work, such
as deriving the crashing {\StateMachine} or the interfering
\glspl{cfg}, and one for the per-\gls{interferingthread} work, such as
deriving \gls{ic-atomic} or the final satisfiability check.  These
timeouts are both set by default to 60 seconds.  I discuss their
effects in more detail in the evaluation.

\section{Generating a verification condition}
\label{sect:using:check_realness}

Previous sections have described how to generate pairs of
{\StateMachines} representing fragments of the program which might
interact in interesting ways when run concurrently.  The next step is
to determine, for each pair, whether running the two {\StateMachines}
in parallel might lead to a crash, and if so under what circumstances.

The core of the approach is to use symbolic execution\cite{King1976}
to convert the {\StateMachines} into two predicates over the
{\StateMachine} state: the \gls{verificationcondition}, which is true
when interleaving the two {\StateMachines} might lead to a crash, and
the \gls{inferredassumption}, which is true when executing them
atomically in series will not.  {\Technique} generates a run-time
\gls{bugenforcer} whenever it cannot show that the conjunction of
these two predicates is unsatisfiable.  Note that it will only use a
run-time enforcer if there is some initial state such that
interleaving the {\StateMachines} might lead to a crash but running
them atomically would not; as discussed in
\autoref{sect:types_of_bugs}, {\technique} is only concerned with
atomicity violation bugs, and this provides a very useful reduction in
the number of false positives which must be checked by run-time
\glspl{bugenforcer}.

\subsection{Symbolically executing {\StateMachines}}
\label{sect:derive:symbolic_execute}

The symbolic execution engine used by {\implementation} is, for the
most part, quite conventional.  I give only a brief overview of the
most important features here:
\begin{itemize}
\item The symbolic execution engine considers only a single
  {\StateMachine} at a time, even when investigating the parallel
  behaviour of two threads.  Rather than investigating thread
  interleavings in the symbolic execution engine, as is done in, for
  instance, ESD\cite{Zamfir2010}, {\technique} instead encodes them
  into special cross-product {\StateMachines}, described in
  \autoref{sect:using:build_cross_product}.

\item The program's memory is represented by the sequence of update
  operations, rather than attempting to maintain separate models for
  particular objects or memory locations.  In effect, the whole of
  memory is modelled as a single array using McCarthy's theory of
  arrays\needCite{}.  \state{Store} operations are implemented by
  simply adding them to the update list, whereas \state{Load}
  operations have to scan back through the list to find a matching
  \state{Store}.

  This is not a particularly efficient approach.  {\Implementation}
  relies on two facts to mitigate the problems.  First, where the
  relationship between \state{Store}s and \state{Load}s is simple,  the
  {\StateMachine} simplifiers will forward data between them before
  the symbolic execution starts, eliminating both from the
  {\StateMachine}.  Second, {\implementation} maintains a cache of
  previous aliasing queries, and so if, for instance, two paths
  through the {\StateMachine} both need to determine whether
  \state{Store} A might alias with \state{Load} B the symbolic
  execution engine usually only needs to do so once.

\item Aliasing queries are resolved lazily.  This means that if the
  engine must execute a \state{Load} operation and cannot immediately
  determine which \state{Store} operation to use, it does not cause an
  immediate fork of its state, but instead causes the \state{Load} to
  return an expression BDD (see \autoref{sect:sm_expr_language}) whose
  internal nodes describe the alias query and whose leaves select the
  appropriate result.

\item The engine does not use any kind of incremental abstraction
  technique such as CEGAR\cite{Clarke2000}.  This is primarily because
  of the use of a flat memory representation: memory is a single
  object, and so it makes little sense to talk about modelling one
  part of it accurately and another part inaccurately, without which
  CEGAR would provide little benefit.  The use of lazy aliasing
  resolution provides some of the benefit of CEGAR, as it allows some
  aliasing queries which do not affect program behaviour to be
  skipped.

\item Unlike most symbolic execution engines, the one used by
  {\implementation} does not attempt to detect when it revisits a
  previously-visited configuration.  This is safe because
  {\StateMachines} are acyclic: any path through a {\StateMachine} can
  visit a given state at most once, and so there is no possibility of
  a single revisiting a configuration and entering an infinite loop.
  It is also, surprisingly, reasonably performant, because it is
  extremely rare for multiple paths to visit the same configuration,
  and so there is little scope for re-using configurations to reduce
  duplicated work.  This is largely because {\technique} simplifies
  the {\StateMachine} before attempting to symbolically execute it and
  these simplifications tend to remove most easily-exploited forms of
  redundancy.
\end{itemize}

\subsection{Deriving the inferred assumption}

\label{sect:derive:inferred_assumption}

The \gls{inferredassumption} is the conjunction of two sub-conditions:
\gls{ci-atomic}, the condition under which running the crashing
    {\StateMachine} and then the interfering one ends at the
    {\stSurvive} state, and \gls{ic-atomic}, the condition under which
    running them in the opposite order does.  These sub-conditions can
    be easily generated by concatenating the two {\StateMachines} in
    the appropriate order and then symbolically executing them.

One might reasonably ask why\editorial{guh} building the composite
{\StateMachine} is superior to simply symbolically executing one
{\StateMachine} until it reaches the {\stSurvive} state and then
starting the other {\StateMachine} in the resulting configuration.
This would correctly implement the desired behaviour, and would be
somewhat simpler to implement.  The advantage of building a composite
{\StateMachine} is that the composite {\StateMachine} can be further
simplified using the standard {\StateMachine} simplification passes,
which usually reduces the complexity of symbolic execution by a useful
amount, even when the input {\StateMachines} were themselves
simplified as far as possible.  This is because the composite
{\StateMachine} is ``closed'': it contains all potentially relevant
operations, and so the simplification passes can assume that there are
no potentially interfering operations in another thread, giving them
far more scope to eliminate memory accesses.  The result is that
symbolically executing the simplified composite {\StateMachine} is
almost always much faster than executing the input {\StateMachines} in
turn.

\subsection{Building cross-product {\StateMachines}}
\label{sect:using:build_cross_product}

{\Technique}'s symbolic execution engine is only capable of exploring
the behaviour of one {\StateMachine} at a time, but the
\gls{verificationcondition} is defined in terms of the parallel
composition of two {\StateMachines}.  {\Technique} therefore builds a
new {\StateMachine}, the \emph{cross-product {\StateMachine}}, which
acts as this parallel composition.

\begin{sanefig}
  \newlength{\extrapadA}
  \setlength{\extrapadA}{6mm}
  \newlength{\extrapadB}
  \setlength{\extrapadB}{3mm}
  \newlength{\extrapadC}
  \setlength{\extrapadC}{0mm}
  \newcommand{\midcolumn}{~\hspace{\extrapadA}\Rightarrow\hspace{\extrapadB}~}
  \newcommand{\lastcolumn}[1]{~\hspace{\extrapadC}\circled{#1}}
  \centerline{
    \begin{tabular}{m{3.7cm}m{11.5cm}}
      Terminals: & {\STateMachine} states \\
      Non-terminals: & Pairs $(A, B)$, where $A$ is a fragment of the crashing {\StateMachine} and $B$ a fragment of the interfering one. \\
      {\raggedright Initial non-terminal:} & $(A_0, B_0)$, where $A_0$ is the entire crashing {\StateMachine} and $B_0$ the entire interfering one, after renaming apart any common {\StateMachine} temporaries. \\
      \raisebox{12pt}{Productions:} &
      \vspace{-24pt}
      \begin{displaymath}
        \begin{array}{lccr}
          \tikz[baseline=(current bounding box.center)]{\node [style=graphNT, minimum height=3cm] (charrr) {\raisebox{-34pt}{$\left(\begin{tikzpicture}[baseline=(current bounding box.center), minimum height = 0]
                \node at (0,6mm) {};
                \node at (0,0) (r) [stateIf] {\stIf{m}};
                \node at (-5mm,-10mm) (A) {$A_0$};
                \node at (5mm,-10mm) (B) {$A_1$};
                \node at (0,-17mm) {};
                \draw[->,ifTrue] (r) -- (A);
                \draw[->,ifFalse] (r) -- (B);
              \end{tikzpicture}, B\right)$}};
          } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
            \node at (0,7mm) {};
            \node at (0,0) (r) [stateIf] {\stIf{m}};
            \node at (-10mm, -10mm) (A) [style=graphNT] { $(A_0, B)$ };
            \node at (10mm, -10mm) (B) [style=graphNT] { $(A_1, B)$ };
            \node at (0,-18mm) {};
            \draw[->,ifTrue] (r) -- (A);
            \draw[->,ifFalse] (r) -- (B);
          \end{tikzpicture} & \lastcolumn{$1_a$} \vspace{6pt}\\
          \tikz[baseline=(current bounding box.center)]{
            \node [style=graphNT, minimum height=3cm] {
              \raisebox{-34pt}{$
                \left(\hspace{0.2mm}A, \begin{tikzpicture}[baseline=(current bounding box.center), minimum height = 0]
                  \node at (0,6mm) {};
                  \node at (0,0) (r) [stateIf] {\stIf{m}};
                  \node at (-5mm,-10mm) (A) {$B_0$};
                  \node at (5mm,-10mm) (B) {$B_1$};
                  \node at (0,-17mm) {};
                  \draw[->,ifTrue] (r) -- (A);
                  \draw[->,ifFalse] (r) -- (B);
                \end{tikzpicture}\right)$}
            };
          } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
            \node at (0,7mm) {};
            \node at (0,0) (r) [stateIf] {\stIf{m}};
            \node at (-10mm, -10mm) (A) [style=graphNT] { $(A, B_0)$ };
            \node at (10mm, -10mm) (B) [style=graphNT] { $(A, B_1)$ };
            \node at (0,-18mm) {};
            \draw[->,ifTrue] (r) -- (A);
            \draw[->,ifFalse] (r) -- (B);
          \end{tikzpicture} & \lastcolumn{$1_b$} \\
          \tikz[baseline=(current bounding box.center)]{
            \node [style=graphNT, minimum height=3cm] {
              \raisebox{-34pt}{$
                \left(\hspace{3.13mm}\begin{tikzpicture}[baseline=(current bounding box.center), minimum height=0cm]
                  \node at (0,6mm) {};
                  \node at (0,0) (r) [stateSideEffect,minimum height=16pt] {$a$};
                  \node at (0,-10mm) (A) {$A$};
                  \node at (0,-17mm) {};
                  \draw[->] (r) -- (A);
                \end{tikzpicture}\hspace{3.2mm},\hspace{3.2mm}\begin{tikzpicture}[baseline=(current bounding box.center), minimum height=0cm]
                  \node at (0,6mm) {};
                  \node at (0,0) (r) [stateSideEffect,minimum width=1.23em] {$b$};
                  \node at (0,-10mm) (A) {$B$};
                  \node at (0,-17mm) {};
                  \draw[->] (r) -- (A);
                \end{tikzpicture}\hspace{3.13mm}\right)$
              }
              \hspace{-2.4mm}
            };
          } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
            \node at (0,7mm) {};
            \node at (0,0) (r) [stateIf] {\stIf{\happensBefore{a}{b}}};;
            \node at (-10mm,-12mm) [stateSideEffect,minimum height=16pt] (A) { $a$ };
            \node at (10mm,-12mm) [stateSideEffect,minimum width=1.23em] (B) { $b$ };
            \node at (-10mm,-22mm) (A2) [style=graphNT, minimum height=1cm, minimum width=0] {
              \hspace{-4mm}
              \raisebox{-17pt}{
                $\left(A, \begin{tikzpicture}[minimum height=0]
                  \node at (0,0) (rr) [stateSideEffect,minimum width=1.23em] {$b$};
                  \node at (0,-10mm) (rb) {$B$};
                  \draw[->] (rr) -- (rb);
                \end{tikzpicture}\right)$
              }
              \hspace{-4mm}
            };
            \node at (10mm,-22mm) (B2) [style=graphNT, minimum height=1cm, minimum width = 0] {
              \hspace{-4mm}
              \raisebox{-17pt}{
                $\left(\begin{tikzpicture}[minimum height=0]
                  \node at (0,0) (rr) [stateSideEffect,minimum width=1.23em] {$a$};
                  \node at (0,-10mm) (rb) {$A$};
                  \draw[->] (rr) -- (rb);
                \end{tikzpicture}, B\right)$
              }
              \hspace{-4mm}
            };
            \draw[->,ifTrue] (r) -- (A);
            \draw[->] (A) -- (A2);
            \draw[->,ifFalse] (r) -- (B);
            \draw[->] (B) -- (B2);
          \end{tikzpicture} & \lastcolumn{2} \\
          \tikz[baseline=(current bounding box.center)]{
            \node [style=graphNT, minimum height=3cm] {
              \raisebox{-34pt}{$
                \left(\hspace{2.5mm}\raisebox{0em}{\tikz[baseline = (charrr.base)]{\node [stateTerminal, minimum height = 0] (charrr) {\state{t}};}}\begin{tikzpicture}[minimum height = 1.3cm]
                  \node at (0,0mm) {};
                  \node at (0,-12mm) {};
                \end{tikzpicture}\hspace{1.5mm},\hspace{4mm}B\hspace{4.4mm}\right)$
              }
            };
          } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
            \node at (0,7mm) {};
            \node at (0,-4mm) (r) [stateTerminal] {\state{t}};
            \node at (0,-18mm) {};
          \end{tikzpicture} & \lastcolumn{3} \vspace{6pt}\\
          \tikz[baseline=(current bounding box.center)]{
            \node [style=graphNT, minimum height=3cm] {
              \raisebox{-34pt}{$
                \left(\hspace{3.13mm}\begin{tikzpicture}[baseline=(current bounding box.center), minimum height = 0]
                  \node at (0,6mm) {};
                  \node at (0,0) (r) [stateSideEffect,minimum height=16pt] {$a$};
                  \node at (0,-10mm) (A) {$A$};
                  \node at (0,-17mm) {};
                  \draw[->] (r) -- (A);
                \end{tikzpicture}\hspace{3.7mm}, \hspace{3.3mm}\tikz[minimum height = 0, baseline = (charrr.base)] {\node [stateTerminal] (charrr) {\state{t}};} \hspace{2.6mm}\right)$
              }
            };
          } & \midcolumn & \begin{tikzpicture}[baseline=(current bounding box.center)]
            \node at (0,7mm) {};
            \node at (0,2mm) (r) [stateSideEffect,minimum height=16pt]{$a$};
            \node at (0,-11mm) (A) [style=graphNT] {\raisebox{-6pt}{
                $\!\!\!\left(A,\raisebox{.35em}{\tikz{\node [stateTerminal] {\state{t}};}}\right)\!\!\!$
              }
            };
            \node at (0,-18mm) {};
            \draw[->] (r) -- (A);
          \end{tikzpicture} & \lastcolumn{4} \\
        \end{array}
      \end{displaymath}
      \vspace{-24pt}
    \end{tabular}
  }
  \caption{A basic {\StateMachine} cross-product algorithm, expressed
    as a node replacement graph generating grammar.  $m$ matches
    boolean BDDs; $A_0$, $A_1$, and $A$ match fragments of the
    crashing {\StateMachine}; $B_0$, $B_1$, and $B$ match fragments of
    the interfering {\StateMachine}; $a$ and $b$ match individual
    states within the crashing and interfering {\StateMachines},
    respectively; \state{t} matches any terminal state.}
  \label{fig:derive:basic_cross_product}
\end{sanefig}

This can be though of as a kind of staged computation, with the first
stage, which runs in the same computational model as {\technique},
building a cross-product {\StateMachine} and the second, embedded,
stage running ``inside'' that {\StateMachine} to emulate the original
{\StateMachines}.  Work performed in the first stage will generally be
far cheaper than work performed in the second, but will not have
access to the current value of program memory or {\StateMachine}
temporaries.  {\Technique}'s cross-product algorithm attempts to put
as much of the work as possible in the first stage, using the second
stage only when necessary.

A simplified version of the algorithm used by {\technique} is shown in
\autoref{fig:derive:basic_cross_product}, as a node replacement graph
grammar (see \autoref{sect:intro:graph_grammar}).  The grammar itself
forms the first stage of the emulation, and the {\StateMachine} it
generates forms the second.  The productions of the grammar are
themselves quite simple.  The first two show how to handle an
\state{If} state in one of the input {\StateMachines}.  The
discriminant $m$ cannot be evaluated in the host model, as it may
depend on {\StateMachine} model state, and so it is deferred to the
second stage computation in the embedded model by adding an \stIf{m}
state to the cross-product {\StateMachine}.  Similarly, production
\circled{2} shows how to handle side effect states: the host model
cannot tell which side effect to do first, and so it defers it to the
embedded model using a $\happensBeforeEdge$ happens-before test,
performing the appropriate side effect based on the result and then
advancing one of the {\StateMachines}.  Finally, productions
\circled{3} and \circled{4} show how to handle terminal states.  If
the crashing {\StateMachine} reaches a terminal state, the emulation
stops and the result is that terminal state; if the interfering
{\StateMachine} reaches one, emulation continues with just the
crashing {\StateMachine} until it also reaches a terminal state.

\begin{sanefig}
  \begin{displaymath}
    \textsc{Configuration} = \left(\begin{array}{rrll}
      \multirow{2}{*}{\bigg\{} & \mathit{crashingState}: & \textsc{{\STateMachine} state}, & \multirow{2}{*}{\bigg\},}\\
                               & \mathit{crashingHasIssued}: & \textsc{Bool}\\
      \multirow{2}{*}{\bigg\{} & \mathit{interferingState}: & \textsc{{\STateMachine} state}, & \multirow{2}{*}{\bigg\},} \\
                               & \mathit{interferingHasIssued}: & \textsc{Bool}\\
      \multicolumn{2}{r}{\mathit{atomic}:} & \multicolumn{2}{l}{\{ \varnothing, \mathit{crashing}, \mathit{interfering} \}}
    \end{array}\right)
  \end{displaymath}
  \caption{\textsc{Configuration} type for the cross-product algorithm.}
  \label{fig:cross_product:configuration}
\end{sanefig}
The actual algorithm used by {\technique} includes several refinements
over this basic one:
\begin{itemize}
\item The input {\StateMachines} may contain atomic blocks delimited
  by {\stStartAtomic} and {\stEndAtomic} states, and the simple
  grammar does not respect them.  This can be fixed by extending the
  grammar's non-terminal structure with a final field which indicates
  which, if any, of the {\StateMachines} are currently within atomic
  blocks.  This $\mathit{atomic}$ field is maintained as the
  {\StateMachines} perform {\stStartAtomic} and {\stEndAtomic} states
  and is used to restrict the interleavings generated by the grammar.
\item The basic grammar will consider every possible interleaving of
  the two {\StateMachines}, including the trivial ones in which one or
  other completes atomically.  This is correct but inefficient, as the
  atomic orderings have already been considered in the
  \gls{inferredassumption}.  Fixing this is, again, simple, by
  extending the grammar's non-terminal structure with two additional
  fields, $\mathit{crashingHasIssued}$ and
  $\mathit{interferingHasIssued}$, which track whether the crashing
  and interfering machines have issued\editorial{undef} any side
  effects\editorial{undef}.  If either {\StateMachine} terminates
  without having done so then the cross-product {\StateMachine} ends
  in the {\stUnreached} state, so that the relevant paths will not be
  included in the \gls{verificationcondition}.

\begin{sanefig}
  \begin{displaymath}
    \mathit{initial} = \left(\begin{array}{rrll}
      \multirow{2}{*}{\bigg\{} & \mathit{crashingState} = & \mathit{crashing}_0, & \multirow{2}{*}{\bigg\},} \\
                               & \mathit{crashingHasIssued} = & \false\\
      \multirow{2}{*}{\bigg\{} & \mathit{interferingState} = & \mathit{interfering}_0, & \multirow{2}{*}{\bigg\},} \\
                               & \mathit{interferingHasIssued} = & \false\\
      \multicolumn{2}{r}{\mathit{atomic} = } & \varnothing \\
    \end{array}\right)
  \end{displaymath}
  \caption{Initial \textsc{Configuration} for the cross-product
    algorithm.  $\mathit{crashing}_0$ is the first state of the
    crashing {\StateMachine} and $\mathit{interfering}_0$ that of the
    interfering one.}
  \label{fig:cross_product:initial}
\end{sanefig}

  Note that this refinement excludes executions in which the
  {\StateMachines} run in series, rather than those in which they run
  linearizably\editorial{Cite, maybe?}.  The latter would perhaps be
  more useful, as it could potentially eliminate more paths, but is
  far more difficult to calculate\editorial{Seems like I need a bit
    more there.}.
\item The basic grammar fails to take account of partial order
  redundancies caused by {\StateMachine}-local side effects such as
  \state{Copy} or \state{Assert}.  The complete grammar ameliorates
  this weakness by only generating happens-before tests for non-local
  side-effects, defined to be those which could possibly influence or
  be influenced by the other {\StateMachine}.  This definition is
  context-dependent: a \state{Load} in the crashing {\StateMachine},
  for instance, will be non-local if it could alias with any of the
  crashing {\StateMachine}'s possible future \state{Store}s, and so
  might switch from being non-local to being local when the crashing
  {\StateMachine} performs a \state{Store}.  Checking whether a side
  effect in one {\StateMachine} is local requires {\technique} to look
  ahead through the other {\StateMachine}, comparing the current side
  effect to every remote one.  A {\stStartAtomic} side effect is
  considered non-local if any of the side effects in the atomic block
  which it starts are non-local\editorial{ugg}.
\item This grammar can sometimes duplicate side effect states, which
  might break static single assignment form.  {\Technique} uses a
  separate post-pass to restore the SSA invariant.
\end{itemize}
The extended non-terminal type, \textsc{Configuration}, is shown in
\autoref{fig:cross_product:configuration}, its initial value in
\autoref{fig:cross_product:initial}, and the extended productions in
\autoref{fig:cross_product:algorithm}.  Many of the productions have
symmetrical versions which simply swap the roles of the crashing and
interfering {\StateMachines}; for the sake of brevity, I show only the
crashing {\StateMachine} production and mark it with a *.

\begin{sidewaysfigure}
\begin{figgure}
  \begin{displaymath}
    \begin{array}{ c c c<{\hspace{-1mm}} >{\raggedright}p{4.5cm} >{\hspace{-1mm}}c<{\hspace{-1mm}} >{\hspace{-1mm}}c<{\hspace{-1mm}}}
      \tikz [baseline = (current bounding box.center)] {
        \node[style = graphNT] {
          \hspace{-10pt}
          \raisebox{-20pt}{
            $\left(\left\{\begin{tikzpicture}[baseline = (current bounding box.center),minimum height = 0,node distance=0.5cm,font=\small]
            \node at (0,0) (r) [stateIf] {\stIf{m}};
            \node at (-5mm, -10mm) (A) {$A_0$};
            \node at (5mm, -10mm) (B) {$A_1$};
            \draw[->,ifTrue] (r) -- (A);
            \draw[->,ifFalse] (r) -- (B);
            \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\},z\right)$
          }
          \hspace{-10pt}
        };
      } & \!\!\!\Rightarrow\!\!\! & \hspace{-5mm}\raisebox{-8mm}{\begin{tikzpicture}[node distance=0.5cm,font=\small]
          \node at (0,0) (r) [stateIf] {\stIf{m} };
          \node at (-20mm, -10mm) (A) [graphNT] { $(\{A_0, i_a\}, \{B, i_b\}, z)$ };
          \node at (20mm, -10mm) (B) [graphNT] { $(\{A_1, i_a\}, \{B, i_b\}, z)$ };
          \draw[->,ifTrue] (r) -- (A);
          \draw[->,ifFalse] (r) -- (B);
        \end{tikzpicture}} & & \circled{$1'$} & *\\

      \tikz [baseline = (current bounding box.center)] {
        \node[style = graphNT] {
          \hspace{-7pt}
          \raisebox{-20pt}{$
            \left(\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center) ]
                \node at (0,0) (r) [stateSideEffect] {$a$};
                \node at (0,-10mm) (A) {$A$};
                \draw[->] (r) -- (A);
            \end{tikzpicture}, i_a\right\},\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center) ]
                \node at (0,0) (r) [stateSideEffect] {$b$};
                \node at (0,-10mm) (A) {$B$};
                \draw[->] (r) -- (A);
            \end{tikzpicture}, i_b\right\}, \varnothing\!\right)
            $}
          \hspace{-7pt}
        };
      }
          & \!\!\!\Rightarrow\!\!\! & \hspace{-5mm}\raisebox{-22mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateIf] {\stIf{\happensBefore{a}{b}}};
          \node at (-25mm, -10mm) [stateSideEffect] (A) {$a$};
          \node at (25mm, -10mm) [stateSideEffect] (C) {$b$};
          \node at (-25mm, -20mm) (B) [graphNT] {
            \hspace{-10pt}
            \raisebox{-18pt}{
              $\left(\{A, \true\}, \left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
              \node at (0,0) (Br) [stateSideEffect] {$b$};
              \node at (0,-10mm) (BA) {$B$};
              \draw[->] (Br) -- (BA);
              \end{tikzpicture}, i_b\right\}, \varnothing\!\right)$
            }
            \hspace{-10pt}
          };
          \node at (25mm, -20mm) (D) [graphNT] {
            \hspace{-10pt}
            \raisebox{-18pt}{
              $\left(\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
              \node at (0,0) (Dr) [stateSideEffect] {$a$};
              \node at (0,-10mm) (DA) {$A$};
              \draw[->] (Dr) -- (DA);
              \end{tikzpicture}, i_a\right\}, \{B, \true\}, \varnothing\!\right)$
            }
            \hspace{-10pt}
          };
          \draw[->,ifTrue] (r) -- (A);
          \draw[->,ifFalse] (r) -- (C);
          \draw[->] (A) -- (B);
          \draw[->] (C) -- (D);
        \end{tikzpicture}}\hspace{-5mm} & When $a$ and $b$ are non-local. & \circled{$2'$} & \\

      \tikz [baseline = (current bounding box.center)] {
        \node[style = graphNT, inner sep = 0] {
          \raisebox{-6pt}{$
            \left(\left\{\begin{tikzpicture}[font=\small, baseline = (char.base)]
                \node [stateTerminal] (char) {\state{t}};
            \end{tikzpicture}, \true\right\}, \{A, \true\}, z\right)
            $}
        };
      }
            & \!\!\!\Rightarrow\!\!\! & \raisebox{-3mm}{\begin{tikzpicture}[font=\small]
          \node [stateTerminal] {\state{t}};
      \end{tikzpicture}} &  & \circled{$3'$} \\

      \tikz [baseline = (current bounding box.center)] {
        \node[style = graphNT, inner sep = 0] {
          \raisebox{-18pt}{$
            \left(\left\{\begin{tikzpicture}[font=\small][baseline = (current bounding box.center)]
                \node at (0,0) (r) [stateSideEffect] {$a$};
                \node at (0,-10mm) (A) {$A$};
                \draw[->] (r) -- (A);
            \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\},z\right)
            $}
        };
      }
      & \!\!\!\Rightarrow\!\!\! & \raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {$a$};
          \node at (0,-10mm) [graphNT] (A) {$(\{A, i_a\}, \{B, i_b\}, z)$};
          \draw[->] (r) -- (A);
        \end{tikzpicture}} & If $a$ is a local side-effect. & \circled{$4'$} & *\\

      \tikz [baseline = (current bounding box.center)] {
        \node[style=graphNT] {
          \hspace{-8pt}
          \raisebox{-6pt}{
            $\left(\left\{\begin{tikzpicture}[font=\small, baseline = (char.base), minimum height=0]
            \node [stateTerminal] (char) {\state{t}};
            \end{tikzpicture}, i_a\right\}, \{A, i_b\}, z\right)$
          }
          \hspace{-8pt}
        };
      } & \!\!\!\Rightarrow\!\!\! & \raisebox{-2mm}{\begin{tikzpicture}[font=\small]
          \node [stateTerminal] {{\stUnreached}};
      \end{tikzpicture}} & If $i_a \wedge i_b = \false$. & \circled{$5'$} & *\\
      
      \tikz [baseline = (current bounding box.center)] {
        \node[style=graphNT] {
          \hspace{-10pt}
          \raisebox{-17pt}{
            $
            \left(\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
                \node at (0,0) (r) [stateSideEffect] {{\stStartAtomic}};
                \node at (0,-10mm) (A) {$A$};
                \draw[->] (r) -- (A);
            \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\}, \varnothing\!\right) 
            $
          }
          \hspace{-10pt}
        };
      }
            & \!\!\!\Rightarrow\!\!\! & \graphNT{$(\{A, i_a\}, \{B, i_b\}, \mathit{crashing})$} & If the atomic block is local. & \circled{$6'$} & *\\

      \tikz [baseline = (current bounding box.center)] {
        \node[style=graphNT] {
          \hspace{-10pt}
          \raisebox{-17pt}{
            $
            \left(\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
                \node at (0,0) (r) [stateSideEffect] {{\stEndAtomic}};
                \node at (0,-10mm) (A) {$A$};
                \draw[->] (r) -- (A);
            \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\}, \mathit{crashing} \right)
            $
          }
          \hspace{-10pt}
        };
      }
            & \!\!\!\Rightarrow\!\!\! & \graphNT{$(\{A, i_a\}, \{B, i_b\}, \varnothing )$} & & \circled{$7'$} & *\\

      \tikz [baseline = (current bounding box.center)] {
        \node[style=graphNT] {
          \hspace{-10pt}
          \raisebox{-17pt}{
            $
            \left(\left\{\begin{tikzpicture}[font=\small, baseline = (current bounding box.center)]
                \node at (0,0) (r) [stateSideEffect] {$a$};
                \node at (0,-10mm) (A) {$A$};
                \draw[->] (r) -- (A);
            \end{tikzpicture}, i_a\right\},\left\{B, i_b\right\},\mathit{crashing}\right)
            $
          }
          \hspace{-10pt}
        };
      }
            & \!\!\!\Rightarrow\!\!\! & \raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {$a$};
          \node at (0,-10mm) [graphNT] (A) {$(\{A, \true\}, \{B, i_b\}, \mathit{crashing})$};
          \draw[->] (r) -- (A);
        \end{tikzpicture}} & If $a$ is a non-local side-effect. & \circled{$8'$} & *
    \end{array}
  \end{displaymath}
  \caption{The cross-product algorithm as a node replacement graph
    grammar.  $A$, $A_0$, and $A_1$ match fragments of the crashing
    {\StateMachine} and $a$ matches a single state from the crashing
    {\StateMachine}.  $B$ and $b$ match fragments of and a single
    state in, respectively, the interfering {\StateMachine}.  $i_a$
    and $i_b$ match either {\true} or {\false}.  $z$ matches any of
    $\varnothing$, $\mathit{crashing}$, or $\mathit{interfering}$.
    $m$ matches a boolean BDD.  \state{T} matches any terminal
    state. *: production also applies with the crashing and
    interfering {\StateMachines} swapped.}
  \label{fig:cross_product:algorithm}
\end{figgure}
\end{sidewaysfigure}
\begin{sanefig}
  \begin{subfloat}
    \begin{tikzpicture}
      \node[stateSideEffect,initial above] (lA) {$A$: \stLoad{1}{x} };
      \node[stateIf,below = of lA] (lB) {$B$: \stIf{\smTmp{1} = 0} };
      \node[stateSideEffect,below = of lB] (lC) {$C$: \stLoad{2}{x} };
      \node[stateIf,below = of lC] (lD) {$D$: \stIf{\smBadPtr{\smTmp{2}}} };
      \node[stateTerminal,below = of lD] (lH) {$H$: \stCrash };
      \node[stateTerminal,right = 0.3 of lC] (lG) {$G$: \stSurvive };
      \draw[->] (lA) -- (lB);
      \draw[->,ifTrue] (lB) -- (lG);
      \draw[->,ifFalse] (lB) -- (lC);
      \draw[->] (lC) -- (lD);
      \draw[->,ifTrue] (lD) -- (lH);
      \draw[->,ifFalse] (lD) -- (lG);
    \end{tikzpicture}
    \caption{Crashing thread {\StateMachine} }
  \end{subfloat}
  \hspace{-7mm}
  \begin{subfloat}
    \begin{tikzpicture}
      \node[stateIf,initial above] (lE) {$E$: \stIf{y \not= 0}};
      \path (node cs:name=lE) ++(2.2,-1.5) node [stateSideEffect] (lF) {$F$: \stStore{0}{x}};
      \node[stateTerminal,below = 2 of lE] (lI) {$I$: \stSurvive };
      \draw[->,ifTrue] (lE) -- (lI);
      \draw[->,ifFalse] (lE) -- (lF);
      \draw[->] (lF) -- (lI);
    \end{tikzpicture}
    \caption{Interfering thread {\StateMachine} }
  \end{subfloat}
  \caption{A pair of {\StateMachines}.  $x$ is a global memory
    location.  Figure~\ref{fig:cross_product_output} shows their
    cross-product.}
  \label{fig:cross_product_input}
\end{sanefig}

The productions of the extended grammar encode these refinements quite
directly:
\begin{itemize}
\item Production \circled{$1'$} corresponds to productions \circled{$1_a$}
  and \circled{$1_b$} in the simple grammar.  Similarly, \circled{$2'$}
  corresponds to \circled{2\phantom{'}} and \circled{$3'$} corresponds to
  \circled{3\phantom{'}}.  These need no further explanation.
\item Production \circled{$4'$} allows either of the {\StateMachines}
  to advance past a local side effect without needing to perform a
  happens-before test, implementing a limited form of partial-order
  reduction.
\item Production \circled{$5'$} causes paths in which one
  {\StateMachine} finishes before the other starts to end in the
  {\stUnreached} state, effectively eliminating those paths from
  consideration.
\item Productions \circled{$6'$}, \circled{$7'$}, and \circled{$8'$}
  implement atomic blocks by allowing {\StateMachines} to,
  respectively, enter a local atomic block, leave a block, and
  progress within a block.  Entering a non-local atomic block requires
  combining productions \circled{$6'$} and \circled{$2'$}, which
  produces a large number of unenlightening special cases, and I do
  not give details here.
\end{itemize}
\begin{sanefig}
  \newcommand{\boxLabel}[2]{\small \raisebox{-1.5pt}{\circled{$#1'$}} #2}
  % \labelBox{x0}{y0}{width}{height}{ruleId}{state}
  \newcommand{\labelBox}[6]{
    \fill [color=blue!20] (#1, #2) rectangle (#1 + #3, #2 + #4);
    \node at (#1, #2 + #4 + .1) [below right] {\boxLabel{#5}{#6}};
  }
  \newcommand{\nodeWidth}{4.3cm}
  \newcommand{\nodeHeight}{1.0cm}
  \begin{tikzpicture}[align=center, node distance = 1 and 0.9]
    \labelBox{-2.4}{-.6}{4.8}{1.65}{1}{\{$A$, {\false}\}, \{$E$, {\false}\}}
    \labelBox{2.8}{-.6}{4.8}{1.65}{5}{(\{$A$, {\false}\}, \{$I$, {\false}\})}

    \fill [color=blue!20] (-2.4, -0.9) -- (7.6, -0.9) -- (7.6, -2.65) -- (2.4, -2.65) -- (2.4, -4.65) -- (-2.4, -4.65);
    \node at (-2.4, -0.8) [below right] {\boxLabel{2}{(\{$A$, {\false}\}, \{$F$, {\false}\})} };

    \labelBox{2.8}{-4.65}{4.8}{1.65}{5}{(\{$A$, {\false}\}, \{$I$, {\true}\})}
    \labelBox{-2.4}{-6.65}{4.8}{1.65}{1}{(\{$B$, {\true}\}, \{$F$, {\false}\})}
    \labelBox{2.8}{-6.65}{4.8}{1.65}{5}{(\{$G$, {\true}\}, \{$F$, {\false}\})}

    \fill [color=blue!20] (-2.4, -6.9) -- (7.6, -6.9) -- (7.6, -8.7) -- (2.4, -8.7) -- (2.4, -10.7) -- (-2.4, -10.7);
    \node at (-2.4, -6.8) [below right] {\boxLabel{2}{(\{$C$, {\true}\}, \{$F$, {\false}\})}};

    \labelBox{8.0}{-8.7}{4.8}{1.8}{4}{(\{$C$, {\true}\}, \{$I$, {\true}\})}
    \labelBox{8.0}{-10.7}{4.8}{1.8}{1}{(\{$D$, {\true}\}, \{$I$, {\true}\})}
    \labelBox{8.0}{-12.7}{4.8}{1.8}{3}{(\{$H$, {\true}\}, \{$I$, {\true}\})}
    \labelBox{2.8}{-12.7}{4.8}{1.8}{3}{(\{$G$, {\true}\}, \{$I$, {\true}\})}
    \labelBox{-2.4}{-12.7}{4.8}{1.8}{1}{(\{$D$, {\true}\}, \{$F$, {\false}\})}

    \labelBox{-2.4}{-14.7}{4.8}{1.8}{1}{(\{$H$, {\true}\}, \{$F$, {\false}\})}
    \labelBox{2.8}{-14.7}{4.8}{1.8}{1}{(\{$G$, {\true}\}, \{$F$, {\false}\})}

    \node at (0,1.5) (start) {start};
    \node at (0,0) [stateIf, minimum width=\nodeWidth, minimum height=\nodeHeight] (A) {\stIf{y \not= 0} };
    \node[stateTerminal, right = of A, minimum width=\nodeWidth, minimum height=\nodeHeight] (B) {{\stUnreached} };

    \node[stateIf, below = of A, minimum width=\nodeWidth, minimum height=\nodeHeight] (C) {\stIf{\happensBefore{A}{F}} };
    \node[stateSideEffect, below = of C, minimum width=\nodeWidth, minimum height=\nodeHeight] (D) {\stLoad{1}{x} };
    \node[stateSideEffect, right = of C, minimum width=\nodeWidth, minimum height=\nodeHeight] (E) {\stStore{0}{x} };
    \node[stateIf, below = of D, minimum width=\nodeWidth, minimum height=\nodeHeight] (F) {\stIf{\smTmp{1} = 0} };
    \node[stateTerminal, below = of E, minimum width=\nodeWidth, minimum height=\nodeHeight] (G) {\stUnreached };
    \node[stateTerminal, right = of F, minimum width=\nodeWidth, minimum height=\nodeHeight] (H) {\stUnreached };
    \node[stateIf, below = of F, minimum width=\nodeWidth, minimum height=\nodeHeight] (I) {\stIf{\happensBefore{C}{F}} };
    \node[stateSideEffect, below = of I, minimum width=\nodeWidth, minimum height=\nodeHeight] (J) {\stLoad{2}{x}};
    \node[stateSideEffect, right = of I, minimum width=\nodeWidth, minimum height=\nodeHeight] (K) {\stStore{0}{x}};
    \node[stateIf, below = of J, minimum width=\nodeWidth, minimum height=\nodeHeight] (L) {\!\!\!\stIf{\smBadPtr{\smTmp{2}}}\!\!\!};
    \node[stateSideEffect, right = of K, minimum width=\nodeWidth, minimum height=\nodeHeight] (M) {\stLoad{2}{x}};
    \node[stateTerminal, below = of L, minimum width=\nodeWidth, minimum height=\nodeHeight] (N) {\stUnreached };
    \node[stateTerminal, right = of N, minimum width=\nodeWidth, minimum height=\nodeHeight] (O) {\stUnreached };
    \node[stateIf, below = of M, minimum width=\nodeWidth, minimum height=\nodeHeight] (P) {\!\!\!\stIf{\smBadPtr{\smTmp{2}}}\!\!\!};
    \node[stateTerminal, minimum width=\nodeWidth, minimum height=\nodeHeight, below = of P] (R) {\stCrash };
    \node[stateTerminal, minimum width=\nodeWidth, minimum height=\nodeHeight] at (R-|K) (Q) {\stSurvive };
    \draw[->] (start) -- (A);
    \draw[->,ifTrue] (A) -- (B);
    \draw[->,ifFalse] (A) -- (C);
    \draw[->,ifTrue] (C) -- (D);
    \draw[->,ifFalse] (C) -- (E);
    \draw[->] (D) -- (F);
    \draw[->] (E) -- (G);
    \draw[->,ifTrue] (F) -- (H);
    \draw[->,ifFalse] (F) -- (I);
    \draw[->,ifTrue] (I) -- (J);
    \draw[->,ifFalse] (I) -- (K);
    \draw[->] (J) -- (L);
    \draw[->] (K) -- (M);
    \draw[->,ifTrue] (L) -- (N);
    \draw[->,ifFalse] (L) -- (O);
    \draw[->] (M) -- (P);
    \draw[->,ifFalse] (P) -- (Q);
    \draw[->,ifTrue] (P) -- (R);
  \end{tikzpicture}
  \caption{Cross product of the {\StateMachines} shown in
    \autoref{fig:cross_product_input}.  Blue boxes show the
    non-terminals and productions used to generate each part of the
    graph.  The $\mathit{atomic}$ field of the non-terminal is always
    $\varnothing$ for these input {\StateMachines} and is not shown.}
  \label{fig:cross_product_output}
\end{sanefig}
The cross-product of the {\StateMachines} in
\autoref{fig:cross_product_input} is given in
\autoref{fig:cross_product_output}, showing the
\textsc{Configuration}s and productions used to produce all of the
output states.  This captures every possible interleaving of the two
input {\StateMachines} into a single cross-product {\StateMachine},
allowing {\technique} to build the \gls{verificationcondition} using a
simple single-threaded symbolic execution engine.  Note that while
this example produced a tree-structured {\StateMachine}, in the
general case the result is a directed acyclic graph, with sub-graphs
shared if the same non-terminal is generated multiple times.  This
reduces the worst-case number of states in the cross-product
{\StateMachine} from $O(\binom{L+S}{L})$ to $O(LS)$, where $L$ is the
number of states in the crashing {\StateMachine} and $S$ the number in
the interfering one, which often reduces the cost of analysing it by a
useful amount.

\todo{blah} Importantly, the cross-product {\StateMachine} can itself
be simplified using the usual {\StateMachine} simplification passes.
Simplifying the example cross-product {\StateMachine} will produce the
{\StateMachine} shown in Figure~\ref{fig:cross_product_output_opt}.
In this case, the actual symbolic execution step will be trivial, and
will report that the program will reach a {\stCrash} state precisely
when $y = 0 \wedge \happensBefore{A}{F} \wedge LD(x) \not= 0 \wedge
\happensBefore{F}{C}$; in other words, if $y$ is nonzero, the initial
value of $x$ is non-zero, and statement F intercedes between
statements A and C.  Comparison to the input {\StateMachines} will
show that this is precisely the desired result\editorial{blurg}.

The simplifications needed in this example are quite simple, and it
would have been possible to include equivalent optimisations in the
symbolic execution engine itself.  This would be more difficult for
more complex simplifications, for two reasons:
\begin{itemize}
\item Simplification passes can easily look ahead in the
  {\StateMachine}, whereas symbolic execution primarily considers a
  single state at a time.  Dead code elimination, for example, is much
  easier to implement as a simplification to the {\StateMachine} than
  as a change to the symbolic execution engine.
\item The results of a simplification pass are inherently shared
  across all paths which reach a particular state, whereas the
  symbolic execution engine needs to perform additional work in order
  to share results.
\end{itemize}
There is also an engineering consideration which argues in favour of
building and simplifying the cross-product {\StateMachine}, rather
than integrating equivalent optimisations into the symbolic execution
engine: {\implementation} already needs all of the simplifiers in
order to build the input {\StateMachines}, and so re-using them here
halves the implementation effort.

\begin{sanefig}
  \centerline{
  \begin{tikzpicture}
    \node[stateSideEffect, initial] (A) {\stAssert{y = 0 \wedge \happensBefore{A}{F} \wedge \smLoad{x} \not= 0 \wedge \happensBefore{F}{C}} };
    \node[stateTerminal, below = of A] (B) {\stCrash };
    \draw[->] (A) -- (B);
  \end{tikzpicture}
  }
  \caption{Result of simplifying {\StateMachine} shown in
    Figure~\ref{fig:cross_product_output}.}
  \label{fig:cross_product_output_opt}
\end{sanefig}

\subsection{Path explosion}

One common problem in symbolic execution systems is path explosion:
the number of paths through a program rises exponentially in the size
of the program, and this can prevent na\"ive symbolic execution
systems from being applied to realistically large programs.  In the
case of \technique, there are two main causes of path explosion:
\begin{itemize}
\item
  \textit{Aliasing}.  If the various simplification passes and the
  dynamic analysis cannot determine how memory accessing instructions
  alias then the symbolic execution engine must exhaustively consider
  every possible way in which every \state{Load} can be satisfied.  If
  there are $n$ \state{Load}s and \state{Store}s then the number of
  such patterns to be considered is $O((m+1)^n)$, which grows very
  quickly in the number of accesses.  The use of lazy alias resolution
  helps mitigate this to some extent, but does not eliminate it
  completely.  This represents one of the major limitations to
  \technique's scalability.
\item
  \textit{Thread interleaving}.  The cross-product {\StateMachine}
  will have $O(nm)$ states, where $n$ is number of states in the
  crashing {\StateMachine} and $m$ the number in the interfering one.
  The number of paths through the combined {\StateMachine} then grows
  as $O(2^{nm})$, which again grows rather quickly.
\end{itemize}
The result is that, in the common case where the read-side
{\StateMachine} consists mostly of \state{Load} operations and
write-side one mostly of \state{Store} ones, the symbolic execution
engine might have to consider up to $O(2^{nm}.m^n)$ distinct
paths\editorial{Not quite -- laziness means that you can share some of
  the aliasing work across multiple interleavings, so you don't
  actually need to multiply them up like that.  Although I suppose I
  could argue that big-O just gives upper bounds.}  when evaluating
the cross-product {\StateMachine}.  This is impractical for even
moderate values of $n$ and $m$.  For good performance, {\technique}
relies on the various simplification and analysis techniques to reduce
$n$ and $m$ to manageably small values.  Fortunately, as discussed in
the evaluation, they are able to do so in a useful set of cases.

\section{The \glsentrytext{w-isolation} property}
\label{sect:derive:w_isolation}

In practice, {\technique}'s most important limitation is the need to
solve a large number of aliasing problems.  This can be somewhat
ameliorated by assuming that the crashing {\StateMachine} never stores
to any memory locations which are subsequently loaded by the
interfering one.  This amounts to restricting the class of bugs
considered from those where two threads are simultaneously working
with a structure to those where one thread is reading from the
structure whilst another updates it.  Such bugs are said to be
I-isolated, or to have the \gls{w-isolation} property, as the
\gls{interferingthread} is effectively isolated from the crashing one.

Restricting the class of bugs considered in this way enables three
main optimisations:

\begin{itemize}
\item
  It directly restricts the aliasing problem, as the analysis no
  longer needs to consider aliasing between stores in the crashing
  {\StateMachine} and loads in the interfering one.
\item
  It reduces the set of interfering \glspl{cfg} which is generated for
  each \gls{crashingthread}, because if the \gls{interferingthread}
  cannot load any location stored to by the \gls{crashingthread} then
  $c2i$ is empty and $\beta = i2c$ (see
  \autoref{sect:derive:write_side}).
\item
  It simplifies the calculation of the \gls{inferredassumption}.
  Normally, the \gls{inferredassumption} is the conjunction of
  \gls{ci-atomic} and \gls{ic-atomic}, but when the \gls{w-isolation}
  property holds this can be replaced by C-atomic, the condition for
  the crashing {\StateMachine} to survive when run in isolation.  This
  is because the \gls{w-isolation} property implies that
  \gls{ci-atomic} = C-atomic $\wedge$ I-atomic, where I-atomic is the
  condition for the interfering {\StateMachine} to survive in
  isolation, and I-atomic is implied by \gls{ic-atomic}.  C-atomic is
  independent of the interfering {\StateMachine}, and so only needs to
  be calculated once for every crashing {\StateMachine} rather than
  once for every pair of crashing and interfering {\StateMachines},
  which can sometimes provide a modest performance improvement.
\end{itemize}
The analysis is almost always faster with the \gls{w-isolation}
assumption, but cannot handle as broad a class of program behaviour.
I evaluate these effects experimentally in
\autoref{sect:eval:w_isolation}.

\section{The \glsentrytext{programmodel}}
\label{sect:program_model}

In addition to the {\StateMachine}-level aliasing analysis,
{\technique} also makes use of an \gls{programmodel}, built before the
main analysis starts, which describes how the program accesses memory
during normal operation.  This is used both for alias analysis during
{\StateMachine} simplification (\autoref{sect:derive:simplify_sm}) and
also to find the $\beta$ and $i2c$ sets when building the interfering
\gls{cfg} (\autoref{sect:derive:write_side}).  The memory access model
is itself composed of two parts: a model of how the program accesses
its stack and a model of accesses to non-stack memory.  I describe
each in turn.

\subsection{The stack model}
\todo{Slightly feeble para} The stack model is built using a fairly
conventional function-local static pointer escape analysis\needCite{}
based on the observation that stack frames are in some sense
``created'' when a function starts, and so there should not be any
pointers to function-local variables unless the function being
analysed creates one.  If one pointer is to a local variable and
another pointer existed before the function started then it is safe to
assume that they do not alias, and so the static analysis attempts to
track which registers and memory locations might contain pointers to
the local stack frame.  This is usually sufficient for the
{\StateMachine} simplifiers to be able to determine which, if any,
stack frames a given memory access might refer to; simple arithmetic
considerations can then usually determine the specific local variable.
As part of this analysis, {\technique} discovers any registers which
have constant value at a particular instruction, or those which are
equal to another register plus a constant, and this information is
also used during {\StateMachine} simplification.

\subsection{The non-stack model}
\todo{This is a bit of a mess.}  It is much more difficult to
characterise the structure of non-stack memory, such as the heap,
using static analysis: not only is the heap structure itself more
complicated, in terms of the number of objects which point at other
objects, but the information is harder to locate, as it is no longer
localised to any particular function or program module.  These
problems make it difficult to accurately model the heap statically
even when the analysis tool has full access to the program's source
code\needCite{}, and attempting to do so given only a binary is
completely infeasible.

{\Technique} instead relies on a dynamic analysis to model accesses to
non-stack locations.  The intended behaviour of this analysis is to
first recover the structure of memory in terms of aggregate structure
types and their fields, identifying the field which each memory
location holds, and to track which program instructions access each
field.  Two instructions are considered to alias if they ever access
the same field.  This behaviour cannot, however, be directly
implemented, because fields and types are source-level constructs, and
{\technique} has access only to the compiled program.  {\Technique}
therefore approximates by identifying fields with the sets of
instructions which access them.  Memory is divided into fixed-size
chunks\footnote{Each chunk is eight bytes for {\implementation}.  This
  represents a reasonable trade-off between precision, which argues
  for smaller chunks, and analysis performance, which argues for
  larger ones.}, each of which is labelled with the set of
instructions which access it; the final result is the set of all such
labels generated by the program.

This approximation is generally quite accurate, and where it does
produce results different from using source-level fields usually
produces a better \gls{programmodel}.  There is, however, one
important situation in which it fails completely: releasing
dynamically-allocated memory with \texttt{free}.  This effectively
changes the aggregate type associated with a memory region, and hence
all of the fields contained within it.  If the dynamic analysis fails
to notice that this has happened then the old fields will be merged
with those of any structure which is later allocated, potentially
leading to a very imprecise aliasing table.  {\Technique} relies on
identifying such type-changing functions manually.  This is not an
unreasonable burden: for MySQL, the necessary annotations amount to an
additional twenty lines across the entire program, and none of the
other programs examined in the evaluation required any annotations at
all.  It might be possible to identify such allocators using a variant
of the techniques described by Cozzie et al.\cite{Cozzie2008}, but I
have not investigated that at this time.

\todo{Talk about what happens if this is incomplete?}

\subsection{Escape analysis}
{\Implementation} includes one minor refinement to the basic dynamic
analysis described above.  It is fairly common for programs to
allocate new heap structures using a function such as \texttt{malloc}
and to then initialise this structure using a series of stores.  These
stores will never race, so it would be helpful to avoid spending
excessive time considering what would happen if they did.
{\Technique} avoids doing so by marking blocks of memory returned from
\texttt{malloc} as thread-local, and they remain so until a pointer to
them is stored in non-stack memory.  Entries in the aliasing table
include a flag indicating whether the access is thread-private or
potentially racing, and this is used by later phases of the analysis
to constrain the aliasing problem.

This policy might seem to be overly conservative: a block of memory is
marked as shared whenever a pointer to it is stored into any non-stack
memory, even when that non-stack memory is itself marked as
thread-private.  This is necessary because the analysis does not
attempt to track the heap reachability graph, and in particular cannot
map from one block to the set of blocks reachable from that block.  It
is therefore not safe to ``upgrade'' a block from thread-private to
thread-shared if there is any possibility of that block containing a
thread-private pointer; upgrading blocks early and pessimistically
means that it is never necessary to do so.

\section{Discussion}

This chapter has shown how to derive {\StateMachines} representing a
particular bug and how to convert these into
\glspl{verificationcondition}.  It has also shown how to derive
      {\StateMachines} for every possible bug of a particular form in
      a program.  These are not, however, particularly useful by
      themselves, as many of the {\StateMachines} will represent false
      positives.  The following chapters show how to convert them
      into, first, \glspl{bugenforcer}, which cause a particular bug
      to reproduce more easily, and, then, fixes, which completely
      eliminate whichever bugs actually do reproduce.
