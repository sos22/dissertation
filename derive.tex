\chapter{Finding bugs}
\label{sect:derive}

The core of {\technique} is a system for taking potential bugs,
described by \glspl{verificationcondition}, and turning them into
either \glspl{bugenforcer}, which check which ones are real, or
{\genfixes}, which eliminate some specific bugs.  This chapter
describes one approach to finding the \glspl{verificationcondition}.
The simplest form of the algorithm assumes that the crashing
instruction has already been identified and then investigates
concurrency errors which might lead to that sort of crash, but it can
also be generalised to finding completely unknown bugs, at the expense
of quite high computational cost.

The basic algorithm proceeds as follows:

\begin{itemize}
\item Identify all of the instructions which the \gls{crashingthread}
  might have executed in the \gls{analysiswindow}.  In other words,
  identify all of the static instructions which might have executed in
  the \gls{alpha} dynamic instructions prior to the crash.  These are
  represented as an acyclic, unrolled, \gls{cfg}; the details are in
  \autoref{sect:derive:build_crashing_cfg}.
\item Compile the \gls{crashingthread}'s \gls{cfg} into a
  {\StateMachine}.  This is essentially a decompilation step,
  converting the program's machine code into a form which is more
  amenable to later analysis.  The {\StateMachine} abstraction is
  described in \autoref{sect:derive:state_machines} and the
  compilation process in \autoref{sect:derive:compile_cfg}.
\item Simplify the {\StateMachine}.  {\StateMachines}, when initially
  derived, are \todo{(almost)} faithful representations of the
  program's behaviour, and as such often contain a large amount of
  information which is irrelevant to the bug under investigation.  The
  simplification step is responsible for removing this redundant
  information.  It is described in \autoref{sect:derive:simplify_sm}.
\item Examine the \gls{crashingthread}'s {\StateMachine} to discover
  what it might have raced with.  This information is then used to
  build \glspl{cfg} and {\StateMachines} for all of the
  \glspl{interferingthread}; this is described in
  \autoref{sect:derive:write_side}.
\item Symbolically execute the crashing and interfering
  {\StateMachines} so as to convert them into a
  \gls{verificationcondition}; this is a predicate over the program's
  state and happens-before graph which is true if there is any
  possibility of the bug under investigation reproducing.  This step
  is described in \autoref{sect:derive:symbolic_execute}.
\end{itemize}

At this stage, {\technique} takes no account of the program's existing
synchronisation, beyond that which is visible in the \glspl{cfg}, and
so the resulting set of \glspl{verificationcondition} will contain a
large number of false positives.  These will be filtered out later by
the \glspl{bugenforcer}.

\section{Building the crashing thread's CFG}
\label{sect:derive:build_crashing_cfg}

{\Technique} considers concurrency bugs caused by unfortunate
interleavings of the \gls{alpha} instructions prior to the crash, and
the first step is to find the \gls{alpha} instructions which might
have executed in the \gls{crashingthread} leading up to the crash.
These are represented as a \gls{cfg}, showing both the instructions
involved and the relationships between them.

Note that the instructions in this \gls{cfg} are \emph{dynamic} rather
than static, so if the same program instruction executes twice on a
single path then it will be represented twice in the \gls{cfg}.  This
means that the generated \gls{cfg} is inherently acyclic, which
simplifies later analysis.  More importantly, it also makes it easy to
identify specific memory-accessing operations, so that it makes sense
to talk about a particular memory access happening before or after
some other access, which is more difficult if accesses can happen
multiple times.

The approach taken by {\technique} is simple: start by deriving a
fragment of the program's static \gls{cfg} which contains all of the
necessary static instructions, and then apply a loop unrolling
algorithm which converts that static \gls{cfg} into a dynamic one by
duplicating instructions as necessary.  The next sub-sections describe
this process in more detail.

\subsection[Building the \glsentrytext{crashingthread}'s static \glsentrytext{cfg}]{Building the \gls{crashingthread}'s static \gls{cfg}}
\label{sect:derive:build_static_cfg}

\begin{figure}
\begin{algorithmic}[1]
\State $\mathit{depth} \gets 0$
\State $\mathit{pendingAtDepth} \gets \queue{\mathit{targetInstrAddress}}$
\State $\mathit{result} \gets \map{}$
\While{$\mathit{depth} < \alpha$}
  \State $\mathit{pendingAtNextDepth} \gets \queue{}$
  \While{$\neg{}\mathit{empty}(\mathit{pendingAtDepth})$}
    \State $\mathit{currentInstr} \gets \mathit{pop}(\mathit{pendingAtDepth})$
    \If {$\mathit{result} \textrm{ has entry for } \mathit{currentInstr}$}
      \State \textbf{continue}
    \EndIf
    \State $\mathit{current} \gets \text{decode instruction at } \mathit{currentInstr}$
    \State $\mapIndex{\mathit{result}}{\mathit{currentInstr}} \gets \mathit{current}$
    \State $\mathit{predecessors} \gets \text{predecessors of } \mathit{currentInstr}$
    \State Add $\mathit{predecessors}$ to $\mathit{pendingAtNextDepth}$
  \EndWhile
  \State $\mathit{pendingAtDepth} \gets \mathit{pendingAtNextDepth}$
  \State $\mathit{depth} \gets \mathit{depth} + 1$
\EndWhile
\end{algorithmic}
\caption{Building a \gls{crashingthread} static \gls{cfg}.}
\label{fig:derive:static_read_cfg_single_function}
\end{figure}

The first step in building the \gls{crashingthread}'s dynamic
\gls{cfg} is to find its static \gls{cfg}.  The algorithm for doing so
is shown in Figure~\ref{fig:derive:static_read_cfg_single_function}.
This implements a depth-limited breadth-first search starting at the
crashing instruction and exploring backwards through the program's
control flow.  Note that this can result in a \gls{cfg} with multiple
roots.

There is a slight subtlety on line 13, which determines the
predecessors of a given instruction.  This is not always completely
trivial given only a program binary.  {\Technique}'s approach depends
on a combination of static and dynamic analysis.  The full details are
given in Section~\ref{sect:program_model:instr_predecessors}.  For
now, assume that there is some function which maps from an instruction
to the set of instructions which might have executed immediately
before that instruction in a single thread, crossing function
boundaries when necessary.

\subsection[Converting the static \glsentrytext{cfg} to a dynamic one]{Converting the static \gls{cfg} to a dynamic one}
\label{sect:derive:handling_loops}

The nodes of the \gls{cfg} generated in
\autoref{sect:derive:build_static_cfg} represent static instructions
in the program, but the \glspl{cfg} used to build {\StateMachines}
must represent the program's dynamic instructions.  {\Technique} must
therefore the static \gls{cfg} into a dynamic one.  For completely
acyclic \glspl{cfg} this is easy, as each static instruction executes
at most once and it is safe to simply identify the static and dynamic
instructions.  Loops are more difficult to handle: the instructions in
a loop might execute multiple times, and so no such simple
correspondence exists.  {\Technique} solves this problem by simply
unrolling such loops until they exceed \gls{alpha} instructions, so
that the loop can only execute at most once during the
\gls{analysiswindow}, restoring the problem to the acyclic case.

\begin{figure}
\begin{tikzpicture}
  [node distance=1 and 0.3]
  \begin{scope}
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$}; 
    \node (C) [CfgInstr] [below=of B] {$C_0$}; 
    \node (D) [CfgInstr] [below=of C] {$D_0$}; 
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C.east) to [bend right=90] (B.east) node (edge1) [right] {};
    \begin{pgfonlayer}{bg}
      \node (box1) [fill=black!10,fit=(A) (B) (C) (D) (edge1)] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=4cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$}; 
    \node (C) [CfgInstr] [below=of B] {$C_0$}; 
    \node (D) [CfgInstr] [below=of C] {$D_0$};  
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (B) to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B);
    \begin{pgfonlayer}{bg}
      \node (box2) [fill=black!10,fit=(A) (B) (C) (D) (C')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=8cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$};
    \node (B') [CfgInstr] [right=of B] {$B_1$};
    \node (C) [CfgInstr] [below=of B] {$C_0$};
    \node (D) [CfgInstr] [below=of C] {$D_0$};
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') to [bend right=10] (C');
    \draw[->] (C') to [bend right=10] (B');
    \begin{pgfonlayer}{bg}
      \node (box3) [fill=black!10,fit=(A) (B) (C) (D) (C') (B')] {};
    \end{pgfonlayer}
  \end{scope}
  \begin{scope}[xshift=12cm]
    \node (A) at (0,2) [CfgInstr] {$A_0$};
    \node (B) [CfgInstr] [below=of A] {$B_0$};
    \node (B') [CfgInstr] [right=of B] {$B_1$};
    \node (C) [CfgInstr] [below=of B] {$C_0$};
    \node (C') [CfgInstr] [right=of C] {$C_1$};
    \node (C'') [CfgInstr] [right=of C'] {$C_2$};
    \node (D) [CfgInstr] [below=of C] {$D_0$};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (C) -- (D);
    \draw[->] (C') -- (B);
    \draw[->] (A) -- (B');
    \draw[->] (B') -- (C');
    \draw[->] (C'') to [bend right=10] (B');
    \draw[->] (B') to [bend right=10] (C'');
    \begin{pgfonlayer}{bg}
      \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
    \end{pgfonlayer}
  \end{scope}
  \draw[->,thick] (box1) -- (box2) node [above,midway] {duplicate $C_0$};
  \draw[->,thick] (box2) -- (box3) node [above,midway] {duplicate $B_0$};
  \draw[->,thick] (box3) -- (box4) node [above,midway] {duplicate $C_1$};
  \draw[->,thick] (box4) -- +(2.5,0) node [above,midway] {...};
\end{tikzpicture}
\caption{A CFG containing a cycle.}
\label{fig:cyclic_cfg}
\end{figure}

As an example, consider the \gls{cfg} shown at the left of
Figure~\ref{fig:cyclic_cfg}, which contains a loop between
instructions $B_0$ and $C_0$.  This loop must be removed from the \gls{cfg}
while maintaining all paths which terminate at $D_0$ and which contain
\gls{alpha} or fewer instructions.  The algorithm starts by
performing a depth-first traversal backwards through the graph from
$D_0$ until it finds an edge which closes a cycle.
In this case, that is the edge from $C_0$ to $B_0$.
\begin{wrapfigure}{r}{3.3cm}
\begin{tikzpicture}
  [node distance=1 and 0.3]
  \node (A) at (0,2) [CfgInstr] {$A_0$};
  \node (B) [CfgInstr] [below=of A] {$B_0$};
  \node (B') [CfgInstr] [right=of B] {$B_1$};
  \node (C) [CfgInstr] [below=of B] {$C_0$};
  \node (C') [CfgInstr] [right=of C] {$C_1$};
  \node (C'') [CfgInstr] [above right=of B'] {$C_2$};
  \node (D) [CfgInstr] [below=of C] {$D_0$};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) -- (D);
  \draw[->] (C') -- (B);
  \draw[->] (A) -- (B');
  \draw[->] (B') -- (C');
  \draw[->] (C'') -- (B');
  \begin{pgfonlayer}{bg}
    \node (box4) [fill=black!10,fit=(A) (B) (C) (D) (C') (B') (C'')] {};
  \end{pgfonlayer}
\end{tikzpicture}
\vspace{-3mm}
\caption{Fully unrolled version of the CFG in
  Figure~\ref{fig:cyclic_cfg}, preserving all paths of length six or
  fewer instructions.  Note that an additional root has been
  introduced at $C_2$.}
\vspace{3mm}
\label{fig:unrolled_cyclic_cfg}
\end{wrapfigure}
{\Technique} will therefore break this edge by duplicating the
instruction at the start of the edge, $C_0$, along with all of its
incoming edges (in this case, just the $B_0$ to $C_0$ edge).  The
$C_0$ to $B_0$ edge can then be redirected to be from $C_1$ to $B_0$,
producing the next diagram in the sequence.  All paths which were
possible in the old graph will also be possible in the new one, if
duplicated nodes are treated as semantically equivalent, and the loop
is now one instruction further away from the target instruction $D_0$.
The process then repeats, moving the cycle steadily further and
further away from $D_0$ until all paths ending of length \gls{alpha}
ending at $D_0$ are acyclic, at which point the cycle can be safely
removed from the graph.

Note that the edge which is modified is the back edge, from $C_0$ to
$B_0$, which points ``away from $D_0$'', and not the forwards edge
from $B_0$ to $C_0$.  Trying to break the $B_0$ to $C_0$ edge would
have moved the cycle away from $A_0$ rather than away from $D_0$,
which would not be helpful.

\begin{figure}
\begin{algorithmic}[1]
  \While {graph is not cycle-free}
     \State $edge \gets \textsc{findEdgeToBeBroken}(targetInstr)$
     \If {$edge$ is at least $\alpha$ instructions from target instruction}
        \State {Erase $edge$ from graph}
     \Else
        \State {$newNode \gets$ duplicate of $edge.source$}
        \For {$i$ incoming edge of $edge.source$}
           \State {Create a new edge from $i.source$ to $newNode$}
        \EndFor
        \State {Replace $edge$ with an edge from $newNode$ to $edge.destination$}
     \EndIf
  \EndWhile
\end{algorithmic}
\caption{Loop unrolling and cycle breaking algorithm.
  \textsc{findEdgeToBeBroken} simply performs a depth-first search of
  the graph backwards from $targetInstr$ and returns the first edge
  which completes a cycle.}
\label{fig:derive:read:unroll_cycle_break}
\end{figure}

The complete algorithm is shown in
Figure~\ref{fig:derive:read:unroll_cycle_break}.  This algorithm is
guaranteed to preserve all paths of length $\alpha$ which end at the
target instruction.  There are only two places in the algorithm which
remove existing edges, so consider each in turn.  The first is the
erasure on line 4.  This can only ever affect edges whose shortest
path to a target is at least $\alpha$ instructions long, and so cannot
eliminate any paths to a target of length $\alpha$, and is therefore
safe.  The other is the replacement step at line 10, which replaces an
edge from $edge.source$ to $edge.destination$ with one from $newNode$
to $edge.destination$.  This is safe provided that every path to
$newNode$ has a matching path to $edge.source$, which is ensured by
duplicating all of $edge.source$'s incoming edges to $newNode$.  At
the same time, no additional paths will be introduced, because every
path to $newNode$ has a matching path to $edge.source$.  As such, the
algorithm correctly preserves all paths of length \gls{alpha},
as desired, and does not introduce any more.

This might appear, on the face of it, to be a rather expensive
algorithm: it must explore every path of length \gls{alpha}
ending at the target instruction, and the number of such paths is
potentially exponential in \gls{alpha}.  This is true, but
there are some important mitigating factors:

\begin{itemize}
\item In practice, most code is not particularly branch-heavy, so that
  relatively few instructions have more than one predecessor and the
  base of the exponential is small.
\item With careful implementation, the constant factor can be made
  small.
\item This is not the only analysis performed by {\technique} which
  has exponential cost in \gls{alpha}.  For instance, later
  phases must determine which memory accesses might alias with which
  other accesses; in the worst case, this might involve considering
  all $2^n$ aliasing pairs individually.
\end{itemize}

The overall result is that, despite having cost $O(n^{\alpha})$, this
phase of the analysis rarely accounts for more than a small percentage
of {\implementation}'s total running time (see
Section~\ref{sect:eval:time_details} for more details).

\section{\STateMachines}
\label{sect:derive:state_machines}

The dynamic \gls{cfg} represents all of the instructions in the
\gls{crashingthread} in the \gls{analysiswindow}, and therefore in
principle contains all of the information needed to analyse the bug.
It is, however, very close to machine code, and is therefore difficult
to work with.  The next step in the algorithm is to decompile it into
a more amenable form.  In the case of {\technique}, this is a
{\StateMachine}: a directed acyclic graph of states, described in
Table~\ref{table:state_machine_states}.

It is important to emphasise at this stage that {\StateMachines}
states do not directly correspond to instructions in the original
program: one state might represent several instructions, or a single
instruction might be represented by multiple states.  For instance, an
instruction in a function $f$ might correspond to one state when $f$
is called from $g$ and another when $f$ is called from $h$, and
instructions which are not relevant to the behaviour being
investigated will not have any corresponding states at all.  This
gives {\technique} a great deal of flexibility when simplifying
{\StateMachines}, and hence allows it to remove most irrelevant
information from the {\StateMachine}.

In addition to the graph of states, {\StateMachines} may also have
some temporary variables.  These are simple slots into which the
values of expressions and the results of \state{Load} operations can
be stored.  Temporaries can only store simple values, with no internal
structure, and there is no concept analogous to a pointer to a
temporary\footnote{For {\implementation}, {\StateMachine} temporaries
  are 128-bit values, so as to be able to store the contents of an XMM
  register in one temporary, but other choices would be possible.}.
It is important to note that {\StateMachine} temporaries do not
necessarily correspond to any particular bit of program state, and
that most program state will not be represented by any temporary.

\begin{table}
\begin{tabular}{lp{11.3cm}}
Expression & Meaning \\
\hline
$\smTmp{A}$ & The value of {\StateMachine} temporary $A$. \\
$\happensBefore{A}{B}$ & True if event $A$ happens before event $B$, false if $B$ happens before $A$. \\
$\entryExpr{\mai{tid}{instr}}$ & True if thread $tid$ starts with instruction $instr$, and false otherwise. \\
$\controlEdge{tid}{A}{B}$ & True if thread $tid$ executed instruction $B$ immediately after instruction $A$. False if it executed some other instruction after $A$ and undefined if it did not execute $A$ at all.\\
$\smBadPtr{expr}$ & True if $expr$ evaluates to a value which is not a valid pointer.\\
$\smLoad{expr}$ & The initial value of the memory at location $expr$. \\
\end{tabular}
\caption{Types of expressions in the {\StateMachine} expression
  language.  The usual arithmetic operators, such as addition,
  multiplication, bit shift, etc., are also supported, but logical
  operators such as $\wedge$ and $\vee$ are not.}
\label{table:state_machine_exprs}
\end{table}

\begin{landscape}
\begin{table}
\begin{tabular}{lllp{5cm}p{12.8cm}}
\multicolumn{2}{l}{State}       & \multicolumn{2}{l}{Fields} & Meaning \\
\hline
\multicolumn{2}{l}{\state{If}}  & \state{cond} & BBDD        & Conditional branch with two successor states.  Evaluates \state{cond}, branching to one successor if it is true and the other if it is false. \\
\hline
\multicolumn{2}{l}{Terminal states} &          &             & Terminal states.  {\STateMachine} execution finishes when it reaches one of these. \\
 & {\stSurvive}              &              &             & The bug has been avoided. \\
 & {\stCrash}                &              &             & The bug will definitely happen. \\
 & {\stUnreached}            &              &             & A contradiction has been reached; this path through the {\StateMachine} should be ignored. \\
\hline
\multicolumn{2}{l}{Side-effect states}\\
 & \state{Load}                 & \state{addr} & Expression BDD & \multirow{2}{12.8cm}{Load from program memory at address \state{addr} and store the result in {\StateMachine} temporary \state{tmp}.} \\
 &                              & \state{tmp}  & {\STateMachine} temporary \\
 & \state{Store}                & \state{addr} & Expression BDD & \multirow{2}{12.8cm}{Store to program memory.  \state{data} and \state{addr} are evaluated to concrete values and the value of \state{data} stored to the address \state{addr}.} \\
 &                              & \state{data} & Expression BDD \\
 & \state{Copy}                 & \state{data} & Expression BDD & Evaluate an expression and store the result in a {\StateMachine} temporary. \\
 &                              & \state{tmp}  & {\STateMachine} temporary \\
\\
 & \state{ImportRegister}       & \state{tid}  & Thread ID       & \multirow{2}{12.8cm}{Copy the value of register \state{reg} in program thread \state{tid} into {\StateMachine} temporary \state{tmp}.} \\
 &                              & \state{reg}  & Register ID \\
 &                              & \state{tmp}  & {\STateMachine} temporary \\
\\
 & \state{Assert}               & \state{cond} & BBDD            & Note that a given condition is true at a particular point in the {\StateMachine}'s execution. \\
 & $\Phi$                       &              &                 & Implement an SSA $\Phi$ node\cite{cytron1991}. \\
\\
 & {\stStartAtomic}          &              &                 & \multirow{2}{12.8cm}{Mark the start and end of atomic blocks, used to constrain the set of schedules which must be considered; see Section~\ref{sect:using:build_cross_product}.} \\
 & {\stEndAtomic}            \\
\end{tabular}
\caption{Types of {\StateMachine} states.}
\label{table:state_machine_states}
\end{table}
\end{landscape}

\subsection{{\STateMachine} expression language}
\label{sect:sm_expr_language}

\todo{Could maybe explain this more easily by making a more explicit
  comparison to SMT-style things?}

Any non-trivial {\StateMachine} will include some expressions over the
original program's state.  These are expressed using an expression
language which is described in Table~\ref{table:state_machine_exprs}.
This language is, for most part, quite conventional, and includes
simple mechanisms for querying the program's behaviour and state and
for obtaining the values of {\StateMachine} temporaries, or for
evaluating simple arithmetic operators.

Note that $\smLoad{}$ expressions always evaluate to the initial value
of the selected memory location, and not to the current value, which
may be different if the {\StateMachine} has executed a \state{Store}
operation.  The only way to access the current contents of memory is
via a \state{Load} side-effect.  This means that $\smLoad{}$ is a pure
function and can be re-ordered freely across side-effecting
operations, simplifying analysis.

This language is missing one important feature: logical connectives
such as $\wedge$ or $\vee$.  These operators are not represented using
the expression language, but are instead encoded into binary decision
diagrams, or BDDs\cite{Brace1990}, which are themselves expressed in
terms of the expression language.  These BDDs allow much more
efficient implementations of many common operations than would be
possible with a scheme based entirely on unconstrained expressions.

\begin{wrapfigure}{r}{6.5cm}
  \begin{tikzpicture}
    \node (x) [BddNode] {$\smTmp{A} = 72$};
    \node (y) [BddNode, below = of x] {$\smLoad{\smTmp{B}} = 9$};
    \node (z) [BddNode, below = of y] {$\smTmp{B} > 912$};
    \node (true) [BddLeaf, below left = of z] {$715$};
    \node (false) [BddLeaf, below right = of z] {$\smTmp{C}$};
    \draw [BddTrue] (x) -- (y);
    \draw [BddFalse] (x.east) to [bend left=30] (false);
    \draw [BddTrue] (y) -- (true);
    \draw [BddFalse] (y) -- (z);
    \draw [BddTrue] (z) -- (true);
    \draw [BddFalse] (z) -- (false);
  \end{tikzpicture}
  \caption{An example expression BDD.  This can evaluate to either
    $715$ or $\smTmp{C}$, depending on the values of $\smTmp{A}$,
    $\smTmp{B}$ and the initial contents of program memory.}
  \label{fig:derive:example_expr_bdd}
  \vspace{-5mm}
\end{wrapfigure}
Two types of BDD used in {\StateMachine} states: expression BDDs and
boolean BDDs.  The difference is that boolean BDDs, or BBDDs, are
constrained to evaluate to a simple boolean, whereas expression BDDs
evaluate to an expression in the expression language.  An example
expression BDD is shown in Figure~\ref{fig:derive:example_expr_bdd}.
This BDD evaluates to $715$ if either $\smTmp{A} \not= 72$ or both
$\smLoad{\smTmp{B}} = 9$ and $\smTmp{B} > 912$, or to $\smTmp{C}$
otherwise.  

The important thing to note here is that both the internal nodes and
the leaves of the BDD are expressions in the expression language,
rather than simple boolean variables or boolean constants.  This means
that expression BDDs can represent complex functions of program state
(they are used, for instance, for the address field of a \state{Load}
state), and do not overly complicate any of the usual BDD manipulation
algorithms.  They do, however, mean that expression and boolean BDDs
are only canonical to the extent that expressions in the expression
language can be canonicalised.  Unfortunately, completely
canonicalising general expressions is impossible\footnote{If nothing
  else, doing so would provide a solution to Hilbert's tenth problem,
  which is known to be impossible\cite{Davis1973}.}, and so while
ordinary BDDs are canonical representations of boolean functions those
used by {\technique} are not.  This lack of canonicalisation can
sometimes lead to poor performance if a single function is represented
in many different forms.  Fortunately, a few very simple
canonicalisation rules, such as sorting the arguments to commutative
operators and respecting transitivity of equality, usually suffice to
reduce redundancy to an acceptable level and avoid the worst
performance problems.

\subsection{Example {\StateMachines}}
\label{sect:derive:simple_toctou_example}

\begin{figure}
  \subfigure[][Crashing thread]{
    \texttt{
    \begin{tabular}{rlll}
              & \multicolumn{3}{l}{crashing\_thread:} \\
      400694: & mov & global\_ptr, &\%rax\\
      40069b: & test & \%rax, &\%rax \\
      40069e: & je   & \multicolumn{2}{l}{4006ad}\\
      4006a0: & mov  & global\_ptr, & \%rax\\
      4006a7: & movl & \$0x5, & (\%rax)\\
    \end{tabular}
    }
    \label{fig:derive:single_threaded_machine_inp:crashing}
  }
  \subfigure[][Interfering thread]{
    \texttt{
      \begin{tabular}{rlll}
        \\
        \\
        & \multicolumn{3}{l}{interfering\_thread:} \\
        4008fb: & movq & \$0x0, &global\_ptr\\
        \\
        \\
      \end{tabular}
      }
    \label{fig:derive:single_threaded_machine_inp:interfering}
    }
  \caption{Two fragments of machine code which, when run in parallel,
    have a bug.  The {\StateMachines} for these fragment are shown in
    \autoref{fig:derive:single_threaded_machine_both}.}
  \label{fig:derive:single_threaded_machine_inp}
\end{figure}

\begin{figure}
  \subfigure[][Crashing {\StateMachine} generated from the machine
    code in
    Figure~\ref{fig:derive:single_threaded_machine_inp:crashing},
    assuming that the bug to be investigated is a crash at 4006a7.]{
  \begin{tikzpicture}
    \node (l1) at (0,2) [stateSideEffect] {l1: \stLoad{1}{\mathrm{global\_ptr}} };
    \node (l2) [stateIf, below=of l1] {l2: \stIf{\smTmp{1} = 0}};
    \node (l4) [stateSideEffect, below=of l2] {l4: \stLoad{2}{\mathrm{global\_ptr}} };
    \node (l3) [stateTerminal, right= of l4] {l3: \stSurvive };
    \node (l5) [stateIf, below=of l4] {l5: \stIf{\smBadPtr{\smTmp{2}}}};
    \node (l6) [stateTerminal, below=of l5] {l6: \stCrash};
    \draw[->] (l1) -- (l2);
    \draw[->,ifTrue] (l2) -- (l3);
    \draw[->,ifFalse] (l2) -- (l4);
    \draw[->] (l4) -- (l5);
    \draw[->,ifFalse] (l5) -- (l3);
    \draw[->,ifTrue] (l5) -- (l6);
  \end{tikzpicture}\hspace{-10mm}
  \label{fig:derive:single_threaded_machine}
  }
  \subfigure[][Interfering {\StateMachine}, generated from the machine code in Figure~\ref{fig:derive:single_threaded_machine_inp:interfering}.]{
    \begin{tikzpicture}
      \node (l7) [stateSideEffect] {l7: \stStore{0}{\mathrm{global\_ptr}}};
    \end{tikzpicture}
    \label{fig:derive:single_threaded_machine_write}
  }
  \caption{}
  \label{fig:derive:single_threaded_machine_both}
\end{figure}

Figure~\ref{fig:derive:single_threaded_machine} shows an example of a
simple single-threaded {\StateMachine}\footnote{This is the crashing
  thread component of the simple\_toctou test discussed in more detail
  in Section~\ref{sect:eval:simple_toctou}.}.  It illustrates a simple
time-of-check, time-of-use race: the program loads from
\verb|global_ptr| twice in quick succession, validating the result of
the first and using the result of the second.  It is straightforward
to read off from this diagram that the program might crash if some
other thread modifies \verb|global_ptr| in between the two loads, and
that it will otherwise survive.  Notice that \verb|4006a7|, the
instruction which crashes, is not itself represented in the
{\StateMachine}: by the time that instruction executes, the program is
either doomed to crash or has definitely avoided the bug, and so that
instruction is irrelevant to determining when and whether the bug can
actually happen.

\begin{sidewaysfigure}
  \begin{tikzpicture}
    \node (lA) [stateIf] { \stIf{\happensBefore{\mai{cfg6}{thread1}}{\mai{cfg8}{thread2}}} };
    \node (lB) [stateSideEffect, below = of lA] { l1: \stLoad{1}{\mathrm{global\_ptr}} };
    \node (lCdummy) [below right = of lA] {};
    \node (lC) [stateSideEffect, right = of lCdummy] {l7: \stStore{0}{\mathrm{global\_ptr}} };
    \node (lD) [stateIf, below = of lB] { l2: \stIf{\smTmp{1} = 0} };
    \node (lE) [stateTerminal, below = of lC] { \stUnreached };
    \node (lF) [stateIf, below left = of lD] {\stIf{\happensBefore{\mai{cfg3}{thread1}}{\mai{cfg8}{thread2}}} };
    \node (lG) [stateTerminal, below right = of lD] {\stSurvive};
    \node (lHdummy) [below right = of lF] {};
    \node (lH) [stateTerminal, right = of lHdummy] {\stUnreached};
    \node (lI) [stateSideEffect, below = of lF] {l7: \stStore{0}{\mathrm{global\_ptr}} };
    \node (lJ) [stateSideEffect, below = of lI] {l4: \stLoad{2}{\mathrm{global\_ptr}} };
    \node (lK) [stateIf, below = of lJ] { l5: \stIf{\smBadPtr{\smTmp{2}}} };
    \node (lL) [stateTerminal, below left = of lK] { \stCrash };
    \node (lM) [stateTerminal, below right = of lK] { \stSurvive };
    \draw[->,ifTrue] (lA) -- (lB);
    \draw[->,ifFalse,draw] (lA) -- (lC);
    \draw[->] (lB) -- (lD);
    \draw[->] (lC) -- (lE);
    \draw[->,ifTrue] (lD) -- (lG);
    \draw[->,ifFalse] (lD) -- (lF);
    \draw[->,ifTrue] (lF) -- (lH);
    \draw[->,ifFalse] (lF) -- (lI);
    \draw[->] (lI) -- (lJ);
    \draw[->] (lJ) -- (lK);
    \draw[->,ifTrue] (lK) -- (lL);
    \draw[->,ifFalse] (lK) -- (lM);
  \end{tikzpicture}
  \caption{Cross-product of the {\StateMachines} shown in
    Figures~\ref{fig:derive:single_threaded_machine}
    and~\ref{fig:derive:single_threaded_machine_write}.}
  \label{fig:derive:cross_thread}
\end{sidewaysfigure}

\begin{figure}
  \begin{center}
  \begin{tikzpicture}
    \node (lA) [stateSideEffect] {\stAssert{0 \not= \smLoad{\mathrm{global\_ptr}} \wedge \happensBefore{\mai{cfg6}{thread1}}{\mai{cfg7}{thread2}}} };
    \node (lB) [stateIf, below = of lA] {\stIf{\happensBefore{\mai{cfg3}{thread1}}{\mai{cfg7}{thread2}}} };
    \node (lC) [stateTerminal, below left = of lB] {\stSurvive};
    \node (lD) [stateTerminal, below right = of lB] {\stCrash};
    \draw [->] (lA) -- (lB);
    \draw [->,ifTrue] (lB) -- (lC);
    \draw [->,ifFalse] (lB) -- (lD);
  \end{tikzpicture}
  \end{center}
  \caption{{\STateMachine} from figure~\ref{fig:derive:cross_thread}
    after {\StateMachine} simplification.}
  \label{fig:derive:cross_thread_opt}
\end{figure}

{\STateMachines} become more interesting when they capture the results
of multiple threads.  Figure~\ref{fig:derive:cross_thread} shows an
example of such a {\StateMachine}.  It is the \gls{crossproduct} of
the {\StateMachines} shown in
Figure~\ref{fig:derive:single_threaded_machine_both}.  Note that even
though the {\StateMachine} completely captures the concurrent
behaviour of the two {\StateMachines}, it is itself completely
deterministic, and hence is relatively easy to simplify.  The result
of these simplifications is shown in
Figure~\ref{fig:derive:cross_thread_opt}.  Simplification is generally
cheaper than symbolic execution, and so this can provide a very
worthwhile performance improvement.

\section[Compiling the dynamic \glsentrytext{cfg} to a \StateMachine]{Compiling the dynamic \gls{cfg} to a \StateMachine}
\label{sect:derive:compile_cfg}

I have already described how to build the \gls{crashingthread}'s
dynamic \gls{cfg}.  The next step is to convert that dynamic \gls{cfg}
into a {\StateMachine}.  The {\StateMachine} analysis language is
powerful enough to make translating individual instructions in
isolation completely straightforward\footnote{{\Implementation} uses
  LibVEX \todo{cite} to decode AMD64 machine code before performing
  this compilation.}.  Connecting them together is, however, slightly
more difficult, as the edges in the dynamic \gls{cfg} do not match up
precisely with those in the original program, so that, for instance,
an instruction which would normally have a single successor might have
multiple successors in the dynamic \gls{cfg}, or one which would
normally have multiple successors might only have one, or even none at
all.  There are three cases which require special care:

\begin{itemize}
\item
  Some edges will be erased from the dynamic \gls{cfg}.  For instance,
  in Figure~\ref{fig:unrolled_cyclic_cfg}, the program's original
  \gls{cfg} contained an edge from $C_0$ to $D_0$ but the unrolled
  \gls{cfg} does not include any branches from $C_1$ to a $D$-like
  instructions.  These are converted to branches to the special
  {\stUnreached} state, reflecting the fact that these paths are of no
  interest to the rest of the analysis.

\item
  Some additional edges will have been introduced which do not
  correspond to anything in the original program.  In the example,
  instruction $A_0$ had a single successor, $B_0$, in the original
  program, but has multiple successors in the dynamic \gls{cfg}.  Each
  of the $B_i$ \gls{cfg} nodes will be represented by a separate
  {\StateMachine} state, but there is no condition on the original
  program's state which can be evaluated at $A_0$ which could
  determine which of $B_i$ states the {\StateMachine} must execute
  next.  {\Technique} handles these by converting them into
  \StateMachine-level control flow using
  $\controlEdge{}{}{}$\editorial{Looks a bit ugly like that.}
  expressions which test the program's path through the \gls{cfg}.
  See, for instance, state $A_0'$ in
  Figure~\ref{fig:state_machine_for_cyclic_cfg}\footnote{There is a
    close analogy here with the loop count variables used in
    loop-extended symbolic execution~\cite{Saxena2009}, and in fact an
    early version of {\implementation} was based on LESE.  Switching
    to $\controlEdge{}{}{}$ expressions lead to a modest performance
    improvement due to avoiding some poor interactions with
    {\technique}'s rather simplistic SMT solver.}.

  Note that $\controlEdge{}{}{}$ expressions test control flow within
  the dynamic \gls{cfg} rather than control flow within the original
  program \gls{cfg}.  This can sometimes complicate the design of
  crash enforcers; see Section~\ref{sect:enforce:succ} for more
  details.

\item
  The \gls{cfg} can sometimes have multiple roots, each
  represented by a separate {\StateMachine} state, but the
  {\StateMachine} itself must have a single entry state.  The most
  direct way of handling this would be to build an initial prefix of
  the {\StateMachine} which examines the program state and selects an
  appropriate starting state, but this would be quite awkward.  In
  particular, checking that the calling context is correct would
  require loading a number of stack locations, which would place a
  completely unnecessary additional burden on the alias analysis.
  {\Technique} instead uses special $\entryExpr{}$ expressions which
  are booleans reflecting where the thread entered the \gls{cfg}; see, for
  instance, the start state in
  Figure~\ref{fig:state_machine_for_cyclic_cfg}.

\end{itemize}

As a somewhat unrealistic example, suppose that the \gls{cfg} in
Figure~\ref{fig:cyclic_cfg} had been generated from a program
something like this:

\begin{verbatim}
A: MOV rdx -> rcx
B: LOAD *(rcx) -> rcx
C: JMP_IF_NOT_EQ *(rcx + 8), 0, B
D: STORE $0 -> *(rcx)
\end{verbatim}

The \verb|JMP_IF_NOT_EQ| instruction is supposed to indicate that
\verb|C| loads from the memory at \verb|rcx+8|, jumping to \verb|B| if
it is non-zero and proceeding to \verb|D| otherwise.  This will
produce an unrolled \gls{cfg} as in
Figure~\ref{fig:unrolled_cyclic_cfg}, as already discussed, and a
{\StateMachine} as shown in
Figure~\ref{fig:state_machine_for_cyclic_cfg}.  The generated
{\StateMachine} can then be converted to static single
assignment\needCite{} form and passed to the rest of the analysis.

\begin{figure}
\begin{tikzpicture}
  \node[stateIf,initial above] (l1) {\stIf{\entryExpr{\mai{threadId}{A_0}}}};
  \node[stateSideEffect,below = of l1] (l2) {$A_0$: \state{Copy} $\smReg{rdx}{} \rightarrow \smReg{rcx}{}$};
  \node[stateIf,below = of l2] (l3) {$A_0'$: \stIf{\controlEdge{threadId}{A_0}{B_0}}};
  \node[stateSideEffect,below = of l3] (l4) {$B_0$: \state{Load} $\ast(\smReg{rcx}{}) \rightarrow \smReg{rcx}{}$};
  \node[stateSideEffect,below = of l4] (l5) {$C_0$: \stLoad{}{\smReg{rcx}{}+8}};
  \node[stateIf,below = of l5] (l6) {\stIf{\smTmp{} = 0}};
  \node[stateIf,below = of l6] (l7) {$D_0$: \stIf{\smBadPtr{\smReg{rcx}{}}}};
  \node[stateSideEffect,below right = of l3] (l8) {$B_1$: \state{Load} $\ast(\smReg{rcx}{}) \rightarrow \smReg{rcx}{}$};
  \node[stateSideEffect,below = of l8] (l9) {$C_1$: \stLoad{}{\smReg{rcx}{}+8}};
  \node[stateIf,below = of l9] (l10) {\stIf{\smTmp{} = 0}};
  \node[stateSideEffect,below right = of l1] (l11) {$C_2$: \stLoad{}{\smReg{rcx}{}+8}};
  \node[stateIf,right = of l3] (l12) {\stIf{\smTmp{} = 0}};
  \node[stateTerminal,below = of l7] (lBeta) {\stCrash};
  \node[stateTerminal,right = of lBeta] (lGamma) {\stSurvive};
  \node[stateTerminal,right = of lGamma] (lAlpha) {\stUnreached};
  \draw[->,ifTrue] (l1) -- (l2);
  \draw[->,ifFalse] (l1) -- (l11);
  \draw[->] (l2) -- (l3);
  \draw[->,ifFalse] (l3) -- (l8);
  \draw[->,ifTrue] (l3) -- (l4);
  \draw[->] (l4) -- (l5);
  \draw[->] (l5) -- (l6);
  \draw[->,ifFalse] (l6) -- (lAlpha);
  \draw[->,ifTrue] (l6) -- (l7);
  \draw[->,ifFalse] (l7) -- (lGamma);
  \draw[->,ifTrue] (l7) -- (lBeta);
  \draw[->] (l8) -- (l9);
  \draw[->] (l9) -- (l10);
  \draw[->,ifTrue] (l10) -- (lAlpha);
  \draw[->,ifFalse] (l10) -- (l4);
  \draw[->] (l11) -- (l12);
  \draw[->,ifTrue] (l12.east) to [bend left=45] (lAlpha);
  \draw[->,ifFalse] (l12) -- (l8);
\end{tikzpicture}
\caption{{\STateMachine} generated from the dynamic \gls{cfg} shown in
  Figure~\ref{fig:cyclic_cfg}.}
\label{fig:state_machine_for_cyclic_cfg}
\end{figure}

\subsection{Handling library functions}
\label{sect:derive:library_functions}

{\Implementation} deals with calls to functions in the operating
system standard library by re-implementing approximations of them as
fragments of {\StateMachine}.  These fragments can then be substituted
into {\StateMachines} as they are being built.  In effect, the library
function is treated as a special sort of instruction, and compiled in
exactly the same way as any other instruction\footnote{In particular,
  memory accesses made by the library function are treated as if they
  were made by the call instruction for the purposes of computing the
  interfering stores set and in the dynamic aliasing analysis.}.  This
is generally straightforwards and I do not give full details here.

Library function handling is particularly important when investigating
double-free bugs, as the actual crash will occur in the library
function rather than in the program under investigation.
{\Technique}'s implementation of the \texttt{free} function is shown
in Figure~\ref{fig:library_free}.  The call to \texttt{free} which we
suspect might release something which has already been released, and
hence suffer a double-free crash, uses the crashing \texttt{free}
implementation, while all of the other calls to \texttt{free} use the
non-crashing one\footnote{Note that this includes other calls to
  \texttt{free} in the \gls{crashingthread}.}.

The effects of these implementations are hopefully reasonably clear:
non-crashing \texttt{free}s set the $\mathit{last\_free}$ address to
the pointer which was released, and the crashing \texttt{free} then
asserts that the pointer which it releases is not the one which was
most recently released.  This scheme is clearly not capable of
detecting all possible double-free bugs, but it is sufficient for the
most common kind.

\begin{figure}
  \subfigure[][Crashing \texttt{free}]{
    \begin{tikzpicture}
      \node (l1) [stateSideEffect] {\stLoad{1}{\mathit{last\_free}} };
      \node (l2) [stateIf, below = of l1] {\stIf{\smTmp{1} = \mathit{arg0}} };
      \node (l3) [stateTerminal, below left = of l2] {\stCrash };
      \node (l4) [stateTerminal, below right = of l2] {\stSurvive };
      \draw[->] (l1) -- (l2);
      \draw[->,ifTrue] (l2) -- (l3);
      \draw[->,ifFalse] (l2) -- (l4);
    \end{tikzpicture}
    \label{fig:library_free:crashing}
  }
  \subfigure[][Non-crashing \texttt{free}]{
    \raisebox{17mm}{
    \begin{tikzpicture}
      \node [stateSideEffect] {\stStore{\mathit{arg0}}{\mathit{last\_free}} };
    \end{tikzpicture}
    }
    \label{fig:library_free:non_crashing}
  }

  \caption{{\StateMachine} implementations of the \texttt{free}
    function. $arg0$ is an expression for the platform ABI's first
    argument register; for Linux on AMD64, this is RDI.
    $\mathit{last\_free}$ can be any fixed memory location which is
    not used by the program; {\implementation} uses an address in
    kernel space.}
  \label{fig:library_free}
\end{figure}

\begin{figure}
  \centerline{
    {\hfill}
  \subfigure[][pthread\_mutex\_lock]{
    \begin{tikzpicture}
      \node (l1) [stateSideEffect] {\stStartAtomic};
      \node (l2) [below = of l1, stateSideEffect] {\stLoad{1}{\mathit{arg0}} \{\textsc{mux}\}};
      \node (l3) [below = of l2, stateSideEffect] {\stAssert{\smTmp{1} = 0}};
      \node (l4) [below = of l3, stateSideEffect] {\stStore{\mathit{tid}}{\mathit{arg0}} \{\textsc{mux}\}};
      \node (l5) [below = of l4, stateSideEffect] {\stEndAtomic};
      \draw[->] (l1) -- (l2);
      \draw[->] (l2) -- (l3);
      \draw[->] (l3) -- (l4);
      \draw[->] (l4) -- (l5);
    \end{tikzpicture}
  }{\hfill}
  \subfigure[][pthread\_mutex\_unlock]{
    \begin{tikzpicture}
      \node (l1) [stateSideEffect] {\stStartAtomic};
      \node (l2) [below = of l1, stateSideEffect] {\stLoad{1}{\mathit{arg0}} \{\textsc{mux}\}};
      \node (l3) [below = of l2, stateSideEffect] {\stAssert{\smTmp{1} = \mathit{tid}}};
      \node (l4) [below = of l3, stateSideEffect] {\stStore{0}{\mathit{arg0}} \{\textsc{mux}\}};
      \node (l5) [below = of l4, stateSideEffect] {\stEndAtomic};
      \draw[->] (l1) -- (l2);
      \draw[->] (l2) -- (l3);
      \draw[->] (l3) -- (l4);
      \draw[->] (l4) -- (l5);
    \end{tikzpicture}
  }
    {\hfill}
  }
  \caption{{\StateMachine} models for the pthread\_mutex\_lock and
    pthread\_mutex\_unlock functions.  $\mathit{arg0}$ is an
    expression for the first argument register.  $\smTmp{1}$ is a
    fresh {\StateMachine} temporary.  $\mathit{tid}$ is a constant
    identifying the current thread; for {\implementation}, this is 0
    for the \gls{crashingthread} and 1 for the
    \gls{interferingthread}.  The \textsc{mux} tag on the memory
    accesses is a hint to the alias analysis that these accesses will
    not alias with normal memory accesses.}
  \label{fig:library_mux}
\end{figure}

Figure~\ref{fig:library_mux} shows how the pthread\_mutex\_lock and
pthread\_mutex\_unlock functions are implemented.  Note that the lock
operation does not include any logic to wait for the lock to be
released, but instead simply asserts that it is not currently held.
That is sufficient: the analysis will not consider any paths on which
an assertion fails, and so these library models will ensure that it
does not consider any paths which are prohibited by the mutex
operations.

\section{Simplifying the {\StateMachine}}
\label{sect:derive:simplify_sm}

The {\StateMachines} generated by this process are faithful
representations of the original program's machine code, and therefore
contain large amounts of information which is not relevant to the bug
which is being investigated.  {\Technique} therefore attempts to
simplify them before moving on to the rest of the analysis.  This
simplification process resembles the optimisation passes of an
optimising compiler, and most of the algorithms used by {\technique}
are straightforward conversions of standard compiler algorithms to
this slightly different context.

The most important simplifications used by {\technique} are:

\begin{itemize}
\item Dead code elimination, to eliminate redundant updates to
  registers and {\StateMachine} temporaries.
\item Register copy propagation, using the algorithm from the
  dcc\needCite{} decompiler.  One minor extension present in
  {\implementation} but not dcc is that {\implementation} can make use
  of \state{Assert} side-effects during this transformation, so that,
  for instance, if $x$ is asserted to be less than $7$ then the
  expression $x > 22$ can be rewritten to \false.  This does not
  require any significant changes to the bulk of the algorithm, beyond
  a few simple rules describing when such rewrites are valid.
\item \state{$\Phi$} elimination.  {\STateMachines} are maintained in
  SSA form throughout most of the analysis process, so as to undo the
  effects of compiler register coallescing and aid analysis, but the
  resulting \state{$\Phi$} side-effects are themselves quite difficult
  to analyse.  The problem is that the effect of a \state{$\Phi$}
  side-effect depends on the control-flow context in which it is
  executed, but it does not make that dependency explicit.  The
  \state{$\Phi$} elimination simplification replaces the
  \state{$\Phi$} side-effects with expression BDDs (see
  \autoref{sect:sm_expr_language}) whose internal nodes are the
  program's control flow expressions and whose leaves are the value
  which would be selected by the \state{$\Phi$} in the appropriate
  control flow context.  By making the control flow dependency
  explicit, these make the \state{$\Phi$} much easier to analyse.
\item Aliasing analysis, which determines how \state{Store} and
  \state{Load} operations might interact.  This allows most
  \state{Load}s to be replaced with simpler \state{Copy} operations if
  it can be shown that they always load the results of the same
  \state{Store}, or with \state{Copy}s from $\smLoad{}$ expressions if
  it can be shown that they always load the initial contents of
  memory, and it allows \state{Store}s to be eliminated completely if
  nothing every loads from that memory location.  This simplification
  makes extensive use of the aliasing information in the
  \gls{programmodel}; see \autoref{sect:program_model:dynamic_alias}.
\end{itemize}

There are also various minor peephole simplifications, such as
removing empty \state{Atomic} regions or combining long chains of
related \state{If} operations.

The effect of these simplification passes is to take a {\StateMachine}
which represents all of the program's behaviour in the
\gls{analysiswindow} and transform it to one which represents only the
behaviour which is most relevant to the bug under investigation.  This
reduces the complexity of symbolically executing them, giving a useful
reduction in the total time to analyse a program\footnote{There is a
  close analogy here with program slicing~\cite{Weiser1981}.  The
  important difference is one of intent: program slices are intended
  to make a bug easier for a human to understand, whereas
  {\technique}'s simplifiers are intended to make it easier for a
  symbolic execution engine to process.  This is discussed in more
  detail in \autoref{sect:rw:auto_characterise}.}.

\section{Building the interfering thread's \StateMachines}
\label{sect:derive:write_side}

At this stage, {\technique} has built the crashing thread's
{\StateMachine} for the bug which is to be investigated.  The next
step is to build the interfering thread's {\StateMachine}.  If we did
not care about computational cost, we would now consider every
possible sequence of \gls{alpha} instructions in the program,
convert each to a {\StateMachine}, and consider every possible
interleaving of the crashing thread's {\StateMachine} with each of
these interfering {\StateMachines}.  This would clearly be completely
impractical for all but the most trivial programs.  Fortunately, it is
possible to significantly reduce the set of sequences which must be
considered by using the \gls{programmodel}.

The \gls{programmodel} includes, for each instruction which reads from
memory, a list of all of the instructions which might store to the
same memory location (see
Section~\ref{sect:program_model:dynamic_alias}).  This allows it to
find all of the store operations which might alias with one of the
load operations in the \gls{crashingthread}; these are referred to as
the \glspl{interferingstore} for the crashing thread.  We can then
immediately reduce the set of sequences which need to be considered to
just those which include some instruction from the
\glspl{interferingstore} set, which is already a useful reduction.
Two further observations make it possible to reduce the set of
sequences further.

Most obviously, any instructions in the interfering thread after the
last \gls{interferingstore} cannot possible influence the behaviour of
the crashing thread, and so cannot possibly affect whether the program
crashes.  They can therefore be completely discarded.

Beyond that, it is also possible to discard instructions prior to the
first \gls{communicatinginstruction}.  A
\gls{communicatinginstruction} is either one of the
\glspl{interferingstore} or a load operation which might alias with
one of the stores in the \gls{crashingthread} {\StateMachine}.
Instructions prior to the first communicating instruction can
influence the behaviour of the interfering thread, but only by
restricting the possible values of thread-local state, such as machine
registers or memory locations, when the interfering thread starts.  In
the absence of any such information, {\technique} will consider all
possible starting configurations, and so discarding these instructions
is sound.

On the other hand, it is not always desirable to discard these
instructions, if the restrictions would have provided useful hints to
later phases of the analysis.  For instance, if the bug to be
investigated is a bad pointer dereference, knowing that the value
stored into a shared structure had previously been dereferenced by the
interfering thread, and hence is definitely a valid pointer, is often
useful.  The approach taken by {\implementation} is to first generate
a set of minimal \glspl{cfg} (i.e. ones in which every root is a
communicating instruction) and to then extend them backwards to
include a small amount of additional context, provided that doing so
does not increase the number of paths through the \gls{cfg} or cause
the length of any path to exceed \gls{alpha}.  In other words, a
\gls{cfg} rooted at instruction A will be extended to include its
predecessor instruction B provided that B is A's only predecessor and
provided that the longest path starting at A is at most $\alpha - 1$
instructions long.

The procedure for building interfering {\StateMachines} is then a
variant of that used for building crashing ones: find all of the
potentially interfering store instructions, build an acyclic \gls{cfg} which
includes all traces of appropriate length which start with a
communicating instruction and end with an interfering one, potentially
extend it with a small amount of extra context, and then compile the
\gls{cfg} down to a {\StateMachine}.  The details are, however, slightly
different, and I describe them in the next couple of sections.

There is a slight subtlety in the definition of
\glspl{interferingstore}.  Some of the \state{Load} operations in the
crashing {\StateMachine} will only be needed to evaluate
\state{Assert}ions, or to select between several possible
\state{Assert}ions.  \state{Assert}ions provide hints to later
analysis phases, but do not actually affect the result of the
{\StateMachine}.  It is therefore not entirely clear whether remote
stores which alias only with these \state{Assert}-related loads should
count as interfering.  {\Implementation} defines them to be
non-interfering, and so will not consider races with remote threads
which only influence the effect of \state{Assert} states.  The
implementation of this is reasonably simple: strip all of the
\state{Assert}s from the crashing {\StateMachine}, simplify it as far
as possible, and only consider a store to be interfering if it aliases
with one of the remaining load operations.  Note that this only
applies while looking for \glspl{interferingstore}: if one of these
\state{Assert}-only \glspl{interferingstore} is included in the
\gls{interferingthread} for some other reason (perhaps because it lies
between two non-\state{Assert} \glspl{interferingstore}) then
{\implementation} will consider all interleavings of it with the
\gls{crashingthread}.

\subsection[Building the \glsentrytext{interferingthread} \glsentrytext{cfg}s]{Build the \gls{interferingthread} \glspl{cfg}}

The input to this phase of the analysis is the set of
\glslink{interferingstore}{interfering} and
\gls{communicatinginstruction}{communicating} instructions, and the
analysis must build a collection of dynamic \glspl{cfg} which cover
all possible paths through the program which start with a
\gls{communicatinginstruction}, end with an \gls{interferingstore},
and contain at most \gls{alpha} instructions.  This is easier than
building the crashing thread's \glspl{cfg} in the sense that both ends
of the \gls{cfg} are ``bounded'' by some well-defined instruction,
whereas the crashing thread's \glspl{cfg} potentially extend
arbitrarily far backwards; it is harder in the sense that the
interfering thread \gls{cfg} builder must also cluster the
instructions, deciding which should be included in a single \gls{cfg}
and which analysed independently, whereas the crashing thread's
\glspl{cfg} concern only a single instruction.  The overall result is
that the interfering thread's \glspl{cfg} tend to be significantly
smaller and easier to analyse than crashing thread ones but the
unrolling algorithm itself is itself more involved.

As when building the crashing \gls{cfg}, the first step is to build
the static \gls{cfg}.  This is simple: starting from each potentially
communicating instruction, {\technique} explores forwards for
\gls{alpha} instructions (crossing function boundaries when
necessary), merges the resulting \gls{cfg} fragments, and then
discards any instructions which cannot reach a potentially interfering
store within \gls{alpha} instructions.

The next step is to convert the resulting static \gls{cfg} to a
dynamic one.  As with the crashing \glspl{cfg}, this is accomplished
by duplicating nodes so as to unroll loops until any path which uses
the loop more than once must be longer than \gls{alpha} instructions,
at which point the loop-closing edges can be safely discarded.  There
is, however, one important difference: in the crashing thread
\gls{cfg}, we are interested in any path which terminates at a
specific point, whereas in the interfering thread \gls{cfg} we need to
preserve any path which goes from a communicating instruction to an
interfering store.  This makes it more difficult to determine when a
loop has been unrolled sufficiently, as it is no longer sufficient to
just check the distance to a nominated target instruction.
{\Technique} solves this problem by labelling each node in the graph
with information about where it might occur in an interesting path.
The label on an instruction $l$ then consists of two maps,
$\mathit{min\_from}$ and $\mathit{min\_to}$, defined as:

\begin{enumerate}
\item
  $\mathit{min\_from}_l(c)$ is the number of instructions on the
  shortest path from $c$ to $l$, where $c \in C$, the set of all
  communicating instructions.
\item
  $\mathit{min\_to}_l(i)$ is the number of instructions on the
  shortest path from $l$ to $i$, where $i \in I$, the set of all
  interfering stores and their duplicates.
\end{enumerate}

The length of the shortest path from an instruction $c \in C$ to $i
\in I$ which goes via the instruction $l$ is then
$\mathit{min\_from}_l(c) + \mathit{min\_to}_l(i)$, and so it is safe
to discard any instruction $l$ where
\begin{displaymath}
\min_{c \in C}\left(\mathit{min\_from}_l(c)\right) + \min_{i \in I}\left(\mathit{min\_to}_l(i)\right) > \alpha
\end{displaymath}

The asymmetry, taking the distance from only true communicating
instructions but to any duplicate of an interfering store, is perhaps
surprising.  The key observation is that every path which starts at a
duplicated communicating instruction will have a matching path which
starts at the original communicating instruction, and so the ones
which start at the duplicate instruction are redundant\footnote{The
  symmetrical statement is also true: every path which ends in a
  duplicate interfering store has a matching path which ends at a true
  interfering store.  It would therefore also be correct to discard
  paths which end at a duplicate interfering store.  It would not,
  however, be correct to combine the two observations and discard all
  paths which either start with a duplicate communicating instruction
  or end with duplicate interfering stores, as there would then be
  little point in having those duplicates.}.

\begin{figure}
\begin{algorithmic}
  \State {Compute initial labelling of graph}
  \For {$t$ in the set of potentially-relevant stores}
    \While {graph rooted at $t$ is not cycle-free}
       \State $\mathit{edge} \gets \textsc{findEdgeToBeBroken}(t, \{\})$
       \State $\mathit{newLabel} \gets \textsc{combineLabels}(\text{current label of } \mathit{edge}.\mathit{start}, \text{current label of } \mathit{edge}.\mathit{end})$
       \If {$\min_c(\mathit{min\_from}_{\mathit{newLabel}}(c)) + \min_i(\mathit{min\_to}_{\mathit{newLabel}}(i)) > \alpha$}
           \State {remove $\mathit{edge}$}
       \Else
           \State $\mathit{newNode} \gets \text{duplicate } \mathit{edge}.\mathit{end}$
           \For {Edges $e$ leaving $\mathit{edge}.\mathit{end}$}
              \State {Create a new edge from $\mathit{newNode}$ to $e.\mathit{end}$}
           \EndFor
           \State {Set label of $\mathit{newNode}$ to $\mathit{newLabel}$}
           \State {Replace $\mathit{edge}$ with an edge from $\mathit{edge}.\mathit{start}$ to $\mathit{newNode}$}
           \State {Recalculate $\mathit{min\_from}$ for $\mathit{edge}.\mathit{end}$ and its successors, if necessary}
       \EndIf
    \EndWhile
  \EndFor
\end{algorithmic}
\caption{Loop unrolling algorithm for interfering thread CFGs.
  \textsc{findEdgeToBeBroken} and \textsc{combineLabels} are described
  in the text.}
\label{fig:derive:store_cfg_unroll_alg}
\end{figure}

The complete algorithm is shown in
Figure~\ref{fig:derive:store_cfg_unroll_alg}.  Note that in this
algorithm duplicating a node duplicates its \emph{outgoing} edges,
whereas when building a crashing thread \gls{cfg} the \emph{incoming} edges
are duplicated.  This reflects the fact that interfering thread \glspl{cfg}
are built up forwards from the communicating instructions while crashing
thread \glspl{cfg} are built up backwards from the target instruction.

The algorithm relies on two utility functions:

\begin{itemize}
\item \textsc{findEdgeToBeBroken} just finds the closing edge of some
  cycle in the graph.  The precise choice of edge is not
  important\editorial{I \emph{think} it'll converge on the same thing
    regardless, but it might be nice to show that.  It's certainly
    guaranteed to be correct, but confluence would also be a nice
    property.}.  In {\implementation}'s implementation, this is a
  breadth-first search starting from some arbitrarily chosen root of
  the \gls{cfg} and reporting the first edge to close a cycle.  If the graph
  reachable from that root is acyclic then {\implementation} moves on
  to the next root.  If the sub-graph reachable from every root is
  acyclic then the whole graph is acyclic and nothing more needs to be
  done.
\item \textsc{combineLabels} is also simple, and is responsible for
  computing the label for the new node which would be produced by
  duplicating $\mathit{edge}.\mathit{end}$.  This node will have the
  same outgoing edges as $\mathit{edge}.\mathit{end}$, and so the same
  $min\_to$ label, and a single incoming edge from
  $\mathit{edge}.\mathit{start}$, and hence a $\mathit{min\_from}$
  label which is just $\mathit{edge}.\mathit{start}$'s
  $\mathit{min\_from}$ with one added to every value.
\end{itemize}

The resulting \gls{cfg} can then be compiled to a {\StateMachine} in
the same way as a \gls{crashingthread}'s \gls{cfg} is.  The only major
difference is that the \gls{interferingthread}'s \glspl{cfg} can
sometimes contain disjoint components, in which case each such
component is compiled to a separate {\StateMachine}.

As an example, consider this cyclic \gls{cfg}:

\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$} edge [in=30,out=-30,loop] ();
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
             & \multicolumn{1}{c}{$\mathit{min\_to}$} & \multicolumn{2}{c}{$\mathit{min\_from}$} & overall min\\
             & $C$ & $A$ & $C$ \\
      $A$    & 2   & 0   & 1 & 2\\
      $B$    & 1   & 1   & 2 & 2\\
      $C$    & 0   & 2   & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}

$A$, in blue, is a communicating instruction and $C$, in green, is an
interfering instruction.  The overall min column is the minimum
$\mathit{min\_to}$ value plus the minimum $\mathit{min\_from}$ one; it
gives the number of edges on the shortest path involving a given node
which starts at a true interfering instruction and ends at any
interfering instruction, whether true or a duplicate.  Assume, for the
purposes of the example, that \gls{alpha} is five.  A
depth-first search starting at $A$ will find the cycle from $B$ back
to itself and attempt to break that cycle by duplicating $B$.  The
resulting graph will look like this:

\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$} edge [in=210,out=150,loop,killEdge] ();
  \node (B1) [NewCfgInstr, right=of B] {$B_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,swungEdge] (B1) to [bend left=10] (B);
  \draw[->] (B1) -- (C);
  \draw[->] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
            & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$ \\
      $A$   & 2   & 0   & 1 & 2\\
      $B$   & 1   & 1   & 2 & 2\\
      $B_1$ & 1   & 2   & 3 & 3\\
      $C$   & 0   & 2   & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}

New nodes are shown in red, as is the edge which is modified, and
edges which have been removed are shown crossed through.  Notice that
whereas the shortest cyclic path starting at $A$ was previously
$A$,$B$,$B$, of length 3, it is now $A$, $B$, $B_1$, $B$, of length 4.
Suppose that the next depth-first iteration discovers the edge from
$C$ to $A$.  The algorithm will then break this edge by duplicating
$A$:

\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (A1) [NewCfgInstr,right=of C] {$A_1$};
  \draw[->] (A) -- (B);
  \draw[->,swungEdge] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->] (B1) -- (C);
  \draw[->] (B1) to [bend left=10] (B);
  \draw[->] (C) -- (A1);
  \draw[->,killEdge] (C) to [bend left=90] (A) node (edge1) [right,midway] {~~~~~~~~};
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (B) (B1) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
      labels & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2   & 0   & $\infty$ & 2\\
      $A_1$ & 2   & 3   & 1        & 3\\
      $B$   & 1   & 1   & 2        & 2\\
      $B_1$ & 1   & 2   & 3        & 3\\
      $C$   & 0   & 2   & 0        & 0\\
    \end{tabular}
  };
\end{tikzpicture}

Suppose it now selects the $B_1$ to $B$ edge as the cycle-completing
edge.  It will then duplicate $B$:

\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [NewCfgInstr, right=of B1] {$B_2$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B);
  \draw[->] (B) -- (C);
  \draw[->] (B) to [bend left=10] (B1);
  \draw[->,killEdge] (B1) to [bend left=10] (B);
  \draw[->,swungEdge] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
            & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2   & 0   & $\infty$ & 2\\
      $A_1$ & 2   & 3 & 1 & 3\\
      $B$   & 1   & 1 & 2 & 2\\
      $B_1$ & 1   & 2 & 3 & 3\\
      $B_2$ & 1   & 3 & 4 & 4\\
      $C$   & 0   & 2 & 0 & 0\\
    \end{tabular}
  };
\end{tikzpicture}

The length of the shortest cyclic path start at $A$ has again
increased, this time from four to five.  Now duplicate $B$ because of
the $A_1$ to $B$ cycle-completing edge:

\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [NewCfgInstr, below=of A1] {$B_3$};
  \draw[->] (A) -- (B);
  \draw[->,killEdge] (A1) -- (B);
  \draw[->,swungEdge] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
      labels & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $A$ & $C$\\
      $A$   & 2 & 0 & $\infty$ & 2\\
      $A_1$ & 2 & 3 & 1        & 3\\
      $B$   & 1 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 2 & 3        & 3\\
      $B_2$ & 1 & 3 & 4        & 4\\
      $B_3$ & 1 & 4 & 2        & 3\\
      $C$   & 0 & 2 & 0        & 0\\
    \end{tabular}
  };\smh{overall min hangs over right hand margin}
\end{tikzpicture}

The next cycle-completing edge considered is that from $B_2$ to $B_1$.
In this case, the new label would have an overall minimum of 5,
matching \gls{alpha}, and so there can be no paths through the
new node which start with a communicating instruction and which end at
an interfering store or a duplicate of it, and so the edge is simply
deleted:

\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [CfgInstr, below=of A1] {$B_3$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) to [bend left=10] (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) to [bend left=10] (B1);
  \draw[->,killEdge] (B2) to [bend left=10] (B1);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (C);
  \draw[->] (B3) to [bend right=45] (B1);
  \draw[->] (C) -- (A1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lcccc}
         & \multicolumn{1}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
         & $C$ & $A$ & $C$\\
      $A$   & 2 & 0 & $\infty$ & 2\\
      $A_1$ & 2 & 3 & 1        & 3\\
      $B$   & 1 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 2 & 3        & 3\\
      $B_2$ & 1 & 3 & 4        & 4\\
      $B_3$ & 1 & 4 & 2 & 3\\
      $C$  & 0 & 2 & 0        & 0\\
      New label & 1 & 4 & 5 & 5\\
    \end{tabular}
  };
\end{tikzpicture}

This process iterates, removing one cycle-completing edge at a time,
until the graph is completely acyclic:

\begin{tikzpicture}
  \node (A) at (0,2) [CommCfgInstr] {$A$};
  \node (B) [CfgInstr, below=of A] {$B$};
  \node (B1) [CfgInstr, right=of B] {$B_1$};
  \node (B2) [CfgInstr, right=of B1] {$B_2$};
  \node (A1) [DupeCommCfgInstr,right=of C] {$A_1$};
  \node (C) [InterferingCfgInstr, below=of B] {$C$};
  \node (B3) [CfgInstr, below=of A1] {$B_3$};
  \node (C1) [DupeInterferingCfgInstr, below=of B3] {$C_1$};
  \node (B4) [CfgInstr, right=of B3] {$B_4$};
  \draw[->] (A) -- (B);
  \draw[->] (A1) -- (B3);
  \draw[->] (B) -- (C);
  \draw[->] (B) -- (B1);
  \draw[->] (B1) -- (B2);
  \draw[->] (B1) -- (C);
  \draw[->] (B2) -- (C);
  \draw[->] (B3) -- (B4);
  \draw[->] (C) -- (A1);
  \draw[->] (B3) -- (C1);
  \draw[->] (B4) -- (C1);
  \begin{pgfonlayer}{bg}
    \node(box1) [fill=black!10,fit=(A) (A1) (B) (B1) (B2) (B3) (C) (C1) (edge1)] {};
  \end{pgfonlayer}
  \draw node [right=of box1] {
    \begin{tabular}{lccccc}
            & \multicolumn{2}{l}{$\mathit{min\_to}$} & \multicolumn{2}{l}{$\mathit{min\_from}$} & overall min\\
            & $C$ & $C_1$ & $A$ & $C$ \\
      $A$   & 2 & 5 & 0 & $\infty$ & 2\\
      $A_1$ & $\infty$ & 2 & 3 & 1 & 3\\
      $B$   & 1 & 4 & 1 & $\infty$ & 2\\
      $B_1$ & 1 & 4 & 2 & $\infty$ & 3\\
      $B_2$ & 1 & 4 & 3 & $\infty$ & 4\\
      $B_3$ & $\infty$ & 1 & 4 & 2 & 3\\
      $B_4$ & $\infty$ & 1 & 5 & 3 & 4\\
      $C$   & 0 & 3 & 2 & 0 & 0\\
      $C_1$ & $\infty$ & 0 & 5 & 3 & 3\\
    \end{tabular}
  };
\end{tikzpicture}

As desired, the graph has been rendered acyclic while preserving all
paths of length up to five instructions.  As a minor optimisation,
{\implementation} will merge node $B_2$ with $B_4$ and $C_3$ with
$C_2$ before converting the \gls{cfg} to a \StateMachine, as the nodes are
semantically identical and this results in a slightly simpler
\StateMachine.


\section{Generating a verification condition}
\label{sect:using:check_realness}

Previous sections have described how to generate pairs of
{\StateMachines} representing fragments of the program which might
interact in interesting ways when run concurrently.  The next step is
take each of those pairs and, for each, determine whether running the
two {\StateMachines} in parallel might lead to a crash, and if so
determine under what circumstances it might do so.

The core of the approach is to take a pair of {\StateMachines}, one
representing the crashing thread and the other representing the
interfering one, and convert them into a
\gls{verificationcondition}\cite{Floyd1967} using symbolic
execution\cite{King1976}.  This \gls{verificationcondition} is
satisfiable precisely when running the fragments of program
represented by the two {\StateMachines} in parallel might cause to the
bug being investigated to reproduce.  This technique is complete in
the sense that if it reports that the program has no bugs then it
definitely does not have any of the form being investigated, provided
that the dynamic analysis is complete and no part of the analysis
times out.  It is not, however, sound, in the sense that some of the
bugs reported might not actually be reproducible, because it ignores
most of the program's existing synchronisation, and this can lead to a
significant number of false positives.
Section~\ref{sect:reproducing_bugs} describes one possible way of
making it sound, at the expense of a significant loss of completeness.

In addition to the verification condition itself, this phase also
generates an \gls{inferredassumption}\editorial{I should probably be
  citing something for that}: a condition which expresses when the
{\StateMachines} would not predict a crash if run atomically, in
either order.  A potential crash is only reported if both the
\gls{inferredassumption} and the \gls{verificationcondition} might be
true simultaneously.  For instance, it might be that the crashing
thread will dereference a pointer stored in a particular stack
location, regardless of the actions of the interfering thread.  The
inferred assumption will then show that that stack location must
contain a valid pointer, and so {\technique} will not report any
potential crashes which depend on it containing an invalid pointer.
This is consistent with the goal of only reporting concurrency bugs,
and not simply any kind of bug in the program.

\subsection{Symbolically executing {\StateMachines}}
\label{sect:derive:symbolic_execute}

{\Implementation} uses a simple symbolic execution engine to evaluate
{\StateMachines} and determine when they will crash.  The details of
this are, for the most part, quite conventional, and I give only a
brief overview of the most important features here:

\begin{itemize}
\item The symbolic execution engine considers only a single
  {\StateMachine} at a time, even when investigating the parallel
  behaviour of two threads.  Rather than investigating interleavings
  of the threads in the symbolic execution engine, as is done in, for
  instance, ESD\cite{Zamfir2010}, {\technique} instead encodes them
  into special cross-product {\StateMachines}, described in
  \autoref{sect:using:build_cross_product}.

\item The program's memory is represented by the sequence of update
  operations, rather than attempting to maintain separate models for
  particular objects or memory locations.  In effect, the whole of
  memory is modelled as a single array using McCarthy's theory of
  arrays\needCite{}.  \state{Store} operations are then implemented by
  simply adding them to the update list, whereas \state{Load}
  operations have to scan back through the list to find a matching
  \state{Store}.

  This is not a particularly efficient approach.  {\Implementation}
  relies on two facts to mitigate the problems.  First, where the
  relationship between \state{Store}s and \state{Load}s is simple the
  {\StateMachine} simplifiers will forward data between them before
  the symbolic execution starts, eliminating both from the
  {\StateMachine}.  Second, {\implementation} maintains a cache of
  previous aliasing queries, and so if, for instance, two paths
  through the {\StateMachine} both need to determine whether
  \state{Store} A might alias with \state{Load} B the symbolic
  execution engine usually only needs to do so once.

\item Aliasing queries are resolved lazily.  This means that if the
  engine must execute a \state{Load} operation and cannot immediately
  determine which \state{Store} operation to use, it does not cause an
  immediate fork of its state, but instead causes the \state{Load} to
  return an expression BDD (see \autoref{sect:sm_expr_language}) whose
  internal nodes describe the alias query and whose leaves select the
  appropriate result.

\item The engine does not use any kind of incremental abstraction
  technique such as CEGAR\cite{Clarke2000}.  This is primarily because
  of the use of a flat memory representation: memory is a single
  object, and so it makes little sense to talk about modelling one
  part of it accurately and another part inaccurately, and without
  that CEGAR would provide little benefit.  The use of lazy aliasing
  resolution provides some of the benefit of CEGAR, as it allows some
  aliasing queries which do not affect program behaviour to be
  skipped\editorial{Might want an example of that?  It's not very
    important, and a pain to explain properly.}.

\item Memory operations, in addition to their main function of
  accessing memory, are also used to infer that certain pointers are
  definitely valid.  For instance, if the engine executes a
  \state{Load} operation which loads from memory with a location given
  by $\mathit{expr}$ it will conclude that $\mathit{expr}$ is a valid
  pointer and ensure that the expression $\smBadPtr{\mathit{expr}}$
  always evaluate to {\false}.  This means that if {\technique} is
  investigating a bad pointer dereference and some fragment of program
  dereferences the same pointer multiple times, it will only ever
  report a bug for the first such dereference operation.

\item Unlike most symbolic execution engines, the one used by
  {\implementation} does not attempt to detect when it revisits a
  previously-visited configuration.  This is safe because
  {\StateMachines} are acyclic: any path through a {\StateMachine} can
  visit a given state at most once, and so there is no possibility of
  a single revisiting a configuration and entering an infinite loop.
  It is also, surprisingly, reasonably performant, because it is
  extremely rare for multiple paths to visit the same configuration,
  and so there is little scope for re-using configurations to reduce
  duplicated work.  This is largely because {\technique} simplifies
  the {\StateMachine} before attempting to symbolically execute it and
  these simplifications tend to remove most easily-exploited forms of
  redundancy.
\end{itemize}

\subsection{Deriving the inferred assumption}

\label{sect:derive:inferred_assumption}

The \gls{inferredassumption} is a condition on the program's state
which ensures that the \glslink{crashingthread}{crashing} and
\glslink{interferingthread}{interfering} threads do not crash when run
atomically, in either order.  It has two parts: the \gls{ci-atomic}
constraint, which gives the conditions under which running the
\glslink{crashingthread}{crashing} {\StateMachine} and then the
\glslink{interferingthread}{interfering} {\StateMachine} survives, and
\gls{ic-atomic}, which gives the conditions under which running the
\glslink{interferingthread}{interfering} {\StateMachine} and then the
\glslink{crashingthread}{crashing} one survives.  Each sub-condition
is formed by concatenating the two input {\StateMachines} in the
appropriate order, symbolically executing them, and then taking the
disjunction of all paths which end in the {\stSurvive}
state\footnote{Note, in particular, that paths which end at
  {\stUnreached} are not included in the \gls{inferredassumption}.}.
The \gls{inferredassumption} itself is then formed from the
conjunction of these two sub-constraints.

Concatenating {\StateMachines} is straightforward: simply take the
first {\StateMachine} and replace all of its {\stSurvive} states
with branches to the start of the second {\StateMachine}.

\todo{Bad para break.} One might reasonably ask why building the
composite {\StateMachine} is superior to simply symbolically executing
one {\StateMachine} until it reaches the {\stSurvive} state and then
starting the other {\StateMachine} in the resulting configuration.
This would correctly implement the desired behaviour, and would be
somewhat simpler to implement.  The great advantage of building a
composite {\StateMachine} is that the composite {\StateMachine} can be
simplified using the standard {\StateMachine} simplification passes,
which usually reduces the complexity of symbolic execution by a useful
amount, even when the input {\StateMachines} were themselves
simplified as far as possible.  This is because the composite
{\StateMachine} is ``closed'': it contains all potentially relevant
operations, and so the simplification passes can assume that there are
no potentially interfering operations in another thread.  This gives
the simplification passes far more scope to eliminate memory accesses.
The result is that symbolically executing the simplified composite
{\StateMachine} is almost always much faster than executing the input
{\StateMachines} in turn.

This is a similar argument to that made previously by Khoo et
al\cite{Khoo2010}.  In that work, the authors improved the efficiency
of a symbolic execution engine by combining it with a much simpler and
faster type checking system, producing something with the precision of
a symbolic execution engine and performance similar to the type
checker.  In much the same way, building the composite {\StateMachine}
and simplifying it before symbolic execution starts is far cheaper
than trying to symbolically execute the {\StateMachines} in isolation
and then stitch them together via the end and start symbolic states.
\todo{I really don't have a great deal of evidence for that.}

\subsection{Building cross-product {\StateMachines}}
\label{sect:using:build_cross_product}

The symbolic execution engine is only capable of exploring one
{\StateMachine} at a time, and so if cross-thread behaviours are to be
investigated then the two single-threaded {\StateMachines} must be
combined into a single multi-threaded one.  This new {\StateMachine}
must capture the complete behaviour of every possible interaction the
two input {\StateMachines}, so that any possible interleaving of their
states is represented by a path through the combined {\StateMachine}.
The approach used by {\technique} is essentially to take a
cross-product of the two {\StateMachines}.  This is a new
{\StateMachine} whose states correspond to pairs of states in the
input {\StateMachine} and whose transitions correspond to advancing
one or other of the two input {\StateMachines}.

There is a close analogy here with the finite state machine
cross-product operation, which is a new FSM whose states correspond to
pairs of states in the input FSMs and whose transitions correspond to
advancing one or other of the input FSMs.  The important difference is
that, in an FSM cross-product, the two FSMs are independent and cannot
influence each other's behaviour, whereas in a {\StateMachine} cross
product they can do, and those influences are precisely what the cross
product is intended to capture.

There are some important subtleties involved in building the
cross-product {\StateMachine}:

\begin{itemize}
\item The input {\StateMachines} might contain {\stStartAtomic} and
  {\stEndAtomic} blocks, which are intended to indicate that some
  fragment of the {\StateMachine} executes atomically.  It is
  important that the output {\StateMachine} is constructed in a way
  which respects those blocks.
\item There will be many pairs of states in the two input
  {\StateMachines} which cannot interact with each other in any way,
  and so can be re-ordered without changing any interesting behaviour.
  The output {\StateMachine} would ideally be constructed in a way
  which avoids forcing later analyses to consider both orderings.  In
  general, there may be many partial-order redundancies\cite{Alur1997}
  between the two {\StateMachines}, and, for efficiency reasons, we
  would like to remove as many of these as possible as early as
  possible.
\item The \gls{crossproduct} {\StateMachine} is only used to
  investigate cases in which the two input {\StateMachines} are
  interleaved, as the cases where they execute atomically will have
  already been considered in the \gls{inferredassumption}.  The
  \gls{crossproduct} {\StateMachine} should therefore be built in a
  way which excludes paths where the input {\StateMachines} run
  atomically.
\end{itemize}

\begin{figure}
  \begin{displaymath}
    \textsc{configuration} = \left(\begin{array}{rrll}
      \multirow{2}{*}{\bigg\{} & \mathit{crashingState}: & \textsc{{\StateMachine} state}, & \multirow{2}{*}{\bigg\},}\\
                               & \mathit{crashingHasIssued}: & \textsc{Bool}\\
      \multirow{2}{*}{\bigg\{} & \mathit{interferingState}: & \textsc{{\StateMachine} state}, & \multirow{2}{*}{\bigg\},} \\
                               & \mathit{interferingHasIssued}: & \textsc{Bool}\\
      \multicolumn{2}{r}{\mathit{atomic}:} & \multicolumn{2}{l}{\{ \varnothing, \mathit{crashing}, \mathit{interfering} \}}
    \end{array}\right)
  \end{displaymath}
  \caption{\textsc{configuration} type for the cross-product algorithm.}
  \label{fig:cross_product:configuration}
\end{figure}

The algorithm used is actually rather simple.  The core idea is to
identify each state of the output {\StateMachine} with a particular
configuration of the two input {\StateMachines}, where the
configuration contains all of the necessary information about the
concurrent interleaving of the two {\StateMachines} and nothing else.
More concretely, a configuration consists of a field showing which, if
either, of the input {\StateMachines} are currently in atomic blocks,
and, for each input {\StateMachine}, that {\StateMachine}'s current
state and a flag indicating whether it has issued any memory accesses.
The output {\StateMachine} starts in a configuration with neither
input {\StateMachine} in an atomic block, both {\StateMachines} at
their initial state, and, and with neither having issued any memory
accesses.  The algorithm then exhaustively explores every
configuration reachable from this one, building the output
{\StateMachine} as it goes.  The rules for advancing from a
configuration are quite simple, and are best explained by means of an
example.  Consider, for instance, the {\StateMachines} shown in
Figure~\ref{fig:cross_product_input}.  $y$ here is supposed to
indicate some value which is local to the interfering thread
{\StateMachine} and $x$ some global variable in memory.  These produce
the cross-product {\StateMachine} shown in
Figure~\ref{fig:cross_product_output}\footnote{These {\StateMachines}
  have no atomic blocks, and so the state related to atomic blocks is
  not shown.}.  I highlight a couple of the more important features of
this diagram:

\begin{figure}
  \begin{subfloat}
    \begin{tikzpicture}
      \node[stateSideEffect,initial above] (lA) {$A$: \stLoad{1}{x} };
      \node[stateIf,below = of lA] (lB) {$B$: \stIf{\smTmp{1} = 0} };
      \node[stateSideEffect,below = of lB] (lC) {$C$: \stLoad{2}{x} };
      \node[stateIf,below = of lC] (lD) {$D$: \stIf{\smBadPtr{\smTmp{2}}} };
      \node[stateTerminal,below = of lD] (lH) {$H$: \stCrash };
      \node[stateTerminal,right = 0.5 of lC] (lG) {$G$: \stSurvive };
      \draw[->] (lA) -- (lB);
      \draw[->,ifTrue] (lB) -- (lG);
      \draw[->,ifFalse] (lB) -- (lC);
      \draw[->] (lC) -- (lD);
      \draw[->,ifTrue] (lD) -- (lH);
      \draw[->,ifFalse] (lD) -- (lG);
    \end{tikzpicture}
    \caption{Crashing thread {\StateMachine} }
  \end{subfloat}
  \hspace{-5mm}
  \begin{subfloat}
    \begin{tikzpicture}
      \node[stateIf,initial above] (lE) {$E$: \stIf{y \not= 0}};
      \path (node cs:name=lE) ++(2.2,-1.5) node [stateSideEffect] (lF) {$F$: \stStore{0}{x}};
      \node[stateTerminal,below = 2 of lE] (lI) {$I$: \stSurvive };
      \draw[->,ifTrue] (lE) -- (lI);
      \draw[->,ifFalse] (lE) -- (lF);
      \draw[->] (lF) -- (lI);
    \end{tikzpicture}
    \caption{Interfering thread {\StateMachine} }
  \end{subfloat}
  \caption{A pair of {\StateMachines}.  $x$ is a global memory
    location.  Figure~\ref{fig:cross_product_output} shows their
    cross-product.}
  \label{fig:cross_product_input}
\end{figure}

\begin{figure}
  \begin{tikzpicture}[align=center]
    \node[stateIf, initial above] (A) {(\{$A$, {\false}\}, \{$E$, {\false}\})\\\stIf{y \not= 0} };
    \node[stateTerminal, right = of A] (B) {(\{$A$, {\false}\}, \{$I$, {\false}\})\\{\stUnreached} };
    \node[stateIf, below = of A] (C) {(\{$A$, {\false}\}, \{$F$, {\false}\})\\\stIf{\happensBefore{A}{F}} };
    \node[stateSideEffect, below = of C] (D) {$A$\\\stLoad{1}{x} };
    \node[stateSideEffect, right = of C] (E) {$F$\\\stStore{0}{x} };
    \node[stateIf, below = of D] (F) {(\{$B$, {\true}\}, \{$E$, {\false}\})\\\stIf{\smTmp{1} = 0} };
    \node[stateTerminal, below = of E] (G) {(\{$A$, {\false}\}, \{$I$, {\true}\})\\\stUnreached };
    \node[stateTerminal, right = of F] (H) {(\{$G$, {\true}\}, \{$E$, {\false}\})\\\stUnreached };
    \node[stateIf, below = of F] (I) {(\{$C$, {\true}\}, \{$E$, {\false}\})\\\stIf{\happensBefore{C}{F}} };
    \node[stateSideEffect, below = of I] (J) {$C$\\\stLoad{2}{x}};
    \node[stateSideEffect, right = of I] (K) {$F$\\\stStore{0}{x}};
    \node[stateIf, below = of J] (L) {(\{D, {\true}\}, \{$E$, {\false}\})\\\stIf{\smBadPtr{\smTmp{2}}}};
    \node[stateSideEffect, right = of K] (M) {(\{$C$, {\true}\}, \{$I$, {\true}\})\\\stLoad{2}{x}};
    \node[stateTerminal, below = of L] (N) {(\{$H$, {\true}\}, \{$E$, {\false}\})\\\stUnreached };
    \node[stateTerminal, right = of N] (O) {(\{$G$, {\true}\}, \{$E$, {\false}\})\\\stUnreached };
    \node[stateIf, below = of M] (P) {(\{D, {\true}\}, \{$I$, {\true}\})\\\stIf{\smBadPtr{\smTmp{2}}} };
    \node[stateTerminal] at (5.5,-14.8) (Q) {(\{$G$, {\true}\}, \{$I$, {\true}\})\\\stSurvive };
    \node[stateTerminal, below = of P] (R) {(\{$G$, {\true}\}, \{$I$, {\true}\})\\\stCrash };
    \draw[->,ifTrue] (A) -- (B);
    \draw[->,ifFalse] (A) -- (C);
    \draw[->,ifTrue] (C) -- (D);
    \draw[->,ifFalse] (C) -- (E);
    \draw[->] (D) -- (F);
    \draw[->] (E) -- (G);
    \draw[->,ifTrue] (F) -- (H);
    \draw[->,ifFalse] (F) -- (I);
    \draw[->,ifTrue] (I) -- (J);
    \draw[->,ifFalse] (I) -- (K);
    \draw[->] (J) -- (L);
    \draw[->] (K) -- (M);
    \draw[->,ifTrue] (L) -- (N);
    \draw[->,ifFalse] (L) -- (O);
    \draw[->] (M) -- (P);
    \draw[->,ifTrue] (P) -- (Q);
    \draw[->,ifFalse] (P) -- (R);
  \end{tikzpicture}
  \caption{Cross product of the {\StateMachines} shown in
    Figure~\ref{fig:cross_product_input}.}
  \label{fig:cross_product_output}
\end{figure}

\begin{itemize}
\item The {\StateMachine} starts in the configuration (\{$A$,
  {\false}\}, \{$E$, {\false}\}), indicating that the crashing thread
  {\StateMachine} is in state $A$, is not in an atomic block, and has
  not issued any memory accesses, and that the interfering thread
  {\StateMachine} is in state $E$, is not in an atomic block, and has
  not issued any memory accesses.  The first state in the interfering
  thread is an \state{If}, which is always thread-local, and so it is
  able to advance immediately.  The resulting state in the output
  {\StateMachine} is therefore an \state{If} state testing the same
  condition as the input \state{If}.  The successor states in the
  output state are then the starting output state with the interfering
  thread's state updated to reflect the result of the conditional
  test.  In other words, when the condition is true, the output
  {\StateMachine} moves to configuration (\{$A$, {\false}\}, \{$I$,
  {\false}\}), because the interfering thread {\StateMachine} will
  move from state E to state I, and when the condition is false the
  output {\StateMachine} moves to configuration (\{$A$, {\false}\},
  \{$F$, {\false}\}), because the interfering thread {\StateMachine}
  moves from $E$ to $F$.

  There is a slight ambiguity here which occurs if both input
  {\StateMachines} are at purely local states.  In that case,
  {\technique} prefers to advance the {\StateMachine} whose current
  state has the smallest number of successor states, as this tends to
  result in the smallest possible output {\StateMachine}.  When both
  input {\StateMachines} have the same number of successor states it
  advances the crashing thread {\StateMachine}.

\item One of the successors of the initial configuration is (\{$A$,
  {\false}\}, \{$I$, {\false}\}).  The interfering thread
  {\StateMachine} has now reached a terminal state, $I$, without
  either input {\StateMachine} issuing any memory accesses.  This
  indicates that both {\StateMachines} have completed atomically,
  which is not an interesting case to investigate, and so {\technique}
  instantiates this output state as {\stUnreached}.  The later
  analysis phases will therefore not investigate this path.  In
  general, any time one of the input {\StateMachines} reaches a
  terminal state and either input {\StateMachine} has failed to issue
  an access the output state should be {\stUnreached}.

\item In the other successor of the initial state, (\{$A$, {\false}\},
  \{$F$, {\false}\}), both {\StateMachines} have reached a memory
  accessing state.  {\Technique} now looks ahead in both input
  {\StateMachines} to determine whether the accesses might involve
  some cross-{\StateMachine} interaction (see below for more details
  of this process).  In this case, they can interact, and so the
  cross-product algorithm uses a happens-before test
  $\happensBeforeEdge{}$ to consider both possible orderings of the
  accesses.  If $\happensBefore{A}{F}$ then the memory access in state
  $A$ should happen before that in state $F$, and so the output
  {\StateMachine} issues the state A access and moves to configuration
  (\{$B$, {\true}\}, \{$F$, {\false}\}).  Otherwise, $F$ should happen
  before $A$, and so it issues the state $F$ access and moves to
  (\{$A$, {\false}\}, \{$I$, {\true}\}).  Other
  potentially-interfering memory accesses are handled similarly.

\item In the configuration (\{$C$, {\true}\}, \{$I$, {\true}\}),
  the crashing thread {\StateMachine} is at a memory accessing state
  $C$ while the interfering thread {\StateMachine} is at a terminal
  state $I$.  There can therefore be no interaction between the access
  at $C$ and any future accesses issued by the interfering thread, and
  so the output {\StateMachine} can issue the $C$ access without needing
  any further $\happensBeforeEdge$ tests.

\item The $F$ state has been duplicated into two places in the output
  {\StateMachine}.  In this case, this is not actually a problem, but
  if F had been a variable-defining state such as \state{Load} or
  \state{Copy} then this would have broken the SSA form.  {\Technique}
  uses a further post-processing step to restore the SSA invariant,
  introducing additional variables and \state{$\Phi$} side-effects as
  necessary.
\end{itemize}

Importantly, the cross-product {\StateMachine} can itself be
simplified using the usual {\StateMachine} simplification passes, and
this can often lead to useful simplifications even when the input
{\StateMachines} have already been simplified as far as possible.
Simplifying the example cross-product {\StateMachine} will produce the
{\StateMachine} shown in Figure~\ref{fig:cross_product_output_opt}.
In this case, the actual symbolic execution step will be trivial, and
will report that the program will reach a {\stCrash} state precisely
when $y = 0 \wedge \happensBefore{A}{F} \wedge LD(x) \not= 0 \wedge
\happensBefore{F}{C}$; in other words, if $y$ is nonzero, the initial
value of $x$ is non-zero, and statement F intercedes between
statements A and C, precisely as desired.

The simplifications needed in this example are quite simple, and it
would have been possible to include equivalent optimisations in the
symbolic execution engine itself.  This would be more difficult for
more complex simplifications, for two reasons:

\begin{landscape}
\begin{figure}
\vspace{-1cm}
  \begin{displaymath}
    \begin{array}{cccp{5cm}}
      \left(\left\{\raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {{\stStartAtomic}};
          \node at (0,-10mm) (A) {B};
          \draw[->] (r) -- (A);
        \end{tikzpicture}}, b_1\right\},\left\{C, b_2\right\}, \varnothing \right) & \!\!\!\Rightarrow\!\!\! & (\{B, b_1\}, \{C, b_2\}, \mathit{crashing}) & And likewise for interfering thread.\\

      \left(\left\{\raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {{\stEndAtomic}};
          \node at (0,-10mm) (A) {B};
          \draw[->] (r) -- (A);
        \end{tikzpicture}}, b_1\right\},\left\{C, b_2\right\}, \mathit{crashing} \right) & \!\!\!\Rightarrow\!\!\! & (\{B, b_1\}, \{C, b_2\}, \varnothing ) & And likewise for interfering thread.\\

      \left(\left\{\raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {$A$: \state{MemoryAccess}};
          \node at (0,-10mm) (A) {B};
          \draw[->] (r) -- (A);
        \end{tikzpicture}}, b_1\right\},\left\{C, b_1\right\},\mathit{crashing}\right) & \!\!\!\Rightarrow\!\!\! & \raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {$A$: \state{MemoryAccess}};
          \node at (0,-10mm) (A) {$(\{B, \true\}, \{C, b_1\}, \mathit{crashing})$};
          \draw[->] (r) -- (A);
        \end{tikzpicture}} & And likewise for interfering thread. \\

      \left(\left\{\raisebox{-8mm}{\begin{tikzpicture}[node distance=0.5cm,font=\small]
          \node at (0,0) (r) [stateIf] {\stIf{c}};
          \node at (-5mm, -10mm) (A) {$A$};
          \node at (5mm, -10mm) (B) {$B$};
          \draw[->,ifTrue] (r) -- (A);
          \draw[->,ifFalse] (r) -- (B);
      \end{tikzpicture}}, b_1\right\},\left\{C, b_2\right\},a\right) & \!\!\!\Rightarrow\!\!\! & \raisebox{-10mm}{\begin{tikzpicture}[node distance=0.5cm,font=\small]
          \node at (0,0) (r) [stateIf] {\stIf{c} };
          \node at (-20mm, -10mm) (A) { $(\{A, b_1\}, \{C, b_2\}, a)$ };
          \node at (20mm, -10mm) (B) { $(\{B, b_1\}, \{C, b_2\}, a)$ };
          \draw[->,ifTrue] (r) -- (A);
          \draw[->,ifFalse] (r) -- (B);
        \end{tikzpicture}} & And likewise for interfering thread.\\

      \left(\left\{\raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {A};
          \node at (0,-10mm) (A) {B};
          \draw[->] (r) -- (A);
        \end{tikzpicture}}, b_1\right\},\left\{C, b_2\right\},a\right) & \!\!\!\Rightarrow\!\!\! & \raisebox{-10mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {A};
          \node at (0,-10mm) (A) {$(\{B, b_1\}, \{C, b_2\}, a)$};
          \draw[->] (r) -- (A);
        \end{tikzpicture}} & If $A$ is a local side-effect.  Likewise for interfering thread.\\

      \left(\left\{\raisebox{-3mm}{\begin{tikzpicture}[font=\small]
          \node [stateTerminal] {\state{Terminal}};
      \end{tikzpicture}}, b_1\right\}, \{A, b_2\}, a\right) & \!\!\!\Rightarrow\!\!\! & \raisebox{-4mm}{\begin{tikzpicture}[font=\small]
          \node [stateTerminal] {{\stUnreached}};
      \end{tikzpicture}} & If $b_1 \wedge b_2 = \false$.  Likewise for interfering thread. \\
      
      \left(\left\{\raisebox{-3mm}{\begin{tikzpicture}[font=\small]
          \node [stateTerminal] {{\stCrash}};
      \end{tikzpicture}}, \true\right\}, \{A, \true\}, a\right) & \!\!\!\Rightarrow\!\!\! & \raisebox{-3mm}{\begin{tikzpicture}[font=\small]
          \node [stateTerminal] {{\stCrash}};
      \end{tikzpicture}} & \\
      
      \left(\left\{\raisebox{-3mm}{\begin{tikzpicture}[font=\small]
          \node [stateTerminal] {{\stSurvive}};
      \end{tikzpicture}}, \true\right\}, \{A, \true\}, a\right) & \!\!\!\Rightarrow\!\!\! & \raisebox{-3mm}{\begin{tikzpicture}[font=\small]
          \node [stateTerminal] {{\stSurvive}};
      \end{tikzpicture}} & \\
      
      \left(\left\{\raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {$A$: \state{MemoryAccess}};
          \node at (0,-10mm) (A) {B};
          \draw[->] (r) -- (A);
        \end{tikzpicture}}, b_1\right\},\left\{\raisebox{-8mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateSideEffect] {$C$: \state{MemoryAccess}};
          \node at (0,-10mm) (A) {D};
          \draw[->] (r) -- (A);
        \end{tikzpicture}}, b_2\right\}, \varnothing\!\!\!\right) & \!\!\!\Rightarrow\!\!\! & \raisebox{-16mm}{\begin{tikzpicture}[font=\small]
          \node at (0,0) (r) [stateIf] {\stIf{\happensBefore{A}{C}}};
          \node at (-22mm, -10mm) [stateSideEffect] (A) {$A$};
          \node at (22mm, -10mm) [stateSideEffect] (C) {$C$};
          \node at (-22mm, -20mm) (B) {$(\{B, \true\}, \{C, b_2\}, \varnothing)$};
          \node at (22mm, -20mm) (D) {$(\{A, b_1\}, \{D, \true\}, \varnothing)$};
          \draw[->,ifTrue] (r) -- (A);
          \draw[->,ifFalse] (r) -- (C);
          \draw[->] (A) -- (B);
          \draw[->] (C) -- (D);
        \end{tikzpicture}} & When $A$ and $C$ are non-local \\

    \end{array}
  \end{displaymath}
  \caption{The cross product algorithm as a node replacement graph
    grammar.  Only the most important features of the algorithm are
    shown here; refer to the text for a more complete description.}
\end{figure}
\end{landscape}

\begin{figure}
  \centerline{
  \begin{tikzpicture}
    \node[stateSideEffect, initial] (A) {\stAssert{y = 0 \wedge \happensBefore{A}{F} \wedge \smLoad{x} \not= 0 \wedge \happensBefore{F}{C}} };
    \node[stateTerminal, below = of A] (B) {\stCrash };
    \draw[->] (A) -- (B);
  \end{tikzpicture}
  }
  \caption{Result of simplifying {\StateMachine} shown in
    Figure~\ref{fig:cross_product_output}.}
  \label{fig:cross_product_output_opt}
\end{figure}

\begin{itemize}
\item Simplification passes can easily look ahead in the
  {\StateMachine}, whereas symbolic execution primarily considers a
  single state at a time.  Dead code elimination, for example, is much
  easier to implement as a simplification to the {\StateMachine} than
  as a change to the symbolic execution engine.
\item The results of a simplification pass are inherently shared
  across all paths which reach a particular state, whereas the
  symbolic execution engine needs to perform additional work in order
  to share results.
\end{itemize}

There is also an engineering consideration which argues in favour of
building and simplifying the cross-product {\StateMachine}, rather
than integrating equivalent optimisations into the symbolic execution
engine: {\technique} already needs all of the simplifiers in order to
build the input {\StateMachines} to begin with, and so re-using them
here halves the implementation effort.

\subsubsection{Other important features of the algorithm}

The cross-product building algorithm used in {\implementation} has a
number of other features not illustrated in the example:

\begin{itemize}
\item
  Atomic blocks are correctly maintained.  The output {\StateMachine}
  configurations have a further field not discussed above which
  indicates which, if either, of the two input {\StateMachines} are
  currently executing within atomic blocks.  When one {\StateMachine}
  is in an atomic block then only that {\StateMachine} is allowed to
  issue operations in the output {\StateMachine}, regardless of what
  the rules illustrated above would suggest.  Implementing
  {\stStartAtomic} and {\stEndAtomic} side-effects is then a
  simple matter of updating this field.

\item The output {\StateMachine} is not necessarily tree-structure,
  but forms a DAG of states.  This means that the number of states in
  the output {\StateMachine} grows as roughly $O(nm)$, where $n$ is
  the number of states in the crashing thread {\StateMachine} and $m$
  the number in the interfering thread one, rather than in proportion
  to the number of potential interleavings, which is
  $O(2^{\mathrm{min}(n,m)})$.  This makes it much easier to scale
  {\technique} to larger {\StateMachines}.

\item The output {\StateMachine} includes a large number of
  {\stUnreached} states.  The {\StateMachine} simplification passes
  are quite effective at removing these, but for efficiency reasons we
  would prefer to not generate them at all.  {\Implementation}
  includes a few simple rules to eliminate some of the more egregious
  ones.  For instance, in the example, no state after C in the
  crashing thread {\StateMachine} can issue any memory accesses, and
  so if the crashing thread ever reaches C before the interfering
  thread has issued any accesses then the crashing thread will
  definitely complete as-if atomically and be replaced by an
  {\stUnreached} state, and so state C in the output
        {\StateMachine} will itself be replaced by an
        {\stUnreached} state.  There is therefore no need to
        actually generate any of (\{D, true\}, \{E, false\}), (\{H,
        true\}, \{E, false\}), or (\{G, true\}, \{E, false\}).  These
        rules are generally quite obvious and unenlightening and so
        are not discussed in detail here.
\end{itemize}

\subsection{Path explosion}

One common problem in symbolic execution systems is path explosion:
the number of paths through a program rises exponentially in the size
of the program, and this can prevent na\"ive symbolic execution
systems from being applied to realistically large programs.  In the
case of \technique, there are two main causes of path explosion:

\begin{itemize}
\item
  Aliasing.  If the various simplification passes and the dynamic
  analysis cannot determine how memory accessing instructions alias
  then the symbolic execution engine must consider every possible
  aliasing pattern, of which there are $O(n^m)$, where $n$ is number
  of \state{Load} operations and $m$ the number of \state{Store} ones.
  This grows rather quickly in the number of unsolvable aliasing
  problems, especially when the number of \state{Store}s in the
  {\StateMachine} rises.  The use of lazy alias resolution helps
  mitigate this to some extent, but does not eliminate it completely.
  This represents one of the major limitations to \technique's
  scalability.
\item
  Thread interleaving.  The cross-product {\StateMachine} will have
  $O(nm)$ states, where $n$ is number of states in the read-side
  {\StateMachine} and $m$ the number in the write-side one.  The
  number of paths through the combined {\StateMachine} then grows as
  $O(2^{nm})$, which again grows rather quickly.
\end{itemize}

The result is that, in the common case where the read-side
{\StateMachine} consists mostly of \state{Load} operations and
write-side one mostly of \state{Store} ones, the symbolic execution
engine might have to consider up to $O((2n)^m)$ distinct paths when
evaluating the cross-product {\StateMachine}.  This is obviously
completely infeasible for even moderate values of $n$ and $m$.  For
good performance, {\technique} relies on the various simplification
and analysis techniques to reduce $n$ and $m$ to manageably small
values.  Fortunately, as discussed in the evaluation, they are able to
do so in a useful set of cases.

\section{The W isolation assumption}
\label{sect:derive:w_isolation}

\todo{The name is perhaps a little obscure.}

As discussed in Section~\ref{sect:intro:overview}, {\technique} is
concerned with bugs in which two threads are simultaneously operating
on the same data structure.  This could sensibly be further restricted
to bugs in which the \gls{interferingthread} is modifying a structure
which the \gls{crashingthread} is reading, rather than having to
consider cases in which both threads modify the structure.  In other
words, it can be restricted to bugs in which the data flow is only
from the \gls{interferingthread} to the crashing one.
{\Implementation} can configured to look only for that kind of bug, by
making the \gls{w-isolation} assumption: that the
\gls{interferingthread} never loads any locations which have been
stored to by the \gls{crashingthread}.

The \gls{w-isolation} assumption enables three main optimisations:

\begin{itemize}
\item
  It directly restricts the aliasing problem, as the analysis no
  longer needs to consider aliasing between stores in the
  \gls{crashingthread} and loads in the \gls{interferingthread}.
\item
  It reduces the set of interfering \glspl{cfg} which is generated for
  each \gls{crashingthread}.  If the \gls{interferingthread} cannot
  load any location stored to by the \gls{crashingthread} then only
  \glspl{interferingstore} can be \glspl{communicatinginstruction},
  and all of the other \glspl{communicatinginstruction} can be
  discarded.
\item
  It simplifies the calculation of the \gls{ci-atomic} constraint.  If
  the \gls{w-isolation} holds then the \gls{crashingthread} cannot
  influence the behaviour of the interfering one, and so
  \gls{ci-atomic} becomes simply C-atomic~$\wedge$~I-atomic, where
  C-atomic is the condition for the \gls{crashingthread} to survive
  when run in isolation and I-atomic the equivalent condition on the
  \gls{interferingthread}.  I-atomic is then always implied by
  \gls{ic-atomic}, because of the way the latter is constructed, and
  so \gls{ci-atomic} can be replaced with just C-atomic.  C-atomic
  depends on the \gls{crashingthread} only, and so does not need to be
  recalculated for every \gls{interferingthread}, which can provide a
  useful performance when there are a large number of
  \glspl{interferingthread} for a single crashing one.
\end{itemize}

The analysis is almost always faster with the \gls{w-isolation}
assumption, but cannot handle as broad a class of program behaviour.
I evaluate these effects experimentally in
\autoref{sect:eval:w_isolation}.

\section{Finding unknown bugs}
\label{sect:derive:unknown_bugs}

The discussion in \autoref{sect:derive:build_crashing_cfg} assumed
that someone had already identified the crashing instruction before
{\technique} starts.  {\Technique} can also be used to discover
unknown bugs.  The scheme used is quite simple: enumerate all of the
instructions in a program which might crash due to a bug of the class
being investigated and then consider each independently in turn.  This
is obviously only feasible if the majority of instructions can be
dismissed quickly.  Fortunately, they can be: {\implementation} takes,
on average, a few hundred of milliseconds per instruction on fairly
modest hardware, allowing even large programs with millions of
instructions to be analysed in a few days.  Parallelisation would
allow this time to be reduced further if necessary.

Identifying instructions which might crash depends on the type of bug
which is to be investigated, but is generally straightforwards.
{\Implementation} considers three types of crash:

\begin{itemize}
\item Assertion-failure type crashes.  These are caused by the program
  calling a function such as \verb|__assert_fail| or \verb|abort|
  provided by an operating system library.  Finding such functions is
  generally straightforward given the usual dynamic linker
  information, and the initial whole-program static analysis phase can
  then find all callers of those functions.
\item Double free errors.  These are caused by the program calling
  \verb|free| in an incorrect manner.  Again, the dynamic linker
  information allows {\implementation} to quickly find all calls to
  \verb|free| in the program, and these calls are used as the
  potentially-crashing instructions in the program.
\item Bad pointer dereferences.  Any memory-accessing instruction
  could potentially dereference a bad pointer, and so
  {\implementation} simply enumerates all memory accessing
  instructions discovered by the initial static analysis.
\end{itemize}

These potentially-crashing instructions are then considered
independently in turn.  This is, again, potentially rather expensive,
as there can be a very large number of these candidate instructions.
On the other hand, many can be dismissed very quickly (90\% of the
instructions in mysql, for instance, can be analysed in under 250ms;
see Section~\ref{sect:eval:mysql} for details), and, because each
instruction is considered independently, the main phase of the
analysis is embarrassingly parallel.  This means that the total cost
of the analysis remains high but tolerable, and that it is likely to
become faster as hardware becomes increasingly parallel, which is
precisely the scenario in which it would be most useful.

\section{The program model}
\label{sect:program_model}

{\Technique} models the part of the program which is directly involved
in a crash via the {\StateMachine} mechanism, but these are only
capable of analysing relatively small fragments of the program.  This
means that they cannot express global properties such as the structure
of the heap.  {\Technique} instead captures these properties in its
\gls{programmodel}, a model of some important aspects of the program
built before the main analysis starts.

\todo{Say more.}

\subsection{Memory access model}
\label{sect:program_model:dynamic_alias}

The most important part of the \gls{programmodel} is the memory access
model, which shows how the program accesses memory during normal
operation.  This is used both for aliasing analysis during
{\StateMachine} simplification (Section~\ref{sect:derive:simplify_sm})
and to find the \glspl{communicatinginstruction} when building the
interfering \gls{cfg} (Section~\ref{sect:derive:write_side}).  The
memory access model is itself composed of two parts: a model of how
the program accesses its stack and a model of accesses to non-stack
memory.

The stack model is built using a fairly conventional function-local
static pointer escape analysis and I give only a high-level
description of it here.  The core idea is to observe that stack frames
are in some sense ``created'' when a function starts, and so there
should not be any pointers to function-local variables unless the
function being analysed creates one.  If the analysis can show that
one pointer is to a local variable and another pointer existed before
the function started then it is safe to assume that the two pointers
definitely do not alias.  This static analysis therefore attempts to
track which registers and memory locations might contain pointers to
the local stack frame.  This is usually sufficient for the
{\StateMachine} simplifiers to be able to determine which, if any,
stack frames a given memory access might refer to that.  Simple
arithmetic considerations are then usually sufficient to determine
accesses to local variables.

The non-stack memory model is more complex.  It is much more difficult
to characterise the structure of non-stack memory, such as the heap,
using static analysis, for two main reasons:

\begin{itemize}
\item First, the structure of non-stack memory is almost always more
  complex than the structure of a stack frame, in terms of what
  objects point at which other objects.  Heap objects are often part
  of complicated pointer graphs, and will both contain and be referred
  to by pointers, whereas most stack locations are simple variables.
\item Non-stack locations can be accessed from anywhere in the
  program, with little indication of which instructions access which
  objects.  Stack locations, by contrast, are almost always accessed
  only by the function which allocated them, with the vast majority of
  other accesses made by functions called by that one.
\end{itemize}

These problems make it difficult to accurately model the heap even
when the analysis tool has full access to the program's source
code\needCite{}, and attempting to do so given only a binary is
completely infeasible.

\todo{This is a slightly abstract way of describing this.  I think
  it's still probably the best way of thinking about it, but perhaps
  not the best way of explaining it.}

{\Technique} instead relies on a dynamic analysis to model accesses to
non-stack locations.  The result of this analysis is a table showing
which pairs of instructions might access the same memory locations.
In the pessimal case, where every instruction might alias with every
other, this table will contain $n^2$ entries, where $n$ is the number
of memory accessing instructions in the program.  That would be
unfeasibly large for non-trivial programs.  Fortunately, the table is
in practice very sparse, and so is usually perfectly manageable.

The intuition behind this analysis is that most fields in most data
structures are accessed by a relatively small number of instructions
in the program, and so if it were possible to identify the field being
accessed by a given instruction then that would make it easy to
determine whether two instructions might interfere.  More concretely,
if we had an instruction $\mathit{instr\_to\_fields}(i)$ mapping from
an instruction to the set of fields which it can access then
\begin{displaymath}
\mathit{alias}(i, i') = (\mathit{instr\_to\_fields}(i) \cap \mathit{instr\_to\_fields}(i') \not= \varnothing)
\end{displaymath}
where $\mathit{alias}(i, i')$ is an alias function which is true when
there is some possibility of instructions $i$ and $i'$ accessing the
same memory location.  Unfortunately, the $\mathit{instr\_to\_field}$
function is difficult to express for binary programs, even with
perfect information, because fields are a high-level language
construct and {\technique} must operate at the level of machine code.

A little bit of algebra allows us to re-express $\mathit{alias}$ like
so:
\begin{displaymath}
\mathit{alias}(i, i') = \left(i' \in \bigcup_{f \in \mathit{instr\_to\_fields}(i)} \mathit{field\_to\_instrs}(f)\right)
\end{displaymath}
We now have two functions which are defined in terms of fields,
$\mathit{instr\_to\_fields}$ and $\mathit{field\_to\_instrs}$, rather
than one, which might appear to have made the problem worse.  The
composition of the two functions, $\mathit{instr\_label}(i) =
\{\mathit{field\_to\_instrs}(f) | f \in
\mathit{instr\_to\_fields}(i)\}$, however, does not require the caller
to make any reference to fields.  This suggests that it should be
implementable, and it is this function which the dynamic analysis
computes\footnote{In fact, the actual output of the {\implementation}
  dynamic analysis phase is the function $i \rightarrow \cup
  \mathit{instr\_label}(i)$, which flattens one level of
  $\mathit{instr\_label}$'s set structure, as that is the most
  convenient form for the later phases of the analysis.  Internally,
  it computes the full $\mathit{instr\_label}$ function.}.

The types of these functions are perhaps informative:

\begin{itemize}
\item $\mathit{instr\_to\_fields}: \mathit{instruction} \rightarrow \mathit{set}(\mathit{field})$
\item $\mathit{field\_to\_instrs}: \mathit{field} \rightarrow \mathit{set}(\mathit{instruction})$
\item $\mathit{instr\_label}: \mathit{instruction} \rightarrow \mathit{set}(\mathit{set}(\mathit{instruction}))$
\end{itemize}

In other words, $\mathit{instr\_label}$ is formed from
$\mathit{instr\_to\_fields}$ by identifying fields with sets of
instructions.  This can be thought of as analogous to the conceptual
switch between data types in a functional language and those in an
object-oriented language\editorial{Little bit too philosophical.}.  In
a functional language, a data type is identified with a set of
values\needCite{}, whereas in an object-oriented one the type is
identified with a set of operators which can be applied to
values\needCite{}.  In the same way, the intended semantics of this
analysis talks about the set of fields which an instruction can
access, which is in some sense like the set of values on which it can
operate, whereas the actual analysis discovers the set of instructions
with which it can interact, which is analogous to a set of operations.

Given this conceptual work, implementing the dynamic analysis itself
is quite simple.  {\Implementation} does so using a Valgrind skin.
The program's memory is divided into fixed-size chunks\footnote{These
  chunks are 8 bytes, for {\implementation}.}, each of which has a
label consisting of a set of accessing
instructions\footnote{{\Implementation} also tracks which accesses are
  reads, which writes, and which both, as this simplifies the
  implementation of the later analysis phases, but this does not
  meaningfully change the algorithm.}.  Any instruction which accesses
that memory chunk adds itself to the set.  The set of all labels
generated by the program, suitable indexed, is then an approximation
to the $\mathit{instr\_label}$ function; if the dynamic analysis is
able to observe all relevant program behaviour then it is precisely
equal to $\mathit{instr\_label}$.

This scheme, as presented, has one important weakness, which is that
it assigns labels to addresses in memory.  Mapping back to high-level
language constructs, this means that it is assuming that each address
in the program, once allocated, corresponds to the same type of data
structure throughout the program's life.  In other words, it assumes
that memory is type stable\cite{Greenwald1996}.  This is generally
reasonable for statically-allocated structures, such as those in the
BSS segment\cite[Section~7.6]{Stevens}, but not for
dynamically-allocated structures such as those allocated via
\texttt{malloc} or \texttt{operator new}.  {\Implementation} relies on
being able to identify such dynamic memory allocators so that it can
reset the labels on memory addresses.  This is easy for allocators
provided by standard system libraries, such as \texttt{malloc}, but
much harder for program-specific allocators.  It might be possible to
identify such allocators using a variant of the techniques described
by Cozzie et al.\cite{Cozzie2008}, but I have not investigated that at
this time.  Instead, {\implementation} relies on manually annotating
allocation functions in the program.  This is not an unreasonable
burden: for mysql, the necessary annotations amount to an additional
twenty lines across the entire program, and none of the other programs
examined in the evaluation required any annotations at all.

{\Implementation} includes one minor refinement to the basic analysis
described above.  It is fairly common for programs to allocate new
heap structures using a function such as \texttt{malloc} and to then
initialise this structure using a series of stores.  These stores will
never race, so it would be helpful to avoid spending excessive time
considering what would happen if they did.  The approach taken is
simple: when a block is returned from \texttt{malloc} it is marked as
thread-private, and remains so until it is stored to non-stack memory.
Entries in the aliasing table then include a flag indicating whether
the access is thread-private or potentially racing.  Later analysis
phases can then assume that two thread-private accesses in different
threads cannot alias with each other.

This policy might seem to be overly conservative: a block of memory is
marked as shared whenever a pointer to it is stored into any non-stack
memory, even when that non-stack memory is itself marked as
thread-private.  This is necessary because the analysis does not
attempt to track the heap reachability graph, and in particular cannot
map from one block to the set of blocks reachable from that block.  It
is therefore not safe to ``upgrade'' a block from thread-private to
thread-shared if there is any possibility of that block containing a
thread-private pointer; upgrading blocks early and pessimistically
means that it is never necessary to do so.

\todo{Talk about what happens if this is incomplete?}

\subsection{Finding the predecessors of an instruction}
\label{sect:program_model:instr_predecessors}

The algorithm for building the \gls{crashingthread} \gls{cfg} given in
Section~\ref{sect:derive:build_crashing_cfg} assumes that there is
some function $\mathit{predecessors}$ which maps from an instruction
to the set of instructions which might execute immediately before it.
This is not completely trivial given only a binary program.  One
obvious approach would be to simply build the program's entire
\gls{cfg}, starting from its entry point and disassembling forwards
from there until the complete set of successor instructions is known
for every instruction.  This \gls{cfg} would then make determining the
predecessors of a given instruction trivial.  Unfortunately, building
it is itself quite complicated:

\begin{itemize}
\item Finding the set of entry points is not always easy, as there can
  be branches into the main program from libraries or from the
  operating system.
\item Any non-trivial program will contain indirect branches:
  instructions whose successors can only be determined at run time.
\end{itemize}

{\Implementation} avoids these issues by using information from the
dynamic analysis.  In addition to the program's memory accessing
patterns, the dynamic analysis also tracks the targets of all indirect
branches and all branches into the program from outside of it, and
then simply assumes that this information is complete.  This allows
the program's complete \gls{cfg} to be computed, making the
instruction predecessor problem trivial.

\subsection{Other information in the program model}

The \gls{programmodel} also includes some information obtained from
static analysis:

\begin{itemize}
\item As discussed previously, a pointer escape analysis used to
  determine when a value loaded from memory might contain a pointer to
  a local variable.
\item A function discovery pass is used to identify the program's
  functions, including those which are not contiguous in
  memory\editorial{Cite Zhou 2005 or US patent 2007/0089106, unless I
    can find something better.  Also mention that MSVC does this in
    real programs.} and in the presence of most forms of tail call
  elimination\needCite{}.
\item A liveness analysis is used to identify the arguments to
  functions, which provides useful hints to some of the other analysis
  steps.
\item A function characterisation analysis identifies some simple
  wrappers around functions such as \texttt{free} and \texttt{abort}.
  These functions can then be replaced by the appropriate library
  model (see Section~\ref{sect:derive:library_functions}) rather than
  needing to be translated into {\StateMachines}.
\item A variant of Value Set Analysis\needCite{} is used to determine
  some simple properties of registers at particular instructions.
  This includes showing that the register definitely is or definitely
  is not a valid pointer, or that one register is equal to another
  register plus or minus a constant.
\end{itemize}

These analyses collectively act to give {\technique} some limited
sensitivity beyond its \gls{analysiswindow} with only modest
computational cost.
