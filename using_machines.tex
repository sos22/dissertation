\section{Checking whether a given bug is actually real}

Previous sections have described how to generate {\StateMachines}
which represent a given fragment of the program, and how to simplify
them down to remove most of the redundant information.  This section
gives a mechanism for using those {\StateMachines} to check whether
running two fragments of the program in parallel might conceivably
lead to a crash and, if so, under what circumstances it might do so.

The core of the approach is to take a pair of {\StateMachines}, one
representing the read thread and the other representing the write
thread, and convert them into a verification condition using symbolic
execution.  This verification condition is satisfiable precisely when
running the fragments of program represented by the two
{\StateMachines} in parallel might lead to the bug being investigated
reproducing.  Note, though, that this process does not consider the
existing synchronisation present in the program i.e. it is
synchronisation context insensitive, and this can lead to a
significant number of false positives.  See
section~\ref{sect:reproducing_bugs} for one possible way of
eliminating these.

SLI's completeness property is that if every possible \StateMachine
pair which could be generated by the above algorithm is generated and
each one is successfully converted to a verification condition
(i.e. no part of the analysis times out), and none of the verification
conditions are satisfiable, and the dynamic analysis is complete, the
program will not contain any bugs of the class being investigated.

The verification conditions generated here have three main components
which correspond closely to the clauses of the bug definition in
section~\ref{sect:finding_bugs:finding_candidate_bugs:formal_definition}.
These are:

\begin{itemize}
\item
  R atomic -- specifies that the read-side {\StateMachine} must
  predict survival when run atomically.  This translates into a
  precondition on the initial state of the program which is satisfied
  precisely when running the read-side of the purported bug in
  isolation would not crash.
\item
  W atomic -- specifies that when the write-side {\StateMachine} is
  run to completion followed by the read-side {\StateMachine} the
  read-side {\StateMachine} must not predict a crash.  This translates
  into a further precondition which is true for initial states which
  the write-side {\StateMachine} will transform into states which
  satisfy the R atomic precondition.
\item
  Crash possible -- requires that interleaving the two
  {\StateMachines} might cause the read-side {\StateMachine} to
  predict a crash.  This is translated into a predicate over both the
  initial state and the program's happens-before graph which will be
  satisfied for those executions which crash due to the bug which is
  being investigated.
\end{itemize}

\todo{``Initial state'' is arguably a slightly deceptive term here
  because it can include things like control-flow properties of an
  execution of the program.}  The final verification condition is the
conjunction of these three preconditions, and a bug is reported if
that condition is satisfiable.

{\Technique} uses symbolic execution over slightly different
{\StateMachines} to build all of the individual predicates:

\begin{itemize}
\item R atomic is built by symbolically executing the read-side
  {\StateMachine} in isolation and taking the disjunction of the path
  constraints for all paths which reach the \state{Survive} state.
  Note that paths which reach the \state{Unreached} state are treated
  identically to those which reach the \state{Crash} state: these
  paths do not need to be considered by the later phases of analysis,
  by the definition of the \state{Unreached} state, and excluding them
  here makes the later symbolic executions somewhat simpler.
\item W atomic is built by concatenating the two machines and
  symbolically executing the result.  Again, \state{Unreached} states
  are treated identically to \state{Crash} ones.  {\Technique} assumes
  that R atomic holds during the W atomic symbolic execution, which
  can sometimes usefully reduce the set of paths which must be
  considered.
\item Crash possible is produced by symbolically executing the
  cross-product of the two {\StateMachines} and taking the disjunction
  of all paths which reach the \state{Crash} state.  This time, the
  \state{Unreached} state is treated as the \state{Survive} one,
  so as to avoid reporting bugs when the cross-product machine
  reaches a state which is supposed to be ignored.
\end{itemize}

Note that the intermediate {\StateMachines} used here are subject to
the usual simplifications before being symbolically executed, even
though both input {\StateMachines} will have already been simplified
as far as possible.  This can sometimes allow further useful
simplifications and hence reduce the cost of the symbolic execution.
In particular, the simplifiers can assume that no other threads will
interfere with the two threads being executed, which can be very
useful in solving aliasing problems.

The next few sections give some details of the symbolic execution
techniques used and a description of the algorithm used to build the
cross-product {\StateMachine}.

\subsubsection{Symbolically executing machines}

SLI uses a simple symbolic execution engine to evaluate machines and
determine when {\StateMachines} will crash.  The details of this are
fairly standard, and I give only a brief overview
here\editorial{\emph{Should} only give a brief overview; this ended up
  much more detailed than I'd expected.}.  The core data structure
used by the execution engine is a queue of (mostly) symbolic
configurations which the {\StateMachine} might occupy, and the main
operation is to take a state out of this queue, determine what the
\StateMachine might do next, and possibly add some additional
configurations to the queue to explore further.  Each configuration
contains:

\begin{itemize}
\item
  A mapping from {\StateMachine} variables to the (symbolic) values of
  those variables.
\item
  A reference to the {\StateMachine}'s current (non-symbolic) state.
\item
  The order in which those temporaries were assigned to.  As discussed
  in \S~\ref{sect:ssa}, SLI uses a slightly unusual form of single
  static assignment in which $\Phi$ nodes select their input variable
  based on which was assigned to most recently, rather than based on
  the preceding control flow, and this keeping track of that ordering
  allows that semantics to be implemented simply.
\item
  A log of all of the memory stores issued by the \StateMachine so far.
\item
  The current path constraint.  This is simply the conjunction of all
  of the conditions which are known to be true at this point in the
  execution.
\end{itemize}\editorial{Plus some debug crap, but we don't care about that here.}

A symbolic execution will eventually reach one of the {\StateMachine}
terminal states: survive, crash, or unreached.  As discussed above,
the unreached state can be treated as either surviving or crashing,
depending on how the results of the symbolic execution are to be used.
``Impossible'' paths, such as those where the {\StateMachine}
dereferences a bad pointer, are treated as if they reached the
unreached state\footnote{Note the program dereferencing a bad pointer
  does not imply that the {\StateMachine} does as well.  In
  particular, if the {\StateMachine} was generated to investigate a
  bad pointer dereference at memory accessing-instruction $I$ then $I$
  will be translated into a test of a $BadPtr$ expression, rather than
  a memory access, and so will never cause the {\StateMachine} to
  dereference a bad pointer.}.

Implementing the various types of \StateMachine operation is
straightforward:

\begin{itemize}
\item Conditional branches.  The condition in the branch is compared
  to the path constraint to determine whether the branch is completely
  determined by the path constraint.  If it is, execution simply moves
  to the appropriate successor state.  Otherwise, two new successor
  configurations are added to the queue, one for the true branch of
  the conditional and the other for the false branch, and the path
  constraints updated as appropriate.
\item \state{Copy} $var = expr$, which evaluates $expr$ ands store it
  in the {\StateMachine}-level variable $var$.  The expression is
  simplified as far as possible using the information in the path
  constraint and stored in the appropriate place in the variable
  table.  The variable is then moved up in the variable assignment
  order table.
\item \state{Store} $value \rightarrow \ast(addr)$, which evaluates
  $addr$ and $value$ and stores the value of $value$ to the memory
  location $addr$.  The symbolic executor simplifies $addr$ and
  $value$ using the path constraint and then adds a new entry to the
  end of the memory store log.

  Depending on the mode of operation, the engine might also test
  whether $addr$ is a bad pointer and, if it is, branch to the
  unreached state, much as it would if the {\StateMachine} had
  asserted $\not{}BadPtr(addr)$.  This extra assertion can then
  sometimes be used to simplify $BadPtr$ expressions which occur later
  in the {\StateMachine}'s execution.  This is often useful if the bug
  to be investigated is itself a bad pointer dereference but tends to
  be less so if the bug is an assertion failure, and so the engine
  only does this in the former case.

\item \state{Load} $\ast(addr) \rightarrow var$, which evaluates
  $addr$ to the address of a memory location and then copies the
  current contents of that location to the {\StateMachine} variable
  $var$.  As usual, symbolic execution starts by simplifying $addr$
  using the path constraint.  The resulting address is then compared
  to every location in the store log to determine which stores might
  possibly satisfy the load and a successor configuration created for
  each.  Each of these successor configurations will have $var$ set to
  an appropriate value and the path constraint extended with enough
  additional clauses to ensure that the \state{Load} is satisfied by
  the desired \state{Store}.  There may also be an additional
  successor configuration for the case where the \state{Load} does not
  match any \state{Store}s and instead returns the initial contents of
  memory, with appropriate values for $var$ and appropriate
  modifications to the path constraint.

  As for \state{Store} operations, \state{Load}s may sometimes also
  introduce a constraint that the dereferenced pointer is valid, if
  the symbolic execution engine is being used in a mode where that is
  likely to be helpful.

\item \state{$\Phi$} $\{var_1,var_2,\ldots{},var_n\} \rightarrow var$
  selects whichever of the $var_i$ inputs has been most recently
  assigned to and copies its value to $var$.  This is simple to
  implement given that each configuration includes a log of the order
  in which variables are assigned to and produces a single successor
  configuration.

\item
  Every other operation is a no-op in the interpreter and simply moves
  to the next state.  In particular, the execution engine ignores
  operations such as \state{StackLayout} and \state{PointsTo} which
  simply provide additional hints to the various \StateMachine
  simplification phases.  One might expect that these would be useful
  for determining which \state{Store} should be used to satisfy a
  given \state{Load} operation, but in almost all cases where these
  hints provide useful information the {\StateMachine} simplification
  passes will have already used it to simplify the {\StateMachine},
  and so there is little to be gained from repeating the analysis
  here.

  The \state{StartAtomic} and \state{EndAtomic} states are also
  treated as no-ops in the symbolic execution engine.  This is because
  the engine only ever processes a single {\StateMachine} at a time,
  and so the entire {\StateMachine} is already implicitly atomic.

  \todo{Actually, the effects get stripped out before we start, rather
    than ignored here, but it ends up being equivalent.}
\end{itemize}

\todo{The path constraint is initialised to whatever assumptions we're
  allowed to make when the symbolic execution starts, so R atomic when
  we're trying to derive W atomic.}

\todo{Surprise: you basically never re-visit an old configuration in
  this model.  Not sure why that is, and it's almost certainly not a
  good thing.}

\subsection{Building cross-product {\StateMachines}}

The symbolic execution engine is only capable of exploring one
{\StateMachine} at a time, and so if cross-thread behaviours are to be
investigated then the single-threaded {\StateMachines} must be
combined into a single multi-threaded one.  This is trivial for the
{\StateMachine} used in deriving W atomic (the two {\StateMachines}
are concatenated together by replacing the terminal state of the
write-side {\StateMachine} with the a copy of the read-side
{\StateMachine}), but the one used to derive the crash-possible
constraint is somewhat more involved.  The obvious approach here would
be to a cross-product of the two {\StateMachines}, but this would be
both inefficient, due to the presence of partial order
redundancies\needCite{}, and incorrect, due to the presence of atomic
blocks within the {\StateMachines}.  {\Technique}'s approach is a
refinement of the cross-product algorithm which avoids this
incorrectness while also mitigating the inefficiency.  It also avoids
running either {\StateMachine} to completion before the other starts,
since the results obtained from executing such a path would be
immediately discarded by the R atomic and W atomic preconditions.

\todo{I want to explain this in terms of supercompilation of an
  abstract interpreter, but it's not quite coming together.}

The algorithm used is actually rather simple.  The core idea is to
identify each state of the output {\StateMachine} with a particular
configuration of the two input {\StateMachines}, where the
configuration contains all of the necessary information about the
concurrent interleaving of the two {\StateMachines} and nothing else.
More concretely, a configuration consists of five fields:

\begin{itemize}
\item The {\StateMachine} state which each machine is to execute next;
\item A field saying which, if any, of the input {\StateMachines} are
  currently in atomic blocks;
\item A flag indicating whether the read-side {\StateMachine} has
  issued any memory accesses yet; and
\item A flag indicating whether the write-side {\StateMachine} has
  issued any stores.
\end{itemize}

The output machine starts in a configuration with both input
{\StateMachines} in their initial state, neither in an atomic block,
and neither having issued any memory accesses.  The algorithm then
exhaustively explores every configuration reachable from this one,
building the output {\StateMachine} as it goes.

As an example, consider the {\StateMachines} shown in
figure~\ref{fig:cross_product_input}.  $y$ here is supposed to
indicate some value which is local to the write-side {\StateMachine}
and $x$ some global variable in memory.  These produce the
cross-product {\StateMachine} shown in
figure~\ref{fig:cross_product_output}.  I highlight a couple of the more important
features of this diagram:

\begin{itemize}
\item The {\StateMachine} starts in the configuration (A, E,
  $\varnothing$, false, false), indicating that the read-side
  {\StateMachine} is in state A, write-side one is in state $E$,
  neither one is in an atomic section, and neither has issued any
  memory accesses.  The write-side machine is about to execute a
  thread-local operation and so the cross-product algorithm allows it
  to advance independently.  In this case, that operation is an
  \state{If} state, so the write-side state has two successors, and so
  the configuration has two successor configurations.
\item One of the successors of this initial state is (A, I,
  $\varnothing$, false, false).  Here, the write-side {\StateMachine}
  has reached a terminal state I before the read-side {\StateMachine}
  issued any memory accesses, as indicated by the first false, and so
  the output state is \state{Unreached} and no further exploration is
  necessary.  Likewise all of the other ways in which one
  {\StateMachine} could complete before the other starts have been
  converted to \state{Unreached} and will be removed by the
  simplifier.
\item In the other success of the initial state, (A, F, $\varnothing$,
  false, false), both {\StateMachines} have reached a memory accessing
  state.  The cross-product algorithm now uses a happens-before test
  to incorporate both possible orderings of these accesses into the
  output {\StateMachine}.  In this way all of the relevant memory
  ordering behaviour of the two input {\StateMachines} is incorporated
  into the output {\StateMachine}.
\item The output {\StateMachine} might no longer be in static single
  assignment form.  This can sometimes reduce the effectiveness of
  {\StateMachine} simplifications on the output {\StateMachine}.

  \todo{This kind of sucks.  I need to think of something clever to
    say here.}

\item In this case, simplification will be effective and will reduce
  the {\StateMachine} to that shown in
  figure~\ref{fig:cross_product_output_opt}; symbolic execution is
  barely necessary to determine the crash constraint.  More complex
  {\StateMachines} will not simplify nearly so well. \todo{Not sure I
    really needed to say that.}
\end{itemize}

\begin{figure}
  \begin{subfloat}
    \begin{tikzpicture}
      \node[stateSideEffect,initial] (lA) {lA: \state{Load} $x \rightarrow tmp$ };
      \node[stateIf,below = of lA] (lB) {lB: \state{If} $tmp = 0$ };
      \node[stateSideEffect,below = of lB] (lC) {lC: \state{Load} $x \rightarrow tmp'$ };
      \node[stateIf,below = of lC] (lD) {lD: \state{If} $BadPtr(tmp')$ };
      \node[stateTerminal,below = of lD] (lH) {lH: \state{Crash} };
      \node[stateTerminal,right = of lC] (lG) {lG: \state{Survive} };
      \draw[->] (lA) -- (lB);
      \draw[->] (lB) to node {true} (lG);
      \draw[->] (lB) to node {false} (lC);
      \draw[->] (lC) -- (lD);
      \draw[->] (lD) to node {true} (lH);
      \draw[->] (lD) to node {false} (lG);
    \end{tikzpicture}
    \caption{Read-side}
  \end{subfloat}
  \begin{subfloat}
    \begin{tikzpicture}
      \node[stateIf,initial] (lE) {lE: \state{If} $y \not= 0$};
      \node[stateSideEffect,below right = of lE] (lF) {lF: \state{Store} $0 \rightarrow x$};
      \node[stateTerminal,below left = of lF] (lI) {lI: \state{Survive} };
      \draw[->] (lE) to node {true} (lI);
      \draw[->] (lE) to node {false} (lF);
      \draw[->] (lF) -- (lI);
    \end{tikzpicture}
    \caption{Write-side}
  \end{subfloat}
  \caption{Some {\StateMachines}.  $x$ is a global memory location.}
  \label{fig:cross_product_input}
\end{figure}

\begin{figure}
  \begin{tikzpicture}[align=center]
    \node[stateIf, initial] (A) {(A, E, $\varnothing$, false, false)\\\state{If} $y \not= 0$ };
    \node[stateTerminal, right = of A] (B) {(A, I, $\varnothing$, false, false)\\\state{Unreached} };
    \node[stateIf, below = of A] (C) {(A, F, $\varnothing$, false, false)\\\state{If} $A \happensBefore F$ };
    \node[stateSideEffect, below = of C] (D) {\state{Load} $x \rightarrow tmp$};
    \node[stateSideEffect, right = of C] (E) {\state{Store} $0 \rightarrow x$};
    \node[stateIf, below = of D] (F) {(B, E, $\varnothing$, true, false)\\\state{If} $tmp = 0$ };
    \node[stateTerminal, below = of E] (G) {(A, I, $\varnothing$, false, true)\\\state{Unreached} };
    \node[stateTerminal, right = of F] (H) {(G, E, $\varnothing$, true, false)\\\state{Unreached} };
    \node[stateIf, below = of F] (I) {(C, E, $\varnothing$, true, false)\\\state{If} $C \happensBefore F$ };
    \node[stateSideEffect, below = of I] (J) {\state{Load} $x \rightarrow tmp'$};
    \node[stateSideEffect, right = of I] (K) {\state{Store} $0 \rightarrow x$};
    \node[stateIf, below = of J] (L) {(D, E, $\varnothing$, true, false)\\\state{If} $BadPtr(tmp')$ };
    \node[stateSideEffect, right = of K] (M) {(C, I, $\varnothing$, true, true)\\\state{Load} $x \rightarrow tmp'$ };
    \node[stateTerminal, below = of L] (N) {(H, E, $\varnothing$, true, false)\\\state{Unreached} };
    \node[stateTerminal, below right = of L] (O) {(G, E, $\varnothing$, true, false)\\\state{Unreached} };
    \node[stateIf, below = of M] (P) {(D, I, $\varnothing$, true, true)\\\state{If} $BadPtr(tmp')$ };
    \node[stateTerminal, left = of P] (Q) {(G, I, $\varnothing$, true, true)\\\state{Survive} };
    \node[stateTerminal, below = of P] (R) {(G, I, $\varnothing$, true, true)\\\state{Crash} };
    \draw[->] (A) to node {true} (B);
    \draw[->] (A) to node {false} (C);
    \draw[->] (C) to node {true} (D);
    \draw[->] (C) to node {false} (E);
    \draw[->] (D) -- (F);
    \draw[->] (E) -- (G);
    \draw[->] (F) to node {true} (H);
    \draw[->] (F) to node {false} (I);
    \draw[->] (I) to node {true} (J);
    \draw[->] (I) to node {false} (K);
    \draw[->] (J) -- (L);
    \draw[->] (K) -- (M);
    \draw[->] (L) to node {true} (N);
    \draw[->] (L) to node {false} (O);
    \draw[->] (M) -- (P);
    \draw[->] (P) to node {true} (Q);
    \draw[->] (P) to node {false} (R);
  \end{tikzpicture}
  \caption{Cross product of the {\StateMachines} shown in
    figure~\ref{fig:cross_product_input}. \todo{redo layout} }
  \label{fig:cross_product_output}
\end{figure}

\begin{figure}
  \begin{tikzpicture}[align=center]
    \node[stateSideEffect, initial] (A) {\state{Assert} $y = 0 \wedge A \happensBefore F \wedge LD(x) \not= 0 \wedge F \happensBefore C$ };
    \node[stateTerminal, below = of A] (B) {\state{Crash} };
    \draw[->] (A) -- (B);
  \end{tikzpicture}
  \caption{Result of simplifying {\StateMachine} shown in figure~\ref{fig:cross_product_output}.}
  \label{fig:cross_product_output_opt}
\end{figure}

The algorithm used in $\implementation$ has a number of other features
not illustrated here:

\begin{itemize}
\item
  Atomic blocks are correctly maintained.  Issuing a
  \state{StartAtomic} side-effect causes the third field of the label
  tuple to be set to the issuing {\StateMachine} and from that point
  only that {\StateMachine} will be allowed to advance until it
  reaches an \state{EndAtomic} side-effect.  

\item
  It is not always necessary to consider both orderings for every
  memory access.  In particular, if the access in one machine cannot
  possibly alias with \emph{any} future access in the other then it is
  safe to dispense with the $\happensBefore$ condition and directly
  issue that access.

  \todo{Shrink this}

  Note that it is \emph{not} safe to simply test the two next accesses
  against each other and skip the $\happensBefore$ test if those two
  accesses happen not to conflict.  TO understand why, consider this
  diagram:

  \begin{tikzpicture}
    \node[draw] (L1) {L1};
    \node[draw, below = of L1] (L2) {L2};
    \node[draw, right = of L1] (S1) {S1};
    \node[draw, below = of S1] (S2) {S2};
    \node[draw, below = of S2] (S3) {S3};
    \draw[->] (L1) -- (L2);
    \draw[->] (S1) -- (S2);
    \draw[->] (S2) -- (S3);
    \draw[<->, dashed] (L1) -- (S2);
    \draw[<->, dashed] (L1) -- (S3);
    \draw[<->, dashed] (L2) -- (S2);
  \end{tikzpicture}

  This is supposed to indicate that the read-side {\StateMachine} has
  two operations, L1 and L2, and the store machine has three, S1
  through S3, and that L1 could alias with S1 or S3 while L2 can only
  alias with S2.  This graph cannot be correctly handled if only
  directly-racing accesses are considered for permutation.  The
  algorithm will start in the configuration (L1, S1, $\varnothing$,
  false, false), and determine that L1 can race with S1 so introduce
  an $\happensBefore$ test to distinguish the two cases.  Consider the
  case where L1 is issued before S1 first.  The algorithm will now
  have to encode a {\StateMachine} for the configuration (L2, S1,
  $\varnothing$, true, false).  L2 does not directly conflict with
  S1, so if only direct races were considered then it would be able to
  issue L2 and S1 in either order.  If it issues L2 first then that
  will lead to the read-side {\StateMachine} completing before the
  write-side one has started, and so all orderings where L1 comes
  before S1 will be ignored.

  Now consider the case where S1 is issued before L1.  The next
  configuration to be considered is then (L1, S2, $\varnothing$,
  false, true), and L1 does not directly interfere with S2, so the
  algorithm can immediately issue S2 and move on to (L1, S3,
  $\varnothing$, false, true).  Now the only option is to issue L1, as
  otherwise the store machine will complete before the load machine
  starts, moving to (L2, S3, $\varnothing$, false, true).  Finally, L2
  does not directly interfere with S3, and so only one ordering of the
  two accesses need be considered; assume the algorithm picks L2
  first.  Now the only access considered by the cross-product
  {\StateMachine} is S1, S2, L1, L2, S3, which clearly does not
  adequately cover the potential interleavings of these two
  {\StateMachines}.

  \todo{Need to mention the magic words ``partial order reduction'' in
    here somewhere.}

\item
  Of course, it is not always entirely clear when two accesses might
  overlap, as the addresses accessed can be completely arbitrary
  expressions.  In that case, the algorithm adds an additional
  assertion to one of the successor states requiring that the accesses
  alias. \todo{Explain why only one.}
\end{itemize}

\subsection{Path explosion}

One common problem in symbolic execution systems is path explosion:
the number of paths through a program rises exponentially in the size
of the program, and this can prevent na\"ive symbolic execution
systems from being applied to realistically large programs.  In the
case of \technique, there are two main causes of path explosion:

\begin{itemize}
\item
  Aliasing.  If the various simplification passes and the dynamic
  analysis cannot determine how memory accessing instructions alias
  then the symbolic execution engine must consider every possible
  aliasing pattern, of which there are $O(n^m)$, where $n$ is number
  of \state{Load} operations and $m$ the number of \state{Store} ones.
  This grows rather quickly in the number of unsolvable aliasing
  problems, especially when the number of \state{Store}s in the
  {\StateMachine} rises.  This represents one of the major limitations
  to \technique's scalability.
\item
  Thread interleaving.  The cross-product {\StateMachine} will have
  $O(nm)$ states, where $n$ is number of states in the read-side
  {\StateMachine} and $m$ the number in the write-side one.  The
  number of paths through the combined {\StateMachine} then grows as
  $O(2^(nm))$, which again grows rather quickly.
\end{itemize}

The result is that, in the common case where the read-side
{\StateMachine} consists mostly of \state{Load} operations and
write-side one mostly of \state{Store} ones, the symbolic execution
engine might have to consider up to $O((2n)^m)$ distinct paths when
evaluating the cross-product {\StateMachine}.  This is obviously
completely infeasible for even moderate values of $n$ and $m$.  For
good performance, {\technique} is completely reliant on the various
simplification and analysis techniques to reduce $n$ and $m$ to
something manageable.  Fortunately, as discussed in the evaluation,
they are able to do so in a useful set of cases.

\subsection{Use of the induction rule}

When analysing a potential crash at instruction $i$, it is sometimes
possible to show that the existence of a bug at $i$ would imply the
existence of a bug at an earlier instruction $i'$.  This allows
{\technique} to use a form of induction to eliminate some potential
bad pointer dereference-type bugs, or at the very lest to reduce the
number of very similar bugs reported.  The idea here is to take the
read-side {\StateMachine} from a candidate bug and truncate it,
cutting it off at some memory-accessing state, in such a way that the
truncated {\StateMachine} will report a crash if the original
{\StateMachine} would have dereferenced a bad pointer at that memory
access.  A new verification condition is then generated using that
truncated {\StateMachine} as the read-side {\StateMachine} and the
original write-side {\StateMachine} as the write-side.  A bug only
needs to be reported for the original pair of {\StateMachines} if
there is some initial state which satisfies the original verification
condition but does not satisfy the truncated one.

Of course, like most forms of induction, this requires a base case in
order to be sound.  The main bug-finding analysis therefore records
every time it had to use the induction rule to eliminate a bug,
including which instruction the eliminated bug was at and which
instruction was assumed to be bug-free in order to eliminate it.  Once
the main analysis is complete this log is checked for cycles.  If it
is cycle free then the induction rule is sound.  I have not, so far,
found any programs for which the graph contains a cycle, but if a
cycle were to be found then it could sensibly be handled by selecting
a single bug from the cycle and reporting that, ignoring all others.

This rule relies on the monotonicity property of the definition of
crashes, as discussed in \S~\ref{sect:monotonicity}.  As such, it is
not sound in the presence of mandatory concurrency, and might lead to
bugs which do not require mandatory concurrency being neglected simply
because they happen to be ``near to'' a bug which does.  As argued
previously, mandatory concurrency should be rare in most programs, and
so I do not expect this to be a problem in practice.  \todo{Might want
  a bit more there?}

\todo{Forward ref eval to say how useful this is.}

\subsection{Completeness}
\todo{I've kind of already covered this, but it might be worth pulling
  all of the completeness discussion together into one place.}

\section{The satisfiability checker}
This doesn't really belong here.  I should figure out where to put it.
I should also figure out whether I'd be better off using someone
else's checker.  The current one is pretty damn stupid; it tries a
couple of normal forms and then if they don't do anything useful does
an implicit analytic tableaux-type thing.

\section{Reproducing bugs}
\label{sect:reproducing_bugs}

Given a pair of {\StateMachines} and a verification condition it is
also possible to build a ``crash enforcer'' which will insert delays
into the program's execution in a way which will make the bug more
likely to reproduce.  This can then be used to determine which of the
many bugs reported by the prior analysis are actually reproducible,
and hence to appropriately target efforts in fixing them.  In effect,
the {\StateMachine} pair is turned into a custom dynamic analysis
which targets just the bug of interest.

I now illustrate the basic approach with a simple example before
giving details of the algorithms used.  Suppose that the bug to be
exhibited involves two threads:

\begin{verbatim}
int *global_ptr[];
void thread1(int idx1) {
    if (global_ptr[idx1])
        *global_ptr[idx1] = 7;
} 
void thread2(int idx2) {
    global_ptr[idx2] = NULL;
}
\end{verbatim}

Suppose further that these functions compile to this machine code:

\begin{verbatim}
thread1:

l1:   ADD global_ptr + idx1 -> reg1
l2:   LOAD *reg1 -> reg2
l3:   CMP 0, reg2
l4:   jmp_if_eq l7
l5:   LOAD *reg1 -> reg3
l6:   STORE 7 -> *reg3
l7:

thread2:

l8:   ADD global_ptr + idx2 -> reg4
l9:   STORE 0 -> *reg4
\end{verbatim}

There is a risk here that \verb|thread1| might crash if \verb|l9| is
interleaved between \verb|l2| and \verb|l5| and \verb|idx1 == idx2|.
The previous analysis phase will produce \StateMachines something like
these:

\begin{tikzpicture}
  \node[stateSideEffect,initial] (l2) {l2: Load $global\_ptr + idx1$ to $tmp1$};
  \node[stateIf,below = of l2] (l4) {l4: If $tmp1 == 0$?};
  \node[stateSideEffect, below = of l4] (l5) {l5: Load $global\_ptr + idx1$ to $tmp2$};
  \node[stateIf,below = of l5] (l6) {If $BadPtr(tmp2)$?};
  \node[stateTerminal,below right = of l6] (crash) {Crash};
  \node[stateTerminal,below left = of l6] (survive) {Survive};
  \draw[->] (l2) -- (l4);
  \draw[->] (l4) -- node {false} (l5);
  \draw[->] (l5) -- (l6);
  \draw[->] (l4.west) to [bend right=80] node {true} (survive);
  \draw[->] (l6.east) to [bend left=70] node {true} (crash);
  \draw[->] (l6.west) to [bend right=75] node {false} (survive);
  \begin{pgfonlayer}{bg}
    \node(box99) [fill=black!10,fit=(l2) (l4) (l5) (l6) (survive) (crash)] {};
  \end{pgfonlayer}

  \begin{scope} [xshift=8cm,yshift=-3cm]
    \node[stateSideEffect,initial] (l9) {l9: Store $0$ to $global\_ptr + idx2$};
    \node[stateTerminal,below=of l9] (end) {Finish};
    \draw[->] (l9) -- (end);
    \begin{pgfonlayer} {bg}
      \node [fill=black!10,fit=(l9) (end)] {};
    \end{pgfonlayer}
  \end{scope}
\end{tikzpicture}

With a verification condition that $idx1 == idx2 \wedge l2
\happensBefore l9 \wedge l8 \happensBefore l5$.  We can therefore
augment the \StateMachines with happens before edges as shown:

\begin{tikzpicture}
  \node[stateSideEffect,initial] (l2) {l2: Load $global\_ptr + idx1$ to $tmp1$};
  \node[stateIf,below = of l2] (l4) {l4: If $tmp1 == 0$?};
  \node[stateSideEffect, below = of l4] (l5) {l5: Load $global\_ptr + idx1$ to $tmp2$};
  \node[stateIf,below = of l5] (l6) {If $BadPtr(tmp2)$?};
  \node[stateTerminal,below right = of l6] (crash) {Crash};
  \node[stateTerminal,below left = of l6] (survive) {Survive};
  \draw[->] (l2) -- (l4);
  \draw[->] (l4) -- node {false} (l5);
  \draw[->] (l5) -- (l6);
  \draw[->] (l4.west) to [bend right=80] node {true} (survive);
  \draw[->] (l6.east) to [bend left=70] node {true} (crash);
  \draw[->] (l6.west) to [bend right=75] node {false} (survive);
  \begin{pgfonlayer}{bg}
    \node(box99) [fill=black!10,fit=(l2) (l4) (l5) (l6) (survive) (crash)] {};
  \end{pgfonlayer}

  \begin{scope} [xshift=8cm,yshift=-3cm]
    \node[stateSideEffect,initial] (l9) {l9: Store $0$ to $global\_ptr + idx2$};
    \node[stateTerminal,below=of l9] (end) {Finish};
    \draw[->] (l9) -- (end);
    \begin{pgfonlayer} {bg}
      \node [fill=black!10,fit=(l9) (end)] {};
    \end{pgfonlayer}
  \end{scope}

  \draw[->,happensBeforeEdge] (l2.south) -- (l9.north);
  \draw[->,happensBeforeEdge] (l9.south) -- (l5.north);
\end{tikzpicture}

The task is then to modify the program so as to make it more likely
that this happen-before graph will be satisfied when the program runs
while leaving its behaviour otherwise unchanged.  This can be
accomplished by inserting small delays into the program's execution.
In this case, simply inserting a delay before \verb|l5| would probably
be sufficient, as that would enlarge the critical section and hence
make it more likely that the critical store will intervene.

\begin{figure}
  \begin{tikzpicture}
    \node[draw] (A) {A};
    \node[draw, below right = of A] (X) {X};
    \node[draw, below left = of X] (B) {B};
    \node[draw, below right = of B] (Y) {Y};
    \node[draw, below left = of Y] (C) {C};
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (X) -- (Y);
    \draw[->,happensBeforeEdge] (A) -- (X);
    \draw[->,happensBeforeEdge] (X) -- (B);
    \draw[->,happensBeforeEdge] (B) -- (Y);
    \draw[->,happensBeforeEdge] (Y) -- (C);
  \end{tikzpicture}
  \caption{A happens-before graph which cannot be reproduced by
    inserting a single delay.}
  \label{fig:enforce_crash:complex_hb}
\end{figure}

More complex happens-before graphs can make it more complex to
determine where delays should be inserted.  Consider, for instance,
the happens-before graph shown in
figure~\ref{fig:enforce_crash:complex_hb}.  This is intended to show
two threads, one which executes instructions A, B and C in order and
the other which executes X and then Y, where the behaviour of interest
only happens with the interleaving A, X, B, Y, C.  There is no simple
critical section structure here, making it less obvious where delays
need to be inserted.  Even once it has been determined where to insert
the delays, deciding their magnitude remains non-trivial: the delay
between X and Y for instance, must be large enough to be confident
that B happens before Y if A happens before X, but not so large that C
is also likely to happen before Y.  Simply picking the largest delay
which avoids unacceptable performance overheads risks masking such
bugs.  Solving such constraints requires a far more detailed model of
the program's structure and the time taken by various instructions,
and such models are both difficult to derive and fragile once they are
available.

SLI solves this problem using a message-passing system.  The core idea
is to model a happens-before ordering X before Y as a message which is
sent by X, after it completes, and collected by Y, before it starts.
These messages are synchronous: the sender will wait for the receiver,
and the receiver will wait for the sender, in both cases with a short
timeout.  In the first example, there will be two messages, one sent
from \verb|l2| to \verb|l9| and the other sent from \verb|l9| to
\verb|l5|.  Delays will be inserted immediately before \verb|l9| and
\verb|l5|, and also immediately after \verb|l2| and \verb|l9|.  These
delays have slightly different functions:

\begin{itemize}
\item
  The delay after \verb|l2| makes the read-side of the critical
  section wait for a matching write-side.  In effect, this delay
  enlarges the read-side critical section in the hope that a
  write-side operation will come along which can be dropped into it.
  This is useful for bugs where the write-side occurs much more
  frequently than the read-side.
\item
  The delay before \verb|l9| makes the write-side wait for a matching
  read-side.  The intuition here is that the enforcer will maintain a
  ``pool'' or write-side operations, which can then be deployed as
  soon as a read-side operation turns up.  This is useful for bugs
  where the read-side occurs much more frequently than the write-side.
\item
  The delays after \verb|l9| and before \verb|l5| help the read- and
  write-sides of the critical section to proceed with the desired
  interleaving.  By this point, the two threads have rendezvoused and
  been bound together, and so there is no need to wait for a matching
  operation to arrive, and the delay is necessary only to wait for the
  paired thread to reach the appropriate place in its control-flow
  graph (or to exit the simulation, causing the message operation to
  fail).  The timeout is in this case necessary only to prevent
  deadlocks: it is possible that the program contains some
  synchronisation structure of which SLI is unaware, so that one
  thread might be waiting for the other, and introducing an additional
  unbounded wait would be unsafe.
\end{itemize}

Setting the sizes of these delays, especially those after \verb|l2|
and \verb|l9|, involves a delicate trade-off between performance and
the likelihood of uncovering bugs.  In general, SLI will choose one of
\verb|l2| or \verb|l9| as a delay-able instruction, according to their
relative frequency, and use a large timeout for the delay-able
instruction and a very small one for the non-delay-able one.  This is
discussed further in section\needCite{}.

Encoding the second example as a message-passing system requires more
message operations but does not introduce any additional conceptual
complexity.

This basic message passing scheme is sufficient to ensure that the
instructions of the program obey the happens-before graph.  That is
sufficient to trigger the desired behaviour in some cases but not all.
In particular, it can be insufficient if there is some additional
non-trivial side-condition required before the bug reproduces, beyond
simply requiring a particular happens-before graph.  Suppose, for
instance, that the read- and write-side {\StateMachines} are
operations on some dynamic structure and that there are many instances
of the structure in the running program.  The bug will only be
reproduced if the two threads happen to access the same instance of
the structure, and, if since there are a large number of such
instances, that is a low-probability event and the probability of the
bug being reproduced remains very low even when all happens-before
relationships are satisfied.  Even worse, the additional delays mean
that the buggy code is likely to run less often than it otherwise
would be, and so enforcing the happens-before graph in isolation might
actually make the bug less likely to reproduce.

Fortunately, {\technique} already knows about all such
side-conditions, as they form the bulk of the verification condition
whose derivation has already been discussed.  This problem can
therefore be avoided if the crash enforcer checks the verification
condition at the same time as it enforces the happens-before graph.
Once a verification condition has failed there is no need to insert
additional delays, reducing the overhead of the enforcement patch so
that the buggy code will run more often in unit time and hence
increasing the likelihood of the desired bug being exhibited.

In the case of the example, the verification condition is $idx1 ==
idx2$, and so we are only interested in executions where the two
indices coincide.  There is then one immediate complication: $idx1$ is
a local variable in one thread, and $idx2$ is a local variable in a
different thread.  There are no points in the original program which
know the values of both variables, and so no obvious place in which to
check the condition.  {\Technique} solves this problem by checking the
condition as part of the \verb|l2| to \verb|l9| message operation.  If
\verb|l2| is delayed while sending the message, it will publish the
value of \verb|idx1| to a globally-accessible location, and \verb|l9|
will then check that as part of its receive operation.  If the indices
do not match, the receive operation fails and \verb|l9| will continue
waiting for another sender subject to its own timeout.  Likewise, if
\verb|l9| is delayed while receiving the message it will publish the
value of \verb|idx2|, and this will then be checked by any other
thread trying to send the message from \verb|l2|.  A timeout balancing
mechanism will then ensure that the timeouts are adjusted so that both
\verb|l9| and \verb|l2| occur with reasonable frequency so that a
message operation is likely to succeed eventually, provided that the
bug really is possible.

The result of this phase is a crash enforcement plan containing two
message operations:

\begin{itemize}
\item Send a message from \verb|l2| to \verb|l9|, including the value
  of $idx1$, provided that $idx1 = idx2$.
\item Send a message from \verb|l9| to \verb|l5|, with no additional
  data or side-condition.
\end{itemize}

The original program can then be modified so as to follow this plan
when possible.  I present two basic mechanisms for doing so: an
interpreter, which must interpret the original program's machine code
while it is executing instructions involved in the plan, and a
compiler, which avoids the interpreter by compiling the plan down to
machine code, hence improving performance but at the expense of less
effective handling of some corner cases.  As a further refinement, I
also present a scheme for combining multiple enforcers, so that
several bugs can be investigated at once.

\todo{The implementation of the compiler is currently broken.  Need to
  decide whether I'm going to fix it or just drop that bit.}

\subsection{Outline of algorithm}

At a high level, the algorithm used has the following phases:

\begin{itemize}
\item
  Heuristically simplify the crash summary.  The main bug-finding
  analysis phase attempts to be sound, within parameters already
  discussed, which can sometimes lead to the verification condition
  containing clauses which might in theory influence the behaviour of
  the program but in practice are highly unlikely to.  The first phase
  of building a crash enforcement plan is to remove some of these
  clauses.  This is discussed in
  section~\ref{sect:enforce:heuristic_simplify}.
\item
  ``Slice'' the verification condition according to the possible
  happens-before graphs. As discussed in
  section~\ref{sect:intro:overview}, {\technique} can correctly
  capture the happens-before graph necessary for a bug to reproduce
  even when that happens-before graph is data-dependent.
  Unfortunately, those data-dependencies make it difficult to encode
  the happens-before graph into a simple message passing system.
  {\Technique} therefore re-expresses the verification condition as a
  function of the happens-before graph, effectively eliminating the
  dependency.  This is discussed in more detail in
  section~\ref{sect:enforce:slice_hb_graph}.
\item
  The algorithm now considers each possible happens-before graph in
  turn and, for each one:

  \begin{itemize}
  \item
    Determines what information is needed to evaluate the verification
    condition, in terms of the machine registers and the contents of
    memory, and where that information first becomes available;
  \item
    Decides where to evaluate the various clauses of the verification
    condition;
  \item
    Defines the ``payload'' data which must be included in the various
    messages.
  \end{itemize}

  This is discussed in section~\ref{sect:enforce:place_vcs}.
\item
  Attempt to optimise the resulting enforcement plan.  The obvious
  algorithm for placing verification conditions can sometimes lead to
  the program spending an excessively large amount of time in the
  interpreter, for instance, and {\technique} uses a few simple
  heuristics to move things to more useful places.  These are
  discussed in section~\ref{sect:enforce:optimise_plan}.
\item
  If appropriate, multiple enforcers can be combined at this point,
  which can sometimes make it easier to discover bugs if the initial
  analyses have produced a very large pool of candidates.  More
  details are given in section~\ref{sect:enforce:combine_enforcers}.
\item
  Decide on a strategy for gaining control of the program at
  appropriate points.  The crash enforcement plan only imposes
  constraints on instructions which are likely to be involved in the
  target bug, which is usually a tiny fragment of the entire program.
  It would be deeply unfortunate if the entire program had to be
  rewritten to insert the necessary modifications; quite aside from
  the greater risk of bugs, the performance would probably be quite
  poor.  The interpreter is obviously far slower than running machine
  code directly, and even the compiler introduces significant
  overheads.  As such, it is necessary to modify the original program
  so that {\technique} can gain control when the program reaches one
  of the interesting fragments of code.
  Section~\ref{label:sect:assign_entry_points} discusses how this is
  done.

\item
  Depending on the desired mode of operation, the plan is then passed
  to either the plan interpreter, discussed in
  section~\ref{sect:enforce:interpreting}, or the compiler, discussed
  in section~\ref{sect:enforce:compiling}.
\end{itemize}

\subsection{Heuristic simplifications of the {\StateMachines} }
\label{sect:enforce:heuristic_simplify}

The crash summaries generated by the main analysis often contain a lot
of redundant information, and this can complicate the process of
enforcing the crash.  The first phase of generating an enforce is
therefore to remove this ancillary information.  Most obviously, all
of the parts of the {\StateMachines} used only to provide hints to the
simplifiers and symbolic execution engine can be dropped:
\state{Assert}, \state{PointsTo}, \state{StartFunction}, and so forth.
This often allows further simplifications by, for instance, making
some of the variables used in the assertions dead, or by making states
which differ only in their calling context more easily unified.

Next, the R atomic and W atomic parts of the verification condition
can be removed.  These restrict the analysis to only consider the case
where the read-side is run completely in isolation and where it is run
atomically after the write-side completes, respectively.  This is
appropriate when trying to determine statically whether a concurrency
bug might exist, but is unlikely to be helpful when trying to enforce
a concurrency bug.  If R atomic fails then the read-side is likely to
crash without any help from a concurrent thread, and so the pattern of
delays which the enforcer applies is unlikely to make a great deal of
difference either way.  Having the enforcer attempt to check R atomic
is therefore rather pointless.  Likewise, seeing the W atomic
constraint fail usually indicates that the program has a bug, but it
is not one which the enforcer can do anything about.  The analysis
therefore re-derives R atomic and W atomic constraints and simplifies
the verification condition under the assumption that both are true.

Note that this is not quite the same as simply replacing the
verification condition with the crash possible constraint, as the
information in the atomic constraints will have been used to constrain
the paths considered during symbolic execution of the cross-product
{\StateMachine}.  \todo{Should really figure out whether that's
  actually a good thing or not.}

\todo{This suggests another possible validation thing for the eval: do
  something which evaluates R atomic and W atomic instead of trying to
  enforce the HB graph and see how often they fail.  R atomic,
  certainly, should never fail.  W atomic might due to synchronisation
  things, so that might turn out to be less interesting.}

\subsubsection{Phase 3 canonicalisation}

Something about load canonicalisation, and why it has to be reversed
later?

Important thing to worry about here is satisfiability of verification condition.

\subsubsection{Equality substitution}
Find equality constraints in the verification condition and use them to eliminate register from the summary.
This is done even when the result is more ``complex'' than the original input summary.
Approach is to find all of the registers which we can eliminate, then pick the one which occurs most often and eliminate it, then repeat until we can't eliminate anything else.

\todo{Need to come up with a coherent explanation of why doing this during analysis is bad.  Experimentally, it is, but it's not entirely obvious why that should be so.}

\subsubsection{Removal of redundant clauses}
The verification condition can sometimes include constraints on registers and memory locations which do not occur anywhere in any of the \StateMachines, usually because the \StateMachines have been simplified after the relevant part of the condition was derived.
In the simplest case, these variables are completely independent of the interesting variables.
\todo{Interesting variables are those which appear, or might appear (e.g. LD aliasing), in the \StateMachines.}
To find these, convert the verification condition to conjunctive normal form and then draw a graph whose nodes are variables and which has an edge between A and B if there is any clause in the verification condition which mentions both A and B.
Now find the connected components in this graph, $C_i$.
The verification condition can then be written as $f_1(C_1) \wedge f_2(C_2) \ldots$.
If any of the $C_i$ don't mention any variables in the interesting set then $f_i(C_i)$ can be set to true and hence discarded.

This is safe if $f_i(C_i)$ is satisfiable, which is the common case anyway.

\subsubsection{Removal of underspecified clauses}

If a free variable (i.e. one which isn't mentioned in the \StateMachines) occurs in precisely one place in the verification condition then it is referred to as being underspecified.
This means that, from the point of view of satisfiability checking, it can be set to anything at all, without reference to the rest of verification condition, which in turn means that certain clauses can become trivially satisfiable.
For instance, if $x$ is underspecified in this sense, and the verification condition includes the clause $x == y$, then a satisfiability checker would be able to select an $x$ to make that either true or false, and so our simplifications can assume that $x == y$ is either true or false according to whatever happens to be most convenient in context.

\subsubsection{Functionalisation/conditional independence}

This is analogous to the SSA transformation, but for boolean expressions rather than for programs.
The idea is that if you have a function of two free variables $f(x, y)$, you can treat $y$ as a function of $x$ to get $f(x, y_x)$
If $x$ and $y$ are boolean variables then you can then do a case split on $x$ to get $(x \wedge f(T, y_T)) \vee (\not{}x \wedge f(F, y_F))$.
$y_F$ and $y_T$ are then separate variables, and the two $f$ cases can be subjected to redundant clause removal and underspecified clause removal independently.

\todo{This is in dire need of an example.}

\todo{This is a lot like re-encoding the program's control flow into
  the verification condition, but in a way which is kind-of minimal
  and only contains the bits of control flow which are actually
  relevant.}

\todo{There are some interesting parallels with Skolemization here.
  In fact, this is almost the precise opposite of conversion to Skolem
  normal form.}


\subsection{Slicing the verification condition by happens-before graph}
\label{sect:enforce:slice_hb_graph}

\subsection{Placing the evaluation of verification conditions}
\label{sect:enforce:place_vcs}
The verification condition has now been factored into a happens-before
graph and a conjunction of side conditions which must be enforced in
addition to the happens-before graph for the bug of interest to
reproduce.  It is now necessary to decide, for each such expression,
where in the CFG it is to be evaluated.  Each expression is placed
independently.  There are several constraints on the placement of
expressions:

\begin{itemize}
\item
  It must be possible to evaluate the expression at that point in the CFG.
  In other words, a thread which is at that place in the CFG must know the values of all of the variables which are used in the expression.
\item
  Expressions should not be evaluated more often than strictly necessary, for simple efficiency reasons.
\item
  Expressions should be evaluated as early as possible, so that threads which are definitely not going to trigger the bug are not unnecessarily delayed.
\end{itemize}

SLI starts by determining the complete set of CFG nodes which satisfy the first constraint and then selecting a subset of those nodes at which to actually evaluate the expression using the second and third constraints.

To determine the set of nodes at which an expression is in principle evaluatable, SLI first builds a map showing which variables are available at each node.
A variable is available at a node if either:

\begin{itemize}
\item
  the node generates the variable, or
\item
  the variable is available at all of the node's possible control-flow predecessors, or
\item
  the node has a happens-before predecessor and the variable is available at that predecessor.
\end{itemize}

Note, in particular, that a variable is available if it has been received over a happens-before edge, even if it isn't available at any control-flow predecessors, and that, at this stage, we effectively assume that all messages carry as payload all of the variables which are available at the source of the message.
An expression is evaluatable at a node if all of the variables used by the expression are available at that node.

Given the set of places at which the expression could conceivably be evaluated we must now select which nodes to actually evaluate at.
We do this by eliminating all of the places at which the expression should definitely not be evaluated and then evaluating it at all of the remaining nodes.
An expression should not be evaluated at a node if either:

\begin{itemize}
\item the expression could be evaluated at all of the control-flow predecessors of the node, or
\item the node has a happens-before predecessor and the expression could be evaluated at that node.
\end{itemize}

This resulting assignment of expressions to nodes satisfies the third requirement, of evaluating expressions as soon as possible, but is guaranteed to satisfy the second one, of evaluating expressions the minimum number of times.
Redundant evaluation is, however, very rare, and is only a minor performance problem when it does happen, so this is not a major problem.
\editorial{I'm at least half convinced that it can't happen at all in practice, but showing that requires lots of complicated interactions with other phases, so I don't want to do that.}

\subsubsection{Computing message payloads}

Once expressions have been assigned to nodes in the graph, it is possible to determine where variables are actually needed, and hence what ancillary information needs to be included in messages.
This is a simple data flow problem.

\todo{This really needs to be discussed somewhere, but it's so simple that it doesn't want a section to itself, and it doesn't really fit anywhere else.}

\subsection{Optimising the crash enforcement plan}
\label{sect:enforce:optimise_plan}
Defer register stash, strip redundant CFG prefix.

\subsection{Combining multiple enforcers}
\label{sect:enforce:combine_enforcers}

\subsection{Determining patch entry points}
\label{sect:enforce:assign_entry_points}
This ends up being far, far more complicated than it has any right to
be (and in fact the first three schemes I came up with didn't work,
for one reason or another).  The most important constraint is that you
can't ever let the original program jump in halfway through one of the
branch instructions we've patched in.  The algorithm used ends up
being an exhaustive search; fortunately, most of the time the problem
is easy and the search completes almost immediately.

\subsection{Enforcing the plan}

\todo{This is massively fiddly to implement, but only really needs a
  few basic ideas to get the message across.  Best way of describing
  it is probably just to give the semantics of the cross machines, and
  then just state that the CFG compiler implements them, rather than
  trying to give the actual compilation algorithm.}

We now have a CFG whose nodes are annotated with several potential additional operations:

\begin{itemize}
\item Store a generated value into a simulation slot.
\item Send a message, with some payload expressions.
\item Receive a message, storing the payload expressions into simulation slots.
\item Evaluate a side condition.
\end{itemize}

And we also have a mapping from locations in the original program to points in the control-flow graph.
Our task now is to force the program to follow this plan.
SLI implements two mechanisms for doing so:

\begin{itemize}
\item
  An interpreter with well-defined semantics, a high likelihood of successfully imposing the plan, and some useful theoretical properties, but very high run-time overheads.
\item
  A compiler which makes far more approximations, and hence is far less likely to impose the plan successfully and far less analytically tractable, but which has slightly lower run-time overhead.
\end{itemize}

The interpreter is somewhat easier to understand and so I discuss it first\editorial{Even though I actually implemented the compiler before the interpreter.}.

\subsubsection{Interpreting the plan}
\label{sect:enforce:interpreting}

The plan interpreter runs in the address space of the target program.
It arranges to take control of the program at the plan entry points and then interprets the program's machine code until the plan either completes or fails.
In either case, the interpreter then restores the target program's register state and branches back to the original program's code.

\todo{Should mention that I pulled the interpreter out of Xen.}

The annotated CFG forms, in effect, a very simple language, with very simple semantics.
Unfortunately, those semantics are non-deterministic, in the sense that the interpreter must often choose between several possible options using information which only becomes available later in the execution.
SLI resolves this issue using a power set-like construction\editorial{Probably want a cite for that, maybe.}.
This means that we must first define a low-level, abstract, semantics for the interpreter, using that look-ahead nondeterministic choice operator, and then implement a higher-level, concrete, interpreter which effectively interprets sets of lower-level interpreters in lockstep parallelism.
In effect, the higher-level interpreter resolves the non-deterministic choice by forking the lower-level interpreter as necessary, allowing them all to execute at first, and then later discarding any which fail.
We present the abstract semantics for the low-level interpreter first, and will then discuss the subtleties involved in implementing the higher-level interpreter afterwards.

The non-deterministic language emulates the annotated CFG one node at a time.
For each node, it proceeds through these stages:

\begin{itemize}
\item[Stash]
  Examine the annotations on the CFG node.
  If these include an instruction to stash a register to a simulation slot, do so now.
  This might make some side-conditions evaluatable.
  If so, evaluate them immediately, exiting the interpreter if any fail.
\item[RX]
  Receive any message demanded by the plan.
  The most important part of a message-receive operation is selecting a message-transmit operation to synchronise with.
  The procedure for doing so depends on the type of receive operation:

  \begin{itemize}
  \item
    Unbound receives.
    The first message operation performed by an interpreter is ``unbound'', meaning that it can synchronise with any other low-level interpreter in a different thread of the program.
    In the abstract semantics, these operations are simple: each has a delay $t$ associated with it, and looks forward $t$ seconds through the program's execution to find all suitable message send operations, and then non-deterministically chooses one to synchronise with.
    The receiving thread is delayed until the chosen message-send operation happens, the two threads are bound together, and the receive proceeds.

    Instantiating this into a concrete interpreter is moderately subtle and is discussed in more detail below.
  \item
    Bound receives.
    Every message operation which is not the first is ``bound'', meaning that there is only one thread which can possibly be synchronised with.
    If that thread is ready to transmit a suitable message then the receive can proceed immediately.
    Otherwise, the receiving thread is delayed until its bound peer is ready to transmit.
  \end{itemize}

  Once a send operation has been selected the message operation can be discharged.
  Relevant simulation slots in the message sender are copied into the message payload area and thence to the receiver's simulation slots.
  This, again, might make further side-conditions evaluatable, and if so they are checked here.
\item[Emul]
  Emulate the original program's instruction corresponding to this CFG node.
  This includes issuing any memory loads which the program would issue at this point, and stashing the values of those loads if the crash enforcement plan says to do so.
  This might make additional side-conditions evaluatable.
  As in the stash phase, these side-conditions are evaluated as soon as possible, and the interpreter exits if they fail.
\item[TX]
  Send any message demanded by the plan.
  This is the converse of the receive operation: select an appropriate receive operation to synchronise with, performing a non-deterministic choice if more than one is available, and then copy local simulation slots to remote ones in accordance with the message payload defined by the crash execution plan.
\item[Succ]
  Find successor instructions.
  The emulation phase will have determined the instruction pointer of the next instruction to be executed.
  This can be compared to the CFG to determine which control-flow nodes might need to be executed next.
  There are three interesting cases:

  \begin{itemize}
  \item
    None of the successor nodes of this CFG node have the desired instruction pointer.
    In that case, the interpreter can proceed no further and exits.
  \item
    Precisely one of the successor nodes has the desired instruction pointer.
    The interpreter simply advances to that node.
  \item
    Multiple successor nodes have the desired instruction pointer.
    The most common reason for this is loop unrolling: if a loop in the program is unrolled three times, say, then the CFG node for the instruction just prior to the loop will have four successors, corresponding to skipping the loop completely or running it once, twice, or three times.
    It is not possible, at this stage, to determine how many times the loop must be run, and so the abstract interpreter makes a non-deterministic choice between all of the available options.
  \end{itemize}
\end{itemize}

\subsubsection{Sending and receiving messages in the abstract semantics}

If both the sending and receiving threads of a message operation are known, the operation can be modelled by this simple Petri net:

\begin{tikzpicture}
  \node[place,tokens=1,label=above:{before receive}] (beforeRx) {};
  \node[place,tokens=1,right = of beforeRx, label=above:{before transmit}] (beforeTx) {};
  \node[place,tokens=0,below=of beforeRx, label=below:{after receive}] (afterRx) {};
  \node[place,tokens=0,below=of beforeTx, label=below:{after transmit}] (afterTx) {};
  \node[transition,below right=of beforeRx, label=right:{discharge message}] (trans) {}
  edge [pre] (beforeRx)
  edge [pre] (beforeTx)
  edge [post] (afterRx)
  edge [post] (afterTx);
\end{tikzpicture}

Discharging a message here means copying the relevant bits of state from the transmitting thread's local state into the receiving thread's local state.
A message can be discharged if there is one thread willing to send it and one thread willing to receive it.
While the message is being discharged the two threads are effectively merged, with only one thread actually executing the message side effects.
Once the message is finished the two threads separate again and continue to execute independently.

A thread is only willing to receive a message if the message would pass the thread's message filter.
This has two parts:

\begin{itemize}
\item
  The message must have the correct message ID.
  This simply means that the send and receive parts of the message operation must be trying to enforce the same happens-before edge.
\item
  The message must pass the receiving thread's message receive filter.
  This filter consists of all of the side-conditions present in the crash enforcement plan which will become evaluatable once the message has been received.
\end{itemize}

However, in a real implementation, the threads are not pre-specified, and most of the complexity of the message algorithm lies in determining them.
When the plan demands that a message be sent or received, one side can be determined trivially, but the other must be discovered.
For a receive operation, the algorithm to do so in the abstract semantics looks like so:

\begin{algorithmic}[1]
  \If {$l$ has a bound thread}
    \If {The bound thread does not have an outgoing message}
      \State {Wait for it to send one}
    \EndIf
    \If {The bound thread has an outgoing message and it passes the message filter}
      \State {Discharge the message}
    \Else
      \State {$l$ has failed; remove it from the active set}
    \EndIf
  \Else
    \State {Examine the set of outstanding unbound message sends and collect all of the ones which pass the message filter into $s$}
    \State {Extend $s$ with $\bot$}
    \State {Choose $s'$ non-deterministically from $s$}
    \If {$s\prime = \bot$}
      \State {Register $l$ as a receiver of unbound messages}
      \State {Wait for the delay specified in this receive operation}
      \State {Unregister $l$ as a receiver of unbound messages}
      \State {Collect all of the unbound sends which pass the filter which started while we were waiting into $s$}
      \State {Select $s'$ non-deterministically from $s$}
    \EndIf
    \State {Discharge $s'$}
  \EndIf
\end{algorithmic}

The send operation is symmetric:

\begin{algorithmic}[1]
  \If {$l$ has a bound thread}
    \If {The bound thread is attempting to receive a message}
      \State {Wait for it to start receiving}
    \EndIf
    \If {The bound thread is receiving a message and this message would pass its filter}
      \State {Discharge the message}
    \Else
      \State {$l$ has failed; remove it from the active set}
    \EndIf
  \Else
    \State {Examine the set of outstanding unbound message receives and collect all of the ones whose filters this message would pass into $s$}
    \State {Extend $s$ with $\bot$}
    \State {Choose $s'$ non-deterministically from $s$}
    \If {$s' = \bot$}
      \State {Register $l$ as a sender of unbound messages}
      \State {Wait for the delay specified in this receive operation}
      \State {Unregister $l$ as a sender of unbound messages}
      \State {Collect all of the unbound receives whose filters this message would pass which started while we were waiting into $s$}
      \State {Select $s'$ non-deterministically from $s$}
    \EndIf
    \State {Discharge $s'$}
  \EndIf
\end{algorithmic}

As shown in figure ..., each message operation effectively defines an interval in time, and a send and receive match up if these windows overlap.
The behaviour when $s' = \bot$ is perhaps somewhat surprising: the thread waits a little while and then selects a peer thread to discharge the message with non-deterministically.
Meanwhile, all of the other threads are simultaneously performing similar non-deterministic choices.
The use of look-ahead nondeterminism means that all of the threads will make these selections in a mutually compatible way, so that there is no danger of A attempting to discharge its message with B while B discharges with C.
The actual implementation must resolve these constraints much more carefully, and is discussed in detail later.

Note that the message receive filter is evaluated as the message is being discharged, while the two threads are merged.
It is possible to imagine an alternative implementation in which the filter is instead evaluated locally in the receiver after the discharge operation is complete.
This would reduce the size of the synchronised section and so would appear, on the face of it, to offer greater parallelism, and hence potentially better performance.
Unfortunately, it does not work.
To illustrate the problem, consider again the example shown in figure \todo{...}.
One thread modifies a shared structure while another thread reads it, and the program will crash if the two threads happen to be operating on the same structure at the same time.
Suppose that the read thread runs far more often than the write one and that there are many instances of the structure.
The timeout balancing logic will quickly reduce the delay on the read side's first message send to zero and increase the delay on the write side's first message receive to compensate.
Now, when the write thread does run, it will stop just before \verb|l9| waiting for a matching read thread to arrive.
By hypothesis, many such threads will arrive, as the read thread runs more often than the write one.
In the alternative design, the read thread cannot evaluate the write thread's receive filter, and so every read thread will attempt to bind to the write thread, forcing the write thread to be duplicated many times.
Because of timeout rebalancing, the read thread will proceed from \verb|l2| immediately and quickly reach \verb|l5|, where it has to receive a message from \verb|l9|.
At this point, there are two possible outcomes:

\begin{itemize}
\item
  The thread is delayed at \verb|l5| waiting for the message from \verb|l9|.
  The write thread is still waiting in case any other threads reach \verb|l2| and attempt to synchronise with it, and so this might potentially be a rather long delay.
  Since the read thread runs far more often than the write thread, this will have a very large performance impact.
  Worse, it will probably be pointless: there, by hypothesis, a large number of instances of the structure which is being examined, and so, with high probability, the write thread will be modifying a different one.
  When the write thread does finally escape from its receive delay and evaluate its receive filter it will discover that the filter fails, and so the write thread will exit.
  The read thread will then discover that its bound thread has exited and be forced to exit as well.
  The crash enforcement plan will therefore not complete and the bug is highly unlikely to reproduce.
  
  Even worse, the performance hit might mean that the read thread will run far less frequently, further reducing the likelihood of the bug reproducing.
  In extreme cases, the attempt at enforcing a crash might actually make the bug less likely to reproduce in unit time.
\item
  The thread is not delayed at \verb|l5|.
  It never receives the message from \verb|l9| and must therefore exit without completing the plan, so is unlikely to reproduce the bug.
  The write thread's high-level interpreter will then accumulate a collection of low-level threads which have bound to exited read threads and which will themselves immediately exit.
\end{itemize}

Neither outcome helps to reproduce the target bug.

By contrast, in the scheme used by SLI, the read thread is able to evaluate the write thread's message receive filter at \verb|l2|.
It will therefore only bind to write threads which modify the structure which it is reading.
That means that the thread can be delayed a relatively long time at \verb|l5| without fear of apocalyptic performance damage, and so the bug will reproduce relatively easily.

\subsubsection{Discuss the timeout balancing bit}

Selecting the size of the various timeouts is important for determining the likelihood of reproducing a bug and the overheads of enforcing the patch.
SLI does so primarily dynamically, in response to the program's observed behaviour.

\subsubsection{Implementing non-deterministic choice in the Succ phase}

It might be that an instruction has several possible successors in the control flow graph in the crash execution plan, and in that case the interpreter must choose one of these successors using look-ahead non-determinism.
This cannot be implemented in any physically-realisable system, as it is non-causal, and so SLI must emulate it.
SLI uses a power-set construction to do so.
Rather than operating a single interpreter context, the actual implementation maintains a set of low-level interpreter contexts, which roughly follow the abstract semantics given above, and interprets them all in lock-step parallelism.
When one of these low-level interpreters needs to perform a non-deterministic choice between $n$ possible values, the high-level interpreter creates $n$ successor low-level interpreter states, one corresponding to each possible outcome of the choice, and inserts all of them into its current-state set.
They are then interpreted in parallel until enough information is available to resolve the earlier choice, at which point all but one of the threads will exit and the interpreter can revert to single-threaded execution.
If a thread is bound when it performs a non-deterministic choice then its bound thread must also be duplicated, to ensure that the new thread has something to bind to.

One subtlety here is that the original program's underlying instruction can only be retired once, and so the high-level interpreter must ensure that all low-level interpreters arrive at that point in the execution cycle at the same time.
SLI actually enforces a slightly stronger constraint, which is that every low-level interpreter in a given high-level interpreter must be at the same phase in the instruction execution cycle.
The only phases for which this is difficult are the message send and receive phases, which are discussed in more detail in the next section.

\subsubsection{Concrete implementations of message send and receive}

The message receive operation looks like this:

\begin{algorithmic}[1]
  \State {$lls \gets $ the set of currently-active low-level interpreter states}
  \State {$newLls \gets $ an empty set of low-level interpreter states}
  \For {$l$ in currently-active low-level interpreter states}
    \If {$l$ does not receive any messages}
      \State {Move $l$ from $lls$ to $newLls$ without changing it}
    \ElsIf {$l$ has a bound thread}
      \If {$l$'s bound thread has exited}
        \State {$l$ exits as well; remove it from $lls$ without adding it to $newLls$}
      \ElsIf {$l$'s bound thread has an outgoing message}
        \If {The bound thread's outgoing message passes the message filter}
          \State {Copy stashed values from the sending low-level interpreter's state to the receiving one}
          \State {Move $l$ from $lls$ to $newLls$}
        \Else
          \State {$l$ exits; remove it from $lls$}
        \EndIf
      \Else
        \State {} \Comment {Wait for a the bound thread to send a message}
      \EndIf
    \Else
      \State {} \Comment{Unbound receive}
      \For {$s$ registered unbound senders}
        \If {$s$'s outgoing message passes the message filter}
          \State {$l' \gets $ duplicate $l$}
          \State {$s' \gets $ duplicate $s$}
          \State {Copy stashed values from $s'$'s state to $l'$'s}
          \State {Bind $l'$ and $s'$ together}
          \State {Insert $l'$ into $newLls$}
          \State {Insert $s'$ into $s$'s high-level interpreter's active low-level interpreter list}
        \EndIf
      \EndFor
      \State {Register $l$ as an unbound receiver}
    \EndIf
  \EndFor

  \If {$lls$ is empty}
    \Return
  \EndIf

  \State {$end \gets now() + bound\_delay$}
  \If {There is a minimum delay}
    \State {Release lock}
    \State {Sleep for the minimum delay}
    \State {Acquire lock}
  \EndIf

  \While {There are bound receives in $lls$ and $now() < end$}
    \State {Release lock}
    \State {Wait for some bound receive to complete, or for the current time to pass $end$}
    \State {Acquire lock}
    \For {$l$ performing bound receives in $lls$}
      \If {$l$'s bound thread has exited}
        \State {Remove $l$ from $lls$}
        \State {$l$ exits}
      \ElsIf {$l$'s receiving-bound-message flag is clear}
        \State {Remove $l$ from $lls$}
        \State {Add $l$ to $newLls$}
      \Else
        \State Continue waiting
      \EndIf
    \EndFor
  \EndWhile

  \For {$l$ in $lls$}
    \If {$l$ is registered as an unbound receiver}
      \State {Unregister $l$ as an unbound receiver}
    \EndIf
    \If {$l$ was attempting a bound receive and the bound thread hasn't sent any messages}
      \State {Exit $l$}
    \ElsIf {$l$ is unbound}
      \State {The thread must have been attempting an unbound receive which failed, so exit $l$}
    \Else
      \State {} \Comment {Receive succeeded}
      \State {Add $l$ to $newLls$}
    \EndIf
  \EndFor

  \State {Set high-level interpreters set of currently-active low-level interpreters to $newLls$}
\end{algorithmic}

The send algorithm is very similar:

\begin{algorithmic}[1]
  \State {$lls \gets $ the set of currently-active low-level interpreter states}
  \State {$newLls \gets $ an empty set of low-level interpreter states}
  \For {$s$ in currently-active low-level interpreter states}
    \If {$s$ does not send a message}
      \State {Move $l$ from $lls$ to $newLls$ without changing it}
    \ElsIf {$s$ has a bound thread}
      \If {$s$'s bound thread has exited}
        \State {$s$ exits as well; remove it from $lls$ without adding it to $newLls$}
      \ElsIf {$s$'s bound thread is waiting to receive a message}
        \If {$s$'s outgoing message passes the message filter}
          \State {Copy stashed values from the sending low-level interpreter's state to the receiving one}
          \State {Move $s$ from $lls$ to $newLls$}
          \State {Clear the bound thread's receiving-bound-message flag}
        \Else
          \State {$s$ exits; remove it from $lls$}
        \EndIf
      \Else
        \State {} \Comment {Wait for a the bound thread to be ready to receive a message}
      \EndIf
    \Else
      \State {} \Comment{Unbound send}
      \For {$l$ registered unbound receivers}
        \If {The outgoing message passes $l$'s message filter}
          \State {$l' \gets $ duplicate $l$}
          \State {$s' \gets $ duplicate $s$}
          \State {Copy stashed values from $s'$'s state to $l'$'s}
          \State {Bind $l'$ and $s'$ together}
          \State {Insert $s'$ into $newLls$}
          \State {Insert $l'$ into $l$'s high-level interpreter's active low-level interpreter list}
        \EndIf
      \EndFor
      \State {Register $s$ as an unbound sender}
    \EndIf
  \EndFor

  \If {$lls$ is empty}
    \Return
  \EndIf

  \State {$end \gets now() + bound\_delay$}
  \If {There is a minimum delay}
    \State {Release lock}
    \State {Sleep for the minimum delay}
    \State {Acquire lock}
  \EndIf

  \While {There are bound sends in $lls$ and $now() < end$}
    \State {Release lock}
    \State {Wait for some bound send to complete, or for the current time to pass $end$}
    \State {Acquire lock}
    \For {$s$ performing bound sends in $lls$}
      \If {$s$'s bound thread has exited}
        \State {Remove $s$ from $lls$}
        \State {$s$ exits}
      \ElsIf {$s$'s sending-bound-message flag is clear}
        \State {Remove $s$ from $lls$}
        \State {Add $s$ to $newLls$}
      \Else
        \State Continue waiting
      \EndIf
    \EndFor
  \EndWhile

  \For {$s$ in $lls$}
    \If {$s$ is registered as an unbound sender}
      \State {Unregister $s$ as an unbound sender}
    \EndIf
    \If {$s$ was attempting a bound send and the bound thread hasn't tried to receive any messages}
      \State {Exit $s$}
    \ElsIf {$s$ is unbound}
      \State {The thread must have been attempting an unbound send which failed, so exit $s$}
    \Else
      \State {} \Comment {Send succeeded}
      \State {Add $s$ to $newLls$}
    \EndIf
  \EndFor

  \State {Set high-level interpreters set of currently-active low-level interpreters to $newLls$}
\end{algorithmic}

One further optimisation, not shown here, avoids redundantly duplicating low-level interpreter contexts in the common case that a message operation is discharged precisely once.

\todo{This is probably not the best way of presenting those
  algorithms.}

\todo{Discuss setting minimum delays here}

\subsubsection{The await-bound-thread-exit state}

When a thread completes its plan, it is sometimes useful for it to wait for its bound thread to exit before proceeding.
This is because the crash summary from which the plan is generated does not have complete information on the structure of the program.
If the last edge in a happens-before graph is from memory access A to memory access B, that generally means that A is a store and B is a load and B must load the value stored by A.
That means that, not only must B happen after A, but B must happen before any other writes to the memory location modified by A.
If there were other stores to that location in the crash summary then the happens-before graph would include additional edges to ensure that that happens, but if there are stores outside of the summary then it will not.
For instance, suppose that the write thread looks like this:

\begin{verbatim}
while (1) {
l1:  *x = 5;
l2:  *x = 7;
     <something_complicated>
}
\end{verbatim}

And the read thread looks like this:

\begin{verbatim}
l3: a = *x;
l4: b = *x;
    crash if a != b;
\end{verbatim}

This program is clearly buggy.
One way of reproducing this bug would be to interleave instructions as \verb|l1|, \verb|l3|, \verb|l2|, \verb|l4|.
SLI will discover this interleaving as the happens-before graph show in figure \todo{...}.
The algorithm described so far will be sufficient to enforce this graph (assuming that the two fragments of code shown can actually execute in parallel).
This is not, however, sufficient to cause the program to crash, because the generated happens-before graph is incomplete: it misses the edge from \verb|l4| to \verb|l1| in the next iteration of the loop.
If the loop completes and reaches the store at \verb|l1| before the load \verb|l4| completes then the bug will not reproduce even though the happens-before graph was successfully enforced.
Any scheme with an analysis horizon based on a simple instruction count will suffer from a similar problem, but this is particularly serious for SLI, because the delays inserted are in almost precisely the right place to maximise the chance of this kind of bug-hiding race.
Returning to the example, SLI's crash enforcement plan will include a delay after \verb|l1| (to implement the \verb|l1| to \verb|l3| happens-before edge), and this delay will be large relative to a short sequence of normal instructions, and so the bug will be hidden completely provided that the delay to wake up the read thread after the final edge is greater than the time taken to execute \verb|<something_complicated>|.
This is perfectly plausible, given that \verb|<something_complicated>| only needs to be a few dozen instructions to exceed SLI's analysis window.
This bug will therefore never reproduce under SLI's crash enforcer, even when the happens-before graph is enforced perfectly.
The fix is simple: have the store thread delay slightly after completing its final message send operation, until the read thread also completes its crash enforcement plan.
This ensures that activity beyond the analysis horizon cannot prevent bug reproduction, and, because it only happens when the plan is mostly complete, and hence happens very rarely, it has very low performance overhead.

\begin{tikzpicture}
\node[CfgInstr] (l1) {l1};
\node[CfgInstr, below = of l1] (l2) {l2};
\node[CfgInstr, right = of l1] (l3) {l3};
\node[CfgInstr, below = of l3] (l4) {l4};
\draw[->] (l1) -- (l2);
\draw[->] (l3) -- (l4);
\draw[->,happensBeforeEdge] (l1) -- (l3);
\draw[->,happensBeforeEdge] (l3) -- (l2);
\draw[->,happensBeforeEdge] (l2) -- (l4);
\end{tikzpicture}


\subsubsection{Compiling the plan}
\label{sect:enforce:compiling}

\todo{Implementation of this is currently massively broken; need to
  decide whether I'm going to fix it or just use a slightly older
  version, or just drop it completely.}

In addition to a plan interpreter, SLI also includes a plan compiler, which combines the plan with the program's original machine code to produce a modified version of the program which performs the necessary enforcement actions without needing an interpreter.
The intent here is to reduce the overhead of the interpreter in the case where SLI is investigating many bugs most of which do not exist.
Making this practical requires several simplifications to the semantics:

\begin{itemize}
\item
  The number of physical program threads operating in the plan is limited.
  In particular, it is assumed that only one program thread will be executing the read side of the plan at any time, and likewise only one thread will be executing the write side.
\item
  The message semantics are simplified: messages are sent asynchronously, with a delay only on the read side, and must be ``cancelled'' when the relevant thread exits.
  This has two important implications: first, that a message send can never fail and, second, the something must keep track of what messages a given thread currently has outstanding.
  Combined with the first simplification, it also means that at most one instance of any given message can be outstanding at any one time, and so it is easy to place the relevant information in a global structure without needing any dynamic memory allocation.
\item
  High-level interpreter contexts only ever access the state of their own low-level interpreters.
  This has two important implications:

  \begin{itemize}
  \item
    Remote low-level interpreters are never duplicated during message operations.
    If the normal semantics would require an interpreter to be duplicated then the local message operation fails.
  \item
    The receive message filter can only be executed by the receiving thread after receiving a message.
  \end{itemize}
\item
  When a low-level interpreter is duplicated due to a non-deterministic choice in the Succ phase the low-level state's stash table is not duplicated.
  Instead, all low-level interpreters in a given high-level interpreter share a single stash table.
\end{itemize}

The result is a system with lower run-time overheads, but also a lower probability of reproducing interesting bugs.
It has a much larger I-cache footprint but a smaller D-cache one\editorial{Not sure where I'm going with that, although it is true and kind of interesting}.

I now briefly outline the implementation of this compiler.
The core idea is to compile the CFG in the enforcement plan into a state machine.
This state machine consists of a large number of smaller intra-instruction state machines, as illustrated in figure~\todo{...}, each of which models a single instruction in the original program.
The label on each state is itself a set of low-level labels which consist of a four-tuple of the plan thread which is executing, a reference to the plan CFG node, a set of messages which have been sent by the thread, and the phase of the intra-instruction state machine.
Each state is compiled to a small fragment of machine code (which might be empty, if this instruction does not have an relevant annotation in the plan) plus a set of relocations specifying the fragment's relationships to the other states.
Once every state has been compiled these relocations can be discharged and the fragments concatenated together to form the final patch.

I now discuss the details of each phase of the intra-instruction machine:

\begin{itemize}
\item[RecvMsg]
  Examine the set of CFG nodes which are active in the current state and determine whether the plan requires any of them to receive messages.
  If so, emit code to examine the global outstanding-message structures to see whether of the desired messages are currently outstanding.
  If there are, receive precisely that message.
  Any other message receive operations are considered to have failed and the relevant CFG nodes removed from the current state label.
  If no message sends are currently outstanding then the physical thread is delayed until either one is available or some timeout is reached.
  If a message becomes available then it is received, and otherwise all receives fail and all receiving CFG nodes are removed from the label.
  Note that this does not necessarily mean that the state label will become empty\footnote{Although that is the common case.} as there may be some CFG nodes which do not need to receive messages.

  Once a message is received its content is simply copied from the global message area into the local thread's stash area.
\item[OrigInstr]
  Store any generated values into simulation slots and issue the original instruction.
  There are three main cases to consider here:

  \begin{itemize}
  \item
    Simple memory loads.
    If the instruction is of the form \verb|LOAD *location -> register|, and the value loaded is to be saved, then it is sufficient to just copy \verb|register| into the simulation slot after the original instruction has completed.
  \item
    Compound memory loads.
    Instructions which load from memory but are not themselves simple loads are more difficult to handle.
    For concreteness, suppose that the instruction is \verb|CMP 76,*loc1|, and the annotation requires us to save the value loaded.
    The instruction loads from \verb|*loc1| but does not leave the result in any locations which we can easily access.
    It would be possible to solve this problem by adding another load of \verb|*loc1|, but that would run the risk of a store in a remote thread modifying \verb|*loc1| between the two loads, leading to very confusing results.
    SLI instead solves this problem by rewriting the instruction to this:

\begin{verbatim}
LOAD *loc1 -> reg1
STORE reg1 -> simslot
CMP 76, reg1
\end{verbatim}

    This exposes the loaded value to the instrumentation framework, allowing it to be stored to the simulation slot as desired.

    Instructions which modify memory in-place, such as \verb|ADD 1 + *loc1 -> *loc1|, pose a similar problem and can be solved in the same way, provided that they do not have a \verb|LOCK| prefix.
    \verb|LOCK|ed instructions are more complex, as separating the load and store phases into separate instructions would violate the semantics of the program and might introduce new bugs.
    SLI solves this problem using a \verb|CMPXCHG| loop.
    For instance, the instruction \verb|LOCK ADD 1 + *loc1 -> *loc1| would be converted to this machine code fragment:

\begin{verbatim}
1: LOAD *loc1 -> reg1
ADD 1 + reg1 -> reg2
LOCK CMPXCHG *loc1, reg1 -> reg2
if_cmpxchg_failed goto l1
STORE reg1 -> simslot
\end{verbatim}

    The \verb|CMPXCHG| instruction here is supposed to be an invented syntax for the x86 machine code instruction which atomically compares \verb|*loc1| to \verb|reg1| and, if they are equal, sets \verb|*loc1| to \verb|reg2|.
    This allows SLI to expose the value of the implicit load while preserving the \verb|LOCK| semantics of the original instruction.
  \item
    Branch instructions are deferred to the FindSucc state.
  \end{itemize}
  
\item[SendMsg]
  Send any outgoing messages.
  This amounts to simply copying the message payload into the global message area, setting a global flag to indicate that the message is currently outstanding, and adding the message ID to the state label's set of sent messages.
  This always succeeds and advances to the FindSucc state.

\item[ExitThread]
  When a low-level thread exits it is necessary to cancel any messages which it has sent.
  The compiler takes the union of all of the sent-messages sets in its low-level, removes the thread which is to exit, and then takes the union again.
  Any messages present in the first set but not the second need to be cancelled by setting the relevant global message-outstanding flag to zero.
  The compiler will then either exit the patch, if the last low-level thread has exited, or resume the intra-instruction state machine at an appropriate place.
\end{itemize}


\begin{tikzpicture}
\node[flowChartState] (RecvMsg) {RecvMsg};
\node[flowChartState,above left = of RecvMsg] (StartThread) {StartThread};
\node[flowChartState,above right = of RecvMsg] (CheckForThreadStart) {CheckForThreadStart};
\node[flowChartState, below = of RecvMsg] (OrigInstr) {OrigInstr};
\node[flowChartState, below = of OrigInstr] (VerfCond) {VerfCond};
\node[flowChartState, below = of VerfCond] (SendMsg) {SendMsg};
\node[flowChartState, below = of SendMsg] (FindSucc) {FindSucc};
\node[flowChartState, right = of VerfCond] (CondFail) {CondFail};
\node[flowChartState, right = of CondFail] (Exit) {Exit};
\node[flowChartState, left = of RecvMsg] (RecvdMsg) {RecvdMsg};
\draw[->] (CheckForThreadStart) -- (RecvMsg) -- (OrigInstr) -- (VerfCond) -- (SendMsg) -- (FindSucc);
\draw[->] (StartThread) to [bend right=10] (CheckForThreadStart);
\draw[->] (CheckForThreadStart) to [bend right=10] (StartThread);
\draw[->,dashed] (FindSucc.east) to [bend right=75] (CheckForThreadStart.east);
\draw[->] (RecvMsg) -- (RecvdMsg) -- (OrigInstr);
\draw[->] (RecvMsg) -- (Exit);
\draw[->] (VerfCond) -- (CondFail) -- (Exit);
\draw[->] (FindSucc) -- (Exit);
\end{tikzpicture}

  

\subsubsection{Compiling entry point stubs}
\label{sect:find_bugs:compile_entry_points}

\todo{Good Lord, this is completely incomprehensible.}

The entry points of the crash enforcement plan are given as mappings from call stacks to CFG nodes.
The program must be patched so that it transfers control to the interpreter whenever the call stack matches up with one of these entry-point call stacks.
This is a multi-step process:

\begin{itemize}
\item
  The last pointer in the call stack is a raw instruction pointer.
  The relevant instruction is patched into a jump into the interpreter trampoline.
\item
  The interpreter trampoline transitions to a different stack, saves the client program's register state, and starts the main interpreter.
\item
  The main interpreter then examines the program's stack to determine whether it matches up with the entry-point stack.
  If it does then a new high-level interpreter state is created and interpretation starts.
  Otherwise, the interpreter exits back to the original program (possibly after emulating enough instructions to avoid returning into the middle of the new jump instruction).
\end{itemize}

The last point is more subtle than it might appear, as the interpreter must be able to find arbitrary return addresses on the program's stack, and this is difficult if the program is compiled without frame pointers or debug symbols.
SLI uses a static analysis, performed when generating the crash enforcement plan, to solve this problem.
The static analyses already described are sufficient to determine the entry point of the function containing a given RIP, and so it is easy to perform a simple abstract interpretation forwards from that point to the given RIP and hence find the number of bytes which the function will push onto the stack on the way\footnote{This is generally fixed for all possible paths}.
The first entry in the call stack is trivially true because of the way the jumps are patched in.
The second one is the return address of the current function, which can be found using that offset.
The third one is the return address of the calling function, and the offset there is just the previous offset plus the offset in the calling function.
In this way all necessary offsets can be found and the entry-point stack checked completely.

\subsubsection{Run-time considerations}

Recovery from spurious segfaults in the patch due to e.g. LD operations.

\subsection{Comparison to schedule memoisation}
\subsection{Comparison to STM block inference stuff}

\section{Fixing bugs}

\subsection{Using global locks}
\label{sect:fix_global_lock}

In addition to finding bugs, SLI can also be used to fix them in a
largely automated fashion.  The basic approach here is to binary patch
the program to introduce a new global lock covering the program's
relevant instructions, preventing them from executing in parallel and
hence preventing the bug from occurring (assuming that the relevant
instructions have been correctly identified).  The relevant
instructions are duplicated into a binary patch, unrolling loops and
tracing across function boundaries in a way which reflects the
function inlining and loop unrolling performed during the initial CFG
generation phase, and the duplicates modified to acquire and release
the lock at appropriate points.  The original program is then patched
branch to the duplicates when necessary.

The first step in producing such a fix is correctly identifying the
instructions which must be included in the critical sections.  These
will be roughly a subset of the instructions involved in the
control-flow graphs associated with \StateMachines; a subset because
some instructions in the CFG do not need to be protected, and roughly
because some instructions not in the CFG will also be included in the
critical section.

As an example of the former, consider a program like this:

\begin{verbatim}
read_side() {
    ptr = complicated_local_calculation();
    dptr = *ptr;
    if (dptr != NULL) {
       dptr = *ptr;
       *dptr = 5;
    }
}
write_side() {
    ptr = complicated_local_calculation();
    *ptr = NULL;
}
\end{verbatim}

Here, the read thread computes some pointer using entirely local
operations, loads from it once and then, if the result is
non-\verb|NULL|, loads from it again and uses the resulting pointer.
Meanwhile, the store thread sets a potentially coincident memory
location to \verb|NULL|.  The read thread clearly has a potential
time-of-check, time-of-use race bug.  The \StateMachines generated by
SLI will include the buggy code itself but might also include part or
all of \verb|complicated_local_calculation()| and a side-condition
which requires the two pointers to match up.  This extra information
is useful when analysing the bug (\needCite) or when attempting to
reproduce it (\needCite) but cannot be used by this kind of
instruction-level fix\footnote{But see future work section~\needCite
  for a possible alternative scheme which would make use of it.}, so
including it in the fix is unhelpful and would tend to lead to
unnecessarily large critical sections.  The fix generating process
must therefore select a useful subset of the instructions in the
control-flow graph.

The approach taken here is simple:

\begin{itemize}
\item
  Reduce the verification condition from the bug summary until it
  contains only happens-before edges.  These entirely capture the
  instruction-interleaving parts of the bug to be fixed, and, since
  instruction-interleaving is the only thing which can be influenced
  by this type of patch, the resulting condition contains all of the
  useful information in the condition.
\item
  Identify all of the CFG nodes which are mentioned in one of those
  happens-before edges.
\item
  Trim the CFG such that every path starts and ends in one of those
  mentioned nodes.  All such paths will be included in a critical
  section, and so no such paths will be permitted to execute in
  parallel.
\end{itemize}

In this way the CFG is restricted to just those instructions which are
involved in the interleaving which is to be prevented.

Note that this is not guaranteed to produce an optimal selection of
critical sections, in the sense that sections can sometimes be larger
than is strictly necessary.  Consider, for example, a program with the
same read side as the previous example but a write side in which the
pointer is assigned to twice:

\begin{verbatim}
write_side() {
    ptr = complicated_local_calculation();
    *ptr = NULL;
    *ptr = NULL;
}
\end{verbatim}
    
There are now two obvious ways of protecting this program:

\begin{itemize}
\item
  Place both loads in the read side in a single critical section and
  both stores in the write side in another one.
\item
  Place both loads in the read side in a single critical section, but
  give each store in the write side its own critical section.  In
  other words, drop and re-acquire the lock in between the two stores.
\end{itemize}

Both approaches correctly eliminate the bug, but they will have
different performance characteristics.  In particular, dropping and
re-acquiring the lock reduces the size of the critical section, which
might improve concurrency and reduce starvation, but imposes higher
overheads due to the greater number of lock operations.  In principle
the happens-before graph implicit in the verification condition
contains enough information to determine whether dropping the lock is
safe, but, in this mode, SLI does not make use of this information,
and always uses the former strategy\footnote{But
  see~\ref{sect:fix_from_drs} for a mode in which it can use the other
  approach.}.

Once the relevant fragment of CFG has been identified, entry point
stubs must be generated and the program patched to branch to them at
appropriate points.  There are two main complications here:

\begin{itemize}
\item
  Instructions on x86 are not all the same size, and, in particular, a
  branch instruction is larger than some of the instructions which
  need to be patched.  This means that replacing an instruction with a
  branch to a patch entry point stub might clobber the following
  instruction; if there are any branches to that instruction from
  anywhere else in the program then the results are unlikely to be
  helpful.
\item
  The control-flow graphs used by SLI can cross function boundaries,
  and in particular are sometimes only valid in a particular function
  call context.  The patch must therefore check that the stack matches
  before attempting to run the CFGs.  Note, in particular, that
  whereas stack context checking is a performance optimisation when
  trying to trigger bugs it is necessary for correctness when trying
  to prevent them.
\end{itemize}

SLI solves the first problem by expanding the critical sections
backwards so that they do not start on dangerous instructions.  The
early static analysis passes discover all of the branches within the
program, and so can detect when inserting a branch would cause a
dangerous clobber, and in that case it simply expands the critical
section to include the instruction's predecessors\footnote{This might,
  of course, mean that the critical section has multiple entry points;
  that is not a problem.}.  The process then iterates until a safe
instruction is found\editorial{What if no safe instruction is found
  e.g. a loop with no suitable instructions in?  That does actually
  work, but, thinking about it, I'm not entirely certain why; should
  check that.}.  One complication here is branches from libraries into
the main program, which will not be detected by the static analysis.
Fortunately, they are detected by the dynamic analysis, and so this is
not a problem\editorial{Assuming that the dynamic analysis is
  complete, which is kind of a big assumption.  Saying that we miss
  some bugs when the analysis is incomplete is one thing; saying that
  we introduce more is a bit more of a big deal.  Probably need to say
  a bit more about that.}.

This problem was also tackled by the AutoPaG project, and the solution
they developed is similar. \todo{Similar but not the same.  They use a
  dominator-based scheme, and hence avoid needing the global branch
  information but can end up with much larger patches and a higher
  risk of deadlocks/starvation problems.  The original version of SLI
  used basically the same algorithm (although I did it first); should
  probably explain why I had to switch.}

The second problem, checking function call contexts, is much simpler,
and SLI solves it by simply emitting machine code to perform the
appropriate checks (using a stack layout derived in the same way as in
\S~\ref{sect:find_bugs:compile_entry_points}) and branch to an
appropriate place in the patch (or return to the original program if
there is none).  One subtlety here is that the original instruction
will have been replaced by a branch, and so returning to it directly
is unlikely to be effective, and so SLI copies the branch into the
patch (and possibly also a small number of clobbered instructions, if
necessary) and executes them from the patch before returning, without
holding the patch lock.\editorial{Rewrite the whole damn paragraph.}

An alternative approach would be to take control of the program using
debug breakpoints rather than jump instructions.  These are either a
single byte (for the \verb|int3| instruction) or no bytes at all (for
debug registers), and so avoid the instruction clobbering problem.
This would work, but would have a couple of important disadvantages:

\begin{itemize}
\item
  Debug breakpoints are far slower than branches.  This might be
  important if the critical section is to be inserted on a
  particularly hot code path.
\item
  Using debug breakpoints in this way would interfere with any other
  debugger which the developer might want to use.  With a branch-style
  patch, standard debuggers work without modification for any part of
  the program which has not been patched, whereas a breakpoint-style
  patch requires extensive coordination between the debugger and the
  patch mechanism for either to work at all.
\item
  Breakpoint registers are of strictly limited number on most
  architectures (four, on x86).  This means that they can never
  provide a complete solution by themselves.
\item
  On most UNIX-type operating systems, including Linux and FreeBSD,
  catching debug breakpoints requires modifying the program's signal
  handling configuration, which requires some level of coordination
  with the program to be modified.  It would be possible to use an
  alternative API, but this would require kernel modifications,
  complicating the use of the generated patches\footnote{Branch-style
    patches also require modifying signal handling configurations in
    order to, for instance, provide a correct faulting instruction
    address in SIGSEGV register configurations.  However, very few
    programs actually require that information for correctness, and so
    it is not usually a major issue if the patch loses control of the
    signal handler to the main program.  In a breakpoint-style patch,
    losing control of the signal handler means, first, that the patch
    is never run, and so cannot hope to fix the bug, and, second, that
    the program receives spurious breakpoint events, which will often
    introduce additional erroneous behaviour.}.
\end{itemize}

SLI therefore generates predominantly branch-style patches.

\todo{I did implement a breakpoint-based scheme, so it might be
  interesting to actually include some numbers on their relative
  effectiveness.}

\subsection{Other ways of fixing the bugs}
\todo{I've come up with an algorithm for doing this using a message
  passing network, like we do for crash enforcement but with the
  delays in slightly different places, but I really don't have time to
  implement it.  It is kind of cool, though; I'd like to include it
  somewhere, even if it's just a future work-type thing.}

\todo{Might also be worth saying a few words about possibly fixing the
  bugs by using STM-like techniques?  I've not implemented any of
  them, either, but they are kind of interesting.  Probably worth a
  paragraph or two.}

\section{Fixing bugs from DRS logs}
\label{sect:fix_from_drs}
The simplest way to fix a bug is to start from a DRS log.
Given a log, identifying the thread and which is most responsible for a crash is generally straightforward.
If the crash is caused by dereferencing a bad pointer then the responsible thread is the one which dereferenced the pointer; if the crash is an assertion failure then the responsible thread is the one which called \verb|abort()| (or equivalent).
The log then makes it trivial to determine what instructions the responsible thread executed before it crashed, and a suffix of these can be compiled into a \StateMachine capturing the most relevant parts of the responsible thread's behaviour.

The log also makes it trivial to determine precisely which stores the read-side thread raced with, and so building a write-side \StateMachine is redundant.
Instead, the read-side \StateMachine is ``slid across'' the log, evaluating it at every step (subject to some typing constraints which ensure that the result is reasonable), and the resulting pattern of safe and unsafe regions converted directly into critical sections, without ever needing to generate an explicit write-side \StateMachines.

\subsection{Building the read-side \StateMachine}
\todo{This is gratuitously different from the non-DRS mode in an enormous number of places.  I should fix that.}

Working from a DRS log provides a lot of information which is not available in SLI's normal mode of operation.
This makes some parts of the algorithm redundant.
In particular, there is no need to generate large numbers of CFGs of fragments of the program which might be relevant, as we know precisely which instructions were executed leading up to the crash.
Instead, in this mode, SLI generates the \StateMachine directly from the log.
It starts with a small stub machine representing just the instruction which crashed and then expands it backwards, incorporating a single instruction from the log at a time.
For instance, suppose that the fragment of program to be investigated looked like this:

\begin{verbatim}
l1: mov $5, %rax
l2: mov (global1), %rbx
l3: mov (%rax + %rbx), %rcx
\end{verbatim}

and the program crashed due to dereferencing a bad pointer at \verb|l3|.
The initial stub \StateMachine will then be just:

\begin{verbatim}
if (BadPtr(rax + rbx)) crash(); else survive();
\end{verbatim}

Incorporating \verb|l2| will transform that to

\begin{verbatim}
LOAD (global1) -> rbx
if (BadPtr(rax + rbx)) crash(); else survive();
\end{verbatim}

Incorporating \verb|l1| will then produce the \StateMachine

\begin{verbatim}
COPY 5 -> rax
LOAD (global1) -> rbx
if (BadPtr(rax + rbx)) crash(); else survive();
\end{verbatim}

Which can be simplified in the usual way to produce

\begin{verbatim}
LOAD (global1) -> rbx
if (BadPtr(rbx)) crash(); else survive();
\end{verbatim}

And this can then be used in the rest of the analysis.

The major subtlety here lies in the handling of control flow, and the parts of the program which are not executed.
One possible approach would be to simply say that any changes to the control flow cause the bug to be avoided, but this is over-optimistic.
Consider, for instance, a program like this one:

\begin{verbatim}
ptr = global;
if (some_condition)
    idx = 1;
else
    idx = 2;
local = ptr[idx];
\end{verbatim}

This program loads a pointer to an array from a global variable and then loads some index in the array, with the index chosen depending on some condition.
Suppose that the race then causes the pointer in \verb|global1| to sometimes be bad, and that the reproduction of the bug was obtained while \verb|some_condition| holds.
The bug itself does not depend on \verb|some_condition|, but if one were to assume that any changes to control flow avoid the bug then SLI would not be able to show this.
This problem can only be avoided by exploring untaken branches, and SLI does so, for some (configurable) number of instructions.
If the control flow rejoins that which is represented in the DRS log then an appropriate branch is included from one part of the \StateMachine to another, and if it does not rejoin then a branch to the \verb|NoCrash| state is used instead.

One complication here is that a given static instruction might be represented multiple times in the DRS instruction trace, and hence multiple times in the \StateMachine, if the instruction is part of some loop.
This makes it ambiguous where the branch should branch to.
SLI solves this problem by taking the earliest instance of the instruction, and hence branching to the place in the \StateMachine nearest the root.
This helps to keep the loop structure of the program intact, subject to the unrolling implicit in the DRS log.

The example shown above might then turn into a \StateMachine something like this:

\begin{verbatim}
LOAD global1 -> ptr
if (some_condition) {
   COPY 1 -> idx1
} else {
   COPY 2 -> idx2
}
if (BadPtr(ptr + idx)) {
   crash();
} else {
   survive();
}
\end{verbatim}

The standard simplified will then transform that to this:

\begin{verbatim}
LOAD global1 -> ptr
if (BadPtr(ptr + (some_condition ? 1 : 2))) {
   crash();
} else {
   survive();
}
\end{verbatim}

SLI uses a rule that \verb|BadPtr(x + k)| is equivalent to \verb|BadPtr(x)| whenever \verb|k| is a small constant, and so correctly determines that the bug is independent of \verb|some_condition| in this case.

\todo{Discuss using more powerful bug definitions here e.g. Valgrind, invariant discovery, etc, by applying them to the log and then converting to stub machines, so that you can apply this to races which lead to bugs other than immediate bad pointer dereferences.}

\subsection{Requirements on the DRS}

\subsection{Finding remote critical regions}

The fixes generated by SLI rely on making the read-side \StateMachine operate as-if atomically and then ensuring that it does not execute in any states where doing so would lead to a crash.
In the normal mode of operation, the regions which would lead to a crash are determined by modelling the rest of the program as a set if write-side \StateMachines and then using symbolic execution, but it is possible to be more accurate\editorial{or possibly precise?} if a full DRS log is available.
Instead of the set of \StateMachines, the log itself can be used as a model for the rest of the program.
The idea here is that the log contains a sequence of possible states of the program, and contains all of the ones which are relevant to this particular way of reproducing the bug of interest.
SLI therefore slides the read-side \StateMachine over this log, evaluating it at every instruction, and hence classifies the log into ``safe'' and ``unsafe'' regions, and the transitions between these two types of region give the boundaries of the write-side critical regions.

As a minor optimisation, SLI only re-evaluates the \StateMachine if there has been a store to some memory location which is loaded by the \StateMachine.
This cannot affect the results in any way, but means that the \StateMachine does not have to be evaluated as often.

One important complication here is the presence of dynamically-allocated data structures.
SLI relies on being able to identify points in the program where these are allocated and released.
The loads in the read \StateMachine will correspond to specific load operations in the DRS log and SLI is then able to check which dynamic instance of structures those accesses access and will only evaluate the \StateMachine while all of the relevant structures remain live.

Once the log has been classified, the classification must be converted into realisable critical sections.
In other words, SLI must identify points in the program at which it must insert lock acquire operations and points where it must insert lock release operations.
Ideally, each unsafe region in the log would correspond to a single critical section, with a single acquire operation and a single release one.
This can fail in several ways:

\begin{itemize}
\item
  The start and end of the unsafe region might be in different threads, if, for instance, one thread violates an invariant and another thread then restores it.
  It is difficult to model a cross-thread operation as a critical section.
  SLI cannot prevent this kind of bug, and the unsafe region is simply ignored.
\item
  There might be non-trivial control flow between the start and end of an unsafe region within a single thread.
  In that case additional acquire and release operations must be inserted to ensure that locks are not leaked, double-acquired, or double-released.
\item
  The program might have additional synchronisation mechanisms which, when combined with the SLI-inferred synchronisation, lead to a deadlock.
\end{itemize}

These are discussed in more detail in \S~\ref{sect:fix_global_lock}.

\todo{This is in dire need of rewriting.}

Note that the definition of a dynamic structure is somewhat subtle here.
Most obviously, \verb|malloc| and \verb|free| represent boundaries in the lifespan of such structures (with \verb|malloc| being the start and \verb|free| being the end), but ``re-typing'' operations can also impose such boundaries.
The intent of the sliding procedure is to capture other operations which the program might perform on the data structures involved in the synchronisation bug, in the same way that write-side \StateMachines do in the non-DRS case.
In effect, the program's behaviour is constrained using a heuristic memory safety property, and this memory safety property must correspond reasonably closely to the program's actual structure.

The underlying hypothesis here is that the program has some kind of internal type system which constrains which operations will be performed on a given memory location.
This means that two pieces of code can only race if they have types which are in some sense compatible, so that they might access overlapping memory locations.
The combination of the read-side \StateMachine and the set of dynamic instances accessed by it defines, in a slightly ill-defined way, a set of types which the read side of the critical section might access.
SLI must then find some other operations on the same types to synchronise against, and this is the aim of the sliding procedure.
In order for this to work, the read \StateMachine must only be slid to places where the current types match up with the types for which it was derived.
SLI must therefore be able to identify points where the types of memory locations change.
This includes things like \verb|malloc| and \verb|free|, but is also likely to include things like program-specific memory allocators or object pools.
The precise set will depend on the program's type system, and so can only be sensibly modelled with assistance from the programmer.





